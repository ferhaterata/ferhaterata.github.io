@inproceedings{erata2025learning,
  title = {Learning Randomized Reductions and Program Properties},
  author = {Ferhat Erata and Orr Paradise and Timos Antonopoulos and ThanhVu Nguyen and Shafi Goldwasser and Ruzica Piskac},
  booktitle = {Proceedings of [Anonymous Conference]},
  year = {2025},
  publisher = {[Anonymous Publisher]},
  abstract = {The correctness of computations remains a significant challenge in computer science, with traditional approaches relying on automated testing or formal verification. Self-testing/correcting programs introduce an alternative paradigm, allowing a program to verify and correct its own outputs via randomized reductions, a concept that previously required manual derivation. In this paper, we present \tool, a method and tool for automated learning of randomized (self)-reductions and program properties in numerical programs. \tool combines symbolic analysis and machine learning, with a surprising finding: polynomial-time linear regression, a basic optimization method, is not only sufficient but also highly effective for deriving complex randomized self-reductions and program invariants, often outperforming sophisticated mixed-integer linear programming solvers. We establish a theoretical framework for learning these reductions and introduce \texttt{RSR-Bench}, a benchmark suite for evaluating \tool's capabilities on scientific and machine learning functions. Our empirical results show that \tool surpasses state-of-the-art tools in scalability, stability, and sample efficiency when evaluated on nonlinear invariant benchmarks like \texttt{NLA-DigBench}. \tool is open-source as a Python package and accessible via a web interface that supports C language programs.},
  note = {The conference and publisher information is anonymized for confidentiality.},
  keywords = {randomized self-reductions, loop invariants, learning, random self-reducibility, linear regression, self-correctness}
}