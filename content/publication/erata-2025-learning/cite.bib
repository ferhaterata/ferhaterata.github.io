@inproceedings{erata2025learning,
  title         = {Learning Randomized Reductions and Program Properties},
  author        = {Ferhat Erata and Orr Paradise and Timos Antonopoulos and ThanhVu Nguyen and Shafi Goldwasser and Ruzica Piskac},
  year          = {2024},
  eprint        = {2412.18134},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2412.18134},
  abstract      = {The correctness of computations remains a significant challenge in computer science, with traditional approaches relying on automated testing or formal verification. Self-testing/correcting programs introduce an alternative paradigm, allowing a program to verify and correct its own outputs via randomized reductions, a concept that previously required manual derivation. In this paper, we present \tool, a method and tool for automated learning of randomized (self)-reductions and program properties in numerical programs. \tool combines symbolic analysis and machine learning, with a surprising finding: polynomial-time linear regression, a basic optimization method, is not only sufficient but also highly effective for deriving complex randomized self-reductions and program invariants, often outperforming sophisticated mixed-integer linear programming solvers. We establish a theoretical framework for learning these reductions and introduce \texttt{RSR-Bench}, a benchmark suite for evaluating \tool's capabilities on scientific and machine learning functions. Our empirical results show that \tool surpasses state-of-the-art tools in scalability, stability, and sample efficiency when evaluated on nonlinear invariant benchmarks like \texttt{NLA-DigBench}. \tool is open-source as a Python package and accessible via a web interface that supports C language programs.},
  keywords      = {randomized self-reductions, loop invariants, learning, random self-reducibility, linear regression, self-correctness}
}