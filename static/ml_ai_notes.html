<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- TODO: Remove this noindex tag when ready to make page publicly searchable -->
    <!-- <meta name="robots" content="noindex, nofollow"> -->
    <title>Ferhat Erata's ML/AI Study Notes</title>
    <style>
        /* CSS Variables for theming */
        :root {
            /* Light theme (default) */
            --bg-primary: #f6f8fa;
            --bg-secondary: #ffffff;
            --bg-sidebar: #24292e;
            --bg-code: #f6f8fa;
            --bg-table-header: #f6f8fa;
            --bg-table-row: #f6f8fa;
            --bg-hover: rgba(255,255,255,0.05);
            --bg-active: rgba(255,255,255,0.1);
            
            --text-primary: #24292e;
            --text-secondary: #6a737d;
            --text-sidebar: #959da5;
            --text-sidebar-primary: #e1e4e8;
            --text-sidebar-active: #58a6ff;
            
            --border-primary: #eaecef;
            --border-table: #dfe2e5;
            
            --link-color: #0366d6;
            --accent-color: #0366d6;
            --accent-active: #58a6ff;
            
            --shadow: rgba(0,0,0,0.12);
            --overlay: rgba(0,0,0,0.5);
        }
        
        /* Dark theme */
        [data-theme="dark"] {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --bg-sidebar: #010409;
            --bg-code: #21262d;
            --bg-table-header: #21262d;
            --bg-table-row: #161b22;
            --bg-hover: rgba(255,255,255,0.05);
            --bg-active: rgba(255,255,255,0.1);
            
            --text-primary: #c9d1d9;
            --text-secondary: #8b949e;
            --text-sidebar: #8b949e;
            --text-sidebar-primary: #c9d1d9;
            --text-sidebar-active: #58a6ff;
            
            --border-primary: #30363d;
            --border-table: #30363d;
            
            --link-color: #58a6ff;
            --accent-color: #58a6ff;
            --accent-active: #79c0ff;
            
            --shadow: rgba(0,0,0,0.4);
            --overlay: rgba(0,0,0,0.7);
        }
        
        * { box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background: var(--bg-primary);
            color: var(--text-primary);
            transition: background 0.3s, color 0.3s;
        }
        
        /* Theme Toggle Button */
        .theme-toggle {
            position: fixed;
            top: 15px;
            right: 15px;
            z-index: 1100;
            background: var(--bg-secondary);
            color: var(--text-primary);
            border: 1px solid var(--border-primary);
            border-radius: 50%;
            width: 44px;
            height: 44px;
            font-size: 18px;
            cursor: pointer;
            box-shadow: 0 2px 12px var(--shadow);
            transition: all 0.2s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            /* backdrop-filter removed for iOS stability */
        }
        .theme-toggle:hover {
            transform: scale(1.05);
            border-color: var(--accent-color);
            box-shadow: 0 4px 16px var(--shadow);
        }
        .theme-toggle .icon-sun { display: none; }
        .theme-toggle .icon-moon { display: block; }
        [data-theme="dark"] .theme-toggle .icon-sun { display: block; }
        [data-theme="dark"] .theme-toggle .icon-moon { display: none; }
        
        
        /* Hamburger Menu Button - Hidden on desktop */
        .menu-toggle {
            display: none;
            position: fixed;
            top: 15px;
            left: 15px;
            z-index: 1100;
            background: var(--bg-secondary);
            color: var(--text-primary);
            border: 1px solid var(--border-primary);
            border-radius: 8px;
            padding: 10px 14px;
            font-size: 18px;
            cursor: pointer;
            box-shadow: 0 2px 12px var(--shadow);
            transition: all 0.2s ease;
            /* backdrop-filter removed for iOS stability */
        }
        .menu-toggle:hover {
            border-color: var(--accent-color);
            box-shadow: 0 4px 16px var(--shadow);
        }
        .menu-toggle:active {
            transform: scale(0.95);
        }
        
        /* Overlay backdrop - Hidden by default */
        .sidebar-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: var(--overlay);
            z-index: 999;
            opacity: 0;
            transition: opacity 0.3s;
        }
        .sidebar-overlay.visible {
            opacity: 1;
        }
        
        /* Sidebar Navigation */
        .sidebar {
            position: fixed;
            top: 0;
            left: 0;
            width: 320px;
            height: 100%;
            /* Removed 100dvh - causes memory issues on iOS */
            background: var(--bg-sidebar);
            color: #fff;
            overflow-y: auto;
            -webkit-overflow-scrolling: touch;
            padding: 20px 0;
            z-index: 1000;
            transition: transform 0.3s ease, background 0.3s;
        }
        .sidebar h2 {
            color: #fff;
            font-size: 14px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            padding: 0 20px;
            margin: 0 0 15px 0;
            border-bottom: none;
        }
        .sidebar ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }
        .sidebar > ul > li > ul > li > a {
            padding-left: 35px;
            font-size: 13px;
        }
        .sidebar > ul > li > ul > li > ul > li > a {
            padding-left: 50px;
            font-size: 12px;
        }
        /* Third level sections - collapsed by default */
        .sidebar > ul > li > ul > li > ul {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        /* Expanded when parent has expanded class */
        .sidebar > ul > li > ul > li.expanded > ul {
            max-height: 2000px;
            transition: max-height 0.5s ease-in;
        }
        /* Toggle indicator for expandable items */
        .sidebar > ul > li > ul > li.has-children > a::before {
            content: '‚ñ∂';
            display: inline-block;
            width: 12px;
            font-size: 8px;
            margin-right: 5px;
            transition: transform 0.2s;
        }
        .sidebar > ul > li > ul > li.has-children.expanded > a::before {
            transform: rotate(90deg);
        }
        .sidebar li {
            margin: 0;
        }
        .sidebar a {
            display: block;
            color: var(--text-sidebar);
            text-decoration: none;
            padding: 8px 20px;
            font-size: 14px;
            border-left: 3px solid transparent;
            transition: all 0.2s;
        }
        .sidebar > ul > li > a {
            color: var(--text-sidebar-primary);
            font-weight: 500;
        }
        .sidebar a:hover {
            color: #fff;
            background: var(--bg-hover);
            border-left-color: var(--accent-color);
        }
        .sidebar a.active {
            color: #fff;
            background: var(--bg-active);
            border-left-color: var(--accent-active);
        }
        .sidebar > ul > li > a.active {
            color: var(--text-sidebar-active);
            font-weight: 600;
        }
        .sidebar .nav-title {
            color: #fff;
            font-weight: 600;
            font-size: 16px;
            padding: 15px 20px 10px;
        }
        
        /* Close button - only on mobile */
        .sidebar-close {
            display: none;
            position: absolute;
            top: 10px;
            right: 10px;
            background: transparent;
            border: none;
            color: var(--text-sidebar);
            font-size: 24px;
            cursor: pointer;
            padding: 5px 10px;
            line-height: 1;
        }
        .sidebar-close:hover {
            color: #fff;
        }
        
        /* Main Content */
        .main-content {
            margin-left: 320px;
            padding: 40px;
            max-width: 920px;
        }
        .content {
            background: var(--bg-secondary);
            padding: 40px;
            border-radius: 6px;
            box-shadow: 0 1px 3px var(--shadow);
            transition: background 0.3s, box-shadow 0.3s;
        }
        
        /* Typography */
        h1 { 
            color: var(--text-primary); 
            border-bottom: 1px solid var(--border-primary); 
            padding-bottom: 0.3em;
            margin-top: 1.5em;
        }
        h1:first-child { margin-top: 0; }
        h2 { 
            color: var(--text-primary); 
            border-bottom: 1px solid var(--border-primary); 
            padding-bottom: 0.3em;
            margin-top: 1.5em;
        }
        h3 { color: var(--text-primary); margin-top: 1.5em; }
        
        /* Code */
        code {
            background: var(--bg-code);
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 0.95em;
            transition: background 0.3s;
        }
        pre {
            background: var(--bg-code);
            padding: 16px;
            border-radius: 6px;
            overflow-x: auto;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 0.95em;
            line-height: 1.5;
            transition: background 0.3s;
        }
        pre code {
            background: none;
            padding: 0;
            font-size: inherit;
        }
        /* Override highlight.js background */
        pre code.hljs {
            background: var(--bg-code);
            padding: 0;
        }
        
        /* Tables */
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1em 0;
        }
        th, td {
            border: 1px solid var(--border-table);
            padding: 8px 12px;
            text-align: left;
        }
        th { background: var(--bg-table-header); font-weight: 600; }
        tr:nth-child(even) { background: var(--bg-table-row); }
        
        /* Other Elements */
        blockquote {
            border-left: 4px solid var(--accent-color);
            margin: 0;
            padding: 0 1em;
            color: var(--text-secondary);
        }
        a { color: var(--link-color); text-decoration: none; }
        a:hover { text-decoration: underline; }
        img { max-width: 100%; }
        hr { border: none; border-top: 1px solid var(--border-primary); margin: 2em 0; }
        .math { overflow-x: auto; }
        
        /* MathJax SVG styling */
        .MathJax_SVG_Display {
            overflow-x: auto;
            overflow-y: hidden;
        }
        mjx-container[jax="SVG"] {
            direction: ltr;
        }
        mjx-container[jax="SVG"][display="true"] {
            display: block;
            text-align: center;
            margin: 1em 0;
        }
        
        /* Mobile & Tablet Styles */
        @media (max-width: 1024px) {
            /* Reposition theme toggle */
            .theme-toggle {
                top: 15px;
                right: 15px;
            }
            
            /* Show hamburger button */
            .menu-toggle {
                display: block;
            }
            
            /* Show close button in sidebar */
            .sidebar-close {
                display: block;
            }
            
            /* Show overlay when sidebar is open */
            .sidebar-overlay {
                display: block;
            }
            
            /* Hide sidebar by default, slide from left */
            .sidebar {
                transform: translateX(-100%);
                width: 300px;
            }
            .sidebar.open {
                transform: translateX(0);
            }
            
            /* Add top padding for nav-title to account for close button */
            .sidebar .nav-title {
                padding-right: 50px;
            }
            
            /* Main content takes full width */
            .main-content {
                margin-left: 0;
                padding: 70px 20px 20px 20px;
                max-width: 100%;
            }
            .content {
                padding: 25px;
            }
        }
        
        /* Disable transitions on mobile - reduces memory pressure on iOS */
        @media (max-width: 1024px) {
            .sidebar, .sidebar a, .sidebar > ul > li > ul,
            .theme-toggle, .menu-toggle, body, .content {
                transition: none !important;
            }
        }
        
        /* Extra small screens (phones) */
        @media (max-width: 480px) {
            .theme-toggle {
                width: 40px;
                height: 40px;
                font-size: 18px;
            }
            .sidebar {
                width: 85vw;
                max-width: 300px;
            }
            .main-content {
                padding: 65px 15px 15px 15px;
            }
            .content {
                padding: 20px 15px;
            }
            h1, h2 {
                font-size: 1.4em;
            }
            pre {
                font-size: 0.85em;
                padding: 12px;
            }
            table {
                font-size: 0.9em;
            }
            th, td {
                padding: 6px 8px;
            }
        }
    </style>
    
    <!-- Inline SVG icons instead of Google Fonts (reduces memory on iOS) -->
    <style>
        .icon { display: inline-block; width: 24px; height: 24px; vertical-align: middle; }
        .icon svg { width: 100%; height: 100%; fill: currentColor; }
    </style>
    
    <!-- highlight.js for syntax highlighting -->
    <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" disabled>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>
    
        
    <!-- MathJax Configuration - BEFORE loading MathJax -->
    <script>
    // MathJax config - note: dollar signs escaped for pandoc template
    window.MathJax = {
        tex: {
            inlineMath: [[String.fromCharCode(36), String.fromCharCode(36)], ["\\(", "\\)"]],
            displayMath: [[String.fromCharCode(36)+String.fromCharCode(36), String.fromCharCode(36)+String.fromCharCode(36)], ["\\[", "\\]"]]
        },
        svg: {
            fontCache: "global"
        },
        options: {
            enableMenu: false,
            renderActions: {
                addMenu: [],
                checkLoading: []
            }
        },
        startup: {
            pageReady: function() {
                return MathJax.startup.defaultPageReady().then(function() {
                    if (window.MathJax && window.MathJax.startup && window.MathJax.startup.document) {
                        var doc = window.MathJax.startup.document;
                        if (doc.menu) {
                            doc.menu.resizeObserver = null;
                        }
                    }
                });
            }
        }
    };
    </script>
    <script
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
    type="text/javascript"></script>
    
    <script>
    // Theme management - runs before DOM ready to prevent flash
    (function() {
        const savedTheme = localStorage.getItem('theme');
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        const theme = savedTheme || (prefersDark ? 'dark' : 'light');
        document.documentElement.setAttribute('data-theme', theme);
        
        // Update highlight.js theme
        function updateHljsTheme(isDark) {
            document.getElementById('hljs-light').disabled = isDark;
            document.getElementById('hljs-dark').disabled = !isDark;
        }
        updateHljsTheme(theme === 'dark');
    })();
    
    document.addEventListener('DOMContentLoaded', function() {
        try {
            // Detect mobile ONCE at load - don't recheck
            var isMobile = window.innerWidth <= 1024 || 
                          ('ontouchstart' in window) || 
                          (navigator.maxTouchPoints > 0);
            
            // Theme toggle functionality - simple, works everywhere
            var themeToggle = document.querySelector('.theme-toggle');
            
            function setTheme(theme) {
                try {
                    document.documentElement.setAttribute('data-theme', theme);
                    localStorage.setItem('theme', theme);
                    var hljsLight = document.getElementById('hljs-light');
                    var hljsDark = document.getElementById('hljs-dark');
                    if (hljsLight) hljsLight.disabled = (theme === 'dark');
                    if (hljsDark) hljsDark.disabled = (theme !== 'dark');
                } catch(e) {}
            }
            
            if (themeToggle) {
                themeToggle.addEventListener('click', function() {
                    var currentTheme = document.documentElement.getAttribute('data-theme') || 'light';
                    setTheme(currentTheme === 'dark' ? 'light' : 'dark');
                });
            }
            
            // Mobile: ONLY setup hamburger menu, nothing else
            if (isMobile) {
                var menuToggle = document.querySelector('.menu-toggle');
                var sidebar = document.querySelector('.sidebar');
                var sidebarOverlay = document.querySelector('.sidebar-overlay');
                var sidebarClose = document.querySelector('.sidebar-close');
                
                function openSidebar() {
                    if (sidebar) sidebar.classList.add('open');
                    if (sidebarOverlay) sidebarOverlay.classList.add('visible');
                }
                
                function closeSidebar() {
                    if (sidebar) sidebar.classList.remove('open');
                    if (sidebarOverlay) sidebarOverlay.classList.remove('visible');
                }
                
                if (menuToggle) menuToggle.addEventListener('click', openSidebar);
                if (sidebarClose) sidebarClose.addEventListener('click', closeSidebar);
                if (sidebarOverlay) sidebarOverlay.addEventListener('click', closeSidebar);
                
                // Close on any nav link click
                if (sidebar) {
                    sidebar.addEventListener('click', function(e) {
                        if (e.target.tagName === 'A') {
                            closeSidebar();
                        }
                    });
                }
                
                // NO resize/orientation listeners on mobile - just let it reflow naturally
                return; // Exit early - skip all desktop nav features
            }
            
            // DESKTOP ONLY: Advanced nav features
            // Mark expandable items (avoid :scope for compatibility)
            var allLis = document.querySelectorAll('.sidebar li');
            for (var j = 0; j < allLis.length; j++) {
                var li = allLis[j];
                var children = li.children;
                for (var c = 0; c < children.length; c++) {
                    if (children[c].tagName === 'UL' && children[c].children.length > 0) {
                        li.classList.add('has-children');
                        break;
                    }
                }
            }
            
            // Get all headings
            var headings = document.querySelectorAll('h1[id], h2[id], h3[id]');
            var headingsArray = [];
            for (var k = 0; k < headings.length; k++) {
                headingsArray.push(headings[k]);
            }
            
            // Map heading IDs to nav links
            var navMap = {};
            var sidebar = document.querySelector('.sidebar');
            var navLinksAll = document.querySelectorAll('.sidebar a[href^="#"]');
            for (var m = 0; m < navLinksAll.length; m++) {
                var link = navLinksAll[m];
                var href = link.getAttribute('href');
                if (href && href.length > 1) {
                    navMap[href.slice(1)] = link;
                }
            }
            
            var currentActive = null;
            
            function updateNav() {
                try {
                    var current = null;
                    for (var n = 0; n < headingsArray.length; n++) {
                        var rect = headingsArray[n].getBoundingClientRect();
                        if (rect.top <= 150) {
                            current = headingsArray[n];
                        } else {
                            break;
                        }
                    }
                    
                    if (!current && headingsArray.length > 0) {
                        current = headingsArray[0];
                    }
                    if (!current) return;
                    
                    var navLink = navMap[current.id];
                    if (navLink && navLink !== currentActive) {
                        if (currentActive) currentActive.classList.remove('active');
                        navLink.classList.add('active');
                        currentActive = navLink;
                        
                        // Expand parent
                        var navLi = navLink.parentElement;
                        var parentUl = navLi ? navLi.parentElement : null;
                        var parentLi = (parentUl && parentUl.parentElement && parentUl.parentElement.tagName === 'LI') 
                                       ? parentUl.parentElement : null;
                        
                        var expanded = document.querySelectorAll('.sidebar li.expanded');
                        for (var p = 0; p < expanded.length; p++) {
                            expanded[p].classList.remove('expanded');
                        }
                        
                        if (parentLi && parentLi.classList.contains('has-children')) {
                            parentLi.classList.add('expanded');
                        } else if (navLi && navLi.classList.contains('has-children')) {
                            navLi.classList.add('expanded');
                        }
                        
                        // Scroll sidebar
                        if (sidebar) {
                            var offset = navLink.offsetTop - sidebar.offsetHeight / 2;
                            sidebar.scrollTop = offset;
                        }
                    }
                } catch(e) {}
            }
            
            var ticking = false;
            window.addEventListener('scroll', function() {
                if (!ticking) {
                    requestAnimationFrame(function() {
                        updateNav();
                        ticking = false;
                    });
                    ticking = true;
                }
            }, { passive: true });
            
            setTimeout(updateNav, 200);
            
            // Expandable nav click
            var hasChildrenLinks = document.querySelectorAll('.sidebar li.has-children > a');
            for (var q = 0; q < hasChildrenLinks.length; q++) {
                hasChildrenLinks[q].addEventListener('click', function() {
                    var clickedLi = this.parentElement;
                    var allExp = document.querySelectorAll('.sidebar li.expanded');
                    for (var r = 0; r < allExp.length; r++) {
                        if (allExp[r] !== clickedLi) allExp[r].classList.remove('expanded');
                    }
                    clickedLi.classList.add('expanded');
                });
            }
            
        } catch(e) {}
    });
    </script>
</head>
<body>
    <!-- Theme toggle button -->
    <button class="theme-toggle" aria-label="Toggle dark/light mode">
        <span class="icon-moon icon"><svg viewBox="0 0 24 24"><path d="M12 3a9 9 0 1 0 9 9c0-.46-.04-.92-.1-1.36a5.389 5.389 0 0 1-4.4 2.26 5.403 5.403 0 0 1-3.14-9.8c-.44-.06-.9-.1-1.36-.1z"/></svg></span>
        <span class="icon-sun icon"><svg viewBox="0 0 24 24"><path d="M12 7c-2.76 0-5 2.24-5 5s2.24 5 5 5 5-2.24 5-5-2.24-5-5-5zM2 13h2c.55 0 1-.45 1-1s-.45-1-1-1H2c-.55 0-1 .45-1 1s.45 1 1 1zm18 0h2c.55 0 1-.45 1-1s-.45-1-1-1h-2c-.55 0-1 .45-1 1s.45 1 1 1zM11 2v2c0 .55.45 1 1 1s1-.45 1-1V2c0-.55-.45-1-1-1s-1 .45-1 1zm0 18v2c0 .55.45 1 1 1s1-.45 1-1v-2c0-.55-.45-1-1-1s-1 .45-1 1zM5.99 4.58a.996.996 0 0 0-1.41 0 .996.996 0 0 0 0 1.41l1.06 1.06c.39.39 1.03.39 1.41 0s.39-1.03 0-1.41L5.99 4.58zm12.37 12.37a.996.996 0 0 0-1.41 0 .996.996 0 0 0 0 1.41l1.06 1.06c.39.39 1.03.39 1.41 0a.996.996 0 0 0 0-1.41l-1.06-1.06zm1.06-10.96a.996.996 0 0 0 0-1.41.996.996 0 0 0-1.41 0l-1.06 1.06c-.39.39-.39 1.03 0 1.41s1.03.39 1.41 0l1.06-1.06zM7.05 18.36a.996.996 0 0 0 0-1.41.996.996 0 0 0-1.41 0l-1.06 1.06c-.39.39-.39 1.03 0 1.41s1.03.39 1.41 0l1.06-1.06z"/></svg></span>
    </button>
    
    <!-- Hamburger menu button (mobile only) -->
    <button class="menu-toggle" aria-label="Open navigation menu">
        <span class="icon"><svg viewBox="0 0 24 24"><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg></span>
    </button>
    
    <!-- Overlay backdrop (mobile only) -->
    <div class="sidebar-overlay"></div>
    
    <nav class="sidebar">
        <button class="sidebar-close" aria-label="Close navigation menu">
            <span class="icon"><svg viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg></span>
        </button>
        <div class="nav-title">Ferhat Erata's ML/AI Study Notes</div>
                <ul>
                <li><a href="#ferhat-eratas-mlai-study-notes"
                id="toc-ferhat-eratas-mlai-study-notes">Ferhat Erata‚Äôs
                ML/AI Study Notes</a>
                <ul>
                <li><a href="#quick-navigation"
                id="toc-quick-navigation">Quick Navigation</a></li>
                </ul></li>
                <li><a href="#part-1-learning-through-examples"
                id="toc-part-1-learning-through-examples">Part 1:
                Learning Through Examples</a>
                <ul>
                <li><a href="#what-is-machine-learning"
                id="toc-what-is-machine-learning">1.1 What is Machine
                Learning?</a></li>
                <li><a href="#simple-linear-regression"
                id="toc-simple-linear-regression">1.2 Simple Linear
                Regression</a>
                <ul>
                <li><a href="#the-problem" id="toc-the-problem">The
                Problem</a></li>
                <li><a href="#the-model-a-line"
                id="toc-the-model-a-line">The Model: A Line</a></li>
                <li><a href="#visualizing-different-choices"
                id="toc-visualizing-different-choices">Visualizing
                Different Choices</a></li>
                <li><a href="#the-loss-function-mean-squared-error"
                id="toc-the-loss-function-mean-squared-error">The Loss
                Function: Mean Squared Error</a></li>
                </ul></li>
                <li><a href="#gradient-descent-finding-the-best-line"
                id="toc-gradient-descent-finding-the-best-line">1.3
                Gradient Descent: Finding the Best Line</a>
                <ul>
                <li><a href="#the-optimization-problem"
                id="toc-the-optimization-problem">The Optimization
                Problem</a></li>
                <li><a href="#the-key-idea" id="toc-the-key-idea">The
                Key Idea</a></li>
                <li><a href="#what-is-a-gradient-intuitive-explanation"
                id="toc-what-is-a-gradient-intuitive-explanation">What
                is a Gradient? (Intuitive Explanation)</a></li>
                <li><a href="#the-gradient-which-way-is-downhill"
                id="toc-the-gradient-which-way-is-downhill">The
                Gradient: Which Way is Downhill?</a></li>
                <li><a href="#the-update-rule"
                id="toc-the-update-rule">The Update Rule</a></li>
                <li><a href="#concrete-example-step-by-step"
                id="toc-concrete-example-step-by-step">Concrete Example:
                Step by Step</a></li>
                <li><a href="#python-implementation"
                id="toc-python-implementation">Python
                Implementation</a></li>
                </ul></li>
                <li><a href="#logistic-regression-classification"
                id="toc-logistic-regression-classification">1.4 Logistic
                Regression: Classification</a>
                <ul>
                <li><a href="#the-problem-binary-classification"
                id="toc-the-problem-binary-classification">The Problem:
                Binary Classification</a></li>
                <li><a href="#why-linear-regression-fails"
                id="toc-why-linear-regression-fails">Why Linear
                Regression Fails</a></li>
                <li><a href="#the-sigmoid-function"
                id="toc-the-sigmoid-function">The Sigmoid
                Function</a></li>
                <li><a
                href="#deriving-the-sigmoid-derivative-chain-rule-example"
                id="toc-deriving-the-sigmoid-derivative-chain-rule-example">Deriving
                the Sigmoid Derivative (Chain Rule Example)</a></li>
                <li><a
                href="#why-the-maximum-derivative-is-14-and-why-it-matters"
                id="toc-why-the-maximum-derivative-is-14-and-why-it-matters">Why
                the Maximum Derivative is 1/4 (and Why It
                Matters)</a></li>
                <li><a href="#the-vanishing-gradient-problem"
                id="toc-the-vanishing-gradient-problem">The Vanishing
                Gradient Problem</a></li>
                <li><a href="#relu-the-solution-to-vanishing-gradients"
                id="toc-relu-the-solution-to-vanishing-gradients">ReLU:
                The Solution to Vanishing Gradients</a></li>
                <li><a
                href="#interview-q-why-do-we-use-relu-instead-of-sigmoid-in-hidden-layers"
                id="toc-interview-q-why-do-we-use-relu-instead-of-sigmoid-in-hidden-layers">Interview
                Q: ‚ÄúWhy do we use ReLU instead of sigmoid in hidden
                layers?‚Äù</a></li>
                <li><a href="#logistic-regression-model"
                id="toc-logistic-regression-model">Logistic Regression
                Model</a></li>
                <li><a href="#the-loss-function-binary-cross-entropy"
                id="toc-the-loss-function-binary-cross-entropy">The Loss
                Function: Binary Cross-Entropy</a></li>
                <li><a
                href="#deriving-cross-entropy-from-maximum-likelihood"
                id="toc-deriving-cross-entropy-from-maximum-likelihood">Deriving
                Cross-Entropy from Maximum Likelihood</a></li>
                <li><a href="#gradient-for-logistic-regression"
                id="toc-gradient-for-logistic-regression">Gradient for
                Logistic Regression</a></li>
                <li><a href="#python-implementation-1"
                id="toc-python-implementation-1">Python
                Implementation</a></li>
                </ul></li>
                <li><a href="#multi-layer-perceptron-mlp"
                id="toc-multi-layer-perceptron-mlp">1.5 Multi-Layer
                Perceptron (MLP)</a>
                <ul>
                <li><a href="#limitation-of-linear-models"
                id="toc-limitation-of-linear-models">Limitation of
                Linear Models</a></li>
                <li><a href="#the-mlp-stacking-layers"
                id="toc-the-mlp-stacking-layers">The MLP: Stacking
                Layers</a></li>
                <li><a href="#forward-pass-step-by-step"
                id="toc-forward-pass-step-by-step">Forward Pass (Step by
                Step)</a></li>
                <li><a href="#activation-functions-why-we-need-them"
                id="toc-activation-functions-why-we-need-them">Activation
                Functions: Why We Need Them</a></li>
                <li><a href="#common-activation-functions"
                id="toc-common-activation-functions">Common Activation
                Functions</a></li>
                <li><a href="#relu-why-it-works"
                id="toc-relu-why-it-works">ReLU: Why It Works</a></li>
                <li><a href="#concrete-example-learning-xor"
                id="toc-concrete-example-learning-xor">Concrete Example:
                Learning XOR</a></li>
                <li><a href="#mlp-solution" id="toc-mlp-solution">MLP
                Solution</a></li>
                <li><a href="#python-implementation-2"
                id="toc-python-implementation-2">Python
                Implementation</a></li>
                </ul></li>
                <li><a href="#the-backpropagation-algorithm"
                id="toc-the-backpropagation-algorithm">1.6 The
                Backpropagation Algorithm</a>
                <ul>
                <li><a
                href="#the-problem-how-to-get-gradients-in-deep-networks"
                id="toc-the-problem-how-to-get-gradients-in-deep-networks">The
                Problem: How to Get Gradients in Deep Networks?</a></li>
                <li><a href="#the-chain-rule-the-key-insight"
                id="toc-the-chain-rule-the-key-insight">The Chain Rule:
                The Key Insight</a></li>
                <li><a
                href="#visualizing-backpropagation-the-computational-graph"
                id="toc-visualizing-backpropagation-the-computational-graph">Visualizing
                Backpropagation: The Computational Graph</a></li>
                <li><a href="#backprop-step-by-step"
                id="toc-backprop-step-by-step">Backprop: Step by
                Step</a></li>
                <li><a href="#why-its-efficient"
                id="toc-why-its-efficient">Why It‚Äôs Efficient</a></li>
                <li><a href="#automatic-differentiation"
                id="toc-automatic-differentiation">Automatic
                Differentiation</a></li>
                </ul></li>
                <li><a href="#softmax-and-multi-class-classification"
                id="toc-softmax-and-multi-class-classification">1.7
                Softmax and Multi-class Classification</a>
                <ul>
                <li><a href="#from-binary-to-multi-class"
                id="toc-from-binary-to-multi-class">From Binary to
                Multi-class</a></li>
                <li><a href="#properties-of-softmax"
                id="toc-properties-of-softmax">Properties of
                Softmax</a></li>
                <li><a href="#why-exponential-not-simple-normalization"
                id="toc-why-exponential-not-simple-normalization">Why
                Exponential (Not Simple Normalization)?</a></li>
                <li><a href="#example-3-class-classification"
                id="toc-example-3-class-classification">Example: 3-Class
                Classification</a></li>
                <li><a href="#multi-class-cross-entropy-loss"
                id="toc-multi-class-cross-entropy-loss">Multi-class
                Cross-Entropy Loss</a></li>
                <li><a href="#pytorch-implementation"
                id="toc-pytorch-implementation">PyTorch
                Implementation</a></li>
                </ul></li>
                <li><a href="#batching-why-and-how"
                id="toc-batching-why-and-how">1.8 Batching: Why and
                How</a>
                <ul>
                <li><a href="#what-this-means-for-beginners"
                id="toc-what-this-means-for-beginners">What This Means
                (For Beginners)</a></li>
                <li><a href="#the-key-formula"
                id="toc-the-key-formula">The Key Formula</a></li>
                <li><a href="#worked-example"
                id="toc-worked-example">Worked Example</a></li>
                <li><a href="#visual-summary"
                id="toc-visual-summary">Visual Summary</a></li>
                <li><a href="#the-pizza-analogy"
                id="toc-the-pizza-analogy">The Pizza Analogy üçï</a></li>
                <li><a href="#why-multiple-epochs"
                id="toc-why-multiple-epochs">Why Multiple
                Epochs?</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-epoch-batch-and-iteration"
                id="toc-interview-q-whats-the-difference-between-epoch-batch-and-iteration">Interview
                Q: ‚ÄúWhat‚Äôs the difference between epoch, batch, and
                iteration?‚Äù</a></li>
                <li><a href="#the-problem-with-single-examples"
                id="toc-the-problem-with-single-examples">The Problem
                with Single Examples</a></li>
                <li><a href="#mini-batch-the-sweet-spot"
                id="toc-mini-batch-the-sweet-spot">Mini-batch: The Sweet
                Spot</a></li>
                <li><a href="#why-batching-works"
                id="toc-why-batching-works">Why Batching Works</a></li>
                <li><a href="#batch-size-tradeoffs"
                id="toc-batch-size-tradeoffs">Batch Size
                Tradeoffs</a></li>
                <li><a href="#the-noise-is-good-insight"
                id="toc-the-noise-is-good-insight">The ‚ÄúNoise is Good‚Äù
                Insight</a></li>
                <li><a href="#practical-guidelines"
                id="toc-practical-guidelines">Practical
                Guidelines</a></li>
                <li><a href="#code-example-batching-in-pytorch"
                id="toc-code-example-batching-in-pytorch">Code Example:
                Batching in PyTorch</a></li>
                </ul></li>
                <li><a href="#data-preprocessing"
                id="toc-data-preprocessing">1.9 Data Preprocessing</a>
                <ul>
                <li><a href="#why-preprocess"
                id="toc-why-preprocess">Why Preprocess?</a></li>
                <li><a href="#standardization-z-score-normalization"
                id="toc-standardization-z-score-normalization">Standardization
                (Z-score Normalization)</a></li>
                <li><a href="#min-max-normalization"
                id="toc-min-max-normalization">Min-Max
                Normalization</a></li>
                <li><a href="#when-to-use-which"
                id="toc-when-to-use-which">When to Use Which?</a></li>
                <li><a href="#image-preprocessing"
                id="toc-image-preprocessing">Image
                Preprocessing</a></li>
                <li><a href="#text-preprocessing"
                id="toc-text-preprocessing">Text Preprocessing</a></li>
                <li><a href="#handling-missing-values"
                id="toc-handling-missing-values">Handling Missing
                Values</a></li>
                <li><a href="#outlier-detection-and-handling"
                id="toc-outlier-detection-and-handling">Outlier
                Detection and Handling</a></li>
                </ul></li>
                <li><a href="#loss-function-comparison"
                id="toc-loss-function-comparison">1.10 Loss Function
                Comparison</a>
                <ul>
                <li><a href="#quick-reference-table"
                id="toc-quick-reference-table">Quick Reference
                Table</a></li>
                <li><a href="#mse-vs-cross-entropy-for-classification"
                id="toc-mse-vs-cross-entropy-for-classification">MSE vs
                Cross-Entropy for Classification</a></li>
                <li><a href="#mse-gradient-problem"
                id="toc-mse-gradient-problem">MSE Gradient
                Problem</a></li>
                <li><a href="#huber-loss-best-of-both-worlds"
                id="toc-huber-loss-best-of-both-worlds">Huber Loss: Best
                of Both Worlds</a></li>
                </ul></li>
                <li><a href="#convolutional-neural-networks-cnns"
                id="toc-convolutional-neural-networks-cnns">1.11
                Convolutional Neural Networks (CNNs)</a>
                <ul>
                <li><a href="#the-problem-with-mlps-for-images"
                id="toc-the-problem-with-mlps-for-images">The Problem
                with MLPs for Images</a></li>
                <li><a href="#the-convolution-operation"
                id="toc-the-convolution-operation">The Convolution
                Operation</a></li>
                <li><a href="#why-convolutions-work"
                id="toc-why-convolutions-work">Why Convolutions
                Work</a></li>
                <li><a href="#key-cnn-concepts"
                id="toc-key-cnn-concepts">Key CNN Concepts</a></li>
                <li><a href="#receptive-fields-what-each-neuron-sees"
                id="toc-receptive-fields-what-each-neuron-sees">Receptive
                Fields: What Each Neuron ‚ÄúSees‚Äù</a></li>
                <li><a href="#a-simple-cnn-architecture"
                id="toc-a-simple-cnn-architecture">A Simple CNN
                Architecture</a></li>
                <li><a href="#pytorch-cnn" id="toc-pytorch-cnn">PyTorch
                CNN</a></li>
                </ul></li>
                <li><a href="#word-embeddings"
                id="toc-word-embeddings">1.12 Word Embeddings</a>
                <ul>
                <li><a
                href="#the-problem-how-to-represent-words-as-numbers"
                id="toc-the-problem-how-to-represent-words-as-numbers">The
                Problem: How to Represent Words as Numbers?</a></li>
                <li><a href="#approach-1-one-hot-encoding"
                id="toc-approach-1-one-hot-encoding">Approach 1: One-Hot
                Encoding</a></li>
                <li><a href="#approach-2-dense-embeddings"
                id="toc-approach-2-dense-embeddings">Approach 2: Dense
                Embeddings</a></li>
                <li><a href="#why-embeddings-work"
                id="toc-why-embeddings-work">Why Embeddings
                Work</a></li>
                <li><a href="#embedding-layer-in-pytorch"
                id="toc-embedding-layer-in-pytorch">Embedding Layer in
                PyTorch</a></li>
                <li><a href="#pretrained-embeddings"
                id="toc-pretrained-embeddings">Pretrained
                Embeddings</a></li>
                <li><a href="#how-word2vec-learns-skip-gram-training"
                id="toc-how-word2vec-learns-skip-gram-training">How
                Word2Vec Learns: Skip-gram Training</a></li>
                <li><a href="#from-word-embeddings-to-transformers"
                id="toc-from-word-embeddings-to-transformers">From Word
                Embeddings to Transformers</a></li>
                </ul></li>
                <li><a href="#the-xor-problem-a-complete-mlp-example"
                id="toc-the-xor-problem-a-complete-mlp-example">1.13 The
                XOR Problem: A Complete MLP Example</a>
                <ul>
                <li><a href="#why-xor" id="toc-why-xor">Why
                XOR?</a></li>
                <li><a href="#the-network-architecture"
                id="toc-the-network-architecture">The Network
                Architecture</a></li>
                <li><a href="#step-1-initialize-weights"
                id="toc-step-1-initialize-weights">Step 1: Initialize
                Weights</a></li>
                <li><a href="#step-2-forward-pass-for-input-1-1"
                id="toc-step-2-forward-pass-for-input-1-1">Step 2:
                Forward Pass (for input [1, 1])</a></li>
                <li><a href="#step-3-all-four-inputs"
                id="toc-step-3-all-four-inputs">Step 3: All Four
                Inputs</a></li>
                <li><a href="#better-tuned-weights-for-xor"
                id="toc-better-tuned-weights-for-xor">Better-Tuned
                Weights for XOR</a></li>
                <li><a href="#step-4-compute-loss-binary-cross-entropy"
                id="toc-step-4-compute-loss-binary-cross-entropy">Step
                4: Compute Loss (Binary Cross-Entropy)</a></li>
                <li><a href="#step-5-backward-pass"
                id="toc-step-5-backward-pass">Step 5: Backward
                Pass</a></li>
                <li><a href="#pytorch-implementation-1"
                id="toc-pytorch-implementation-1">PyTorch
                Implementation</a></li>
                <li><a href="#key-insights" id="toc-key-insights">Key
                Insights</a></li>
                </ul></li>
                <li><a href="#weight-initialization"
                id="toc-weight-initialization">1.14 Weight
                Initialization</a>
                <ul>
                <li><a href="#why-does-initialization-matter"
                id="toc-why-does-initialization-matter">Why Does
                Initialization Matter?</a></li>
                <li><a href="#the-goal" id="toc-the-goal">The
                Goal</a></li>
                <li><a href="#xavierglorot-initialization-2010"
                id="toc-xavierglorot-initialization-2010">Xavier/Glorot
                Initialization (2010)</a></li>
                <li><a href="#hekaiming-initialization-2015"
                id="toc-hekaiming-initialization-2015">He/Kaiming
                Initialization (2015)</a></li>
                <li><a
                href="#deriving-he-initialization-variance-propagation"
                id="toc-deriving-he-initialization-variance-propagation">Deriving
                He Initialization: Variance Propagation</a></li>
                <li><a href="#quick-reference"
                id="toc-quick-reference">Quick Reference</a></li>
                <li><a href="#what-about-biases"
                id="toc-what-about-biases">What About Biases?</a></li>
                <li><a href="#example-why-bad-init-fails"
                id="toc-example-why-bad-init-fails">Example: Why Bad
                Init Fails</a></li>
                </ul></li>
                <li><a href="#dropout" id="toc-dropout">1.15 Dropout</a>
                <ul>
                <li><a href="#what-is-dropout"
                id="toc-what-is-dropout">What is Dropout?</a></li>
                <li><a href="#why-does-it-work"
                id="toc-why-does-it-work">Why Does It Work?</a></li>
                <li><a href="#training-vs-inference"
                id="toc-training-vs-inference">Training vs
                Inference</a></li>
                <li><a href="#pytorch-usage"
                id="toc-pytorch-usage">PyTorch Usage</a></li>
                <li><a href="#typical-dropout-rates"
                id="toc-typical-dropout-rates">Typical Dropout
                Rates</a></li>
                <li><a href="#dropout-variants"
                id="toc-dropout-variants">Dropout Variants</a></li>
                <li><a href="#interview-q-why-use-dropout-instead-of-l2"
                id="toc-interview-q-why-use-dropout-instead-of-l2">Interview
                Q: ‚ÄúWhy use dropout instead of L2?‚Äù</a></li>
                </ul></li>
                <li><a href="#overfitting-regularization-preview"
                id="toc-overfitting-regularization-preview">1.16
                Overfitting &amp; Regularization Preview</a>
                <ul>
                <li><a href="#the-central-problem"
                id="toc-the-central-problem">The Central
                Problem</a></li>
                <li><a href="#visual-the-overfitting-spectrum"
                id="toc-visual-the-overfitting-spectrum">Visual: The
                Overfitting Spectrum</a></li>
                <li><a href="#learning-curves-your-diagnostic-tool"
                id="toc-learning-curves-your-diagnostic-tool">Learning
                Curves: Your Diagnostic Tool</a></li>
                <li><a href="#the-regularization-toolbox"
                id="toc-the-regularization-toolbox">The Regularization
                Toolbox</a></li>
                <li><a href="#l2-regularization-weight-decay"
                id="toc-l2-regularization-weight-decay">L2
                Regularization (Weight Decay)</a></li>
                <li><a href="#l1-regularization-lasso"
                id="toc-l1-regularization-lasso">L1 Regularization
                (Lasso)</a></li>
                <li><a href="#l1-vs-l2-when-to-use-which"
                id="toc-l1-vs-l2-when-to-use-which">L1 vs L2: When to
                Use Which?</a></li>
                <li><a href="#quick-decision-tree"
                id="toc-quick-decision-tree">Quick Decision
                Tree</a></li>
                <li><a href="#the-bias-variance-tradeoff-preview"
                id="toc-the-bias-variance-tradeoff-preview">The
                Bias-Variance Tradeoff (Preview)</a></li>
                </ul></li>
                <li><a
                href="#putting-it-all-together-complete-neural-network-setup"
                id="toc-putting-it-all-together-complete-neural-network-setup">1.17
                Putting It All Together: Complete Neural Network
                Setup</a>
                <ul>
                <li><a href="#complete-keras-example"
                id="toc-complete-keras-example">Complete Keras
                Example</a></li>
                <li><a href="#quick-reference-component-options"
                id="toc-quick-reference-component-options">Quick
                Reference: Component Options</a></li>
                <li><a href="#task-specific-configurations"
                id="toc-task-specific-configurations">Task-Specific
                Configurations</a></li>
                <li><a href="#common-mistakes-to-avoid"
                id="toc-common-mistakes-to-avoid">Common Mistakes to
                Avoid</a></li>
                <li><a href="#pytorch-equivalent-for-reference"
                id="toc-pytorch-equivalent-for-reference">PyTorch
                Equivalent (for reference)</a></li>
                </ul></li>
                <li><a href="#batch-normalization"
                id="toc-batch-normalization">1.18 Batch
                Normalization</a>
                <ul>
                <li><a href="#the-problem-internal-covariate-shift"
                id="toc-the-problem-internal-covariate-shift">The
                Problem: Internal Covariate Shift</a></li>
                <li><a href="#modern-perspective-beyond-ics"
                id="toc-modern-perspective-beyond-ics">Modern
                Perspective: Beyond ICS</a></li>
                <li><a href="#the-solution-normalize-each-layer"
                id="toc-the-solution-normalize-each-layer">The Solution:
                Normalize Each Layer</a></li>
                <li><a href="#learnable-parameters"
                id="toc-learnable-parameters">Learnable
                Parameters</a></li>
                <li><a href="#training-vs-inference-1"
                id="toc-training-vs-inference-1">Training vs
                Inference</a></li>
                <li><a href="#where-to-place-batchnorm"
                id="toc-where-to-place-batchnorm">Where to Place
                BatchNorm?</a></li>
                <li><a href="#benefits-of-batch-normalization"
                id="toc-benefits-of-batch-normalization">Benefits of
                Batch Normalization</a></li>
                <li><a href="#pytorch-implementation-2"
                id="toc-pytorch-implementation-2">PyTorch
                Implementation</a></li>
                <li><a href="#layer-normalization-alternative"
                id="toc-layer-normalization-alternative">Layer
                Normalization (Alternative)</a></li>
                <li><a href="#rmsnorm-the-modern-llm-standard"
                id="toc-rmsnorm-the-modern-llm-standard">RMSNorm: The
                Modern LLM Standard</a></li>
                <li><a href="#normalization-comparison-summary"
                id="toc-normalization-comparison-summary">Normalization
                Comparison Summary</a></li>
                </ul></li>
                <li><a href="#vanishing-and-exploding-gradients"
                id="toc-vanishing-and-exploding-gradients">1.19
                Vanishing and Exploding Gradients</a>
                <ul>
                <li><a href="#the-problem-1" id="toc-the-problem-1">The
                Problem</a></li>
                <li><a href="#vanishing-gradients"
                id="toc-vanishing-gradients">Vanishing
                Gradients</a></li>
                <li><a href="#exploding-gradients"
                id="toc-exploding-gradients">Exploding
                Gradients</a></li>
                <li><a href="#solutions"
                id="toc-solutions">Solutions</a></li>
                <li><a href="#gradient-clipping"
                id="toc-gradient-clipping">Gradient Clipping</a></li>
                <li><a href="#skip-connections-resnet"
                id="toc-skip-connections-resnet">Skip Connections
                (ResNet)</a></li>
                </ul></li>
                <li><a href="#numerical-stability"
                id="toc-numerical-stability">1.20 Numerical
                Stability</a>
                <ul>
                <li><a href="#what-is-numerical-stability"
                id="toc-what-is-numerical-stability">What is Numerical
                Stability?</a></li>
                <li><a
                href="#the-original-sin-floating-point-non-associativity"
                id="toc-the-original-sin-floating-point-non-associativity">The
                Original Sin: Floating-Point Non-Associativity</a></li>
                <li><a href="#determinism-vs-batch-invariance-advanced"
                id="toc-determinism-vs-batch-invariance-advanced">Determinism
                vs Batch Invariance (Advanced)</a></li>
                <li><a href="#the-log-sum-exp-trick"
                id="toc-the-log-sum-exp-trick">The Log-Sum-Exp
                Trick</a></li>
                <li><a href="#cross-entropy-with-logits"
                id="toc-cross-entropy-with-logits">Cross-Entropy with
                Logits</a></li>
                <li><a href="#epsilon-for-logarithms"
                id="toc-epsilon-for-logarithms">Epsilon for
                Logarithms</a></li>
                <li><a href="#common-numerical-issues-and-fixes"
                id="toc-common-numerical-issues-and-fixes">Common
                Numerical Issues and Fixes</a></li>
                </ul></li>
                <li><a href="#optimizers-sgd-momentum-and-adam"
                id="toc-optimizers-sgd-momentum-and-adam">1.21
                Optimizers: SGD, Momentum, and Adam</a>
                <ul>
                <li><a href="#why-different-optimizers"
                id="toc-why-different-optimizers">Why Different
                Optimizers?</a></li>
                <li><a href="#sgd-with-momentum"
                id="toc-sgd-with-momentum">SGD with Momentum</a></li>
                <li><a href="#adam-adaptive-moment-estimation"
                id="toc-adam-adaptive-moment-estimation">Adam: Adaptive
                Moment Estimation</a></li>
                <li><a href="#adamw-decoupled-weight-decay"
                id="toc-adamw-decoupled-weight-decay">AdamW: Decoupled
                Weight Decay</a></li>
                <li><a href="#quick-comparison"
                id="toc-quick-comparison">Quick Comparison</a></li>
                <li><a href="#when-to-use-which-1"
                id="toc-when-to-use-which-1">When to Use Which?</a></li>
                <li><a href="#the-sgd-vs-adam-generalization-debate"
                id="toc-the-sgd-vs-adam-generalization-debate">The SGD
                vs Adam Generalization Debate</a></li>
                </ul></li>
                <li><a href="#common-interview-questions-summary"
                id="toc-common-interview-questions-summary">1.22 Common
                Interview Questions Summary</a>
                <ul>
                <li><a href="#architecture-training"
                id="toc-architecture-training">Architecture &amp;
                Training</a></li>
                <li><a href="#optimization"
                id="toc-optimization">Optimization</a></li>
                <li><a href="#regularization"
                id="toc-regularization">Regularization</a></li>
                <li><a href="#softmax-classification"
                id="toc-softmax-classification">Softmax &amp;
                Classification</a></li>
                </ul></li>
                <li><a href="#debugging-checklist"
                id="toc-debugging-checklist">1.23 Debugging
                Checklist</a>
                <ul>
                <li><a href="#loss-is-nan-or-inf"
                id="toc-loss-is-nan-or-inf">1. Loss is NaN or
                Inf</a></li>
                <li><a href="#loss-doesnt-decrease"
                id="toc-loss-doesnt-decrease">2. Loss Doesn‚Äôt
                Decrease</a></li>
                <li><a
                href="#training-loss-good-validation-loss-bad-overfitting"
                id="toc-training-loss-good-validation-loss-bad-overfitting">3.
                Training Loss Good, Validation Loss Bad
                (Overfitting)</a></li>
                <li><a href="#both-losses-high-underfitting"
                id="toc-both-losses-high-underfitting">4. Both Losses
                High (Underfitting)</a></li>
                <li><a href="#quick-sanity-checks"
                id="toc-quick-sanity-checks">5. Quick Sanity
                Checks</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-2-core-theory"
                id="toc-part-2-core-theory">Part 2: Core Theory</a>
                <ul>
                <li><a href="#the-core-update-rule"
                id="toc-the-core-update-rule">2.1 The Core Update
                Rule</a>
                <ul>
                <li><a href="#the-question" id="toc-the-question">The
                Question</a></li>
                <li><a href="#short-answer" id="toc-short-answer">Short
                Answer</a></li>
                <li><a href="#breaking-down-each-component"
                id="toc-breaking-down-each-component">Breaking Down Each
                Component</a></li>
                </ul></li>
                <li><a href="#why-gradients-point-the-way"
                id="toc-why-gradients-point-the-way">2.2 Why Gradients
                Point the Way</a>
                <ul>
                <li><a href="#intuition-hiking-in-the-fog"
                id="toc-intuition-hiking-in-the-fog">Intuition: Hiking
                in the Fog</a></li>
                <li><a href="#the-gradient-as-a-direction"
                id="toc-the-gradient-as-a-direction">The Gradient as a
                Direction</a></li>
                <li><a href="#directional-derivatives"
                id="toc-directional-derivatives">Directional
                Derivatives</a></li>
                <li><a href="#taylor-expansion-perspective"
                id="toc-taylor-expansion-perspective">Taylor Expansion
                Perspective</a></li>
                </ul></li>
                <li><a href="#stochastic-gradient-descent-sgd"
                id="toc-stochastic-gradient-descent-sgd">2.3 Stochastic
                Gradient Descent (SGD)</a>
                <ul>
                <li><a href="#how-sgd-works" id="toc-how-sgd-works">How
                SGD Works</a></li>
                <li><a
                href="#concrete-example-linear-regression-with-sgd"
                id="toc-concrete-example-linear-regression-with-sgd">Concrete
                Example: Linear Regression with SGD</a></li>
                <li><a href="#full-batch-vs-mini-batch-gradient-descent"
                id="toc-full-batch-vs-mini-batch-gradient-descent">Full-Batch
                vs Mini-Batch Gradient Descent</a></li>
                <li><a href="#why-use-sgd" id="toc-why-use-sgd">Why Use
                SGD?</a></li>
                <li><a
                href="#why-noise-can-help-implicit-regularization"
                id="toc-why-noise-can-help-implicit-regularization">Why
                Noise Can Help (Implicit Regularization)</a></li>
                <li><a href="#batch-size-tradeoffs-1"
                id="toc-batch-size-tradeoffs-1">Batch Size
                Tradeoffs</a></li>
                <li><a href="#sgd-vs-full-batch-gradient-descent"
                id="toc-sgd-vs-full-batch-gradient-descent">SGD vs
                Full-Batch Gradient Descent</a></li>
                <li><a href="#key-variations"
                id="toc-key-variations">Key Variations</a></li>
                <li><a href="#pytorch-implementation-3"
                id="toc-pytorch-implementation-3">PyTorch
                Implementation</a></li>
                </ul></li>
                <li><a href="#momentum-methods"
                id="toc-momentum-methods">2.4 Momentum Methods</a>
                <ul>
                <li><a
                href="#the-problem-oscillations-in-ill-conditioned-landscapes"
                id="toc-the-problem-oscillations-in-ill-conditioned-landscapes">The
                Problem: Oscillations in Ill-Conditioned
                Landscapes</a></li>
                <li><a href="#classical-momentum"
                id="toc-classical-momentum">Classical Momentum</a></li>
                <li><a href="#intuition-ball-rolling-downhill"
                id="toc-intuition-ball-rolling-downhill">Intuition: Ball
                Rolling Downhill</a></li>
                <li><a href="#effective-step-size"
                id="toc-effective-step-size">Effective Step
                Size</a></li>
                <li><a href="#why-Œ≤-0.9" id="toc-why-Œ≤-0.9">Why Œ≤ =
                0.9?</a></li>
                <li><a href="#nesterov-accelerated-gradient-nag"
                id="toc-nesterov-accelerated-gradient-nag">Nesterov
                Accelerated Gradient (NAG)</a></li>
                <li><a href="#why-it-helps" id="toc-why-it-helps">Why It
                Helps</a></li>
                <li><a href="#pytorch-implementation-4"
                id="toc-pytorch-implementation-4">PyTorch
                Implementation</a></li>
                </ul></li>
                <li><a href="#adaptive-learning-rate-methods"
                id="toc-adaptive-learning-rate-methods">2.5 Adaptive
                Learning Rate Methods</a>
                <ul>
                <li><a href="#the-problem-with-fixed-learning-rates"
                id="toc-the-problem-with-fixed-learning-rates">The
                Problem with Fixed Learning Rates</a></li>
                <li><a href="#the-evolution-adagrad-rmsprop-adam-adamw"
                id="toc-the-evolution-adagrad-rmsprop-adam-adamw">The
                Evolution: AdaGrad ‚Üí RMSprop ‚Üí Adam ‚Üí AdamW</a></li>
                <li><a href="#adagrad-2011-the-first-adaptive-method"
                id="toc-adagrad-2011-the-first-adaptive-method">AdaGrad
                (2011): The First Adaptive Method</a></li>
                <li><a href="#rmsprop-2012-forgetting-the-past"
                id="toc-rmsprop-2012-forgetting-the-past">RMSprop
                (2012): Forgetting the Past</a></li>
                <li><a href="#adam-2015-the-best-of-both-worlds"
                id="toc-adam-2015-the-best-of-both-worlds">Adam (2015):
                The Best of Both Worlds</a></li>
                <li><a href="#why-bias-correction-matters"
                id="toc-why-bias-correction-matters">Why Bias Correction
                Matters</a></li>
                <li><a href="#adamw-2019-the-interview-question"
                id="toc-adamw-2019-the-interview-question">AdamW (2019):
                The Interview Question!</a></li>
                <li><a href="#the-problem-adam-breaks-weight-decay"
                id="toc-the-problem-adam-breaks-weight-decay">The
                Problem: Adam Breaks Weight Decay</a></li>
                <li><a href="#the-fix-decouple-weight-decay"
                id="toc-the-fix-decouple-weight-decay">The Fix: Decouple
                Weight Decay</a></li>
                <li><a href="#why-it-matters-in-practice"
                id="toc-why-it-matters-in-practice">Why It Matters in
                Practice</a></li>
                <li><a href="#pytorch-implementation-5"
                id="toc-pytorch-implementation-5">PyTorch
                Implementation</a></li>
                </ul></li>
                <li><a href="#learning-rate-schedules"
                id="toc-learning-rate-schedules">2.6 Learning Rate
                Schedules</a>
                <ul>
                <li><a href="#why-schedules-matter"
                id="toc-why-schedules-matter">Why Schedules
                Matter</a></li>
                <li><a href="#warmup-why-transformers-need-it"
                id="toc-warmup-why-transformers-need-it">Warmup: Why
                Transformers Need It</a></li>
                <li><a href="#common-decay-schedules"
                id="toc-common-decay-schedules">Common Decay
                Schedules</a></li>
                <li><a href="#comparison"
                id="toc-comparison">Comparison</a></li>
                <li><a href="#warmup-cosine-standard-llm-recipe"
                id="toc-warmup-cosine-standard-llm-recipe">Warmup +
                Cosine (Standard LLM Recipe)</a></li>
                <li><a href="#optimizer-schedule-decision-guide"
                id="toc-optimizer-schedule-decision-guide">Optimizer +
                Schedule Decision Guide</a></li>
                <li><a href="#quick-recommendations"
                id="toc-quick-recommendations">Quick
                Recommendations</a></li>
                <li><a href="#decision-flowchart"
                id="toc-decision-flowchart">Decision Flowchart</a></li>
                <li><a href="#complete-training-loop-example"
                id="toc-complete-training-loop-example">Complete
                Training Loop Example</a></li>
                </ul></li>
                <li><a href="#convergence-and-loss-landscapes"
                id="toc-convergence-and-loss-landscapes">2.7 Convergence
                and Loss Landscapes</a>
                <ul>
                <li><a href="#what-does-the-loss-landscape-look-like"
                id="toc-what-does-the-loss-landscape-look-like">What
                Does the Loss Landscape Look Like?</a></li>
                <li><a
                href="#sharp-vs-flat-minima-why-it-matters-for-generalization"
                id="toc-sharp-vs-flat-minima-why-it-matters-for-generalization">Sharp
                vs Flat Minima: Why It Matters for
                Generalization</a></li>
                <li><a href="#how-sgd-finds-flat-minima"
                id="toc-how-sgd-finds-flat-minima">How SGD Finds Flat
                Minima</a></li>
                <li><a href="#when-optimization-converges"
                id="toc-when-optimization-converges">When Optimization
                ‚ÄúConverges‚Äù</a></li>
                <li><a
                href="#the-effect-of-architecture-on-the-landscape"
                id="toc-the-effect-of-architecture-on-the-landscape">The
                Effect of Architecture on the Landscape</a></li>
                </ul></li>
                <li><a href="#practical-considerations"
                id="toc-practical-considerations">2.8 Practical
                Considerations</a>
                <ul>
                <li><a href="#gradient-clipping-1"
                id="toc-gradient-clipping-1">Gradient Clipping</a></li>
                <li><a href="#weight-initialization-1"
                id="toc-weight-initialization-1">Weight
                Initialization</a></li>
                <li><a href="#debugging-checklist-1"
                id="toc-debugging-checklist-1">Debugging
                Checklist</a></li>
                </ul></li>
                <li><a href="#regularization-l1-and-l2"
                id="toc-regularization-l1-and-l2">2.9 Regularization: L1
                and L2</a>
                <ul>
                <li><a href="#the-problem-overfitting"
                id="toc-the-problem-overfitting">The Problem:
                Overfitting</a></li>
                <li><a href="#the-solution-penalize-large-weights"
                id="toc-the-solution-penalize-large-weights">The
                Solution: Penalize Large Weights</a></li>
                <li><a href="#l2-regularization-ridge-weight-decay"
                id="toc-l2-regularization-ridge-weight-decay">L2
                Regularization (Ridge / Weight Decay)</a></li>
                <li><a href="#l1-regularization-lasso-1"
                id="toc-l1-regularization-lasso-1">L1 Regularization
                (Lasso)</a></li>
                <li><a href="#l1-vs-l2-comparison"
                id="toc-l1-vs-l2-comparison">L1 vs L2:
                Comparison</a></li>
                <li><a href="#when-to-use-which-2"
                id="toc-when-to-use-which-2">When to Use Which</a></li>
                <li><a href="#elastic-net-best-of-both"
                id="toc-elastic-net-best-of-both">Elastic Net: Best of
                Both</a></li>
                <li><a
                href="#code-example-linear-regression-with-regularization"
                id="toc-code-example-linear-regression-with-regularization">Code
                Example: Linear Regression with Regularization</a></li>
                <li><a href="#regularization-in-deep-learning"
                id="toc-regularization-in-deep-learning">Regularization
                in Deep Learning</a></li>
                <li><a href="#summary-regularization-mental-model"
                id="toc-summary-regularization-mental-model">Summary:
                Regularization Mental Model</a></li>
                </ul></li>
                <li><a
                href="#gradient-instability-vanishing-and-exploding-gradients"
                id="toc-gradient-instability-vanishing-and-exploding-gradients">2.10
                Gradient Instability: Vanishing and Exploding
                Gradients</a>
                <ul>
                <li><a href="#the-problem-2" id="toc-the-problem-2">The
                Problem</a></li>
                <li><a href="#why-it-happens-chain-rule-multiplication"
                id="toc-why-it-happens-chain-rule-multiplication">Why It
                Happens: Chain Rule Multiplication</a></li>
                <li><a href="#vanishing-gradients-1"
                id="toc-vanishing-gradients-1">Vanishing
                Gradients</a></li>
                <li><a href="#exploding-gradients-1"
                id="toc-exploding-gradients-1">Exploding
                Gradients</a></li>
                <li><a href="#gradient-clipping-in-detail"
                id="toc-gradient-clipping-in-detail">Gradient Clipping
                in Detail</a></li>
                <li><a href="#residual-connections-the-key-innovation"
                id="toc-residual-connections-the-key-innovation">Residual
                Connections: The Key Innovation</a></li>
                </ul></li>
                <li><a href="#training-at-scale-memory-and-compute"
                id="toc-training-at-scale-memory-and-compute">2.11
                Training at Scale: Memory and Compute</a>
                <ul>
                <li><a href="#memory-breakdown-for-training"
                id="toc-memory-breakdown-for-training">Memory Breakdown
                for Training</a></li>
                <li><a href="#gradientactivation-checkpointing"
                id="toc-gradientactivation-checkpointing">Gradient/Activation
                Checkpointing</a></li>
                <li><a href="#mixed-precision-training"
                id="toc-mixed-precision-training">Mixed-Precision
                Training</a></li>
                <li><a href="#gradient-accumulation"
                id="toc-gradient-accumulation">Gradient
                Accumulation</a></li>
                <li><a href="#normalization-batchnorm-vs-layernorm"
                id="toc-normalization-batchnorm-vs-layernorm">Normalization:
                BatchNorm vs LayerNorm</a></li>
                </ul></li>
                <li><a href="#distributed-training"
                id="toc-distributed-training">2.12 Distributed
                Training</a>
                <ul>
                <li><a href="#why-distribute"
                id="toc-why-distribute">Why Distribute?</a></li>
                <li><a href="#parallelism-strategies"
                id="toc-parallelism-strategies">Parallelism
                Strategies</a></li>
                <li><a href="#data-parallelism-ddp"
                id="toc-data-parallelism-ddp">Data Parallelism
                (DDP)</a></li>
                <li><a href="#zero-zero-redundancy-optimizer"
                id="toc-zero-zero-redundancy-optimizer">ZeRO: Zero
                Redundancy Optimizer</a></li>
                <li><a href="#summary-which-parallelism-when"
                id="toc-summary-which-parallelism-when">Summary: Which
                Parallelism When?</a></li>
                </ul></li>
                <li><a href="#summary-comparison-table"
                id="toc-summary-comparison-table">2.13 Summary
                Comparison Table</a>
                <ul>
                <li><a href="#hyperparameter-starting-points"
                id="toc-hyperparameter-starting-points">Hyperparameter
                Starting Points</a></li>
                </ul></li>
                <li><a href="#references"
                id="toc-references">References</a></li>
                </ul></li>
                <li><a href="#part-3-math-foundations"
                id="toc-part-3-math-foundations">Part 3: Math
                Foundations</a>
                <ul>
                <li><a href="#linear-algebra"
                id="toc-linear-algebra">3.1 Linear Algebra</a>
                <ul>
                <li><a href="#vectors-and-matrices"
                id="toc-vectors-and-matrices">Vectors and
                Matrices</a></li>
                <li><a href="#tensors" id="toc-tensors">Tensors</a></li>
                <li><a
                href="#shape-notation-1d-tensors-vs-rowcolumn-vectors"
                id="toc-shape-notation-1d-tensors-vs-rowcolumn-vectors">Shape
                Notation: 1D Tensors vs Row/Column Vectors</a></li>
                <li><a href="#tensor-shape-management-in-deep-learning"
                id="toc-tensor-shape-management-in-deep-learning">Tensor
                Shape Management in Deep Learning</a></li>
                <li><a href="#key-matrix-operations"
                id="toc-key-matrix-operations">Key Matrix
                Operations</a></li>
                <li><a href="#linear-independence-and-dependence"
                id="toc-linear-independence-and-dependence">Linear
                Independence and Dependence</a></li>
                <li><a href="#how-to-calculate-determinants"
                id="toc-how-to-calculate-determinants">How to Calculate
                Determinants</a></li>
                <li><a href="#eigenvalues-and-eigenvectors"
                id="toc-eigenvalues-and-eigenvectors">Eigenvalues and
                Eigenvectors</a></li>
                <li><a href="#how-to-calculate-eigenvalues-step-by-step"
                id="toc-how-to-calculate-eigenvalues-step-by-step">How
                to Calculate Eigenvalues (Step-by-Step)</a></li>
                <li><a href="#worked-example-22-matrix"
                id="toc-worked-example-22-matrix">Worked Example: 2√ó2
                Matrix</a></li>
                <li><a href="#what-the-eigenvalues-tell-us"
                id="toc-what-the-eigenvalues-tell-us">What the
                Eigenvalues Tell Us</a></li>
                <li><a href="#singular-value-decomposition-svd"
                id="toc-singular-value-decomposition-svd">Singular Value
                Decomposition (SVD)</a></li>
                <li><a href="#principal-component-analysis-pca"
                id="toc-principal-component-analysis-pca">Principal
                Component Analysis (PCA)</a></li>
                <li><a href="#pca-worked-example-2d-1d"
                id="toc-pca-worked-example-2d-1d">PCA Worked Example: 2D
                ‚Üí 1D</a></li>
                </ul></li>
                <li><a href="#probability-and-statistics"
                id="toc-probability-and-statistics">3.2 Probability and
                Statistics</a>
                <ul>
                <li><a href="#probability-distributions"
                id="toc-probability-distributions">Probability
                Distributions</a></li>
                <li><a href="#bayes-theorem"
                id="toc-bayes-theorem">Bayes‚Äô Theorem</a></li>
                <li><a
                href="#what-bayes-theorem-actually-means-verbal-intuition"
                id="toc-what-bayes-theorem-actually-means-verbal-intuition">What
                Bayes‚Äô Theorem Actually Means (Verbal
                Intuition)</a></li>
                <li><a href="#maximum-likelihood-estimation-mle"
                id="toc-maximum-likelihood-estimation-mle">Maximum
                Likelihood Estimation (MLE)</a></li>
                <li><a
                href="#from-maximization-to-minimization-why-we-minimize-negative-log-likelihood"
                id="toc-from-maximization-to-minimization-why-we-minimize-negative-log-likelihood">From
                Maximization to Minimization: Why We MINIMIZE Negative
                Log-Likelihood</a></li>
                <li><a href="#why-log-likelihood-5-important-reasons"
                id="toc-why-log-likelihood-5-important-reasons">Why
                Log-Likelihood? (5 Important Reasons)</a></li>
                <li><a href="#worked-example-coin-flip-bernoulli-mle"
                id="toc-worked-example-coin-flip-bernoulli-mle">Worked
                Example: Coin Flip (Bernoulli MLE)</a></li>
                <li><a
                href="#worked-example-complete-mle-for-normal-distribution-Œº-and-œÉ¬≤"
                id="toc-worked-example-complete-mle-for-normal-distribution-Œº-and-œÉ¬≤">Worked
                Example: Complete MLE for Normal Distribution (Œº and
                œÉ¬≤)</a></li>
                <li><a href="#important-mle-variance-is-biased"
                id="toc-important-mle-variance-is-biased">‚ö†Ô∏è Important:
                MLE Variance is Biased!</a></li>
                <li><a href="#mle-for-logistic-regression-cross-entropy"
                id="toc-mle-for-logistic-regression-cross-entropy">MLE
                for Logistic Regression ‚Üí Cross-Entropy</a></li>
                <li><a
                href="#cross-entropy-vs-nll-two-derivations-same-formula"
                id="toc-cross-entropy-vs-nll-two-derivations-same-formula">Cross-Entropy
                vs NLL: Two Derivations, Same Formula</a></li>
                <li><a href="#mle-for-regression-mse"
                id="toc-mle-for-regression-mse">MLE for Regression ‚Üí
                MSE</a></li>
                <li><a
                href="#loss-functions-as-mle-under-different-distributions"
                id="toc-loss-functions-as-mle-under-different-distributions">Loss
                Functions as MLE Under Different Distributions</a></li>
                <li><a href="#mse-from-mle-the-gaussian-derivation"
                id="toc-mse-from-mle-the-gaussian-derivation">MSE from
                MLE: The Gaussian Derivation</a></li>
                <li><a href="#mae-from-mle-the-laplace-derivation"
                id="toc-mae-from-mle-the-laplace-derivation">MAE from
                MLE: The Laplace Derivation</a></li>
                <li><a href="#why-this-matters-in-practice"
                id="toc-why-this-matters-in-practice">Why This Matters
                in Practice</a></li>
                <li><a href="#maximum-a-posteriori-map-estimation"
                id="toc-maximum-a-posteriori-map-estimation">Maximum A
                Posteriori (MAP) Estimation</a></li>
                <li><a href="#the-key-insight-priors-are-regularization"
                id="toc-the-key-insight-priors-are-regularization">The
                Key Insight: Priors ARE Regularization!</a></li>
                <li><a
                href="#worked-example-gaussian-prior-l2-regularization"
                id="toc-worked-example-gaussian-prior-l2-regularization">Worked
                Example: Gaussian Prior ‚Üí L2 Regularization</a></li>
                <li><a
                href="#worked-example-laplace-prior-l1-regularization"
                id="toc-worked-example-laplace-prior-l1-regularization">Worked
                Example: Laplace Prior ‚Üí L1 Regularization</a></li>
                <li><a
                href="#why-different-priors-lead-to-different-sparsity"
                id="toc-why-different-priors-lead-to-different-sparsity">Why
                Different Priors Lead to Different Sparsity</a></li>
                <li><a href="#mle-vs-map-comparison"
                id="toc-mle-vs-map-comparison">MLE vs MAP
                Comparison</a></li>
                <li><a href="#when-to-use-mle-vs-map"
                id="toc-when-to-use-mle-vs-map">When to Use MLE vs
                MAP</a></li>
                <li><a href="#concrete-example-coin-flip-with-prior"
                id="toc-concrete-example-coin-flip-with-prior">Concrete
                Example: Coin Flip with Prior</a></li>
                <li><a href="#markov-chains"
                id="toc-markov-chains">Markov Chains</a></li>
                <li><a href="#worked-example-weather-markov-chain"
                id="toc-worked-example-weather-markov-chain">Worked
                Example: Weather Markov Chain</a></li>
                <li><a href="#mcmc-markov-chain-monte-carlo"
                id="toc-mcmc-markov-chain-monte-carlo">MCMC: Markov
                Chain Monte Carlo</a></li>
                <li><a href="#metropolis-hastings-algorithm"
                id="toc-metropolis-hastings-algorithm">Metropolis-Hastings
                Algorithm</a></li>
                <li><a
                href="#simple-mcmc-example-sampling-from-a-mixture-of-gaussians"
                id="toc-simple-mcmc-example-sampling-from-a-mixture-of-gaussians">Simple
                MCMC Example: Sampling from a Mixture of
                Gaussians</a></li>
                <li><a href="#key-statistical-concepts"
                id="toc-key-statistical-concepts">Key Statistical
                Concepts</a></li>
                <li><a href="#the-central-limit-theorem-clt"
                id="toc-the-central-limit-theorem-clt">The Central Limit
                Theorem (CLT)</a></li>
                <li><a href="#understanding-the-notation"
                id="toc-understanding-the-notation">Understanding the
                Notation</a></li>
                <li><a
                href="#what-the-formula-actually-says-step-by-step"
                id="toc-what-the-formula-actually-says-step-by-step">What
                the Formula Actually Says (Step by Step)</a></li>
                <li><a
                href="#why-does-averaging-produce-normality-intuition"
                id="toc-why-does-averaging-produce-normality-intuition">Why
                Does Averaging Produce Normality? (Intuition)</a></li>
                <li><a href="#worked-example-dice-rolling"
                id="toc-worked-example-dice-rolling">Worked Example:
                Dice Rolling</a></li>
                <li><a href="#clt-with-different-starting-distributions"
                id="toc-clt-with-different-starting-distributions">CLT
                with Different Starting Distributions</a></li>
                <li><a href="#the-standard-error-sigma-sqrtn"
                id="toc-the-standard-error-sigma-sqrtn">The Standard
                Error: <span class="math inline">\(\sigma /
                \sqrt{n}\)</span></a></li>
                <li><a href="#why-clt-matters-for-machine-learning"
                id="toc-why-clt-matters-for-machine-learning">Why CLT
                Matters for Machine Learning</a></li>
                <li><a href="#interview-questions"
                id="toc-interview-questions">Interview
                Questions</a></li>
                <li><a
                href="#hypothesis-testing-and-statistical-inference"
                id="toc-hypothesis-testing-and-statistical-inference">Hypothesis
                Testing and Statistical Inference</a></li>
                <li><a href="#latent-variables"
                id="toc-latent-variables">Latent Variables</a></li>
                </ul></li>
                <li><a href="#calculus-for-machine-learning"
                id="toc-calculus-for-machine-learning">3.3 Calculus for
                Machine Learning</a>
                <ul>
                <li><a href="#partial-derivatives-and-gradients"
                id="toc-partial-derivatives-and-gradients">Partial
                Derivatives and Gradients</a></li>
                <li><a href="#chain-rule-for-neural-networks"
                id="toc-chain-rule-for-neural-networks">Chain Rule for
                Neural Networks</a></li>
                <li><a
                href="#backpropagation-worked-example-2-layer-network"
                id="toc-backpropagation-worked-example-2-layer-network">Backpropagation
                Worked Example: 2-Layer Network</a></li>
                <li><a href="#computational-graph-view"
                id="toc-computational-graph-view">Computational Graph
                View</a></li>
                <li><a href="#jacobian-matrix"
                id="toc-jacobian-matrix">Jacobian Matrix</a></li>
                <li><a href="#hessian-matrix"
                id="toc-hessian-matrix">Hessian Matrix</a></li>
                <li><a
                href="#hessian-eigenvalues-and-critical-point-stability-interview-deep-dive"
                id="toc-hessian-eigenvalues-and-critical-point-stability-interview-deep-dive">Hessian
                Eigenvalues and Critical Point Stability (Interview Deep
                Dive)</a></li>
                <li><a href="#important-derivatives-for-ml"
                id="toc-important-derivatives-for-ml">Important
                Derivatives for ML</a></li>
                </ul></li>
                <li><a href="#information-theory"
                id="toc-information-theory">3.4 Information Theory</a>
                <ul>
                <li><a href="#why-information-theory-matters-for-ml"
                id="toc-why-information-theory-matters-for-ml">Why
                Information Theory Matters for ML</a></li>
                <li><a href="#entropy-measuring-uncertainty"
                id="toc-entropy-measuring-uncertainty">Entropy:
                Measuring Uncertainty</a></li>
                <li><a href="#example-coin-flips"
                id="toc-example-coin-flips">Example: Coin Flips</a></li>
                <li><a href="#cross-entropy-comparing-distributions"
                id="toc-cross-entropy-comparing-distributions">Cross-Entropy:
                Comparing Distributions</a></li>
                <li><a href="#why-cross-entropy-is-our-loss-function"
                id="toc-why-cross-entropy-is-our-loss-function">Why
                Cross-Entropy is Our Loss Function</a></li>
                <li><a
                href="#kl-divergence-distance-between-distributions"
                id="toc-kl-divergence-distance-between-distributions">KL
                Divergence: Distance Between Distributions</a></li>
                <li><a href="#forward-vs-reverse-kl"
                id="toc-forward-vs-reverse-kl">Forward vs Reverse
                KL</a></li>
                <li><a href="#kl-in-rlhf-and-dpo"
                id="toc-kl-in-rlhf-and-dpo">KL in RLHF and DPO</a></li>
                <li><a href="#mutual-information"
                id="toc-mutual-information">Mutual Information</a></li>
                <li><a
                href="#perplexity-the-language-models-confusion-metric"
                id="toc-perplexity-the-language-models-confusion-metric">Perplexity:
                The Language Model‚Äôs Confusion Metric</a></li>
                <li><a href="#the-nll-perplexity-connection"
                id="toc-the-nll-perplexity-connection">The NLL ‚ÜîÔ∏é
                Perplexity Connection</a></li>
                <li><a
                href="#intuition-perplexity-as-effective-vocabulary-size"
                id="toc-intuition-perplexity-as-effective-vocabulary-size">Intuition:
                Perplexity as ‚ÄúEffective Vocabulary Size‚Äù</a></li>
                <li><a href="#worked-example-computing-perplexity"
                id="toc-worked-example-computing-perplexity">Worked
                Example: Computing Perplexity</a></li>
                <li><a href="#why-perplexity-over-raw-accuracy"
                id="toc-why-perplexity-over-raw-accuracy">Why Perplexity
                Over Raw Accuracy?</a></li>
                <li><a href="#typical-perplexity-values"
                id="toc-typical-perplexity-values">Typical Perplexity
                Values</a></li>
                <li><a href="#perplexity-limitations"
                id="toc-perplexity-limitations">Perplexity
                Limitations</a></li>
                <li><a
                href="#per-token-vs-per-character-vs-per-word-perplexity"
                id="toc-per-token-vs-per-character-vs-per-word-perplexity">Per-Token
                vs Per-Character vs Per-Word Perplexity</a></li>
                <li><a
                href="#interview-q-what-is-perplexity-and-how-does-it-relate-to-cross-entropy"
                id="toc-interview-q-what-is-perplexity-and-how-does-it-relate-to-cross-entropy">Interview
                Q: ‚ÄúWhat is perplexity and how does it relate to
                cross-entropy?‚Äù</a></li>
                <li><a href="#summary-table"
                id="toc-summary-table">Summary Table</a></li>
                <li><a
                href="#interview-q-why-use-cross-entropy-loss-instead-of-kl-divergence"
                id="toc-interview-q-why-use-cross-entropy-loss-instead-of-kl-divergence">Interview
                Q: ‚ÄúWhy use cross-entropy loss instead of KL
                divergence?‚Äù</a></li>
                <li><a
                href="#interview-q-whats-the-relationship-between-cross-entropy-and-mle"
                id="toc-interview-q-whats-the-relationship-between-cross-entropy-and-mle">Interview
                Q: ‚ÄúWhat‚Äôs the relationship between cross-entropy and
                MLE?‚Äù</a></li>
                </ul></li>
                <li><a href="#additional-mathematical-foundations"
                id="toc-additional-mathematical-foundations">3.5
                Additional Mathematical Foundations</a>
                <ul>
                <li><a href="#trace-and-frobenius-norm"
                id="toc-trace-and-frobenius-norm">Trace and Frobenius
                Norm</a></li>
                <li><a href="#matrix-calculus-identities"
                id="toc-matrix-calculus-identities">Matrix Calculus
                Identities</a></li>
                <li><a
                href="#constrained-optimization-lagrange-multipliers"
                id="toc-constrained-optimization-lagrange-multipliers">Constrained
                Optimization: Lagrange Multipliers</a></li>
                <li><a
                href="#worked-example-maximum-entropy-distribution"
                id="toc-worked-example-maximum-entropy-distribution">Worked
                Example: Maximum Entropy Distribution</a></li>
                <li><a href="#kkt-conditions-inequality-constraints"
                id="toc-kkt-conditions-inequality-constraints">KKT
                Conditions (Inequality Constraints)</a></li>
                <li><a href="#matrix-decompositions-summary"
                id="toc-matrix-decompositions-summary">Matrix
                Decompositions Summary</a></li>
                <li><a href="#numerical-stability-log-sum-exp-trick"
                id="toc-numerical-stability-log-sum-exp-trick">Numerical
                Stability: Log-Sum-Exp Trick</a></li>
                <li><a href="#positive-semi-definite-psd-matrices"
                id="toc-positive-semi-definite-psd-matrices">Positive
                Semi-Definite (PSD) Matrices</a></li>
                </ul></li>
                <li><a href="#common-interview-gotchas"
                id="toc-common-interview-gotchas">3.6 Common Interview
                Gotchas</a></li>
                <li><a href="#quick-reference-card"
                id="toc-quick-reference-card">3.7 Quick Reference
                Card</a>
                <ul>
                <li><a href="#linear-algebra-essentials"
                id="toc-linear-algebra-essentials">Linear Algebra
                Essentials</a></li>
                <li><a href="#key-derivatives"
                id="toc-key-derivatives">Key Derivatives</a></li>
                <li><a href="#probability-essentials"
                id="toc-probability-essentials">Probability
                Essentials</a></li>
                <li><a href="#critical-relationships"
                id="toc-critical-relationships">Critical
                Relationships</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-4-ml-fundamentals"
                id="toc-part-4-ml-fundamentals">Part 4: ML
                Fundamentals</a>
                <ul>
                <li><a href="#bias-variance-tradeoff"
                id="toc-bias-variance-tradeoff">4.1 Bias-Variance
                Tradeoff</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-1"
                id="toc-what-this-means-for-beginners-1">What This Means
                (For Beginners)</a></li>
                <li><a href="#the-decomposition"
                id="toc-the-decomposition">The Decomposition</a></li>
                <li><a href="#intuition"
                id="toc-intuition">Intuition</a></li>
                <li><a href="#complexity-tradeoff"
                id="toc-complexity-tradeoff">Complexity
                Tradeoff</a></li>
                <li><a href="#mathematical-example"
                id="toc-mathematical-example">Mathematical
                Example</a></li>
                <li><a href="#controlling-the-tradeoff"
                id="toc-controlling-the-tradeoff">Controlling the
                Tradeoff</a></li>
                <li><a href="#interview-follow-ups"
                id="toc-interview-follow-ups">Interview
                Follow-ups</a></li>
                </ul></li>
                <li><a href="#overfitting-and-underfitting"
                id="toc-overfitting-and-underfitting">4.2 Overfitting
                and Underfitting</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-2"
                id="toc-what-this-means-for-beginners-2">What This Means
                (For Beginners)</a></li>
                <li><a href="#what-is-overfitting"
                id="toc-what-is-overfitting">What is
                Overfitting?</a></li>
                <li><a href="#what-is-underfitting"
                id="toc-what-is-underfitting">What is
                Underfitting?</a></li>
                <li><a href="#detecting-overfitting"
                id="toc-detecting-overfitting">Detecting
                Overfitting</a></li>
                <li><a href="#prevention-techniques"
                id="toc-prevention-techniques">Prevention
                Techniques</a></li>
                <li><a href="#early-stopping"
                id="toc-early-stopping">Early Stopping</a></li>
                <li><a href="#the-curse-of-dimensionality"
                id="toc-the-curse-of-dimensionality">The Curse of
                Dimensionality</a></li>
                <li><a
                href="#the-manifold-hypothesis-why-deep-learning-works"
                id="toc-the-manifold-hypothesis-why-deep-learning-works">The
                Manifold Hypothesis: Why Deep Learning Works</a></li>
                <li><a href="#the-no-free-lunch-theorem"
                id="toc-the-no-free-lunch-theorem">The No Free Lunch
                Theorem</a></li>
                <li><a href="#interview-follow-ups-1"
                id="toc-interview-follow-ups-1">Interview
                Follow-ups</a></li>
                </ul></li>
                <li><a href="#model-selection-and-evaluation"
                id="toc-model-selection-and-evaluation">4.3 Model
                Selection and Evaluation</a>
                <ul>
                <li><a href="#trainvalidationtest-split"
                id="toc-trainvalidationtest-split">Train/Validation/Test
                Split</a></li>
                <li><a href="#k-fold-cross-validation"
                id="toc-k-fold-cross-validation">K-Fold
                Cross-Validation</a></li>
                <li><a href="#evaluation-metrics"
                id="toc-evaluation-metrics">Evaluation Metrics</a></li>
                <li><a href="#hyperparameter-tuning"
                id="toc-hyperparameter-tuning">Hyperparameter
                Tuning</a></li>
                </ul></li>
                <li><a href="#generative-vs-discriminative-models"
                id="toc-generative-vs-discriminative-models">4.4
                Generative vs Discriminative Models</a>
                <ul>
                <li><a href="#the-key-distinction"
                id="toc-the-key-distinction">The Key
                Distinction</a></li>
                <li><a href="#discriminative-models"
                id="toc-discriminative-models">Discriminative
                Models</a></li>
                <li><a href="#generative-models"
                id="toc-generative-models">Generative Models</a></li>
                <li><a
                href="#how-generative-models-enable-classification"
                id="toc-how-generative-models-enable-classification">How
                Generative Models Enable Classification</a></li>
                <li><a href="#modern-generative-models"
                id="toc-modern-generative-models">Modern Generative
                Models</a></li>
                <li><a href="#when-to-use-which-3"
                id="toc-when-to-use-which-3">When to Use Which?</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-generative-and-discriminative-models"
                id="toc-interview-q-whats-the-difference-between-generative-and-discriminative-models">Interview
                Q: ‚ÄúWhat‚Äôs the difference between generative and
                discriminative models?‚Äù</a></li>
                <li><a
                href="#interview-q-why-are-llms-considered-generative-models"
                id="toc-interview-q-why-are-llms-considered-generative-models">Interview
                Q: ‚ÄúWhy are LLMs considered generative models?‚Äù</a></li>
                </ul></li>
                <li><a href="#curriculum-learning"
                id="toc-curriculum-learning">4.5 Curriculum Learning</a>
                <ul>
                <li><a href="#what-is-curriculum-learning"
                id="toc-what-is-curriculum-learning">What is Curriculum
                Learning?</a></li>
                <li><a href="#motivation-how-humans-learn"
                id="toc-motivation-how-humans-learn">Motivation: How
                Humans Learn</a></li>
                <li><a href="#how-it-works-1"
                id="toc-how-it-works-1">How It Works</a></li>
                <li><a href="#defining-difficulty"
                id="toc-defining-difficulty">Defining
                Difficulty</a></li>
                <li><a href="#self-paced-learning"
                id="toc-self-paced-learning">Self-Paced
                Learning</a></li>
                <li><a href="#why-does-it-work-1"
                id="toc-why-does-it-work-1">Why Does It Work?</a></li>
                <li><a href="#curriculum-learning-in-sequence-models"
                id="toc-curriculum-learning-in-sequence-models">Curriculum
                Learning in Sequence Models</a></li>
                <li><a href="#examples-in-practice"
                id="toc-examples-in-practice">Examples in
                Practice</a></li>
                <li><a href="#anti-curriculum-hard-first"
                id="toc-anti-curriculum-hard-first">Anti-Curriculum
                (Hard First)</a></li>
                <li><a
                href="#interview-q-what-is-curriculum-learning-and-why-does-it-help"
                id="toc-interview-q-what-is-curriculum-learning-and-why-does-it-help">Interview
                Q: ‚ÄúWhat is curriculum learning and why does it
                help?‚Äù</a></li>
                <li><a
                href="#interview-q-how-would-you-implement-curriculum-learning-for-a-language-model"
                id="toc-interview-q-how-would-you-implement-curriculum-learning-for-a-language-model">Interview
                Q: ‚ÄúHow would you implement curriculum learning for a
                language model?‚Äù</a></li>
                </ul></li>
                <li><a
                href="#transfer-learning-domain-adaptation-and-few-shot-learning"
                id="toc-transfer-learning-domain-adaptation-and-few-shot-learning">4.6
                Transfer Learning, Domain Adaptation, and Few-Shot
                Learning</a>
                <ul>
                <li><a href="#transfer-learning"
                id="toc-transfer-learning">Transfer Learning</a></li>
                <li><a href="#domain-adaptation"
                id="toc-domain-adaptation">Domain Adaptation</a></li>
                <li><a href="#one-shot-learning"
                id="toc-one-shot-learning">One-Shot Learning</a></li>
                <li><a href="#zero-shot-learning"
                id="toc-zero-shot-learning">Zero-Shot Learning</a></li>
                <li><a href="#comparison-table-1"
                id="toc-comparison-table-1">Comparison Table</a></li>
                <li><a href="#in-context-learning-gpt-3-style-few-shot"
                id="toc-in-context-learning-gpt-3-style-few-shot">In-Context
                Learning (GPT-3 Style Few-Shot)</a></li>
                <li><a
                href="#interview-q-whats-the-relationship-between-zero-shot-learning-and-transfer-learning"
                id="toc-interview-q-whats-the-relationship-between-zero-shot-learning-and-transfer-learning">Interview
                Q: ‚ÄúWhat‚Äôs the relationship between zero-shot learning
                and transfer learning?‚Äù</a></li>
                <li><a
                href="#interview-q-how-does-gpt-3s-in-context-learning-differ-from-traditional-few-shot-learning"
                id="toc-interview-q-how-does-gpt-3s-in-context-learning-differ-from-traditional-few-shot-learning">Interview
                Q: ‚ÄúHow does GPT-3‚Äôs in-context learning differ from
                traditional few-shot learning?‚Äù</a></li>
                </ul></li>
                <li><a href="#soft-labels-vs-hard-labels"
                id="toc-soft-labels-vs-hard-labels">4.7 Soft Labels vs
                Hard Labels</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-4"
                id="toc-what-this-means-for-beginners-4">What This Means
                (For Beginners)</a></li>
                <li><a href="#what-are-hard-labels"
                id="toc-what-are-hard-labels">What Are Hard
                Labels?</a></li>
                <li><a href="#what-are-soft-labels"
                id="toc-what-are-soft-labels">What Are Soft
                Labels?</a></li>
                <li><a href="#label-smoothing"
                id="toc-label-smoothing">Label Smoothing</a></li>
                <li><a href="#why-use-soft-labels"
                id="toc-why-use-soft-labels">Why Use Soft
                Labels?</a></li>
                <li><a href="#the-overconfidence-problem"
                id="toc-the-overconfidence-problem">The Overconfidence
                Problem</a></li>
                <li><a href="#loss-function-for-soft-labels"
                id="toc-loss-function-for-soft-labels">Loss Function for
                Soft Labels</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-hard-and-soft-labels"
                id="toc-interview-q-whats-the-difference-between-hard-and-soft-labels">Interview
                Q: ‚ÄúWhat‚Äôs the difference between hard and soft
                labels?‚Äù</a></li>
                </ul></li>
                <li><a href="#knowledge-distillation"
                id="toc-knowledge-distillation">4.8 Knowledge
                Distillation</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-5"
                id="toc-what-this-means-for-beginners-5">What This Means
                (For Beginners)</a></li>
                <li><a href="#what-is-knowledge-distillation"
                id="toc-what-is-knowledge-distillation">What is
                Knowledge Distillation?</a></li>
                <li><a href="#why-soft-predictions-dark-knowledge"
                id="toc-why-soft-predictions-dark-knowledge">Why Soft
                Predictions (‚ÄúDark Knowledge‚Äù)?</a></li>
                <li><a href="#temperature-in-distillation"
                id="toc-temperature-in-distillation">Temperature in
                Distillation</a></li>
                <li><a href="#distillation-loss"
                id="toc-distillation-loss">Distillation Loss</a></li>
                <li><a href="#knowledge-distillation-implementation"
                id="toc-knowledge-distillation-implementation">Knowledge
                Distillation Implementation</a></li>
                <li><a href="#types-of-distillation"
                id="toc-types-of-distillation">Types of
                Distillation</a></li>
                <li><a href="#examples-in-practice-1"
                id="toc-examples-in-practice-1">Examples in
                Practice</a></li>
                <li><a href="#why-does-distillation-work"
                id="toc-why-does-distillation-work">Why Does
                Distillation Work?</a></li>
                <li><a
                href="#interview-q-how-does-knowledge-distillation-work-and-why-is-it-effective"
                id="toc-interview-q-how-does-knowledge-distillation-work-and-why-is-it-effective">Interview
                Q: ‚ÄúHow does knowledge distillation work and why is it
                effective?‚Äù</a></li>
                </ul></li>
                <li><a href="#contrastive-learning"
                id="toc-contrastive-learning">4.9 Contrastive
                Learning</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-6"
                id="toc-what-this-means-for-beginners-6">What This Means
                (For Beginners)</a></li>
                <li><a href="#what-is-contrastive-learning"
                id="toc-what-is-contrastive-learning">What is
                Contrastive Learning?</a></li>
                <li><a href="#where-do-pairs-come-from"
                id="toc-where-do-pairs-come-from">Where Do Pairs Come
                From?</a></li>
                <li><a href="#the-infonce-loss-nt-xent"
                id="toc-the-infonce-loss-nt-xent">The InfoNCE Loss
                (NT-Xent)</a></li>
                <li><a href="#simclr-a-simple-framework"
                id="toc-simclr-a-simple-framework">SimCLR: A Simple
                Framework</a></li>
                <li><a href="#simclr-implementation"
                id="toc-simclr-implementation">SimCLR
                Implementation</a></li>
                <li><a href="#why-does-contrastive-learning-work"
                id="toc-why-does-contrastive-learning-work">Why Does
                Contrastive Learning Work?</a></li>
                <li><a
                href="#clip-contrastive-language-image-pretraining"
                id="toc-clip-contrastive-language-image-pretraining">CLIP:
                Contrastive Language-Image Pretraining</a></li>
                <li><a href="#comparison-contrastive-vs-generative"
                id="toc-comparison-contrastive-vs-generative">Comparison:
                Contrastive vs Generative</a></li>
                <li><a
                href="#interview-q-what-is-contrastive-learning-and-why-is-it-effective"
                id="toc-interview-q-what-is-contrastive-learning-and-why-is-it-effective">Interview
                Q: ‚ÄúWhat is contrastive learning and why is it
                effective?‚Äù</a></li>
                </ul></li>
                <li><a href="#decision-trees-ensemble-methods"
                id="toc-decision-trees-ensemble-methods">4.10 Decision
                Trees &amp; Ensemble Methods</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-7"
                id="toc-what-this-means-for-beginners-7">What This Means
                (For Beginners)</a></li>
                <li><a href="#decision-trees"
                id="toc-decision-trees">4.10.1 Decision Trees</a></li>
                <li><a href="#ensemble-methods-bagging-vs-boosting"
                id="toc-ensemble-methods-bagging-vs-boosting">4.10.2
                Ensemble Methods: Bagging vs Boosting</a></li>
                <li><a href="#random-forest"
                id="toc-random-forest">4.10.3 Random Forest</a></li>
                <li><a href="#gradient-boosting-xgboostlightgbm"
                id="toc-gradient-boosting-xgboostlightgbm">4.10.4
                Gradient Boosting (XGBoost/LightGBM)</a></li>
                <li><a href="#trees-vs-neural-networks-when-to-use-what"
                id="toc-trees-vs-neural-networks-when-to-use-what">4.10.5
                Trees vs Neural Networks: When to Use What</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-5-optimization"
                id="toc-part-5-optimization">Part 5: Optimization</a>
                <ul>
                <li><a href="#the-core-update-rule-1"
                id="toc-the-core-update-rule-1">5.1 The Core Update
                Rule</a></li>
                <li><a href="#stochastic-gradient-descent-sgd-1"
                id="toc-stochastic-gradient-descent-sgd-1">5.2
                Stochastic Gradient Descent (SGD)</a>
                <ul>
                <li><a href="#full-batch-vs-mini-batch"
                id="toc-full-batch-vs-mini-batch">Full-Batch vs
                Mini-Batch</a></li>
                <li><a href="#variance-batch-size-tradeoff"
                id="toc-variance-batch-size-tradeoff">Variance-Batch
                Size Tradeoff</a></li>
                <li><a href="#why-noise-can-help"
                id="toc-why-noise-can-help">Why Noise Can Help</a></li>
                </ul></li>
                <li><a href="#momentum-methods-1"
                id="toc-momentum-methods-1">5.3 Momentum Methods</a>
                <ul>
                <li><a href="#the-problem-oscillations"
                id="toc-the-problem-oscillations">The Problem:
                Oscillations</a></li>
                <li><a href="#classical-momentum-1"
                id="toc-classical-momentum-1">Classical
                Momentum</a></li>
                <li><a href="#nesterov-accelerated-gradient-nag-1"
                id="toc-nesterov-accelerated-gradient-nag-1">Nesterov
                Accelerated Gradient (NAG)</a></li>
                </ul></li>
                <li><a href="#adaptive-learning-rate-methods-1"
                id="toc-adaptive-learning-rate-methods-1">5.4 Adaptive
                Learning Rate Methods</a>
                <ul>
                <li><a href="#adam-2015" id="toc-adam-2015">Adam
                (2015)</a></li>
                <li><a href="#adamw-the-interview-question"
                id="toc-adamw-the-interview-question">AdamW: The
                Interview Question!</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-adam-and-adamw"
                id="toc-interview-q-whats-the-difference-between-adam-and-adamw">Interview
                Q: ‚ÄúWhat‚Äôs the difference between Adam and
                AdamW?‚Äù</a></li>
                </ul></li>
                <li><a href="#learning-rate-schedules-1"
                id="toc-learning-rate-schedules-1">5.5 Learning Rate
                Schedules</a>
                <ul>
                <li><a href="#warmup-critical-for-transformers"
                id="toc-warmup-critical-for-transformers">Warmup:
                Critical for Transformers</a></li>
                <li><a href="#cosine-annealing-1"
                id="toc-cosine-annealing-1">Cosine Annealing</a></li>
                <li><a href="#comparison-1"
                id="toc-comparison-1">Comparison</a></li>
                </ul></li>
                <li><a href="#regularization-l1-and-l2-1"
                id="toc-regularization-l1-and-l2-1">5.6 Regularization:
                L1 and L2</a>
                <ul>
                <li><a href="#l2-regularization-ridge-weight-decay-1"
                id="toc-l2-regularization-ridge-weight-decay-1">L2
                Regularization (Ridge / Weight Decay)</a></li>
                <li><a href="#l1-regularization-lasso-2"
                id="toc-l1-regularization-lasso-2">L1 Regularization
                (Lasso)</a></li>
                <li><a href="#comparison-2"
                id="toc-comparison-2">Comparison</a></li>
                </ul></li>
                <li><a href="#gradient-instability"
                id="toc-gradient-instability">5.7 Gradient
                Instability</a>
                <ul>
                <li><a href="#vanishing-gradients-2"
                id="toc-vanishing-gradients-2">Vanishing
                Gradients</a></li>
                <li><a
                href="#vanishing-gradients-the-mathematical-story-interview-deep-dive"
                id="toc-vanishing-gradients-the-mathematical-story-interview-deep-dive">Vanishing
                Gradients: The Mathematical Story (Interview Deep
                Dive)</a></li>
                <li><a href="#exploding-gradients-2"
                id="toc-exploding-gradients-2">Exploding
                Gradients</a></li>
                <li><a
                href="#exploding-gradients-the-mathematical-story-interview-deep-dive"
                id="toc-exploding-gradients-the-mathematical-story-interview-deep-dive">Exploding
                Gradients: The Mathematical Story (Interview Deep
                Dive)</a></li>
                <li><a href="#gradient-clipping-2"
                id="toc-gradient-clipping-2">Gradient Clipping</a></li>
                <li><a href="#residual-connections"
                id="toc-residual-connections">Residual
                Connections</a></li>
                </ul></li>
                <li><a href="#batch-normalization-1"
                id="toc-batch-normalization-1">5.8 Batch
                Normalization</a>
                <ul>
                <li><a href="#the-problem-5" id="toc-the-problem-5">The
                Problem</a></li>
                <li><a href="#the-solution-normalize-each-layer-1"
                id="toc-the-solution-normalize-each-layer-1">The
                Solution: Normalize Each Layer</a></li>
                <li><a href="#where-to-apply"
                id="toc-where-to-apply">Where to Apply?</a></li>
                <li><a href="#training-vs-inference-2"
                id="toc-training-vs-inference-2">Training vs
                Inference</a></li>
                <li><a href="#benefits-1"
                id="toc-benefits-1">Benefits</a></li>
                <li><a href="#batch-norm-vs-layer-norm"
                id="toc-batch-norm-vs-layer-norm">Batch Norm vs Layer
                Norm</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-batchnorm-and-layernorm"
                id="toc-interview-q-whats-the-difference-between-batchnorm-and-layernorm">Interview
                Q: ‚ÄúWhat‚Äôs the difference between BatchNorm and
                LayerNorm?‚Äù</a></li>
                <li><a
                href="#interview-q-what-does-batch-normalization-do-and-why-does-it-help-training"
                id="toc-interview-q-what-does-batch-normalization-do-and-why-does-it-help-training">Interview
                Q: ‚ÄúWhat does batch normalization do and why does it
                help training?‚Äù</a></li>
                </ul></li>
                <li><a href="#newtons-method-second-order-optimization"
                id="toc-newtons-method-second-order-optimization">5.9
                Newton‚Äôs Method (Second-Order Optimization)</a>
                <ul>
                <li><a href="#the-idea-1" id="toc-the-idea-1">The
                Idea</a></li>
                <li><a href="#why-its-faster-in-theory"
                id="toc-why-its-faster-in-theory">Why It‚Äôs Faster (In
                Theory)</a></li>
                <li><a href="#why-we-dont-use-it-for-deep-learning"
                id="toc-why-we-dont-use-it-for-deep-learning">Why We
                Don‚Äôt Use It for Deep Learning</a></li>
                <li><a href="#approximations-used-in-practice"
                id="toc-approximations-used-in-practice">Approximations
                Used in Practice</a></li>
                <li><a
                href="#interview-q-why-dont-we-use-newtons-method-for-neural-networks"
                id="toc-interview-q-why-dont-we-use-newtons-method-for-neural-networks">Interview
                Q: ‚ÄúWhy don‚Äôt we use Newton‚Äôs method for neural
                networks?‚Äù</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-6-sequence-models"
                id="toc-part-6-sequence-models">Part 6: Sequence
                Models</a>
                <ul>
                <li><a href="#recurrent-neural-networks-rnns"
                id="toc-recurrent-neural-networks-rnns">6.1 Recurrent
                Neural Networks (RNNs)</a>
                <ul>
                <li><a href="#the-problem-variable-length-sequences"
                id="toc-the-problem-variable-length-sequences">The
                Problem: Variable-Length Sequences</a></li>
                <li><a href="#rnn-architecture"
                id="toc-rnn-architecture">RNN Architecture</a></li>
                <li><a href="#forward-pass"
                id="toc-forward-pass">Forward Pass</a></li>
                <li><a href="#backpropagation-through-time-bptt"
                id="toc-backpropagation-through-time-bptt">Backpropagation
                Through Time (BPTT)</a></li>
                <li><a href="#the-vanishing-gradient-problem-1"
                id="toc-the-vanishing-gradient-problem-1">The Vanishing
                Gradient Problem</a></li>
                <li><a href="#teacher-forcing"
                id="toc-teacher-forcing">Teacher Forcing</a></li>
                </ul></li>
                <li><a href="#lstm-and-gru" id="toc-lstm-and-gru">6.2
                LSTM and GRU</a>
                <ul>
                <li><a href="#lstm-long-short-term-memory"
                id="toc-lstm-long-short-term-memory">LSTM (Long
                Short-Term Memory)</a></li>
                <li><a href="#lstm-equations"
                id="toc-lstm-equations">LSTM Equations</a></li>
                <li><a href="#why-lstm-solves-vanishing-gradients"
                id="toc-why-lstm-solves-vanishing-gradients">Why LSTM
                Solves Vanishing Gradients</a></li>
                <li><a href="#gru-gated-recurrent-unit"
                id="toc-gru-gated-recurrent-unit">GRU (Gated Recurrent
                Unit)</a></li>
                <li><a href="#lstm-vs-gru" id="toc-lstm-vs-gru">LSTM vs
                GRU</a></li>
                <li><a href="#bidirectional-rnns"
                id="toc-bidirectional-rnns">Bidirectional RNNs</a></li>
                <li><a href="#interview-qa-lstm-and-gru"
                id="toc-interview-qa-lstm-and-gru">Interview Q&amp;A:
                LSTM and GRU</a></li>
                </ul></li>
                <li><a href="#seq2seq-and-attention-pre-transformer"
                id="toc-seq2seq-and-attention-pre-transformer">6.3
                Seq2Seq and Attention (Pre-Transformer)</a>
                <ul>
                <li><a href="#the-seq2seq-architecture"
                id="toc-the-seq2seq-architecture">The Seq2Seq
                Architecture</a></li>
                <li><a href="#attention-the-core-intuition"
                id="toc-attention-the-core-intuition">Attention: The
                Core Intuition</a></li>
                <li><a href="#attention-looking-back-at-the-encoder"
                id="toc-attention-looking-back-at-the-encoder">Attention:
                Looking Back at the Encoder</a></li>
                <li><a href="#step-by-step-attention-in-translation"
                id="toc-step-by-step-attention-in-translation">Step-by-Step:
                Attention in Translation</a></li>
                <li><a href="#attention-as-soft-alignment"
                id="toc-attention-as-soft-alignment">Attention as Soft
                Alignment</a></li>
                <li><a href="#why-attention-works-summary"
                id="toc-why-attention-works-summary">Why Attention Works
                (Summary)</a></li>
                <li><a href="#from-seq2seq-attention-to-self-attention"
                id="toc-from-seq2seq-attention-to-self-attention">From
                Seq2Seq Attention to Self-Attention</a></li>
                <li><a href="#this-is-the-foundation-for-transformers"
                id="toc-this-is-the-foundation-for-transformers">This is
                the Foundation for Transformers!</a></li>
                </ul></li>
                <li><a href="#transformers" id="toc-transformers">6.4
                Transformers</a>
                <ul>
                <li><a href="#why-transformers"
                id="toc-why-transformers">Why Transformers?</a></li>
                <li><a href="#self-attention-vs-cross-attention"
                id="toc-self-attention-vs-cross-attention">Self-Attention
                vs Cross-Attention</a></li>
                <li><a href="#worked-example-attention-computation"
                id="toc-worked-example-attention-computation">Worked
                Example: Attention Computation</a></li>
                <li><a href="#self-attention-mechanism"
                id="toc-self-attention-mechanism">Self-Attention
                Mechanism</a></li>
                <li><a href="#step-by-step-attention"
                id="toc-step-by-step-attention">Step-by-Step
                Attention</a></li>
                <li><a href="#example-the-cat-sat-on-the-mat"
                id="toc-example-the-cat-sat-on-the-mat">Example: ‚ÄúThe
                cat sat on the mat‚Äù</a></li>
                <li><a href="#why-scale-by-sqrtd_k-deep-dive"
                id="toc-why-scale-by-sqrtd_k-deep-dive">Why Scale by
                <span class="math inline">\(\sqrt{d_k}\)</span>? (Deep
                Dive)</a></li>
                <li><a href="#multi-head-attention"
                id="toc-multi-head-attention">Multi-Head
                Attention</a></li>
                <li><a href="#attention-variants-modern"
                id="toc-attention-variants-modern">Attention Variants
                (Modern)</a></li>
                <li><a href="#positional-encoding"
                id="toc-positional-encoding">Positional
                Encoding</a></li>
                <li><a href="#rotary-position-embedding-rope"
                id="toc-rotary-position-embedding-rope">Rotary Position
                Embedding (RoPE)</a></li>
                <li><a href="#transformer-architecture"
                id="toc-transformer-architecture">Transformer
                Architecture</a></li>
                <li><a
                href="#why-residual-connections-prevent-vanishing-gradients-math"
                id="toc-why-residual-connections-prevent-vanishing-gradients-math">Why
                Residual Connections Prevent Vanishing Gradients
                (Math)</a></li>
                <li><a href="#rmsnorm-root-mean-square-normalization"
                id="toc-rmsnorm-root-mean-square-normalization">RMSNorm
                (Root Mean Square Normalization)</a></li>
                <li><a href="#causal-masking-deep-dive"
                id="toc-causal-masking-deep-dive">Causal Masking (Deep
                Dive)</a></li>
                <li><a href="#decoder-only-vs-encoder-decoder-deep-dive"
                id="toc-decoder-only-vs-encoder-decoder-deep-dive">Decoder-Only
                vs Encoder-Decoder (Deep Dive)</a></li>
                <li><a href="#layernorm-placement"
                id="toc-layernorm-placement">LayerNorm
                Placement</a></li>
                <li><a
                href="#interview-q-walk-me-through-how-attention-works"
                id="toc-interview-q-walk-me-through-how-attention-works">Interview
                Q: ‚ÄúWalk me through how attention works‚Äù</a></li>
                </ul></li>
                <li><a href="#decoding-strategies"
                id="toc-decoding-strategies">6.5 Decoding Strategies</a>
                <ul>
                <li><a href="#greedy-decoding"
                id="toc-greedy-decoding">Greedy Decoding</a></li>
                <li><a href="#beam-search" id="toc-beam-search">Beam
                Search</a></li>
                <li><a href="#other-decoding-methods"
                id="toc-other-decoding-methods">Other Decoding
                Methods</a></li>
                </ul></li>
                <li><a href="#efficient-inference-kv-cache"
                id="toc-efficient-inference-kv-cache">6.6 Efficient
                Inference: KV Cache</a>
                <ul>
                <li><a href="#the-problem-redundant-computation"
                id="toc-the-problem-redundant-computation">The Problem:
                Redundant Computation</a></li>
                <li><a href="#the-key-insight-k-and-v-dont-change"
                id="toc-the-key-insight-k-and-v-dont-change">The Key
                Insight: K and V Don‚Äôt Change</a></li>
                <li><a href="#the-solution-kv-cache"
                id="toc-the-solution-kv-cache">The Solution: KV
                Cache</a></li>
                <li><a href="#visual-comparison"
                id="toc-visual-comparison">Visual Comparison</a></li>
                <li><a href="#why-only-cache-k-and-v-not-q"
                id="toc-why-only-cache-k-and-v-not-q">Why Only Cache K
                and V (Not Q)?</a></li>
                <li><a href="#complexity-improvement"
                id="toc-complexity-improvement">Complexity
                Improvement</a></li>
                <li><a href="#memory-cost" id="toc-memory-cost">Memory
                Cost</a></li>
                <li><a href="#why-gqamqa-matters-for-kv-cache"
                id="toc-why-gqamqa-matters-for-kv-cache">Why GQA/MQA
                Matters for KV Cache</a></li>
                <li><a
                href="#interview-q-how-does-kv-caching-speed-up-transformer-inference"
                id="toc-interview-q-how-does-kv-caching-speed-up-transformer-inference">Interview
                Q: ‚ÄúHow does KV caching speed up transformer
                inference?‚Äù</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-7-llm-training-pipeline"
                id="toc-part-7-llm-training-pipeline">Part 7: LLM
                Training Pipeline</a>
                <ul>
                <li><a href="#overview-the-three-stage-pipeline"
                id="toc-overview-the-three-stage-pipeline">7.1 Overview:
                The Three-Stage Pipeline</a>
                <ul>
                <li><a href="#why-three-stages"
                id="toc-why-three-stages">Why Three Stages?</a></li>
                </ul></li>
                <li><a href="#pretraining" id="toc-pretraining">7.2
                Pretraining</a>
                <ul>
                <li><a href="#objective-causal-language-modeling"
                id="toc-objective-causal-language-modeling">Objective:
                Causal Language Modeling</a></li>
                <li><a
                href="#loss-function-causal-language-modeling-clm"
                id="toc-loss-function-causal-language-modeling-clm">Loss
                Function: Causal Language Modeling (CLM)</a></li>
                <li><a href="#masked-language-modeling-mlm-loss"
                id="toc-masked-language-modeling-mlm-loss">Masked
                Language Modeling (MLM) Loss</a></li>
                <li><a href="#clm-vs-mlm-key-differences"
                id="toc-clm-vs-mlm-key-differences">CLM vs MLM: Key
                Differences</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-mlm-and-clm-loss"
                id="toc-interview-q-whats-the-difference-between-mlm-and-clm-loss">Interview
                Q: ‚ÄúWhat‚Äôs the difference between MLM and CLM
                loss?‚Äù</a></li>
                <li><a href="#evaluating-language-models-perplexity"
                id="toc-evaluating-language-models-perplexity">Evaluating
                Language Models: Perplexity</a></li>
                <li><a href="#training-data"
                id="toc-training-data">Training Data</a></li>
                <li><a href="#chinchilla-scaling-laws"
                id="toc-chinchilla-scaling-laws">Chinchilla Scaling
                Laws</a></li>
                <li><a href="#interview-q-how-do-you-pretrain-an-llm"
                id="toc-interview-q-how-do-you-pretrain-an-llm">Interview
                Q: ‚ÄúHow do you pretrain an LLM?‚Äù</a></li>
                </ul></li>
                <li><a href="#supervised-fine-tuning-sft"
                id="toc-supervised-fine-tuning-sft">7.3 Supervised
                Fine-Tuning (SFT)</a>
                <ul>
                <li><a href="#what-is-sft" id="toc-what-is-sft">What is
                SFT?</a></li>
                <li><a href="#data-format" id="toc-data-format">Data
                Format</a></li>
                <li><a href="#loss-function" id="toc-loss-function">Loss
                Function</a></li>
                <li><a href="#key-considerations"
                id="toc-key-considerations">Key Considerations</a></li>
                <li><a href="#lora-efficient-fine-tuning"
                id="toc-lora-efficient-fine-tuning">LoRA: Efficient
                Fine-Tuning</a></li>
                <li><a href="#interview-q-what-is-lora-and-why-use-it"
                id="toc-interview-q-what-is-lora-and-why-use-it">Interview
                Q: ‚ÄúWhat is LoRA and why use it?‚Äù</a></li>
                <li><a
                href="#why-low-rank-works-the-deep-dive-interview-topic"
                id="toc-why-low-rank-works-the-deep-dive-interview-topic">Why
                Low Rank Works: The Deep Dive (Interview Topic)</a></li>
                </ul></li>
                <li><a href="#rl-foundations-for-llm-alignment"
                id="toc-rl-foundations-for-llm-alignment">7.4 RL
                Foundations for LLM Alignment</a>
                <ul>
                <li><a href="#policy-gradient-the-foundation"
                id="toc-policy-gradient-the-foundation">Policy Gradient:
                The Foundation</a></li>
                <li><a href="#baselines-reducing-variance"
                id="toc-baselines-reducing-variance">Baselines: Reducing
                Variance</a></li>
                <li><a href="#actor-critic-learning-the-baseline"
                id="toc-actor-critic-learning-the-baseline">Actor-Critic:
                Learning the Baseline</a></li>
                <li><a href="#connection-to-llm-alignment"
                id="toc-connection-to-llm-alignment">Connection to LLM
                Alignment</a></li>
                <li><a href="#how-dpo-and-grpo-simplify-this"
                id="toc-how-dpo-and-grpo-simplify-this">How DPO and GRPO
                Simplify This</a></li>
                <li><a
                href="#interview-q-what-is-actor-critic-and-how-does-it-relate-to-rlhf"
                id="toc-interview-q-what-is-actor-critic-and-how-does-it-relate-to-rlhf">Interview
                Q: ‚ÄúWhat is Actor-Critic and how does it relate to
                RLHF?‚Äù</a></li>
                </ul></li>
                <li><a
                href="#rlhf-reinforcement-learning-from-human-feedback"
                id="toc-rlhf-reinforcement-learning-from-human-feedback">7.5
                RLHF: Reinforcement Learning from Human Feedback</a>
                <ul>
                <li><a href="#why-rlhf" id="toc-why-rlhf">Why
                RLHF?</a></li>
                <li><a href="#the-rlhf-pipeline"
                id="toc-the-rlhf-pipeline">The RLHF Pipeline</a></li>
                <li><a href="#step-1-reward-model-training"
                id="toc-step-1-reward-model-training">Step 1: Reward
                Model Training</a></li>
                <li><a href="#step-2-ppo-proximal-policy-optimization"
                id="toc-step-2-ppo-proximal-policy-optimization">Step 2:
                PPO (Proximal Policy Optimization)</a></li>
                <li><a href="#challenges-with-rlhf"
                id="toc-challenges-with-rlhf">Challenges with
                RLHF</a></li>
                <li><a href="#interview-q-explain-the-rlhf-pipeline"
                id="toc-interview-q-explain-the-rlhf-pipeline">Interview
                Q: ‚ÄúExplain the RLHF pipeline‚Äù</a></li>
                </ul></li>
                <li><a href="#dpo-direct-preference-optimization"
                id="toc-dpo-direct-preference-optimization">7.6 DPO:
                Direct Preference Optimization</a>
                <ul>
                <li><a href="#the-key-insight"
                id="toc-the-key-insight">The Key Insight</a></li>
                <li><a href="#the-math" id="toc-the-math">The
                Math</a></li>
                <li><a href="#dpo-loss" id="toc-dpo-loss">DPO
                Loss</a></li>
                <li><a href="#dpo-vs-rlhf" id="toc-dpo-vs-rlhf">DPO vs
                RLHF</a></li>
                <li><a href="#dpo-implementation"
                id="toc-dpo-implementation">DPO Implementation</a></li>
                <li><a href="#interview-q-how-does-dpo-differ-from-rlhf"
                id="toc-interview-q-how-does-dpo-differ-from-rlhf">Interview
                Q: ‚ÄúHow does DPO differ from RLHF?‚Äù</a></li>
                </ul></li>
                <li><a href="#grpo-group-relative-policy-optimization"
                id="toc-grpo-group-relative-policy-optimization">7.7
                GRPO: Group Relative Policy Optimization</a>
                <ul>
                <li><a href="#the-deepseek-innovation"
                id="toc-the-deepseek-innovation">The DeepSeek
                Innovation</a></li>
                <li><a href="#how-grpo-works"
                id="toc-how-grpo-works">How GRPO Works</a></li>
                <li><a href="#grpo-algorithm"
                id="toc-grpo-algorithm">GRPO Algorithm</a></li>
                <li><a href="#grpo-vs-ppo" id="toc-grpo-vs-ppo">GRPO vs
                PPO</a></li>
                <li><a href="#why-it-works-2"
                id="toc-why-it-works-2">Why It Works</a></li>
                <li><a href="#when-to-use-grpo-vs.-ppo-vs.-dpo"
                id="toc-when-to-use-grpo-vs.-ppo-vs.-dpo">When to Use
                GRPO vs.¬†PPO vs.¬†DPO</a></li>
                <li><a href="#interview-q-whats-new-in-grpo"
                id="toc-interview-q-whats-new-in-grpo">Interview Q:
                ‚ÄúWhat‚Äôs new in GRPO?‚Äù</a></li>
                </ul></li>
                <li><a href="#quantization" id="toc-quantization">7.8
                Quantization</a>
                <ul>
                <li><a href="#why-quantize" id="toc-why-quantize">Why
                Quantize?</a></li>
                <li><a href="#number-formats"
                id="toc-number-formats">Number Formats</a></li>
                <li><a href="#quantization-formula"
                id="toc-quantization-formula">Quantization
                Formula</a></li>
                <li><a href="#post-training-quantization-ptq"
                id="toc-post-training-quantization-ptq">Post-Training
                Quantization (PTQ)</a></li>
                <li><a href="#quantization-aware-training-qat"
                id="toc-quantization-aware-training-qat">Quantization-Aware
                Training (QAT)</a></li>
                <li><a href="#popular-quantization-methods"
                id="toc-popular-quantization-methods">Popular
                Quantization Methods</a></li>
                <li><a href="#gptq-how-it-works"
                id="toc-gptq-how-it-works">GPTQ: How It Works</a></li>
                <li><a href="#awq-activation-aware"
                id="toc-awq-activation-aware">AWQ:
                Activation-Aware</a></li>
                <li><a href="#quality-vs-size-tradeoff"
                id="toc-quality-vs-size-tradeoff">Quality vs Size
                Tradeoff</a></li>
                <li><a
                href="#interview-q-whats-the-tradeoff-of-4-bit-quantization"
                id="toc-interview-q-whats-the-tradeoff-of-4-bit-quantization">Interview
                Q: ‚ÄúWhat‚Äôs the tradeoff of 4-bit quantization?‚Äù</a></li>
                </ul></li>
                <li><a href="#sampling-techniques-for-text-generation"
                id="toc-sampling-techniques-for-text-generation">7.9
                Sampling Techniques for Text Generation</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-8"
                id="toc-what-this-means-for-beginners-8">What This Means
                (For Beginners)</a></li>
                <li><a href="#greedy-decoding-1"
                id="toc-greedy-decoding-1">Greedy Decoding</a></li>
                <li><a href="#temperature-scaling"
                id="toc-temperature-scaling">Temperature
                Scaling</a></li>
                <li><a href="#top-k-sampling"
                id="toc-top-k-sampling">Top-k Sampling</a></li>
                <li><a href="#top-p-nucleus-sampling"
                id="toc-top-p-nucleus-sampling">Top-p (Nucleus)
                Sampling</a></li>
                <li><a href="#beam-search-1" id="toc-beam-search-1">Beam
                Search</a></li>
                <li><a href="#comparison-table-3"
                id="toc-comparison-table-3">Comparison Table</a></li>
                <li><a href="#common-combinations"
                id="toc-common-combinations">Common
                Combinations</a></li>
                <li><a
                href="#interview-q-when-would-you-use-beam-search-vs-nucleus-sampling"
                id="toc-interview-q-when-would-you-use-beam-search-vs-nucleus-sampling">Interview
                Q: ‚ÄúWhen would you use beam search vs nucleus
                sampling?‚Äù</a></li>
                <li><a
                href="#practical-guidelines-choosing-your-sampling-strategy"
                id="toc-practical-guidelines-choosing-your-sampling-strategy">Practical
                Guidelines: Choosing Your Sampling Strategy</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-8-distributed-training"
                id="toc-part-8-distributed-training">Part 8: Distributed
                Training</a>
                <ul>
                <li><a
                href="#understanding-the-distributed-training-landscape"
                id="toc-understanding-the-distributed-training-landscape">Understanding
                the Distributed Training Landscape</a></li>
                <li><a href="#distributed-data-parallelism-ddp"
                id="toc-distributed-data-parallelism-ddp">8.1
                Distributed Data Parallelism (DDP)</a>
                <ul>
                <li><a href="#the-core-problem-single-gpu-bottleneck"
                id="toc-the-core-problem-single-gpu-bottleneck">The Core
                Problem: Single GPU Bottleneck</a></li>
                <li><a href="#the-key-insight-batch-splitting"
                id="toc-the-key-insight-batch-splitting">The Key
                Insight: Batch Splitting</a></li>
                <li><a href="#why-its-mathematically-equivalent"
                id="toc-why-its-mathematically-equivalent">Why It‚Äôs
                Mathematically Equivalent</a></li>
                <li><a
                href="#allreduce-the-critical-communication-operation"
                id="toc-allreduce-the-critical-communication-operation">AllReduce:
                The Critical Communication Operation</a></li>
                <li><a href="#overlapping-computation-and-communication"
                id="toc-overlapping-computation-and-communication">Overlapping
                Computation and Communication</a></li>
                <li><a href="#pytorch-ddp-implementation"
                id="toc-pytorch-ddp-implementation">PyTorch DDP
                Implementation</a></li>
                <li><a href="#scaling-efficiency-1"
                id="toc-scaling-efficiency-1">Scaling
                Efficiency</a></li>
                <li><a
                href="#interview-q-walk-me-through-how-ddp-synchronizes-gradients"
                id="toc-interview-q-walk-me-through-how-ddp-synchronizes-gradients">Interview
                Q: ‚ÄúWalk me through how DDP synchronizes
                gradients‚Äù</a></li>
                </ul></li>
                <li><a href="#tensor-parallelism"
                id="toc-tensor-parallelism">8.2 Tensor Parallelism</a>
                <ul>
                <li><a href="#when-data-parallelism-isnt-enough"
                id="toc-when-data-parallelism-isnt-enough">When Data
                Parallelism Isn‚Äôt Enough</a></li>
                <li><a href="#the-core-idea-split-weight-matrices"
                id="toc-the-core-idea-split-weight-matrices">The Core
                Idea: Split Weight Matrices</a></li>
                <li><a href="#column-parallel-vs-row-parallel"
                id="toc-column-parallel-vs-row-parallel">Column Parallel
                vs Row Parallel</a></li>
                <li><a href="#transformer-ffn-with-tensor-parallelism"
                id="toc-transformer-ffn-with-tensor-parallelism">Transformer
                FFN with Tensor Parallelism</a></li>
                <li><a href="#attention-with-tensor-parallelism"
                id="toc-attention-with-tensor-parallelism">Attention
                with Tensor Parallelism</a></li>
                <li><a href="#communication-analysis"
                id="toc-communication-analysis">Communication
                Analysis</a></li>
                <li><a href="#why-tp-is-usually-2-8"
                id="toc-why-tp-is-usually-2-8">Why TP is Usually
                2-8</a></li>
                <li><a
                href="#interview-q-why-is-tensor-parallelism-usually-limited-to-2-8-gpus"
                id="toc-interview-q-why-is-tensor-parallelism-usually-limited-to-2-8-gpus">Interview
                Q: ‚ÄúWhy is tensor parallelism usually limited to 2-8
                GPUs?‚Äù</a></li>
                </ul></li>
                <li><a href="#pipeline-parallelism"
                id="toc-pipeline-parallelism">8.3 Pipeline
                Parallelism</a>
                <ul>
                <li><a href="#the-problem-model-too-large-tp-not-enough"
                id="toc-the-problem-model-too-large-tp-not-enough">The
                Problem: Model Too Large, TP Not Enough</a></li>
                <li><a href="#the-core-idea-layer-partitioning"
                id="toc-the-core-idea-layer-partitioning">The Core Idea:
                Layer Partitioning</a></li>
                <li><a href="#the-bubble-problem"
                id="toc-the-bubble-problem">The Bubble Problem</a></li>
                <li><a href="#micro-batching-the-solution"
                id="toc-micro-batching-the-solution">Micro-batching: The
                Solution</a></li>
                <li><a href="#gpipe-vs-pipedream"
                id="toc-gpipe-vs-pipedream">GPipe vs PipeDream</a></li>
                <li><a href="#communication-in-pipeline-parallelism"
                id="toc-communication-in-pipeline-parallelism">Communication
                in Pipeline Parallelism</a></li>
                <li><a href="#memory-analysis"
                id="toc-memory-analysis">Memory Analysis</a></li>
                <li><a
                href="#interview-q-explain-the-bubble-problem-in-pipeline-parallelism"
                id="toc-interview-q-explain-the-bubble-problem-in-pipeline-parallelism">Interview
                Q: ‚ÄúExplain the bubble problem in pipeline
                parallelism‚Äù</a></li>
                </ul></li>
                <li><a href="#zero-zero-redundancy-optimizer-1"
                id="toc-zero-zero-redundancy-optimizer-1">8.4 ZeRO: Zero
                Redundancy Optimizer</a>
                <ul>
                <li><a href="#the-memory-problem-in-data-parallelism"
                id="toc-the-memory-problem-in-data-parallelism">The
                Memory Problem in Data Parallelism</a></li>
                <li><a
                href="#zero-key-insight-partition-instead-of-replicate"
                id="toc-zero-key-insight-partition-instead-of-replicate">ZeRO
                Key Insight: Partition Instead of Replicate</a></li>
                <li><a href="#zero-stage-1-partition-optimizer-states"
                id="toc-zero-stage-1-partition-optimizer-states">ZeRO
                Stage 1: Partition Optimizer States</a></li>
                <li><a href="#zero-stage-2-partition-gradients"
                id="toc-zero-stage-2-partition-gradients">ZeRO Stage 2:
                + Partition Gradients</a></li>
                <li><a href="#zero-stage-3-partition-parameters"
                id="toc-zero-stage-3-partition-parameters">ZeRO Stage 3:
                + Partition Parameters</a></li>
                <li><a href="#communication-trade-offs"
                id="toc-communication-trade-offs">Communication
                Trade-offs</a></li>
                <li><a href="#fsdp-pytorchs-native-zero"
                id="toc-fsdp-pytorchs-native-zero">FSDP: PyTorch‚Äôs
                Native ZeRO</a></li>
                <li><a href="#zero-offload-use-cpu-memory-too"
                id="toc-zero-offload-use-cpu-memory-too">ZeRO-Offload:
                Use CPU Memory Too</a></li>
                <li><a
                href="#when-to-use-zerofsdp-vs-tensor-parallelism"
                id="toc-when-to-use-zerofsdp-vs-tensor-parallelism">When
                to Use ZeRO/FSDP vs Tensor Parallelism</a></li>
                <li><a
                href="#interview-q-when-would-you-use-zero-3-vs-tensor-parallelism"
                id="toc-interview-q-when-would-you-use-zero-3-vs-tensor-parallelism">Interview
                Q: ‚ÄúWhen would you use ZeRO-3 vs Tensor
                Parallelism?‚Äù</a></li>
                </ul></li>
                <li><a href="#memory-optimization-techniques"
                id="toc-memory-optimization-techniques">8.5 Memory
                Optimization Techniques</a>
                <ul>
                <li><a href="#gradient-accumulation-1"
                id="toc-gradient-accumulation-1">Gradient
                Accumulation</a></li>
                <li><a href="#mixed-precision-training-1"
                id="toc-mixed-precision-training-1">Mixed Precision
                Training</a></li>
                <li><a href="#fp16-vs-bf16-vs-fp32"
                id="toc-fp16-vs-bf16-vs-fp32">FP16 vs BF16 vs
                FP32</a></li>
                <li><a href="#loss-scaling-fp16-only"
                id="toc-loss-scaling-fp16-only">Loss Scaling (FP16
                only)</a></li>
                <li><a href="#which-operations-use-fp32"
                id="toc-which-operations-use-fp32">Which Operations Use
                FP32?</a></li>
                <li><a href="#pytorch-amp-automatic-mixed-precision"
                id="toc-pytorch-amp-automatic-mixed-precision">PyTorch
                AMP (Automatic Mixed Precision)</a></li>
                <li><a href="#memory-savings"
                id="toc-memory-savings">Memory Savings</a></li>
                <li><a
                href="#gradient-checkpointing-activation-checkpointing"
                id="toc-gradient-checkpointing-activation-checkpointing">Gradient
                Checkpointing (Activation Checkpointing)</a></li>
                <li><a href="#how-it-works-2"
                id="toc-how-it-works-2">How It Works</a></li>
                <li><a href="#implementation"
                id="toc-implementation">Implementation</a></li>
                <li><a href="#checkpointing-strategies"
                id="toc-checkpointing-strategies">Checkpointing
                Strategies</a></li>
                <li><a href="#combining-memory-optimizations"
                id="toc-combining-memory-optimizations">Combining Memory
                Optimizations</a></li>
                <li><a
                href="#interview-q-how-does-gradient-checkpointing-trade-compute-for-memory"
                id="toc-interview-q-how-does-gradient-checkpointing-trade-compute-for-memory">Interview
                Q: ‚ÄúHow does gradient checkpointing trade compute for
                memory?‚Äù</a></li>
                <li><a
                href="#interview-q-why-does-mixed-precision-training-need-loss-scaling-for-fp16-but-not-bf16"
                id="toc-interview-q-why-does-mixed-precision-training-need-loss-scaling-for-fp16-but-not-bf16">Interview
                Q: ‚ÄúWhy does mixed precision training need loss scaling
                for FP16 but not BF16?‚Äù</a></li>
                </ul></li>
                <li><a href="#d-parallelism-putting-it-all-together"
                id="toc-d-parallelism-putting-it-all-together">8.6 3D
                Parallelism: Putting It All Together</a>
                <ul>
                <li><a href="#which-dimension-first"
                id="toc-which-dimension-first">Which Dimension
                First?</a></li>
                <li><a href="#hardware-topology-why-it-matters"
                id="toc-hardware-topology-why-it-matters">Hardware
                Topology: Why It Matters</a></li>
                <li><a href="#real-world-configurations"
                id="toc-real-world-configurations">Real-World
                Configurations</a></li>
                </ul></li>
                <li><a href="#summary-parallelism-decision-guide"
                id="toc-summary-parallelism-decision-guide">Summary:
                Parallelism Decision Guide</a>
                <ul>
                <li><a href="#quick-reference-communication-patterns"
                id="toc-quick-reference-communication-patterns">Quick
                Reference: Communication Patterns</a></li>
                <li><a href="#typical-modern-configurations"
                id="toc-typical-modern-configurations">Typical Modern
                Configurations</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-9-reinforcement-learning"
                id="toc-part-9-reinforcement-learning">Part 9:
                Reinforcement Learning</a>
                <ul>
                <li><a href="#markov-decision-processes-mdps"
                id="toc-markov-decision-processes-mdps">9.1 Markov
                Decision Processes (MDPs)</a>
                <ul>
                <li><a href="#what-is-an-mdp"
                id="toc-what-is-an-mdp">What is an MDP?</a></li>
                <li><a href="#the-markov-property"
                id="toc-the-markov-property">The Markov
                Property</a></li>
                <li><a href="#example-grid-world"
                id="toc-example-grid-world">Example: Grid World</a></li>
                <li><a href="#trajectory-and-return"
                id="toc-trajectory-and-return">Trajectory and
                Return</a></li>
                <li><a href="#why-discount-gamma"
                id="toc-why-discount-gamma">Why Discount (<span
                class="math inline">\(\gamma\)</span>)?</a></li>
                <li><a href="#policy" id="toc-policy">Policy</a></li>
                <li><a
                href="#interview-q-whats-the-markov-property-and-why-is-it-important"
                id="toc-interview-q-whats-the-markov-property-and-why-is-it-important">Interview
                Q: ‚ÄúWhat‚Äôs the Markov property and why is it
                important?‚Äù</a></li>
                </ul></li>
                <li><a href="#value-functions-and-bellman-equations"
                id="toc-value-functions-and-bellman-equations">9.2 Value
                Functions and Bellman Equations</a>
                <ul>
                <li><a href="#state-value-function-vpis"
                id="toc-state-value-function-vpis">State Value Function
                <span class="math inline">\(V^\pi(s)\)</span></a></li>
                <li><a href="#action-value-function-qpis-a"
                id="toc-action-value-function-qpis-a">Action Value
                Function <span class="math inline">\(Q^\pi(s,
                a)\)</span></a></li>
                <li><a href="#relationship-between-v-and-q"
                id="toc-relationship-between-v-and-q">Relationship
                Between V and Q</a></li>
                <li><a href="#bellman-expectation-equation"
                id="toc-bellman-expectation-equation">Bellman
                Expectation Equation</a></li>
                <li><a href="#optimal-value-functions"
                id="toc-optimal-value-functions">Optimal Value
                Functions</a></li>
                <li><a href="#bellman-optimality-equation"
                id="toc-bellman-optimality-equation">Bellman Optimality
                Equation</a></li>
                <li><a href="#example-simple-mdp"
                id="toc-example-simple-mdp">Example: Simple MDP</a></li>
                <li><a href="#interview-q-explain-the-bellman-equation"
                id="toc-interview-q-explain-the-bellman-equation">Interview
                Q: ‚ÄúExplain the Bellman equation‚Äù</a></li>
                </ul></li>
                <li><a href="#monte-carlo-methods"
                id="toc-monte-carlo-methods">9.3 Monte Carlo Methods</a>
                <ul>
                <li><a href="#the-idea-2" id="toc-the-idea-2">The
                Idea</a></li>
                <li><a href="#first-visit-mc-prediction"
                id="toc-first-visit-mc-prediction">First-Visit MC
                Prediction</a></li>
                <li><a href="#every-visit-mc"
                id="toc-every-visit-mc">Every-Visit MC</a></li>
                <li><a href="#mc-for-q-values-control"
                id="toc-mc-for-q-values-control">MC for Q-Values
                (Control)</a></li>
                <li><a href="#mc-properties" id="toc-mc-properties">MC
                Properties</a></li>
                <li><a href="#importance-sampling"
                id="toc-importance-sampling">Importance
                Sampling</a></li>
                </ul></li>
                <li><a href="#temporal-difference-learning"
                id="toc-temporal-difference-learning">9.4 Temporal
                Difference Learning</a>
                <ul>
                <li><a href="#the-key-insight-1"
                id="toc-the-key-insight-1">The Key Insight</a></li>
                <li><a href="#td0-update" id="toc-td0-update">TD(0)
                Update</a></li>
                <li><a href="#td-vs-mc" id="toc-td-vs-mc">TD vs
                MC</a></li>
                <li><a href="#the-bias-variance-tradeoff"
                id="toc-the-bias-variance-tradeoff">The Bias-Variance
                Tradeoff</a></li>
                <li><a href="#tdlambda-blending-mc-and-td"
                id="toc-tdlambda-blending-mc-and-td">TD(<span
                class="math inline">\(\lambda\)</span>): Blending MC and
                TD</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-td-and-monte-carlo"
                id="toc-interview-q-whats-the-difference-between-td-and-monte-carlo">Interview
                Q: ‚ÄúWhat‚Äôs the difference between TD and Monte
                Carlo?‚Äù</a></li>
                </ul></li>
                <li><a href="#q-learning" id="toc-q-learning">9.5
                Q-Learning</a>
                <ul>
                <li><a href="#the-algorithm" id="toc-the-algorithm">The
                Algorithm</a></li>
                <li><a href="#q-learning-implementation"
                id="toc-q-learning-implementation">Q-Learning
                Implementation</a></li>
                <li><a href="#sarsa-on-policy-alternative"
                id="toc-sarsa-on-policy-alternative">SARSA: On-Policy
                Alternative</a></li>
                <li><a href="#q-learning-vs-sarsa"
                id="toc-q-learning-vs-sarsa">Q-Learning vs
                SARSA</a></li>
                <li><a href="#deep-q-network-dqn"
                id="toc-deep-q-network-dqn">Deep Q-Network
                (DQN)</a></li>
                <li><a
                href="#interview-q-explain-q-learning-and-why-its-off-policy"
                id="toc-interview-q-explain-q-learning-and-why-its-off-policy">Interview
                Q: ‚ÄúExplain Q-learning and why it‚Äôs off-policy‚Äù</a></li>
                </ul></li>
                <li><a href="#policy-gradient-methods"
                id="toc-policy-gradient-methods">9.6 Policy Gradient
                Methods</a>
                <ul>
                <li><a href="#the-problem-with-value-based-methods"
                id="toc-the-problem-with-value-based-methods">The
                Problem with Value-Based Methods</a></li>
                <li><a href="#policy-gradient-idea"
                id="toc-policy-gradient-idea">Policy Gradient
                Idea</a></li>
                <li><a href="#the-policy-gradient-theorem"
                id="toc-the-policy-gradient-theorem">The Policy Gradient
                Theorem</a></li>
                <li><a href="#reinforce-algorithm"
                id="toc-reinforce-algorithm">REINFORCE
                Algorithm</a></li>
                <li><a href="#variance-reduction-baseline"
                id="toc-variance-reduction-baseline">Variance Reduction:
                Baseline</a></li>
                <li><a href="#why-policy-gradients"
                id="toc-why-policy-gradients">Why Policy
                Gradients?</a></li>
                <li><a href="#drawbacks"
                id="toc-drawbacks">Drawbacks</a></li>
                <li><a
                href="#interview-q-derive-the-policy-gradient-theorem"
                id="toc-interview-q-derive-the-policy-gradient-theorem">Interview
                Q: ‚ÄúDerive the policy gradient theorem‚Äù</a></li>
                </ul></li>
                <li><a href="#actor-critic-methods"
                id="toc-actor-critic-methods">9.7 Actor-Critic
                Methods</a>
                <ul>
                <li><a href="#the-idea-3" id="toc-the-idea-3">The
                Idea</a></li>
                <li><a href="#why-actor-critic"
                id="toc-why-actor-critic">Why Actor-Critic?</a></li>
                <li><a href="#advantage-actor-critic-a2c"
                id="toc-advantage-actor-critic-a2c">Advantage
                Actor-Critic (A2C)</a></li>
                <li><a href="#a3c-asynchronous-advantage-actor-critic"
                id="toc-a3c-asynchronous-advantage-actor-critic">A3C:
                Asynchronous Advantage Actor-Critic</a></li>
                <li><a href="#generalized-advantage-estimation-gae"
                id="toc-generalized-advantage-estimation-gae">Generalized
                Advantage Estimation (GAE)</a></li>
                <li><a
                href="#interview-q-whats-the-advantage-function-and-why-use-it"
                id="toc-interview-q-whats-the-advantage-function-and-why-use-it">Interview
                Q: ‚ÄúWhat‚Äôs the advantage function and why use
                it?‚Äù</a></li>
                </ul></li>
                <li><a href="#proximal-policy-optimization-ppo"
                id="toc-proximal-policy-optimization-ppo">9.8 Proximal
                Policy Optimization (PPO)</a>
                <ul>
                <li><a href="#the-problem-with-vanilla-policy-gradient"
                id="toc-the-problem-with-vanilla-policy-gradient">The
                Problem with Vanilla Policy Gradient</a></li>
                <li><a href="#trust-region-policy-optimization-trpo"
                id="toc-trust-region-policy-optimization-trpo">Trust
                Region Policy Optimization (TRPO)</a></li>
                <li><a href="#ppo-simpler-alternative"
                id="toc-ppo-simpler-alternative">PPO: Simpler
                Alternative</a></li>
                <li><a href="#how-clipping-works"
                id="toc-how-clipping-works">How Clipping Works</a></li>
                <li><a href="#ppo-implementation"
                id="toc-ppo-implementation">PPO Implementation</a></li>
                <li><a href="#ppo-hyperparameters"
                id="toc-ppo-hyperparameters">PPO
                Hyperparameters</a></li>
                <li><a href="#why-ppo-for-rlhf"
                id="toc-why-ppo-for-rlhf">Why PPO for RLHF?</a></li>
                <li><a href="#interview-q-why-does-ppo-use-clipping"
                id="toc-interview-q-why-does-ppo-use-clipping">Interview
                Q: ‚ÄúWhy does PPO use clipping?‚Äù</a></li>
                </ul></li>
                <li><a href="#exploration-vs-exploitation"
                id="toc-exploration-vs-exploitation">9.9 Exploration vs
                Exploitation</a>
                <ul>
                <li><a href="#the-dilemma" id="toc-the-dilemma">The
                Dilemma</a></li>
                <li><a href="#exploration-methods"
                id="toc-exploration-methods">Exploration
                Methods</a></li>
                <li><a href="#exploration-in-deep-rl"
                id="toc-exploration-in-deep-rl">Exploration in Deep
                RL</a></li>
                <li><a href="#the-multi-armed-bandit"
                id="toc-the-multi-armed-bandit">The Multi-Armed
                Bandit</a></li>
                <li><a
                href="#interview-q-whats-the-exploration-exploitation-tradeoff"
                id="toc-interview-q-whats-the-exploration-exploitation-tradeoff">Interview
                Q: ‚ÄúWhat‚Äôs the exploration-exploitation
                tradeoff?‚Äù</a></li>
                </ul></li>
                <li><a href="#connection-to-llm-alignment-1"
                id="toc-connection-to-llm-alignment-1">9.10 Connection
                to LLM Alignment</a>
                <ul>
                <li><a href="#why-rl-matters-for-llms"
                id="toc-why-rl-matters-for-llms">Why RL Matters for
                LLMs</a></li>
                <li><a href="#key-insights-from-rl-for-llm-alignment"
                id="toc-key-insights-from-rl-for-llm-alignment">Key
                Insights from RL for LLM Alignment</a></li>
                <li><a
                href="#interview-q-how-does-rlhf-relate-to-standard-rl"
                id="toc-interview-q-how-does-rlhf-relate-to-standard-rl">Interview
                Q: ‚ÄúHow does RLHF relate to standard RL?‚Äù</a></li>
                </ul></li>
                <li><a href="#model-based-vs-model-free-rl"
                id="toc-model-based-vs-model-free-rl">9.11 Model-Based
                vs Model-Free RL</a>
                <ul>
                <li><a href="#the-fundamental-distinction"
                id="toc-the-fundamental-distinction">The Fundamental
                Distinction</a></li>
                <li><a href="#model-free-rl-what-weve-covered"
                id="toc-model-free-rl-what-weve-covered">Model-Free RL
                (What We‚Äôve Covered)</a></li>
                <li><a href="#model-based-rl"
                id="toc-model-based-rl">Model-Based RL</a></li>
                <li><a href="#the-dyna-architecture"
                id="toc-the-dyna-architecture">The Dyna
                Architecture</a></li>
                <li><a href="#world-models-learning-to-dream"
                id="toc-world-models-learning-to-dream">World Models:
                Learning to Dream</a></li>
                <li><a href="#sample-efficiency-comparison"
                id="toc-sample-efficiency-comparison">Sample Efficiency
                Comparison</a></li>
                <li><a href="#when-to-use-which-4"
                id="toc-when-to-use-which-4">When to Use Which?</a></li>
                <li><a href="#challenges-with-model-based-rl"
                id="toc-challenges-with-model-based-rl">Challenges with
                Model-Based RL</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-model-based-and-model-free-rl"
                id="toc-interview-q-whats-the-difference-between-model-based-and-model-free-rl">Interview
                Q: ‚ÄúWhat‚Äôs the difference between model-based and
                model-free RL?‚Äù</a></li>
                </ul></li>
                <li><a href="#offline-rl-batch-rl"
                id="toc-offline-rl-batch-rl">9.12 Offline RL / Batch
                RL</a>
                <ul>
                <li><a href="#the-problem-learning-without-interaction"
                id="toc-the-problem-learning-without-interaction">The
                Problem: Learning Without Interaction</a></li>
                <li><a href="#why-offline-rl-matters"
                id="toc-why-offline-rl-matters">Why Offline RL
                Matters</a></li>
                <li><a href="#the-distribution-shift-problem"
                id="toc-the-distribution-shift-problem">The Distribution
                Shift Problem</a></li>
                <li><a href="#naive-approaches-fail"
                id="toc-naive-approaches-fail">Naive Approaches
                Fail</a></li>
                <li><a href="#solution-1-conservative-q-learning-cql"
                id="toc-solution-1-conservative-q-learning-cql">Solution
                1: Conservative Q-Learning (CQL)</a></li>
                <li><a href="#solution-2-behavior-cloning-constraints"
                id="toc-solution-2-behavior-cloning-constraints">Solution
                2: Behavior Cloning + Constraints</a></li>
                <li><a href="#solution-3-decision-transformer"
                id="toc-solution-3-decision-transformer">Solution 3:
                Decision Transformer</a></li>
                <li><a href="#connection-to-llms"
                id="toc-connection-to-llms">Connection to LLMs</a></li>
                <li><a href="#comparison-of-offline-rl-methods"
                id="toc-comparison-of-offline-rl-methods">Comparison of
                Offline RL Methods</a></li>
                <li><a
                href="#interview-q-whats-the-main-challenge-in-offline-rl"
                id="toc-interview-q-whats-the-main-challenge-in-offline-rl">Interview
                Q: ‚ÄúWhat‚Äôs the main challenge in offline RL?‚Äù</a></li>
                <li><a
                href="#interview-q-how-does-decision-transformer-relate-to-standard-rl"
                id="toc-interview-q-how-does-decision-transformer-relate-to-standard-rl">Interview
                Q: ‚ÄúHow does Decision Transformer relate to standard
                RL?‚Äù</a></li>
                </ul></li>
                <li><a href="#multi-agent-rl-marl"
                id="toc-multi-agent-rl-marl">9.13 Multi-Agent RL
                (MARL)</a>
                <ul>
                <li><a href="#the-setting" id="toc-the-setting">The
                Setting</a></li>
                <li><a href="#types-of-multi-agent-settings"
                id="toc-types-of-multi-agent-settings">Types of
                Multi-Agent Settings</a></li>
                <li><a href="#the-non-stationarity-problem"
                id="toc-the-non-stationarity-problem">The
                Non-Stationarity Problem</a></li>
                <li><a href="#solution-1-independent-learning"
                id="toc-solution-1-independent-learning">Solution 1:
                Independent Learning</a></li>
                <li><a
                href="#solution-2-centralized-training-decentralized-execution-ctde"
                id="toc-solution-2-centralized-training-decentralized-execution-ctde">Solution
                2: Centralized Training, Decentralized Execution
                (CTDE)</a></li>
                <li><a href="#maddpg-multi-agent-ddpg"
                id="toc-maddpg-multi-agent-ddpg">MADDPG: Multi-Agent
                DDPG</a></li>
                <li><a href="#self-play-learning-by-playing-yourself"
                id="toc-self-play-learning-by-playing-yourself">Self-Play:
                Learning by Playing Yourself</a></li>
                <li><a href="#nash-equilibrium"
                id="toc-nash-equilibrium">Nash Equilibrium</a></li>
                <li><a href="#emergent-communication"
                id="toc-emergent-communication">Emergent
                Communication</a></li>
                <li><a href="#challenges-in-marl"
                id="toc-challenges-in-marl">Challenges in MARL</a></li>
                <li><a href="#marl-in-practice"
                id="toc-marl-in-practice">MARL in Practice</a></li>
                <li><a
                href="#interview-q-what-makes-multi-agent-rl-harder-than-single-agent"
                id="toc-interview-q-what-makes-multi-agent-rl-harder-than-single-agent">Interview
                Q: ‚ÄúWhat makes multi-agent RL harder than
                single-agent?‚Äù</a></li>
                </ul></li>
                <li><a href="#monte-carlo-tree-search-mcts"
                id="toc-monte-carlo-tree-search-mcts">9.14 Monte Carlo
                Tree Search (MCTS)</a>
                <ul>
                <li><a href="#what-is-mcts" id="toc-what-is-mcts">What
                is MCTS?</a></li>
                <li><a href="#the-four-steps-of-mcts"
                id="toc-the-four-steps-of-mcts">The Four Steps of
                MCTS</a></li>
                <li><a href="#step-1-selection-ucb1"
                id="toc-step-1-selection-ucb1">Step 1: Selection
                (UCB1)</a></li>
                <li><a href="#step-2-expansion"
                id="toc-step-2-expansion">Step 2: Expansion</a></li>
                <li><a href="#step-3-simulation-rollout"
                id="toc-step-3-simulation-rollout">Step 3: Simulation
                (Rollout)</a></li>
                <li><a href="#step-4-backpropagation"
                id="toc-step-4-backpropagation">Step 4:
                Backpropagation</a></li>
                <li><a href="#full-mcts-algorithm"
                id="toc-full-mcts-algorithm">Full MCTS
                Algorithm</a></li>
                <li><a href="#mcts-neural-networks-alphagoalphazero"
                id="toc-mcts-neural-networks-alphagoalphazero">MCTS +
                Neural Networks: AlphaGo/AlphaZero</a></li>
                <li><a href="#alphazero-training-loop"
                id="toc-alphazero-training-loop">AlphaZero Training
                Loop</a></li>
                <li><a href="#why-mcts-nn-is-so-powerful"
                id="toc-why-mcts-nn-is-so-powerful">Why MCTS + NN is So
                Powerful</a></li>
                <li><a
                href="#interview-q-how-does-mcts-work-and-why-was-it-crucial-for-alphago"
                id="toc-interview-q-how-does-mcts-work-and-why-was-it-crucial-for-alphago">Interview
                Q: ‚ÄúHow does MCTS work and why was it crucial for
                AlphaGo?‚Äù</a></li>
                </ul></li>
                <li><a href="#distributional-rl"
                id="toc-distributional-rl">9.15 Distributional RL</a>
                <ul>
                <li><a href="#the-idea-model-the-full-distribution"
                id="toc-the-idea-model-the-full-distribution">The Idea:
                Model the Full Distribution</a></li>
                <li><a href="#why-distribution-matters"
                id="toc-why-distribution-matters">Why Distribution
                Matters</a></li>
                <li><a href="#c51-algorithm" id="toc-c51-algorithm">C51
                Algorithm</a></li>
                <li><a href="#qr-dqn-quantile-regression"
                id="toc-qr-dqn-quantile-regression">QR-DQN: Quantile
                Regression</a></li>
                <li><a href="#iqn-implicit-quantile-networks"
                id="toc-iqn-implicit-quantile-networks">IQN: Implicit
                Quantile Networks</a></li>
                <li><a href="#rainbow-dqn" id="toc-rainbow-dqn">Rainbow
                DQN</a></li>
                <li><a
                href="#interview-q-whats-distributional-rl-and-why-does-it-help"
                id="toc-interview-q-whats-distributional-rl-and-why-does-it-help">Interview
                Q: ‚ÄúWhat‚Äôs distributional RL and why does it
                help?‚Äù</a></li>
                </ul></li>
                </ul></li>
                <li><a
                href="#part-10-ml-systems-high-performance-computing"
                id="toc-part-10-ml-systems-high-performance-computing">Part
                10: ML Systems &amp; High-Performance Computing</a>
                <ul>
                <li><a
                href="#spmd-computing-single-program-multiple-data"
                id="toc-spmd-computing-single-program-multiple-data">10.1
                SPMD Computing (Single Program Multiple Data)</a>
                <ul>
                <li><a href="#what-is-spmd" id="toc-what-is-spmd">What
                is SPMD?</a></li>
                <li><a href="#spmd-vs-other-paradigms"
                id="toc-spmd-vs-other-paradigms">SPMD vs Other
                Paradigms</a></li>
                <li><a href="#jaxxla-model-for-spmd"
                id="toc-jaxxla-model-for-spmd">JAX/XLA Model for
                SPMD</a></li>
                <li><a href="#sharding-strategies"
                id="toc-sharding-strategies">Sharding
                Strategies</a></li>
                <li><a
                href="#interview-q-what-is-spmd-and-how-does-data-parallelism-relate-to-it"
                id="toc-interview-q-what-is-spmd-and-how-does-data-parallelism-relate-to-it">Interview
                Q: ‚ÄúWhat is SPMD and how does data parallelism relate to
                it?‚Äù</a></li>
                </ul></li>
                <li><a href="#communication-collectives"
                id="toc-communication-collectives">10.2 Communication
                Collectives</a>
                <ul>
                <li><a href="#why-communication-matters"
                id="toc-why-communication-matters">Why Communication
                Matters</a></li>
                <li><a href="#the-core-collectives"
                id="toc-the-core-collectives">The Core
                Collectives</a></li>
                <li><a
                href="#allreduce-the-workhorse-of-data-parallelism"
                id="toc-allreduce-the-workhorse-of-data-parallelism">AllReduce:
                The Workhorse of Data Parallelism</a></li>
                <li><a href="#allgather-collecting-distributed-data"
                id="toc-allgather-collecting-distributed-data">AllGather:
                Collecting Distributed Data</a></li>
                <li><a href="#reducescatter-efficient-gradient-handling"
                id="toc-reducescatter-efficient-gradient-handling">ReduceScatter:
                Efficient Gradient Handling</a></li>
                <li><a href="#reduce-broadcast-and-scatter"
                id="toc-reduce-broadcast-and-scatter">Reduce, Broadcast,
                and Scatter</a></li>
                <li><a href="#ring-allreduce-algorithm"
                id="toc-ring-allreduce-algorithm">Ring AllReduce
                Algorithm</a></li>
                <li><a href="#tree-allreduce-algorithm"
                id="toc-tree-allreduce-algorithm">Tree AllReduce
                Algorithm</a></li>
                <li><a href="#communication-cost-analysis"
                id="toc-communication-cost-analysis">Communication Cost
                Analysis</a></li>
                <li><a
                href="#interview-q-explain-allreduce-and-when-youd-use-ring-vs-tree"
                id="toc-interview-q-explain-allreduce-and-when-youd-use-ring-vs-tree">Interview
                Q: ‚ÄúExplain AllReduce and when you‚Äôd use Ring vs
                Tree‚Äù</a></li>
                </ul></li>
                <li><a href="#numerical-computing-essentials"
                id="toc-numerical-computing-essentials">10.3 Numerical
                Computing Essentials</a>
                <ul>
                <li><a href="#floating-point-formats"
                id="toc-floating-point-formats">Floating Point
                Formats</a></li>
                <li><a href="#bf16-vs-fp16-why-bf16-wins-for-training"
                id="toc-bf16-vs-fp16-why-bf16-wins-for-training">BF16 vs
                FP16: Why BF16 Wins for Training</a></li>
                <li><a href="#mixed-precision-training-2"
                id="toc-mixed-precision-training-2">Mixed Precision
                Training</a></li>
                <li><a href="#loss-scaling-for-fp16"
                id="toc-loss-scaling-for-fp16">Loss Scaling (for
                FP16)</a></li>
                <li><a href="#numerical-stability-softmax"
                id="toc-numerical-stability-softmax">Numerical
                Stability: Softmax</a></li>
                <li><a href="#numerical-stability-log-sum-exp"
                id="toc-numerical-stability-log-sum-exp">Numerical
                Stability: Log-Sum-Exp</a></li>
                <li><a href="#numerical-stability-cross-entropy"
                id="toc-numerical-stability-cross-entropy">Numerical
                Stability: Cross-Entropy</a></li>
                <li><a href="#gradient-accumulation-2"
                id="toc-gradient-accumulation-2">Gradient
                Accumulation</a></li>
                <li><a
                href="#interview-q-why-use-bf16-instead-of-fp16-for-training"
                id="toc-interview-q-why-use-bf16-instead-of-fp16-for-training">Interview
                Q: ‚ÄúWhy use BF16 instead of FP16 for training?‚Äù</a></li>
                </ul></li>
                <li><a href="#memory-and-compute-analysis"
                id="toc-memory-and-compute-analysis">10.4 Memory and
                Compute Analysis</a>
                <ul>
                <li><a href="#what-this-means-for-beginners-10"
                id="toc-what-this-means-for-beginners-10">What This
                Means (For Beginners)</a></li>
                <li><a href="#memory-breakdown-for-training-1"
                id="toc-memory-breakdown-for-training-1">Memory
                Breakdown for Training</a></li>
                <li><a href="#activation-memory"
                id="toc-activation-memory">Activation Memory</a></li>
                <li><a
                href="#activation-checkpointing-gradient-checkpointing"
                id="toc-activation-checkpointing-gradient-checkpointing">Activation
                Checkpointing (Gradient Checkpointing)</a></li>
                <li><a href="#compute-vs-memory-bound"
                id="toc-compute-vs-memory-bound">Compute vs Memory
                Bound</a></li>
                <li><a href="#roofline-model"
                id="toc-roofline-model">Roofline Model</a></li>
                <li><a href="#throughput-optimization"
                id="toc-throughput-optimization">Throughput
                Optimization</a></li>
                <li><a href="#common-memory-optimization-techniques"
                id="toc-common-memory-optimization-techniques">Common
                Memory Optimization Techniques</a></li>
                <li><a
                href="#interview-q-how-would-you-estimate-memory-requirements-for-training-a-7b-parameter-model"
                id="toc-interview-q-how-would-you-estimate-memory-requirements-for-training-a-7b-parameter-model">Interview
                Q: ‚ÄúHow would you estimate memory requirements for
                training a 7B parameter model?‚Äù</a></li>
                </ul></li>
                <li><a href="#practical-systems-problems-pseudo-code"
                id="toc-practical-systems-problems-pseudo-code">10.5
                Practical Systems Problems (Pseudo-code)</a>
                <ul>
                <li><a href="#problem-1-distributed-training-loop"
                id="toc-problem-1-distributed-training-loop">Problem 1:
                Distributed Training Loop</a></li>
                <li><a
                href="#problem-2-implementing-allreduce-with-sendrecv"
                id="toc-problem-2-implementing-allreduce-with-sendrecv">Problem
                2: Implementing AllReduce with Send/Recv</a></li>
                <li><a href="#problem-3-gradient-accumulation"
                id="toc-problem-3-gradient-accumulation">Problem 3:
                Gradient Accumulation</a></li>
                <li><a
                href="#problem-4-computing-global-batch-statistics"
                id="toc-problem-4-computing-global-batch-statistics">Problem
                4: Computing Global Batch Statistics</a></li>
                <li><a href="#problem-5-sharding-a-weight-matrix"
                id="toc-problem-5-sharding-a-weight-matrix">Problem 5:
                Sharding a Weight Matrix</a></li>
                <li><a href="#problem-6-mixed-precision-training-step"
                id="toc-problem-6-mixed-precision-training-step">Problem
                6: Mixed Precision Training Step</a></li>
                </ul></li>
                <li><a href="#summary-and-key-takeaways"
                id="toc-summary-and-key-takeaways">10.6 Summary and Key
                Takeaways</a>
                <ul>
                <li><a href="#the-big-picture-1"
                id="toc-the-big-picture-1">The Big Picture</a></li>
                <li><a href="#decision-tree-how-do-i-train-this-model"
                id="toc-decision-tree-how-do-i-train-this-model">Decision
                Tree: ‚ÄúHow Do I Train This Model?‚Äù</a></li>
                <li><a href="#cheat-sheet-memory-formula"
                id="toc-cheat-sheet-memory-formula">Cheat Sheet: Memory
                Formula</a></li>
                </ul></li>
                <li><a
                href="#quick-reference-ml-systems-interview-questions"
                id="toc-quick-reference-ml-systems-interview-questions">Quick
                Reference: ML Systems Interview Questions</a>
                <ul>
                <li><a href="#spmd-distributed-computing"
                id="toc-spmd-distributed-computing">SPMD &amp;
                Distributed Computing</a></li>
                <li><a href="#communication-collectives-1"
                id="toc-communication-collectives-1">Communication
                Collectives</a></li>
                <li><a href="#numerical-computing"
                id="toc-numerical-computing">Numerical
                Computing</a></li>
                <li><a href="#memory-compute"
                id="toc-memory-compute">Memory &amp; Compute</a></li>
                <li><a href="#practical-systems"
                id="toc-practical-systems">Practical Systems</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-11-advanced-transformer-topics"
                id="toc-part-11-advanced-transformer-topics">Part 11:
                Advanced Transformer Topics</a>
                <ul>
                <li><a href="#mixture-of-experts-moe"
                id="toc-mixture-of-experts-moe">11.1 Mixture of Experts
                (MoE)</a>
                <ul>
                <li><a href="#what-is-moe" id="toc-what-is-moe">What is
                MoE?</a></li>
                <li><a href="#the-key-insight-conditional-computation"
                id="toc-the-key-insight-conditional-computation">The Key
                Insight: Conditional Computation</a></li>
                <li><a href="#routergating-mechanism"
                id="toc-routergating-mechanism">Router/Gating
                Mechanism</a></li>
                <li><a href="#expert-architecture"
                id="toc-expert-architecture">Expert
                Architecture</a></li>
                <li><a href="#the-load-balancing-problem"
                id="toc-the-load-balancing-problem">The Load Balancing
                Problem</a></li>
                <li><a href="#token-choice-vs-expert-choice"
                id="toc-token-choice-vs-expert-choice">Token Choice vs
                Expert Choice</a></li>
                <li><a href="#moe-in-practice-mixtral-deepseek-moe"
                id="toc-moe-in-practice-mixtral-deepseek-moe">MoE in
                Practice: Mixtral, DeepSeek-MoE</a></li>
                <li><a href="#expert-parallelism"
                id="toc-expert-parallelism">Expert Parallelism</a></li>
                <li><a href="#moe-challenges"
                id="toc-moe-challenges">MoE Challenges</a></li>
                <li><a
                href="#interview-q-how-does-moe-achieve-better-scaling-than-dense-models"
                id="toc-interview-q-how-does-moe-achieve-better-scaling-than-dense-models">Interview
                Q: ‚ÄúHow does MoE achieve better scaling than dense
                models?‚Äù</a></li>
                <li><a
                href="#interview-q-whats-the-difference-between-token-choice-and-expert-choice-routing"
                id="toc-interview-q-whats-the-difference-between-token-choice-and-expert-choice-routing">Interview
                Q: ‚ÄúWhat‚Äôs the difference between token choice and
                expert choice routing?‚Äù</a></li>
                </ul></li>
                <li><a href="#flash-attention"
                id="toc-flash-attention">11.2 Flash Attention</a>
                <ul>
                <li><a href="#the-problem-attention-is-memory-bound"
                id="toc-the-problem-attention-is-memory-bound">The
                Problem: Attention is Memory-Bound</a></li>
                <li><a href="#the-memory-hierarchy"
                id="toc-the-memory-hierarchy">The Memory
                Hierarchy</a></li>
                <li><a href="#flash-attention-tiling-recomputation"
                id="toc-flash-attention-tiling-recomputation">Flash
                Attention: Tiling + Recomputation</a></li>
                <li><a href="#the-algorithm-1"
                id="toc-the-algorithm-1">The Algorithm</a></li>
                <li><a href="#online-softmax-the-key-trick"
                id="toc-online-softmax-the-key-trick">Online Softmax:
                The Key Trick</a></li>
                <li><a href="#io-complexity-analysis"
                id="toc-io-complexity-analysis">IO Complexity
                Analysis</a></li>
                <li><a href="#flash-attention-2-improvements"
                id="toc-flash-attention-2-improvements">Flash Attention
                2 Improvements</a></li>
                <li><a href="#flash-attention-3-hopper-gpus"
                id="toc-flash-attention-3-hopper-gpus">Flash Attention 3
                (Hopper GPUs)</a></li>
                <li><a href="#when-flash-attention-helps-most"
                id="toc-when-flash-attention-helps-most">When Flash
                Attention Helps Most</a></li>
                <li><a href="#code-example-using-flash-attention"
                id="toc-code-example-using-flash-attention">Code
                Example: Using Flash Attention</a></li>
                <li><a
                href="#interview-q-why-is-flash-attention-faster-despite-doing-more-flops"
                id="toc-interview-q-why-is-flash-attention-faster-despite-doing-more-flops">Interview
                Q: ‚ÄúWhy is Flash Attention faster despite doing more
                FLOPs?‚Äù</a></li>
                <li><a
                href="#interview-q-how-does-flash-attention-handle-the-backward-pass"
                id="toc-interview-q-how-does-flash-attention-handle-the-backward-pass">Interview
                Q: ‚ÄúHow does Flash Attention handle the backward
                pass?‚Äù</a></li>
                </ul></li>
                <li><a href="#kv-cache-and-inference-optimization"
                id="toc-kv-cache-and-inference-optimization">11.3
                KV-Cache and Inference Optimization</a>
                <ul>
                <li><a
                href="#the-problem-autoregressive-generation-is-slow"
                id="toc-the-problem-autoregressive-generation-is-slow">The
                Problem: Autoregressive Generation is Slow</a></li>
                <li><a href="#kv-cache-cache-past-key-values"
                id="toc-kv-cache-cache-past-key-values">KV-Cache: Cache
                Past Key-Values</a></li>
                <li><a href="#implementation-1"
                id="toc-implementation-1">Implementation</a></li>
                <li><a href="#kv-cache-memory-analysis"
                id="toc-kv-cache-memory-analysis">KV-Cache Memory
                Analysis</a></li>
                <li><a href="#the-memory-problem-at-scale"
                id="toc-the-memory-problem-at-scale">The Memory Problem
                at Scale</a></li>
                <li><a href="#solution-1-multi-query-attention-mqa"
                id="toc-solution-1-multi-query-attention-mqa">Solution
                1: Multi-Query Attention (MQA)</a></li>
                <li><a href="#solution-2-grouped-query-attention-gqa"
                id="toc-solution-2-grouped-query-attention-gqa">Solution
                2: Grouped-Query Attention (GQA)</a></li>
                <li><a href="#solution-3-paged-attention-vllm"
                id="toc-solution-3-paged-attention-vllm">Solution 3:
                Paged Attention (vLLM)</a></li>
                <li><a href="#continuous-batching"
                id="toc-continuous-batching">Continuous
                Batching</a></li>
                <li><a href="#speculative-decoding-connection"
                id="toc-speculative-decoding-connection">Speculative
                Decoding Connection</a></li>
                <li><a
                href="#interview-q-whats-the-memory-cost-of-kv-cache-for-a-70b-model-at-128k-context"
                id="toc-interview-q-whats-the-memory-cost-of-kv-cache-for-a-70b-model-at-128k-context">Interview
                Q: ‚ÄúWhat‚Äôs the memory cost of KV-cache for a 70B model
                at 128K context?‚Äù</a></li>
                <li><a
                href="#interview-q-explain-the-difference-between-mha-mqa-and-gqa"
                id="toc-interview-q-explain-the-difference-between-mha-mqa-and-gqa">Interview
                Q: ‚ÄúExplain the difference between MHA, MQA, and
                GQA‚Äù</a></li>
                </ul></li>
                <li><a href="#speculative-decoding"
                id="toc-speculative-decoding">11.4 Speculative
                Decoding</a>
                <ul>
                <li><a
                href="#the-problem-autoregressive-decoding-is-slow"
                id="toc-the-problem-autoregressive-decoding-is-slow">The
                Problem: Autoregressive Decoding is Slow</a></li>
                <li><a href="#the-key-insight-2"
                id="toc-the-key-insight-2">The Key Insight</a></li>
                <li><a href="#speculative-decoding-algorithm"
                id="toc-speculative-decoding-algorithm">Speculative
                Decoding Algorithm</a></li>
                <li><a href="#the-acceptance-criterion"
                id="toc-the-acceptance-criterion">The Acceptance
                Criterion</a></li>
                <li><a href="#implementation-2"
                id="toc-implementation-2">Implementation</a></li>
                <li><a href="#speedup-analysis"
                id="toc-speedup-analysis">Speedup Analysis</a></li>
                <li><a href="#practical-considerations-1"
                id="toc-practical-considerations-1">Practical
                Considerations</a></li>
                <li><a href="#self-speculative-decoding"
                id="toc-self-speculative-decoding">Self-Speculative
                Decoding</a></li>
                <li><a
                href="#interview-q-how-does-speculative-decoding-achieve-speedup-without-changing-output-distribution"
                id="toc-interview-q-how-does-speculative-decoding-achieve-speedup-without-changing-output-distribution">Interview
                Q: ‚ÄúHow does speculative decoding achieve speedup
                without changing output distribution?‚Äù</a></li>
                </ul></li>
                <li><a href="#state-space-models-mamba"
                id="toc-state-space-models-mamba">11.5 State Space
                Models (Mamba)</a>
                <ul>
                <li><a href="#the-problem-attention-is-on¬≤"
                id="toc-the-problem-attention-is-on¬≤">The Problem:
                Attention is O(N¬≤)</a></li>
                <li><a href="#state-space-models-a-different-approach"
                id="toc-state-space-models-a-different-approach">State
                Space Models: A Different Approach</a></li>
                <li><a href="#why-ssms-are-efficient"
                id="toc-why-ssms-are-efficient">Why SSMs are
                Efficient</a></li>
                <li><a
                href="#the-key-innovation-selective-state-spaces-s6mamba"
                id="toc-the-key-innovation-selective-state-spaces-s6mamba">The
                Key Innovation: Selective State Spaces
                (S6/Mamba)</a></li>
                <li><a href="#mamba-architecture"
                id="toc-mamba-architecture">Mamba Architecture</a></li>
                <li><a href="#efficient-implementation-parallel-scan"
                id="toc-efficient-implementation-parallel-scan">Efficient
                Implementation: Parallel Scan</a></li>
                <li><a href="#mamba-vs-transformers-comparison"
                id="toc-mamba-vs-transformers-comparison">Mamba vs
                Transformers: Comparison</a></li>
                <li><a href="#when-to-use-mamba-vs-transformers"
                id="toc-when-to-use-mamba-vs-transformers">When to Use
                Mamba vs Transformers</a></li>
                <li><a href="#hybrid-architectures-jamba-etc."
                id="toc-hybrid-architectures-jamba-etc.">Hybrid
                Architectures (Jamba, etc.)</a></li>
                <li><a href="#mamba-2-improvements"
                id="toc-mamba-2-improvements">Mamba-2
                Improvements</a></li>
                <li><a
                href="#interview-q-how-is-mamba-different-from-transformers"
                id="toc-interview-q-how-is-mamba-different-from-transformers">Interview
                Q: ‚ÄúHow is Mamba different from Transformers?‚Äù</a></li>
                <li><a
                href="#interview-q-whats-the-key-insight-behind-selective-state-spaces"
                id="toc-interview-q-whats-the-key-insight-behind-selective-state-spaces">Interview
                Q: ‚ÄúWhat‚Äôs the key insight behind selective state
                spaces?‚Äù</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#part-12-question-bank"
                id="toc-part-12-question-bank">Part 12: Question
                Bank</a>
                <ul>
                <li><a href="#theoretical-foundations---math-ml-theory"
                id="toc-theoretical-foundations---math-ml-theory">12.1
                Theoretical Foundations - Math &amp; ML Theory</a>
                <ul>
                <li><a href="#linear-algebra-1"
                id="toc-linear-algebra-1">12.1.1 Linear Algebra</a></li>
                <li><a href="#calculus-and-optimization"
                id="toc-calculus-and-optimization">12.1.2 Calculus and
                Optimization</a></li>
                <li><a href="#probability-and-statistics-1"
                id="toc-probability-and-statistics-1">12.1.3 Probability
                and Statistics</a></li>
                </ul></li>
                <li><a href="#ml-coding-implementation-from-scratch"
                id="toc-ml-coding-implementation-from-scratch">12.2 ML
                Coding &amp; Implementation from Scratch</a>
                <ul>
                <li><a href="#the-transformer-implementation"
                id="toc-the-transformer-implementation">12.2.1 The
                Transformer Implementation</a></li>
                <li><a
                href="#implementing-gradient-descent-from-scratch"
                id="toc-implementing-gradient-descent-from-scratch">12.2.2
                Implementing Gradient Descent from Scratch</a></li>
                <li><a href="#k-means-clustering-from-scratch"
                id="toc-k-means-clustering-from-scratch">12.2.3 K-Means
                Clustering from Scratch</a></li>
                <li><a href="#auc-from-scratch"
                id="toc-auc-from-scratch">12.2.4 AUC from
                Scratch</a></li>
                </ul></li>
                <li><a href="#ml-debugging" id="toc-ml-debugging">12.3
                ML Debugging</a>
                <ul>
                <li><a href="#approach-to-ml-debugging"
                id="toc-approach-to-ml-debugging">Approach to ML
                Debugging</a></li>
                <li><a href="#bug-1-broadcasting-silently-gone-wrong"
                id="toc-bug-1-broadcasting-silently-gone-wrong">Bug #1:
                Broadcasting Silently Gone Wrong</a></li>
                <li><a href="#bug-4-missing-optimizer.zero_grad"
                id="toc-bug-4-missing-optimizer.zero_grad">Bug #4:
                Missing optimizer.zero_grad()</a></li>
                <li><a
                href="#bug-5-dataloader-shufflefalse-for-training"
                id="toc-bug-5-dataloader-shufflefalse-for-training">Bug
                #5: DataLoader shuffle=False for Training</a></li>
                <li><a href="#bug-6-learning-rate-issues"
                id="toc-bug-6-learning-rate-issues">Bug #6: Learning
                Rate Issues</a></li>
                <li><a href="#bug-7-wrong-loss-function-for-task"
                id="toc-bug-7-wrong-loss-function-for-task">Bug #7:
                Wrong Loss Function for Task</a></li>
                <li><a href="#ml-debugging-checklist"
                id="toc-ml-debugging-checklist">ML Debugging
                Checklist</a></li>
                </ul></li>
                <li><a href="#ml-system-design---distributed-training"
                id="toc-ml-system-design---distributed-training">12.4 ML
                System Design - Distributed Training</a>
                <ul>
                <li><a
                href="#q-how-would-you-train-a-100b-parameter-model"
                id="toc-q-how-would-you-train-a-100b-parameter-model"><strong>Q:
                ‚ÄúHow would you train a 100B+ parameter
                model?‚Äù</strong></a></li>
                <li><a
                href="#q-whats-the-straggler-problem-how-do-you-handle-it"
                id="toc-q-whats-the-straggler-problem-how-do-you-handle-it"><strong>Q:
                ‚ÄúWhat‚Äôs the straggler problem? How do you handle
                it?‚Äù</strong></a></li>
                </ul></li>
                <li><a href="#inference-optimization"
                id="toc-inference-optimization">12.5 Inference
                Optimization</a>
                <ul>
                <li><a
                href="#q-explain-kv-caching-and-why-it-matters-for-inference."
                id="toc-q-explain-kv-caching-and-why-it-matters-for-inference."><strong>Q:
                ‚ÄúExplain KV caching and why it matters for
                inference.‚Äù</strong></a></li>
                <li><a
                href="#q-whats-speculative-decoding-why-is-it-exciting-for-2025"
                id="toc-q-whats-speculative-decoding-why-is-it-exciting-for-2025"><strong>Q:
                ‚ÄúWhat‚Äôs speculative decoding? Why is it exciting for
                2025?‚Äù</strong></a></li>
                <li><a
                href="#q-explain-the-trade-offs-of-quantization-for-llm-inference."
                id="toc-q-explain-the-trade-offs-of-quantization-for-llm-inference."><strong>Q:
                ‚ÄúExplain the trade-offs of quantization for LLM
                inference.‚Äù</strong></a></li>
                </ul></li>
                <li><a
                href="#quick-reference-common-interview-questions"
                id="toc-quick-reference-common-interview-questions">12.6
                Quick Reference: Common Interview Questions</a>
                <ul>
                <li><a href="#linear-algebra-2"
                id="toc-linear-algebra-2">Linear Algebra</a></li>
                <li><a href="#calculus-optimization"
                id="toc-calculus-optimization">Calculus &amp;
                Optimization</a></li>
                <li><a href="#probability-statistics"
                id="toc-probability-statistics">Probability &amp;
                Statistics</a></li>
                <li><a href="#transformer-implementation"
                id="toc-transformer-implementation">Transformer
                Implementation</a></li>
                <li><a href="#ml-debugging-deepmind-style"
                id="toc-ml-debugging-deepmind-style">ML Debugging
                (DeepMind Style)</a></li>
                <li><a href="#distributed-training-1"
                id="toc-distributed-training-1">Distributed
                Training</a></li>
                <li><a href="#inference-optimization-1"
                id="toc-inference-optimization-1">Inference
                Optimization</a></li>
                </ul></li>
                <li><a href="#appendix-verbal-expression-templates"
                id="toc-appendix-verbal-expression-templates">Appendix:
                Verbal Expression Templates</a>
                <ul>
                <li><a href="#starting-an-answer"
                id="toc-starting-an-answer">Starting an Answer</a></li>
                <li><a href="#explaining-trade-offs"
                id="toc-explaining-trade-offs">Explaining
                Trade-offs</a></li>
                <li><a href="#connecting-to-practical-implications"
                id="toc-connecting-to-practical-implications">Connecting
                to Practical Implications</a></li>
                <li><a href="#admitting-uncertainty"
                id="toc-admitting-uncertainty">Admitting
                Uncertainty</a></li>
                <li><a href="#asking-clarifying-questions"
                id="toc-asking-clarifying-questions">Asking Clarifying
                Questions</a></li>
                </ul></li>
                </ul></li>
                </ul>
            </nav>
    <div class="main-content">
        <div class="content">
            <h1 id="ferhat-eratas-mlai-study-notes">Ferhat Erata‚Äôs ML/AI
            Study Notes</h1>
            <blockquote>
            <p><strong>Comprehensive notes</strong> covering machine
            learning fundamentals, optimization theory, math
            foundations, deep learning, sequence models, LLM training
            pipelines, and distributed systems.</p>
            </blockquote>
            <h2 id="quick-navigation">Quick Navigation</h2>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 61%" />
            </colgroup>
            <thead>
            <tr>
            <th>Topic</th>
            <th>Key Areas</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>1.</strong> <a
            href="#part-1-learning-through-examples">Learning Through
            Examples</a></td>
            <td>Linear/Logistic Regression, MLP, Backprop, CNNs</td>
            </tr>
            <tr>
            <td><strong>2.</strong> <a href="#part-2-core-theory">Core
            Theory</a></td>
            <td>Gradient Descent, Adam/AdamW, Learning Rates</td>
            </tr>
            <tr>
            <td><strong>3.</strong> <a
            href="#part-3-math-foundations">Math Foundations</a></td>
            <td>Linear Algebra, Probability, Calculus</td>
            </tr>
            <tr>
            <td><strong>4.</strong> <a href="#part-4-ml-fundamentals">ML
            Fundamentals</a></td>
            <td>Bias-Variance, Overfitting, Evaluation</td>
            </tr>
            <tr>
            <td><strong>5.</strong> <a
            href="#part-5-optimization">Optimization</a></td>
            <td>SGD, Momentum, Regularization</td>
            </tr>
            <tr>
            <td><strong>6.</strong> <a
            href="#part-6-sequence-models">Sequence Models</a></td>
            <td>RNNs, LSTMs, Transformers</td>
            </tr>
            <tr>
            <td><strong>7.</strong> <a
            href="#part-7-llm-training-pipeline">LLM Training</a></td>
            <td>Pretraining, SFT, RLHF, DPO</td>
            </tr>
            <tr>
            <td><strong>8.</strong> <a
            href="#part-8-distributed-training">Distributed
            Training</a></td>
            <td>DDP, Tensor/Pipeline Parallelism, ZeRO</td>
            </tr>
            <tr>
            <td><strong>9.</strong> <a
            href="#part-9-reinforcement-learning">Reinforcement
            Learning</a></td>
            <td>MDPs, Q-Learning, Policy Gradients, PPO</td>
            </tr>
            <tr>
            <td><strong>10.</strong> <a
            href="#part-10-ml-systems-high-performance-computing">ML
            Systems</a></td>
            <td>SPMD, Collectives, Memory Analysis</td>
            </tr>
            <tr>
            <td><strong>11.</strong> <a
            href="#part-11-advanced-transformer-topics">Advanced
            Transformers</a></td>
            <td>MoE, Flash Attention, KV-Cache, Mamba</td>
            </tr>
            <tr>
            <td><strong>12.</strong> <a
            href="#part-12-question-bank">Question Bank</a></td>
            <td>Q&amp;A, ML Debugging, System Design</td>
            </tr>
            </tbody>
            </table>
            <h1 id="part-1-learning-through-examples">Part 1: Learning
            Through Examples</h1>
            <h2 id="what-is-machine-learning">1.1 What is Machine
            Learning?</h2>
            <p>Machine learning is about <strong>finding patterns in
            data</strong>. Instead of writing explicit rules, we:</p>
            <ol type="1">
            <li><strong>Define a model</strong> with adjustable
            parameters (weights)</li>
            <li><strong>Define a loss function</strong> that measures
            how wrong we are</li>
            <li><strong>Use an algorithm</strong> to adjust parameters
            to reduce the loss</li>
            </ol>
            <p><strong>The goal</strong>: Find parameter values that
            make our model‚Äôs predictions match the real data as closely
            as possible.</p>
            <pre><code>Data ‚Üí [Model with parameters w] ‚Üí Predictions
                    ‚Üì
           Compare to real answers
                    ‚Üì
               Loss (error)
                    ‚Üì
         Adjust w to reduce loss  ‚Üê Gradient descent!</code></pre>
            <hr />
            <h2 id="simple-linear-regression">1.2 Simple Linear
            Regression</h2>
            <h3 id="the-problem">The Problem</h3>
            <p>You have data about houses: square footage (x) and sale
            price (y). You want to predict price from size.</p>
            <table>
            <thead>
            <tr>
            <th>House</th>
            <th>Size (sq ft)</th>
            <th>Price ($1000s)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>1000</td>
            <td>200</td>
            </tr>
            <tr>
            <td>2</td>
            <td>1500</td>
            <td>280</td>
            </tr>
            <tr>
            <td>3</td>
            <td>2000</td>
            <td>350</td>
            </tr>
            <tr>
            <td>4</td>
            <td>2500</td>
            <td>400</td>
            </tr>
            <tr>
            <td>5</td>
            <td>3000</td>
            <td>500</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-model-a-line">The Model: A Line</h3>
            <p>We assume price is (roughly) linear in size:</p>
            <p><span class="math display">\[
            \hat{y} = w \cdot x + b
            \]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(x\)</span> = input (square
            footage)</li>
            <li><span class="math inline">\(\hat{y}\)</span> = predicted
            price</li>
            <li><span class="math inline">\(w\)</span> =
            <strong>weight</strong> (slope of the line ‚Äî how much price
            increases per sq ft)</li>
            <li><span class="math inline">\(b\)</span> =
            <strong>bias</strong> (intercept ‚Äî base price)</li>
            </ul>
            <p><strong>Our job</strong>: Find the best values of <span
            class="math inline">\(w\)</span> and <span
            class="math inline">\(b\)</span>.</p>
            <h3 id="visualizing-different-choices">Visualizing Different
            Choices</h3>
            <pre><code>Price ($1000s)
    ‚Üë
500 |                    * (3000, 500)
400 |               * (2500, 400)
350 |          * (2000, 350)
280 |     * (1500, 280)
200 | * (1000, 200)
    |________________________‚Üí Size (sq ft)
      1000  1500  2000  2500  3000</code></pre>
            <p>Bad line (<span class="math inline">\(w = 0.05\)</span>,
            <span class="math inline">\(b = 150\)</span>):</p>
            <p><span class="math display">\[\hat{y} = 0.05 \cdot x +
            150\]</span></p>
            <table>
            <thead>
            <tr>
            <th>x</th>
            <th>Actual y</th>
            <th>Predicted <span
            class="math inline">\(\hat{y}\)</span></th>
            <th>Error</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1000</td>
            <td>200</td>
            <td>200</td>
            <td>0</td>
            </tr>
            <tr>
            <td>1500</td>
            <td>280</td>
            <td>225</td>
            <td>-55</td>
            </tr>
            <tr>
            <td>2000</td>
            <td>350</td>
            <td>250</td>
            <td>-100</td>
            </tr>
            <tr>
            <td>2500</td>
            <td>400</td>
            <td>275</td>
            <td>-125</td>
            </tr>
            <tr>
            <td>3000</td>
            <td>500</td>
            <td>300</td>
            <td>-200</td>
            </tr>
            </tbody>
            </table>
            <p>The line is too flat!</p>
            <p>Better line (<span class="math inline">\(w =
            0.15\)</span>, <span class="math inline">\(b =
            50\)</span>):</p>
            <p><span class="math display">\[\hat{y} = 0.15 \cdot x +
            50\]</span></p>
            <table>
            <thead>
            <tr>
            <th>x</th>
            <th>Actual y</th>
            <th>Predicted <span
            class="math inline">\(\hat{y}\)</span></th>
            <th>Error</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1000</td>
            <td>200</td>
            <td>200</td>
            <td>0</td>
            </tr>
            <tr>
            <td>1500</td>
            <td>280</td>
            <td>275</td>
            <td>-5</td>
            </tr>
            <tr>
            <td>2000</td>
            <td>350</td>
            <td>350</td>
            <td>0</td>
            </tr>
            <tr>
            <td>2500</td>
            <td>400</td>
            <td>425</td>
            <td>+25</td>
            </tr>
            <tr>
            <td>3000</td>
            <td>500</td>
            <td>500</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>Much better!</p>
            <p><img src="figures/linear_regression_fit.png"
            alt="Linear Regression Fit" /> <em>Figure: Comparing a bad
            fit (left) with large residuals (MSE = 13,730) to a good fit
            (right) with minimized residuals (MSE = 130). The dashed
            lines show the errors ‚Äî gradient descent minimizes the sum
            of squared errors.</em></p>
            <h3 id="the-loss-function-mean-squared-error">The Loss
            Function: Mean Squared Error</h3>
            <p>How do we quantify ‚Äúhow wrong‚Äù a line is? Use
            <strong>Mean Squared Error (MSE)</strong>:</p>
            <p><span class="math display">\[
            \text{Loss} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
            = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w \cdot x_i + b))^2
            \]</span></p>
            <p>Why squared?</p>
            <ul>
            <li><strong>Positive</strong>: Errors don‚Äôt cancel out</li>
            <li><strong>Penalizes big errors more</strong>: An error of
            100 contributes <span class="math inline">\(100^2 =
            10000\)</span>, not just 100</li>
            <li><strong>Smooth</strong>: Has nice derivatives for
            optimization</li>
            </ul>
            <p>For our ‚Äúbad line‚Äù: <span class="math display">\[
            \text{Loss} = \frac{1}{5}(0^2 + 55^2 + 100^2 + 125^2 +
            200^2) = \frac{1}{5}(0 + 3025 + 10000 + 15625 + 40000) =
            13730
            \]</span></p>
            <p>For our ‚Äúbetter line‚Äù: <span class="math display">\[
            \text{Loss} = \frac{1}{5}(0^2 + 5^2 + 0^2 + 25^2 + 0^2) =
            \frac{1}{5}(0 + 25 + 0 + 625 + 0) = 130
            \]</span></p>
            <p>Lower loss = better fit!</p>
            <hr />
            <h2 id="gradient-descent-finding-the-best-line">1.3 Gradient
            Descent: Finding the Best Line</h2>
            <h3 id="the-optimization-problem">The Optimization
            Problem</h3>
            <p>We want to find <span class="math inline">\((w^*,
            b^*)\)</span> that minimize the loss:</p>
            <p><span class="math display">\[
            (w^*, b^*) = \arg\min_{w, b} \frac{1}{N} \sum_{i=1}^{N} (y_i
            - (w \cdot x_i + b))^2
            \]</span></p>
            <p>For simple linear regression, there‚Äôs a closed-form
            solution. But for neural networks, there isn‚Äôt ‚Äî we need
            <strong>gradient descent</strong>.</p>
            <p><img src="figures/loss_landscape.png"
            alt="Loss Landscape" /> <em>Figure: 3D visualization of a
            loss landscape showing the optimization surface and gradient
            descent path.</em></p>
            <h3 id="the-key-idea">The Key Idea</h3>
            <p>The loss is a function of <span
            class="math inline">\(w\)</span> and <span
            class="math inline">\(b\)</span>. If we plot loss vs
            parameters, we get a <strong>surface</strong>:</p>
            <p><img src="figures/loss_valley.png"
            alt="Loss Landscape - Finding the Valley" /> <em>Figure: The
            loss function forms a landscape over parameter space.
            Gradient descent finds the minimum (valley) by taking steps
            in the direction of steepest descent.</em></p>
            <p><strong>Gradient descent</strong>: Start somewhere,
            repeatedly take small steps ‚Äúdownhill‚Äù until you reach the
            bottom.</p>
            <h3 id="what-is-a-gradient-intuitive-explanation">What is a
            Gradient? (Intuitive Explanation)</h3>
            <p>Before diving into the math, let‚Äôs build intuition about
            what a gradient actually <em>is</em>.</p>
            <p><strong>The core idea</strong>: A gradient is a vector
            that points in the direction of steepest increase of a
            function.</p>
            <p><strong>Physical analogy</strong>: Imagine standing on a
            hilly terrain. The gradient at your location is like an
            arrow pointing directly uphill‚Äîthe direction you‚Äôd go if you
            wanted to climb as steeply as possible. If you want to
            descend to the valley (minimize elevation), you walk in the
            <em>opposite</em> direction of this arrow.</p>
            <p><strong>Why is it a vector?</strong> Because we have
            multiple parameters to adjust! If we have two parameters
            (<span class="math inline">\(w\)</span> and <span
            class="math inline">\(b\)</span>), the gradient has two
            components:</p>
            <ul>
            <li>The first component tells us: ‚ÄúHow much does the loss
            change if I nudge <span class="math inline">\(w\)</span> a
            tiny bit?‚Äù</li>
            <li>The second component tells us: ‚ÄúHow much does the loss
            change if I nudge <span class="math inline">\(b\)</span> a
            tiny bit?‚Äù</li>
            </ul>
            <p>Together, these form a vector that points toward the
            steepest uphill direction in the 2D parameter space.</p>
            <p><strong>Mathematical definition</strong>: The gradient of
            a function <span class="math inline">\(f(w, b)\)</span> is
            the vector of all its partial derivatives:</p>
            <p><span class="math display">\[\nabla f = \begin{bmatrix}
            \frac{\partial f}{\partial w} \\ \frac{\partial f}{\partial
            b} \end{bmatrix}\]</span></p>
            <p>Each partial derivative measures the ‚Äúsensitivity‚Äù of the
            output to changes in one input while holding others
            fixed.</p>
            <p><strong>Key insight for optimization</strong>: Since the
            gradient points uphill, we go the <em>opposite</em>
            direction to minimize the loss: <span
            class="math display">\[\text{new parameters} = \text{old
            parameters} - \alpha \cdot \nabla \text{Loss}\]</span></p>
            <p>where <span class="math inline">\(\alpha\)</span> is the
            learning rate (step size).</p>
            <h3 id="the-gradient-which-way-is-downhill">The Gradient:
            Which Way is Downhill?</h3>
            <p>The <strong>gradient</strong> tells us the direction of
            steepest <strong>uphill</strong>. So we go the
            <strong>opposite</strong> direction!</p>
            <p>For our linear regression:</p>
            <p><span class="math display">\[
            \frac{\partial \text{Loss}}{\partial w} = \frac{1}{N}
            \sum_{i=1}^{N} -2x_i(y_i - (w \cdot x_i + b)) = -\frac{2}{N}
            \sum_{i=1}^{N} x_i(y_i - \hat{y}_i)
            \]</span></p>
            <p><span class="math display">\[
            \frac{\partial \text{Loss}}{\partial b} = \frac{1}{N}
            \sum_{i=1}^{N} -2(y_i - (w \cdot x_i + b)) = -\frac{2}{N}
            \sum_{i=1}^{N} (y_i - \hat{y}_i)
            \]</span></p>
            <p><strong>Intuition</strong>:</p>
            <ul>
            <li>If predictions are too low (<span
            class="math inline">\(y_i - \hat{y}_i &gt; 0\)</span>), we
            need to increase <span class="math inline">\(w\)</span> and
            <span class="math inline">\(b\)</span></li>
            <li>The gradient is negative ‚Üí subtracting it makes <span
            class="math inline">\(w\)</span> and <span
            class="math inline">\(b\)</span> bigger ‚úì</li>
            </ul>
            <h3 id="the-update-rule">The Update Rule</h3>
            <p><span class="math display">\[
            w_{\text{new}} = w_{\text{old}} - \alpha \cdot
            \frac{\partial \text{Loss}}{\partial w}
            \]</span></p>
            <p><span class="math display">\[
            b_{\text{new}} = b_{\text{old}} - \alpha \cdot
            \frac{\partial \text{Loss}}{\partial b}
            \]</span></p>
            <p>where <span class="math inline">\(\alpha\)</span> is the
            <strong>learning rate</strong> (step size).</p>
            <h3 id="concrete-example-step-by-step">Concrete Example:
            Step by Step</h3>
            <p>Let‚Äôs use our housing data: <span class="math inline">\(x
            = [1000, 1500, 2000, 2500, 3000]\)</span>, <span
            class="math inline">\(y = [200, 280, 350, 400,
            500]\)</span></p>
            <p><strong>Initialize</strong>: <span
            class="math inline">\(w = 0.0\)</span>, <span
            class="math inline">\(b = 0.0\)</span>, <span
            class="math inline">\(\alpha = 0.0000001\)</span> (tiny
            because our <span class="math inline">\(x\)</span> values
            are large)</p>
            <p><strong>Step 1: Compute predictions</strong> <span
            class="math display">\[
            \hat{y} = [0, 0, 0, 0, 0]
            \]</span></p>
            <p><strong>Step 2: Compute errors</strong> <span
            class="math display">\[
            y - \hat{y} = [200, 280, 350, 400, 500]
            \]</span></p>
            <p><strong>Step 3: Compute gradients</strong> <span
            class="math display">\[
            \frac{\partial L}{\partial w} = -\frac{2}{5}(1000 \cdot 200
            + 1500 \cdot 280 + 2000 \cdot 350 + 2500 \cdot 400 + 3000
            \cdot 500)
            \]</span></p>
            <p><span class="math display">\[
            = -\frac{2}{5}(200000 + 420000 + 700000 + 1000000 + 1500000)
            = -\frac{2}{5}(3820000) = -1528000
            \]</span></p>
            <p><span class="math display">\[
            \frac{\partial L}{\partial b} = -\frac{2}{5}(200 + 280 + 350
            + 400 + 500) = -\frac{2}{5}(1730) = -692
            \]</span></p>
            <p><strong>Step 4: Update parameters</strong> <span
            class="math display">\[
            w = 0 - 0.0000001 \cdot (-1528000) = 0.1528
            \]</span></p>
            <p><span class="math display">\[
            b = 0 - 0.0000001 \cdot (-692) = 0.0000692
            \]</span></p>
            <p>After just one step, <span class="math inline">\(w
            \approx 0.15\)</span> ‚Äî already close to the good value!</p>
            <p><strong>Repeat</strong> until loss stops decreasing.</p>
            <h3 id="python-implementation">Python Implementation</h3>
            <div class="sourceCode" id="cb3"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">1000</span>, <span class="dv">1500</span>, <span class="dv">2000</span>, <span class="dv">2500</span>, <span class="dv">3000</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">200</span>, <span class="dv">280</span>, <span class="dv">350</span>, <span class="dv">400</span>, <span class="dv">500</span>])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize parameters</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0000001</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predictions</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> w <span class="op">*</span> X <span class="op">+</span> b</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss (MSE)</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean((y <span class="op">-</span> y_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradients</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>n <span class="op">*</span> np.<span class="bu">sum</span>(X <span class="op">*</span> (y <span class="op">-</span> y_pred))</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">/</span>n <span class="op">*</span> np.<span class="bu">sum</span>(y <span class="op">-</span> y_pred)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> w <span class="op">-</span> learning_rate <span class="op">*</span> dw</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b <span class="op">-</span> learning_rate <span class="op">*</span> db</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: w=</span><span class="sc">{</span>w<span class="sc">:.4f}</span><span class="ss">, b=</span><span class="sc">{</span>b<span class="sc">:.4f}</span><span class="ss">, loss=</span><span class="sc">{</span>loss<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Final: Step 900: w=0.1480, b=53.7000, loss=133.83</span></span></code></pre></div>
            <hr />
            <h2 id="logistic-regression-classification">1.4 Logistic
            Regression: Classification</h2>
            <h3 id="the-problem-binary-classification">The Problem:
            Binary Classification</h3>
            <p>Now instead of predicting a continuous value, we want to
            classify: <strong>Is this email spam or not?</strong></p>
            <table>
            <thead>
            <tr>
            <th>Email</th>
            <th>Contains ‚ÄúFREE‚Äù</th>
            <th>Contains ‚Äú!‚Äù</th>
            <th>Has link</th>
            <th>Label</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>1</td>
            <td>1</td>
            <td>1</td>
            <td>Spam (1)</td>
            </tr>
            <tr>
            <td>2</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>Not spam (0)</td>
            </tr>
            <tr>
            <td>3</td>
            <td>1</td>
            <td>1</td>
            <td>0</td>
            <td>Spam (1)</td>
            </tr>
            <tr>
            <td>4</td>
            <td>0</td>
            <td>1</td>
            <td>1</td>
            <td>Not spam (0)</td>
            </tr>
            <tr>
            <td>5</td>
            <td>1</td>
            <td>0</td>
            <td>1</td>
            <td>Spam (1)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="why-linear-regression-fails">Why Linear Regression
            Fails</h3>
            <p>If we use <span class="math inline">\(\hat{y} = w_1 x_1 +
            w_2 x_2 + w_3 x_3 + b\)</span>, we might get outputs like
            <span class="math inline">\(-0.5\)</span> or <span
            class="math inline">\(2.3\)</span> ‚Äî but we need
            probabilities between 0 and 1!</p>
            <h3 id="the-sigmoid-function">The Sigmoid Function</h3>
            <p>We ‚Äúsquash‚Äù the linear output through the
            <strong>sigmoid</strong> function:</p>
            <p><span class="math display">\[
            \sigma(z) = \frac{1}{1 + e^{-z}}
            \]</span></p>
            <p><img src="figures/sigmoid_function.png"
            alt="Sigmoid Function" /> <em>Figure: The sigmoid function
            œÉ(z) = 1/(1+e^(-z)) squashes any input to a value between 0
            and 1.</em></p>
            <p>Properties:</p>
            <ul>
            <li>Always outputs between 0 and 1 ‚úì</li>
            <li><span class="math inline">\(\sigma(0) =
            0.5\)</span></li>
            <li><span class="math inline">\(\sigma(\text{large
            positive}) \approx 1\)</span></li>
            <li><span class="math inline">\(\sigma(\text{large
            negative}) \approx 0\)</span></li>
            </ul>
            <h3
            id="deriving-the-sigmoid-derivative-chain-rule-example">Deriving
            the Sigmoid Derivative (Chain Rule Example)</h3>
            <p>Understanding the sigmoid derivative is crucial for
            backpropagation. Let‚Äôs derive it step-by-step using the
            <strong>chain rule</strong>.</p>
            <p><strong>Starting point</strong>: <span
            class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}} = (1
            + e^{-z})^{-1}\]</span></p>
            <p><strong>Step 1: Apply the chain rule</strong></p>
            <p>Let <span class="math inline">\(u = 1 + e^{-z}\)</span>,
            so <span class="math inline">\(\sigma = u^{-1}\)</span></p>
            <p>We need <span class="math inline">\(\frac{d\sigma}{dz} =
            \frac{d\sigma}{du} \cdot \frac{du}{dz}\)</span></p>
            <p><strong>Step 2: Compute each part</strong></p>
            <ul>
            <li><p><strong>Power rule</strong>: <span
            class="math inline">\(\frac{d\sigma}{du} =
            \frac{d}{du}(u^{-1}) = -u^{-2} =
            -\frac{1}{(1+e^{-z})^2}\)</span></p></li>
            <li><p><strong>Chain rule on exponential</strong>: <span
            class="math inline">\(\frac{du}{dz} = \frac{d}{dz}(1 +
            e^{-z}) = -e^{-z}\)</span></p></li>
            </ul>
            <p><strong>Step 3: Multiply</strong></p>
            <p><span class="math display">\[\frac{d\sigma}{dz} =
            -\frac{1}{(1+e^{-z})^2} \cdot (-e^{-z}) =
            \frac{e^{-z}}{(1+e^{-z})^2}\]</span></p>
            <p><strong>Step 4: Simplify to the elegant form</strong></p>
            <p>Notice that: <span
            class="math display">\[\frac{e^{-z}}{(1+e^{-z})^2} =
            \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} = \sigma(z)
            \cdot \frac{e^{-z}}{1+e^{-z}}\]</span></p>
            <p>And <span class="math inline">\(\frac{e^{-z}}{1+e^{-z}} =
            \frac{1+e^{-z}-1}{1+e^{-z}} = 1 - \frac{1}{1+e^{-z}} = 1 -
            \sigma(z)\)</span></p>
            <p><strong>Final result</strong>: <span
            class="math display">\[\boxed{\sigma&#39;(z) = \sigma(z)(1 -
            \sigma(z))}\]</span></p>
            <p>This is remarkably elegant ‚Äî the derivative depends only
            on the output, not the input!</p>
            <h3
            id="why-the-maximum-derivative-is-14-and-why-it-matters">Why
            the Maximum Derivative is 1/4 (and Why It Matters)</h3>
            <p>The sigmoid derivative <span
            class="math inline">\(\sigma&#39;(z) = \sigma(z)(1 -
            \sigma(z))\)</span> has a <strong>maximum value of
            0.25</strong>. Let‚Äôs prove this and understand its
            implications.</p>
            <p><img src="figures/sigmoid_derivative.png"
            alt="Sigmoid Derivative" /> <em>Figure: The sigmoid function
            and its derivative. The derivative reaches its maximum of
            0.25 at z=0, where œÉ(z)=0.5.</em></p>
            <p><strong>Step 1: Reframe the problem</strong></p>
            <p>Let <span class="math inline">\(y = \sigma(z)\)</span>.
            Since sigmoid outputs are in <span class="math inline">\((0,
            1)\)</span>, we need to find the maximum of: <span
            class="math display">\[f(y) = y(1 - y) = y - y^2 \quad
            \text{for } y \in (0, 1)\]</span></p>
            <p><strong>Step 2: Find the maximum</strong></p>
            <p>This is a downward-opening parabola. Taking the
            derivative: <span class="math display">\[\frac{df}{dy} = 1 -
            2y\]</span></p>
            <p>Setting to zero: <span class="math inline">\(1 - 2y = 0
            \Rightarrow y = 0.5\)</span></p>
            <p><strong>Step 3: Calculate the maximum value</strong></p>
            <p><span class="math display">\[f(0.5) = 0.5 \times (1 -
            0.5) = 0.5 \times 0.5 = \boxed{0.25 =
            \frac{1}{4}}\]</span></p>
            <p><strong>What this means</strong>: At the optimal point
            (<span class="math inline">\(z = 0\)</span>, where <span
            class="math inline">\(\sigma(z) = 0.5\)</span>), the
            gradient through a sigmoid is only <strong>0.25</strong>. In
            the <strong>saturation regions</strong> (where <span
            class="math inline">\(\sigma \to 0\)</span> or <span
            class="math inline">\(\sigma \to 1\)</span>), the derivative
            approaches <strong>0</strong>.</p>
            <h3 id="the-vanishing-gradient-problem">The Vanishing
            Gradient Problem</h3>
            <p>This maximum of 1/4 has catastrophic implications for
            deep networks:</p>
            <p><strong>During backpropagation</strong>, gradients are
            multiplied at each layer: <span
            class="math display">\[\frac{\partial L}{\partial W_1} =
            \frac{\partial L}{\partial z_n} \cdot \sigma&#39;(z_{n-1})
            \cdot \sigma&#39;(z_{n-2}) \cdots
            \sigma&#39;(z_1)\]</span></p>
            <p><strong>Even in the best case</strong> (all neurons at
            <span class="math inline">\(\sigma = 0.5\)</span>):</p>
            <table>
            <thead>
            <tr>
            <th>Layers</th>
            <th>Gradient Factor</th>
            <th>Result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>5</td>
            <td><span class="math inline">\(0.25^5\)</span></td>
            <td>0.001 (1/1000)</td>
            </tr>
            <tr>
            <td>10</td>
            <td><span class="math inline">\(0.25^{10}\)</span></td>
            <td><span class="math inline">\(10^{-6}\)</span> (one
            millionth!)</td>
            </tr>
            <tr>
            <td>20</td>
            <td><span class="math inline">\(0.25^{20}\)</span></td>
            <td><span class="math inline">\(10^{-12}\)</span></td>
            </tr>
            </tbody>
            </table>
            <p><strong>In practice, it‚Äôs worse</strong>: Neurons rarely
            sit at <span class="math inline">\(\sigma = 0.5\)</span>. In
            saturation regions, <span class="math inline">\(\sigma&#39;
            \approx 0\)</span>, making gradients vanish even faster.</p>
            <p><strong>Symptoms of vanishing gradients</strong>: - Early
            layers (near input) barely update - Loss decreases very
            slowly - Deep networks fail to train</p>
            <p><img src="figures/gradient_magnitude_layers.png"
            alt="Gradient Magnitude Through Layers" /> <em>Figure:
            Gradient magnitude decay during backpropagation for
            different activation functions. Sigmoid‚Äôs max derivative of
            0.25 causes exponential decay ‚Äî after 10 layers, gradients
            shrink to 10‚Åª‚Å∂ (best case). ReLU maintains gradient = 1 for
            positive activations, enabling training of very deep
            networks. The bar chart shows how quickly sigmoid gradients
            become negligible.</em></p>
            <p><img src="figures/sigmoid_vs_relu_derivative.png"
            alt="Sigmoid vs ReLU Derivative" /> <em>Figure: Comparison
            of sigmoid and ReLU derivatives. ReLU has gradient = 1 for
            positive inputs, avoiding exponential decay.</em></p>
            <h3 id="relu-the-solution-to-vanishing-gradients">ReLU: The
            Solution to Vanishing Gradients</h3>
            <p><strong>ReLU (Rectified Linear Unit)</strong> solves this
            problem:</p>
            <p><span class="math display">\[\text{ReLU}(z) = \max(0,
            z)\]</span></p>
            <p><strong>ReLU derivative</strong>: <span
            class="math display">\[\text{ReLU}&#39;(z) = \begin{cases} 1
            &amp; \text{if } z &gt; 0 \\ 0 &amp; \text{if } z \leq 0
            \end{cases}\]</span></p>
            <p><strong>Why ReLU works</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 36%" />
            <col style="width: 24%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>Sigmoid</th>
            <th>ReLU</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Max derivative</td>
            <td>0.25</td>
            <td><strong>1</strong></td>
            </tr>
            <tr>
            <td>After 10 layers (best case)</td>
            <td><span class="math inline">\(0.25^{10} \approx
            10^{-6}\)</span></td>
            <td><span class="math inline">\(1^{10} = 1\)</span></td>
            </tr>
            <tr>
            <td>Gradient decay</td>
            <td>Exponential</td>
            <td><strong>None!</strong></td>
            </tr>
            <tr>
            <td>Saturation</td>
            <td>Both directions</td>
            <td>Only negative</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The key insight</strong>: With ReLU, gradients
            pass through unchanged (multiplied by 1) for positive
            activations. There‚Äôs no exponential decay, so deep networks
            can actually train!</p>
            <p><strong>Trade-off</strong>: ReLU has ‚Äúdead neurons‚Äù ‚Äî if
            a neuron‚Äôs input is always negative, its gradient is always
            0 and it never updates. Solutions include: - <strong>Leaky
            ReLU</strong>: Small gradient for negative inputs (<span
            class="math inline">\(0.01x\)</span> instead of <span
            class="math inline">\(0\)</span>) -
            <strong>ELU/GELU</strong>: Smooth alternatives with better
            properties</p>
            <h3
            id="interview-q-why-do-we-use-relu-instead-of-sigmoid-in-hidden-layers">Interview
            Q: ‚ÄúWhy do we use ReLU instead of sigmoid in hidden
            layers?‚Äù</h3>
            <p><strong>A</strong>: Sigmoid has a maximum derivative of
            0.25, causing <strong>vanishing gradients</strong> in deep
            networks. After just 10 layers, even in the best case,
            gradients shrink to <span class="math inline">\(0.25^{10}
            \approx 10^{-6}\)</span>. Early layers barely learn. ReLU
            has derivative = 1 for positive inputs, so gradients pass
            through without decay. This enables training of deep
            networks. The trade-off is ‚Äúdead neurons‚Äù (gradient = 0 for
            negative inputs), addressed by variants like Leaky ReLU.</p>
            <h3 id="logistic-regression-model">Logistic Regression
            Model</h3>
            <p><span class="math display">\[
            P(y=1|x) = \sigma(w^\top x + b) = \frac{1}{1 + e^{-(w^\top x
            + b)}}
            \]</span></p>
            <p>Interpretation: ‚ÄúThe probability this email is spam,
            given its features.‚Äù</p>
            <h3 id="the-loss-function-binary-cross-entropy">The Loss
            Function: Binary Cross-Entropy</h3>
            <p>For classification, we use <strong>cross-entropy
            loss</strong> (not MSE):</p>
            <p><span class="math display">\[
            \text{Loss} = -\frac{1}{N}\sum_{i=1}^{N} \left[ y_i
            \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
            \]</span></p>
            <p>Why this?</p>
            <ul>
            <li>If true label is 1 and we predict 1: <span
            class="math inline">\(-\log(1) = 0\)</span> (no penalty)
            ‚úì</li>
            <li>If true label is 1 and we predict 0.01: <span
            class="math inline">\(-\log(0.01) \approx 4.6\)</span> (big
            penalty!) ‚úì</li>
            <li>Works with probability outputs from sigmoid</li>
            </ul>
            <h3
            id="deriving-cross-entropy-from-maximum-likelihood">Deriving
            Cross-Entropy from Maximum Likelihood</h3>
            <p>The cross-entropy loss isn‚Äôt arbitrary‚Äîit comes directly
            from <strong>Maximum Likelihood Estimation (MLE)</strong>.
            Understanding this derivation reveals <em>why</em>
            cross-entropy is the natural loss for classification.</p>
            <p><strong>Step 1: Model the output as a
            probability</strong></p>
            <p>In logistic regression, we model the probability that
            <span class="math inline">\(y = 1\)</span> given input <span
            class="math inline">\(x\)</span>:</p>
            <p><span class="math display">\[P(y = 1 | x) = \hat{y} =
            \sigma(w^\top x + b)\]</span></p>
            <p>This means <span class="math inline">\(P(y = 0 | x) = 1 -
            \hat{y}\)</span>.</p>
            <p><strong>Step 2: Write the likelihood of one data
            point</strong></p>
            <p>For a single example <span class="math inline">\((x_i,
            y_i)\)</span> where <span class="math inline">\(y_i \in \{0,
            1\}\)</span>:</p>
            <p><span class="math display">\[P(y_i | x_i) =
            \hat{y}_i^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}\]</span></p>
            <p>This elegant formula works because:</p>
            <ul>
            <li>If <span class="math inline">\(y_i = 1\)</span>: <span
            class="math inline">\(P = \hat{y}_i^1 \cdot (1 -
            \hat{y}_i)^0 = \hat{y}_i\)</span> ‚úì</li>
            <li>If <span class="math inline">\(y_i = 0\)</span>: <span
            class="math inline">\(P = \hat{y}_i^0 \cdot (1 -
            \hat{y}_i)^1 = 1 - \hat{y}_i\)</span> ‚úì</li>
            </ul>
            <p><strong>Step 3: Write the likelihood of all data
            (assuming independence)</strong></p>
            <p><span class="math display">\[\mathcal{L}(w, b) =
            \prod_{i=1}^{N} P(y_i | x_i) = \prod_{i=1}^{N}
            \hat{y}_i^{y_i} \cdot (1 - \hat{y}_i)^{1 - y_i}\]</span></p>
            <p><strong>Step 4: Take the log
            (log-likelihood)</strong></p>
            <p>Products are numerically unstable and hard to optimize.
            Taking the log converts products to sums:</p>
            <p><span class="math display">\[\log \mathcal{L} =
            \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1
            - \hat{y}_i) \right]\]</span></p>
            <p><strong>Step 5: Maximize log-likelihood = Minimize
            negative log-likelihood</strong></p>
            <p>We want to <em>maximize</em> the likelihood, but
            optimizers <em>minimize</em>. So we minimize the
            <strong>negative log-likelihood (NLL)</strong>:</p>
            <p><span class="math display">\[\text{NLL} = -\log
            \mathcal{L} = -\sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) +
            (1 - y_i) \log(1 - \hat{y}_i) \right]\]</span></p>
            <p>Dividing by <span class="math inline">\(N\)</span> for
            the average gives us exactly the <strong>binary
            cross-entropy loss</strong>!</p>
            <p><span class="math display">\[\boxed{\text{BCE} =
            -\frac{1}{N}\sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 -
            y_i) \log(1 - \hat{y}_i) \right]}\]</span></p>
            <p><strong>Key insight</strong>: Cross-entropy is the
            <em>principled</em> loss for classification because it
            directly maximizes the probability of the correct labels
            under our model. This connection to MLE also explains:</p>
            <ul>
            <li>Why it gives clean gradients: <span
            class="math inline">\(\frac{\partial \text{BCE}}{\partial z}
            = \hat{y} - y\)</span></li>
            <li>Why it‚Äôs related to information theory: <span
            class="math inline">\(\text{BCE}(p, q) = H(p) + D_{KL}(p ||
            q)\)</span></li>
            <li>Why it‚Äôs the natural choice for probabilistic
            outputs</li>
            </ul>
            <h3 id="gradient-for-logistic-regression">Gradient for
            Logistic Regression</h3>
            <p>The gradient has a beautiful form:</p>
            <p><span class="math display">\[
            \frac{\partial \text{Loss}}{\partial w} = \frac{1}{N}
            \sum_{i=1}^{N} (\hat{y}_i - y_i) x_i
            \]</span></p>
            <p>Just the error times the input ‚Äî same form as linear
            regression!</p>
            <blockquote>
            <p><strong>Note</strong>: This gradient computes the
            <strong>average</strong> over all examples in the batch:
            <code>dw = X.T @ error / len(y)</code>. The division by
            <code>len(y)</code> ensures the gradient magnitude doesn‚Äôt
            depend on batch size.</p>
            </blockquote>
            <h3 id="python-implementation-1">Python Implementation</h3>
            <div class="sourceCode" id="cb4"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Data: [contains_FREE, contains_!, has_link]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>]])</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.zeros(<span class="dv">3</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> X <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> sigmoid(z)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss (cross-entropy)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-15</span>  <span class="co"># avoid log(0)</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="op">-</span>np.mean(y <span class="op">*</span> np.log(y_pred <span class="op">+</span> eps) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_pred <span class="op">+</span> eps))</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradients</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> y_pred <span class="op">-</span> y</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> X.T <span class="op">@</span> error <span class="op">/</span> <span class="bu">len</span>(y)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> np.mean(error)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> dw</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> b <span class="op">-</span> lr <span class="op">*</span> db</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> (y_pred <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        accuracy <span class="op">=</span> np.mean(predictions <span class="op">==</span> y)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: loss=</span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">, accuracy=</span><span class="sc">{</span>accuracy<span class="sc">:.0%}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Final: w=[1.23, 0.41, 0.41], b=-0.82, accuracy=100%</span></span></code></pre></div>
            <hr />
            <h2 id="multi-layer-perceptron-mlp">1.5 Multi-Layer
            Perceptron (MLP)</h2>
            <h3 id="limitation-of-linear-models">Limitation of Linear
            Models</h3>
            <p>Linear and logistic regression can only learn
            <strong>linear decision boundaries</strong>:</p>
            <pre><code>Linearly separable:         NOT separable (XOR):

  x‚ÇÇ‚Üë                         x‚ÇÇ‚Üë
    |  + + +                    |  -     +
    | + + +                     |
----+--------‚Üí x‚ÇÅ          -----+-----‚Üí x‚ÇÅ
    | - - -                     |
    |  - -                      |  +     -</code></pre>
            <p>To learn complex patterns, we need
            <strong>non-linear</strong> models.</p>
            <h3 id="the-mlp-stacking-layers">The MLP: Stacking
            Layers</h3>
            <p>An MLP adds <strong>hidden layers</strong> between input
            and output:</p>
            <p><img src="figures/mlp_architecture.png"
            alt="MLP Architecture" /> <em>Figure: Multi-layer perceptron
            with input layer, hidden layer, and output layer. Each
            connection represents a weight.</em></p>
            <h3 id="forward-pass-step-by-step">Forward Pass (Step by
            Step)</h3>
            <p>For a network with one hidden layer:</p>
            <p><strong>Layer 1</strong>: Linear transformation +
            activation <span class="math display">\[
            z_1 = W_1 x + b_1
            \]</span></p>
            <p><span class="math display">\[
            h = \text{ReLU}(z_1) = \max(0, z_1)
            \]</span></p>
            <p><strong>Layer 2</strong>: Another linear transformation
            <span class="math display">\[
            z_2 = W_2 h + b_2
            \]</span></p>
            <p><span class="math display">\[
            \hat{y} = \text{softmax}(z_2) \quad \text{(for
            classification)}
            \]</span></p>
            <h3 id="activation-functions-why-we-need-them">Activation
            Functions: Why We Need Them</h3>
            <p>Without activation functions, stacking linear layers is
            useless:</p>
            <p><span class="math display">\[
            W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + W_2 b_1 + b_2 = W&#39;
            x + b&#39;
            \]</span></p>
            <p>Still linear! Activation functions introduce
            <strong>non-linearity</strong>.</p>
            <h3 id="common-activation-functions">Common Activation
            Functions</h3>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 25%" />
            <col style="width: 19%" />
            <col style="width: 27%" />
            </colgroup>
            <thead>
            <tr>
            <th>Function</th>
            <th>Formula</th>
            <th>Range</th>
            <th>Used For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Sigmoid</td>
            <td><span
            class="math inline">\(\frac{1}{1+e^{-x}}\)</span></td>
            <td><span class="math inline">\((0, 1)\)</span></td>
            <td>Binary classification output</td>
            </tr>
            <tr>
            <td>Tanh</td>
            <td><span class="math inline">\(\frac{e^x - e^{-x}}{e^x +
            e^{-x}}\)</span></td>
            <td><span class="math inline">\((-1, 1)\)</span></td>
            <td>Hidden layers (older)</td>
            </tr>
            <tr>
            <td>ReLU</td>
            <td><span class="math inline">\(\max(0, x)\)</span></td>
            <td><span class="math inline">\([0, \infty)\)</span></td>
            <td>Hidden layers (modern default)</td>
            </tr>
            <tr>
            <td>Softmax</td>
            <td><span class="math inline">\(\frac{e^{x_i}}{\sum_j
            e^{x_j}}\)</span></td>
            <td><span class="math inline">\((0, 1)\)</span>, sums to
            1</td>
            <td>Multi-class output</td>
            </tr>
            </tbody>
            </table>
            <p><img src="figures/activation_functions.png"
            alt="Activation Functions" /> <em>Figure: Common activation
            functions including ReLU, Sigmoid, Tanh, and their
            variants.</em></p>
            <h3 id="relu-why-it-works">ReLU: Why It Works</h3>
            <ul>
            <li><strong>Simple</strong>: Fast to compute</li>
            <li><strong>Sparse</strong>: Many neurons output 0
            (efficient)</li>
            <li><strong>No vanishing gradient</strong>: Gradient is 1
            for positive inputs</li>
            </ul>
            <h3 id="concrete-example-learning-xor">Concrete Example:
            Learning XOR</h3>
            <p>XOR function:</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(x_1\)</span></th>
            <th><span class="math inline">\(x_2\)</span></th>
            <th>XOR</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            </tr>
            <tr>
            <td>0</td>
            <td>1</td>
            <td>1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0</td>
            <td>1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>1</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>No line can separate 0s from 1s!</p>
            <h3 id="mlp-solution">MLP Solution</h3>
            <p>Hidden layer (2 neurons): <span class="math display">\[
            h_1 = \text{ReLU}(x_1 + x_2 - 0.5) \quad \text{(detects
            &quot;at least one 1&quot;)}
            \]</span></p>
            <p><span class="math display">\[
            h_2 = \text{ReLU}(x_1 + x_2 - 1.5) \quad \text{(detects
            &quot;both are 1&quot;)}
            \]</span></p>
            <p>Output: <span class="math display">\[
            \hat{y} = h_1 - 2 \cdot h_2
            \]</span></p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(x_1\)</span></th>
            <th><span class="math inline">\(x_2\)</span></th>
            <th><span class="math inline">\(h_1\)</span></th>
            <th><span class="math inline">\(h_2\)</span></th>
            <th><span class="math inline">\(\hat{y}\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            </tr>
            <tr>
            <td>0</td>
            <td>1</td>
            <td>0.5</td>
            <td>0</td>
            <td>0.5 ‚Üí 1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0</td>
            <td>0.5</td>
            <td>0</td>
            <td>0.5 ‚Üí 1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>1</td>
            <td>1</td>
            <td>0.5</td>
            <td>0 ‚Üí 0</td>
            </tr>
            </tbody>
            </table>
            <p>The hidden layer creates a <strong>new
            representation</strong> where the problem becomes linearly
            separable!</p>
            <p><img src="figures/xor_transformation.png"
            alt="XOR Transformation" /> <em>Figure: The power of hidden
            layers. Left: In the original input space, XOR is NOT
            linearly separable ‚Äî no single line can separate the red
            class (0) from the blue class (1). Middle: The hidden layer
            computes new features <span
            class="math inline">\(h_1\)</span> (‚Äúat least one 1‚Äù) and
            <span class="math inline">\(h_2\)</span> (‚Äúboth are 1‚Äù).
            Right: In the transformed space, the problem becomes
            linearly separable! This is the key insight behind deep
            learning: each layer transforms data into more useful
            representations.</em></p>
            <h3 id="python-implementation-2">Python Implementation</h3>
            <div class="sourceCode" id="cb6"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_derivative(x):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">&gt;</span> <span class="dv">0</span>).astype(<span class="bu">float</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># XOR data</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Network: 2 inputs ‚Üí 4 hidden ‚Üí 1 output</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">4</span>) <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">4</span>))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.5</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> X <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> relu(z1)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> sigmoid(z2)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean((y <span class="op">-</span> y_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass (will explain in next section!)</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    d_z2 <span class="op">=</span> (y_pred <span class="op">-</span> y) <span class="op">*</span> y_pred <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> y_pred)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    d_W2 <span class="op">=</span> h.T <span class="op">@</span> d_z2</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    d_b2 <span class="op">=</span> np.<span class="bu">sum</span>(d_z2, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    d_h <span class="op">=</span> d_z2 <span class="op">@</span> W2.T</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    d_z1 <span class="op">=</span> d_h <span class="op">*</span> relu_derivative(z1)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    d_W1 <span class="op">=</span> X.T <span class="op">@</span> d_z1</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    d_b1 <span class="op">=</span> np.<span class="bu">sum</span>(d_z1, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">-=</span> lr <span class="op">*</span> d_W2</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">-=</span> lr <span class="op">*</span> d_b2</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">-=</span> lr <span class="op">*</span> d_W1</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">-=</span> lr <span class="op">*</span> d_b1</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: loss=</span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Predictions:&quot;</span>)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.<span class="bu">round</span>(y_pred, <span class="dv">2</span>))</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="co"># [[0.02], [0.98], [0.98], [0.02]] ‚úì</span></span></code></pre></div>
            <hr />
            <h2 id="the-backpropagation-algorithm">1.6 The
            Backpropagation Algorithm</h2>
            <h3
            id="the-problem-how-to-get-gradients-in-deep-networks">The
            Problem: How to Get Gradients in Deep Networks?</h3>
            <p>For linear regression: straightforward calculus.</p>
            <p>For deep networks with millions of parameters: we need an
            efficient algorithm.</p>
            <p><strong>Backpropagation</strong> =
            <strong>Back</strong>ward <strong>propagation</strong> of
            errors</p>
            <h3 id="the-chain-rule-the-key-insight">The Chain Rule: The
            Key Insight</h3>
            <p>If <span class="math inline">\(y = f(g(x))\)</span>,
            then:</p>
            <p><span class="math display">\[
            \frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}
            \]</span></p>
            <p>In neural networks, we have a <strong>chain</strong> of
            operations:</p>
            <p><span class="math display">\[
            x \to z_1 \to h \to z_2 \to \hat{y} \to \text{Loss}
            \]</span></p>
            <p>To find <span class="math inline">\(\frac{\partial
            \text{Loss}}{\partial W_1}\)</span>, we apply the chain rule
            through each step.</p>
            <h3
            id="visualizing-backpropagation-the-computational-graph">Visualizing
            Backpropagation: The Computational Graph</h3>
            <p>A <strong>computational graph</strong> makes
            backpropagation intuitive. Each node represents either a
            variable (data) or an operation. Edges show data flow.</p>
            <p><img src="figures/computational_graph.png"
            alt="Computational Graph" /> <em>Figure: A computational
            graph for a 2-layer MLP. Blue arrows show forward pass (data
            flowing left to right). Red arrows show backward pass
            (gradients flowing right to left). Each operation node
            computes a ‚Äúlocal gradient‚Äù that gets multiplied along the
            path.</em></p>
            <p><strong>Key insight</strong>: During backprop, we
            traverse the same graph in reverse, multiplying local
            gradients along each path.</p>
            <p><strong>The algorithm</strong>:</p>
            <ol type="1">
            <li><strong>Forward pass</strong>: Compute all intermediate
            values, storing them for later</li>
            <li><strong>Backward pass</strong>: Starting from the loss,
            compute gradients by:
            <ul>
            <li>For each operation node, multiply the incoming gradient
            by the local gradient</li>
            <li>Sum gradients when paths merge (multiple outputs from
            one node)</li>
            </ul></li>
            </ol>
            <p><strong>Example with specific values</strong>:</p>
            <pre><code>Forward: x=2 ‚Üí [√óW‚ÇÅ=3] ‚Üí z‚ÇÅ=6 ‚Üí [ReLU] ‚Üí h=6 ‚Üí [√óW‚ÇÇ=0.5] ‚Üí z‚ÇÇ=3 ‚Üí [œÉ] ‚Üí ≈∑=0.95

If y=1:  Loss = -log(0.95) = 0.05

Backward:
  ‚àÇL/‚àÇ≈∑ = -1/0.95 = -1.05
  ‚àÇL/‚àÇz‚ÇÇ = -1.05 √ó œÉ&#39;(3) = -1.05 √ó 0.95 √ó 0.05 = -0.05
  ‚àÇL/‚àÇW‚ÇÇ = -0.05 √ó h = -0.05 √ó 6 = -0.30
  ‚àÇL/‚àÇh  = -0.05 √ó W‚ÇÇ = -0.05 √ó 0.5 = -0.025
  ‚àÇL/‚àÇz‚ÇÅ = -0.025 √ó 1 (ReLU&#39;=1 since z‚ÇÅ&gt;0) = -0.025
  ‚àÇL/‚àÇW‚ÇÅ = -0.025 √ó x = -0.025 √ó 2 = -0.05</code></pre>
            <p><strong>Why computational graphs matter</strong>: -
            <strong>Automatic differentiation</strong> frameworks
            (PyTorch, TensorFlow) build these graphs automatically - Any
            differentiable computation can be expressed as a graph -
            Backprop becomes mechanical: just apply the chain rule at
            each node</p>
            <h3 id="backprop-step-by-step">Backprop: Step by Step</h3>
            <p>Consider our 2-layer MLP:</p>
            <p><strong>Forward pass</strong> (compute and cache
            everything):</p>
            <pre><code>z‚ÇÅ = W‚ÇÅx + b‚ÇÅ
h = ReLU(z‚ÇÅ)
z‚ÇÇ = W‚ÇÇh + b‚ÇÇ
≈∑ = sigmoid(z‚ÇÇ)
L = MSE(y, ≈∑)</code></pre>
            <p><strong>Backward pass</strong> (work backwards from
            loss):</p>
            <ol type="1">
            <li><p><strong>Gradient of loss w.r.t. output</strong>:
            <span class="math display">\[
            \frac{\partial L}{\partial \hat{y}} = \hat{y} - y
            \]</span></p></li>
            <li><p><strong>Through sigmoid</strong>: The sigmoid
            derivative has an elegant form. Starting from <span
            class="math inline">\(\sigma(z) =
            \frac{1}{1+e^{-z}}\)</span>:</p></li>
            </ol>
            <p><span class="math display">\[\sigma&#39;(z) =
            \frac{d}{dz}\frac{1}{1+e^{-z}} = \frac{e^{-z}}{(1+e^{-z})^2}
            = \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} =
            \sigma(z)(1-\sigma(z))\]</span></p>
            <p>Therefore: <span class="math display">\[
            \frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial
            \hat{y}} \cdot \sigma&#39;(z_2) = (\hat{y} - y) \cdot
            \hat{y}(1 - \hat{y})
            \]</span></p>
            <ol start="3" type="1">
            <li><p><strong>Gradient for <span
            class="math inline">\(W_2\)</span></strong>: <span
            class="math display">\[
            \frac{\partial L}{\partial W_2} = h^\top \cdot
            \frac{\partial L}{\partial z_2}
            \]</span></p></li>
            <li><p><strong>Propagate to hidden layer</strong>: <span
            class="math display">\[
            \frac{\partial L}{\partial h} = \frac{\partial L}{\partial
            z_2} \cdot W_2^\top
            \]</span></p></li>
            <li><p><strong>Through ReLU</strong>: <span
            class="math display">\[
            \frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial
            h} \cdot \mathbf{1}_{z_1 &gt; 0}
            \]</span></p></li>
            <li><p><strong>Gradient for <span
            class="math inline">\(W_1\)</span></strong>: <span
            class="math display">\[
            \frac{\partial L}{\partial W_1} = x^\top \cdot
            \frac{\partial L}{\partial z_1}
            \]</span></p></li>
            </ol>
            <h3 id="why-its-efficient">Why It‚Äôs Efficient</h3>
            <ul>
            <li><strong>Forward pass</strong>: <span
            class="math inline">\(O(\text{network size})\)</span> ‚Äî just
            matrix multiplications</li>
            <li><strong>Backward pass</strong>: <span
            class="math inline">\(O(\text{network size})\)</span> ‚Äî same
            complexity as forward!</li>
            <li><strong>Total</strong>: <span
            class="math inline">\(O(\text{network size})\)</span> per
            example</li>
            </ul>
            <p>Without backprop, computing each parameter‚Äôs gradient
            separately would be <span
            class="math inline">\(O((\text{network
            size})^2)\)</span>.</p>
            <h3 id="automatic-differentiation">Automatic
            Differentiation</h3>
            <p>Modern frameworks (PyTorch, TensorFlow) implement
            backprop automatically:</p>
            <div class="sourceCode" id="cb9"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define network</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">4</span>),</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">4</span>, <span class="dv">1</span>),</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid()</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(y_pred, y)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward (automatically computes all gradients!)</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: loss=</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
            <hr />
            <hr />
            <h2 id="softmax-and-multi-class-classification">1.7 Softmax
            and Multi-class Classification</h2>
            <h3 id="from-binary-to-multi-class">From Binary to
            Multi-class</h3>
            <p>Logistic regression handles 2 classes. For <strong>K
            classes</strong>, we use <strong>softmax</strong>:</p>
            <p><span class="math display">\[P(y = k | x) =
            \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}\]</span></p>
            <p>where <span class="math inline">\(z_k = w_k \cdot x +
            b_k\)</span> is the <strong>logit</strong> for class <span
            class="math inline">\(k\)</span>.</p>
            <h3 id="properties-of-softmax">Properties of Softmax</h3>
            <ol type="1">
            <li><strong>Outputs sum to 1</strong>: <span
            class="math inline">\(\sum_k P(y = k | x) = 1\)</span>
            ‚úì</li>
            <li><strong>All outputs positive</strong>: <span
            class="math inline">\(P(y = k | x) &gt; 0\)</span> ‚úì</li>
            <li><strong>Preserves ranking</strong>: Largest logit ‚Üí
            highest probability</li>
            </ol>
            <h3 id="why-exponential-not-simple-normalization">Why
            Exponential (Not Simple Normalization)?</h3>
            <p><strong>Interview Question</strong>: ‚ÄúWhy does softmax
            use <span class="math inline">\(e^{z_i}\)</span> instead of
            just <span class="math inline">\(\frac{z_i}{\sum_j
            z_j}\)</span>?‚Äù</p>
            <p>Simple normalization would be: <span
            class="math inline">\(\text{normalize}(z_i) =
            \frac{z_i}{\sum_j z_j}\)</span></p>
            <p>This fails for several critical reasons:</p>
            <p><strong>Problem 1: Negative Values Break
            Everything</strong></p>
            <pre><code>Logits: [2, -3, 1]
Sum = 0 ‚Üí Division by zero!

Logits: [2, -5, 1]  
Sum = -2
&quot;Probabilities&quot;: [2/-2, -5/-2, 1/-2] = [-1, 2.5, -0.5]
‚Üí Negative &quot;probabilities&quot;! Invalid!</code></pre>
            <p>Exponentials are <strong>always positive</strong>: <span
            class="math inline">\(e^x &gt; 0\)</span> for all <span
            class="math inline">\(x\)</span>, so softmax outputs are
            always valid probabilities.</p>
            <p><strong>Problem 2: Beautiful Gradient
            Properties</strong></p>
            <p>The softmax + cross-entropy gradient has an elegant
            form:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z_i} = p_i - y_i\]</span></p>
            <p>where <span class="math inline">\(p_i\)</span> is
            predicted probability and <span
            class="math inline">\(y_i\)</span> is the target (1 for
            correct class, 0 otherwise).</p>
            <p>This clean gradient comes directly from the exponential.
            Simple normalization would give messier, harder-to-optimize
            gradients.</p>
            <p><strong>Problem 3: Amplification with Temperature
            Control</strong></p>
            <p>Exponentials amplify differences between logits:</p>
            <pre><code>Logits: [2.0, 1.0, 0.5]

Simple normalize: [2/3.5, 1/3.5, 0.5/3.5] = [0.57, 0.29, 0.14]
Softmax:          [0.59, 0.24, 0.17]  # Winner amplified</code></pre>
            <p><strong>Temperature</strong> provides explicit control
            over this amplification:</p>
            <p><span class="math display">\[\text{softmax}(z_i / T) =
            \frac{e^{z_i/T}}{\sum_j e^{z_j/T}}\]</span></p>
            <ul>
            <li><span class="math inline">\(T \to 0\)</span>: Approaches
            argmax (one-hot, hard selection)</li>
            <li><span class="math inline">\(T = 1\)</span>: Standard
            softmax</li>
            <li><span class="math inline">\(T \to \infty\)</span>:
            Approaches uniform distribution</li>
            </ul>
            <p>This is used in knowledge distillation (soft targets) and
            sampling diversity control.</p>
            <p><strong>Problem 4: Theoretical Grounding (Maximum
            Entropy)</strong></p>
            <p>Softmax is the <strong>maximum entropy</strong>
            distribution subject to linear constraints on expected
            features. This connects to:</p>
            <ul>
            <li>Statistical mechanics (Boltzmann distribution)</li>
            <li>Information theory (exponential family
            distributions)</li>
            <li>Principled probabilistic modeling</li>
            </ul>
            <p><strong>Summary</strong>: Exponentials ensure:</p>
            <ul>
            <li>‚úÖ Always positive outputs (valid probabilities)</li>
            <li>‚úÖ Sum to 1 (proper distribution)<br />
            </li>
            <li>‚úÖ Clean gradients for efficient learning</li>
            <li>‚úÖ Controllable sharpness via temperature</li>
            <li>‚úÖ Theoretically principled (max entropy)</li>
            </ul>
            <h3 id="example-3-class-classification">Example: 3-Class
            Classification</h3>
            <pre><code>Input x ‚Üí [Linear Layer] ‚Üí Logits z ‚Üí [Softmax] ‚Üí Probabilities

z = [2.0, 1.0, 0.1]

softmax(z):
  e^2.0 = 7.39
  e^1.0 = 2.72  
  e^0.1 = 1.11
  sum = 11.22

  P = [7.39/11.22, 2.72/11.22, 1.11/11.22]
    = [0.66, 0.24, 0.10]</code></pre>
            <p><img src="figures/softmax_temperature.png"
            alt="Softmax Temperature Effects" /> <em>Figure: Effect of
            temperature on softmax distribution. Lower temperature makes
            the distribution sharper (more confident), higher
            temperature makes it more uniform.</em></p>
            <h3 id="multi-class-cross-entropy-loss">Multi-class
            Cross-Entropy Loss</h3>
            <p>For one-hot label <span class="math inline">\(y\)</span>
            (e.g., <span class="math inline">\(y = [0, 1, 0]\)</span>
            for class 2):</p>
            <p><span class="math display">\[\mathcal{L} =
            -\sum_{k=1}^{K} y_k \log(\hat{y}_k) =
            -\log(\hat{y}_c)\]</span></p>
            <p>where <span class="math inline">\(c\)</span> is the true
            class. We‚Äôre penalizing low probability on the correct
            class.</p>
            <h3 id="pytorch-implementation">PyTorch Implementation</h3>
            <div class="sourceCode" id="cb13"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 1: Separate softmax and loss</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">1</span>)(logits)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.NLLLoss()(torch.log(probs), labels)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 2: Combined (numerically stable, preferred!)</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)  <span class="co"># Takes raw logits!</span></span></code></pre></div>
            <p>‚ö†Ô∏è <strong>Common mistake</strong>:
            <code>CrossEntropyLoss</code> expects
            <strong>logits</strong>, not probabilities!</p>
            <hr />
            <h2 id="batching-why-and-how">1.8 Batching: Why and How</h2>
            <h3 id="what-this-means-for-beginners">What This Means (For
            Beginners)</h3>
            <p>When training a neural network, three terms come up
            constantly: <strong>Epoch</strong>, <strong>Batch</strong>,
            and <strong>Iteration</strong>. Understanding how they
            relate is fundamental.</p>
            <p><strong>Epoch</strong> = One complete pass through the
            <strong>entire</strong> training dataset</p>
            <p>Think of studying for an exam: - <strong>1
            epoch</strong>: You go through the entire syllabus once -
            <strong>5 epochs</strong>: You study the same syllabus five
            times, each time understanding more</p>
            <p><strong>Batch</strong> = A <strong>subset</strong> of the
            training data processed together</p>
            <p>Since the entire dataset is often too large to fit in GPU
            memory at once, we divide it into smaller chunks called
            batches. Each batch is processed in one forward pass and one
            backward pass.</p>
            <p><strong>Iteration</strong> = One forward + backward pass
            on <strong>one batch</strong></p>
            <p>Each iteration updates the model‚Äôs weights once.</p>
            <h3 id="the-key-formula">The Key Formula</h3>
            <p><span class="math display">\[\text{Iterations per epoch}
            = \frac{\text{Total training samples}}{\text{Batch
            size}}\]</span></p>
            <h3 id="worked-example">Worked Example</h3>
            <p>Suppose you have <strong>1,000 training samples</strong>
            and set <strong>batch size = 100</strong>:</p>
            <pre><code>Total samples:     1,000
Batch size:        100
Iterations/epoch:  1,000 / 100 = 10

What happens in 1 epoch:
  Iteration 1:  Process samples 1-100    ‚Üí Update weights
  Iteration 2:  Process samples 101-200  ‚Üí Update weights
  Iteration 3:  Process samples 201-300  ‚Üí Update weights
  ...
  Iteration 10: Process samples 901-1000 ‚Üí Update weights

After 10 iterations, you&#39;ve completed 1 epoch!</code></pre>
            <p><strong>For 3 epochs with batch_size=100 on 1,000
            samples:</strong> - Total iterations = 3 epochs √ó 10
            iterations/epoch = <strong>30 weight updates</strong> - Each
            sample is seen <strong>3 times</strong> total (once per
            epoch)</p>
            <h3 id="visual-summary">Visual Summary</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FULL DATASET (1000 samples)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Batch 1 ‚îÇ Batch 2 ‚îÇ Batch 3 ‚îÇ Batch 4 ‚îÇ    ...    ‚îÇ  Batch 10  ‚îÇ
‚îÇ (100)   ‚îÇ (100)   ‚îÇ (100)   ‚îÇ (100)   ‚îÇ           ‚îÇ   (100)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì         ‚Üì         ‚Üì         ‚Üì                      ‚Üì
  Iter 1    Iter 2    Iter 3    Iter 4       ...      Iter 10
     
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                      = 1 EPOCH</code></pre>
            <h3 id="the-pizza-analogy">The Pizza Analogy üçï</h3>
            <ul>
            <li><strong>The entire pizza</strong> = Full training
            dataset</li>
            <li><strong>Each slice</strong> = One batch</li>
            <li><strong>Eating one slice</strong> = One iteration
            (process one batch, update weights)</li>
            <li><strong>Finishing the whole pizza</strong> = One epoch
            (processed all samples once)</li>
            </ul>
            <p>If the pizza has 8 slices and you eat 1 slice at a time:
            - You need <strong>8 iterations</strong> to finish 1 pizza
            (1 epoch) - Training for 3 epochs = eating 3 pizzas = 24
            slices eaten = 24 iterations</p>
            <h3 id="why-multiple-epochs">Why Multiple Epochs?</h3>
            <p>Training for a single epoch usually isn‚Äôt enough: - The
            model makes one pass through each sample - Weights are
            updated based on each batch, but may not have converged - By
            repeating for multiple epochs, the model <strong>gradually
            refines</strong> its understanding</p>
            <pre><code>Epoch 1: Model learns rough patterns
Epoch 2: Model refines understanding  
Epoch 3: Model fine-tunes details
...
Epoch N: Model converges (loss stops decreasing)</code></pre>
            <p><strong>Warning</strong>: Too many epochs can lead to
            <strong>overfitting</strong>! The model memorizes the
            training data instead of learning generalizable patterns.
            This is why we monitor validation loss.</p>
            <h3
            id="interview-q-whats-the-difference-between-epoch-batch-and-iteration">Interview
            Q: ‚ÄúWhat‚Äôs the difference between epoch, batch, and
            iteration?‚Äù</h3>
            <p><strong>A</strong>: An <strong>epoch</strong> is one
            complete pass through the entire training dataset. A
            <strong>batch</strong> is a subset of the data processed
            together in one forward/backward pass. An
            <strong>iteration</strong> is one weight update, which
            happens after processing one batch.</p>
            <p>The relationship:
            <code>Iterations per epoch = Dataset size / Batch size</code></p>
            <p>For example, with 10,000 samples and batch size 100, you
            have 100 iterations per epoch. Training for 5 epochs means
            500 total weight updates, with each sample seen 5 times.</p>
            <hr />
            <h3 id="the-problem-with-single-examples">The Problem with
            Single Examples</h3>
            <p>Computing gradient on one example at a time:</p>
            <ul>
            <li>Very noisy updates</li>
            <li>Can‚Äôt utilize GPU parallelism</li>
            <li>Slow convergence</li>
            </ul>
            <p>Computing gradient on entire dataset:</p>
            <ul>
            <li>Very stable but slow</li>
            <li>One update per epoch</li>
            <li>Memory can‚Äôt hold all data</li>
            </ul>
            <h3 id="mini-batch-the-sweet-spot">Mini-batch: The Sweet
            Spot</h3>
            <pre><code>Dataset: 10,000 examples
Batch size: 32

‚Üí 10,000 / 32 = 312 batches per epoch
‚Üí 312 gradient updates per epoch</code></pre>
            <h3 id="why-batching-works">Why Batching Works</h3>
            <p>The mini-batch gradient is an <strong>unbiased
            estimator</strong> of the full gradient:</p>
            <p><span
            class="math display">\[\mathbb{E}\left[\frac{1}{B}\sum_{i
            \in \text{batch}} \nabla \ell_i\right] =
            \frac{1}{N}\sum_{i=1}^{N} \nabla \ell_i\]</span></p>
            <h3 id="batch-size-tradeoffs">Batch Size Tradeoffs</h3>
            <table>
            <thead>
            <tr>
            <th>Small Batch (32)</th>
            <th>Large Batch (4096)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>High variance (noisy)</td>
            <td>Low variance (stable)</td>
            </tr>
            <tr>
            <td>Good generalization</td>
            <td>May generalize worse</td>
            </tr>
            <tr>
            <td>More updates per epoch</td>
            <td>Fewer updates</td>
            </tr>
            <tr>
            <td>Slower per update</td>
            <td>Faster per update (GPU)</td>
            </tr>
            <tr>
            <td>Works on any GPU</td>
            <td>Needs large GPU memory</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-noise-is-good-insight">The ‚ÄúNoise is Good‚Äù
            Insight</h3>
            <p>Small batch noise acts as <strong>implicit
            regularization</strong>:</p>
            <ul>
            <li>Helps escape sharp minima (which generalize poorly)</li>
            <li>Finds flatter minima (which generalize better)</li>
            </ul>
            <p><img src="figures/sharp_flat_minima.png"
            alt="Sharp vs Flat Minima" /> <em>Figure: Sharp minima
            (left) overfit because small weight changes cause large loss
            changes. Flat minima (right) generalize better because
            they‚Äôre robust to perturbations.</em></p>
            <h3 id="practical-guidelines">Practical Guidelines</h3>
            <table>
            <thead>
            <tr>
            <th>Dataset Size</th>
            <th>Typical Batch Size</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>&lt; 1,000</td>
            <td>8-32</td>
            </tr>
            <tr>
            <td>1,000-100,000</td>
            <td>32-128</td>
            </tr>
            <tr>
            <td>&gt; 100,000</td>
            <td>128-512</td>
            </tr>
            <tr>
            <td>LLM pretraining</td>
            <td>1M-4M tokens</td>
            </tr>
            </tbody>
            </table>
            <h3 id="code-example-batching-in-pytorch">Code Example:
            Batching in PyTorch</h3>
            <div class="sourceCode" id="cb18"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset and loader</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> TensorDataset(X_tensor, y_tensor)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_X, batch_y <span class="kw">in</span> loader:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass on batch</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model(batch_X)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(predictions, batch_y)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div>
            <hr />
            <h2 id="data-preprocessing">1.9 Data Preprocessing</h2>
            <h3 id="why-preprocess">Why Preprocess?</h3>
            <p>Raw features often have different scales:</p>
            <ul>
            <li>Age: 0-100</li>
            <li>Salary: 20,000-500,000</li>
            <li>Height: 1.5-2.0 meters</li>
            </ul>
            <p>Without preprocessing:</p>
            <ul>
            <li>Large-scale features dominate gradients</li>
            <li>Small learning rates needed for some features</li>
            <li>Optimization is slow and unstable</li>
            </ul>
            <h3
            id="standardization-z-score-normalization">Standardization
            (Z-score Normalization)</h3>
            <p><span class="math display">\[x&#39; = \frac{x -
            \mu}{\sigma}\]</span></p>
            <p><strong>Result</strong>: Mean = 0, Std = 1</p>
            <div class="sourceCode" id="cb19"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training set</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> X_train.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> X_train.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to all sets using TRAINING statistics!</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> (X_train <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> (X_test <span class="op">-</span> mean) <span class="op">/</span> std  <span class="co"># Same mean/std!</span></span></code></pre></div>
            <p>‚ö†Ô∏è <strong>Critical</strong>: Always compute statistics
            on training data only, then apply to test data!</p>
            <h3 id="min-max-normalization">Min-Max Normalization</h3>
            <p><span class="math display">\[x&#39; = \frac{x -
            x_{\min}}{x_{\max} - x_{\min}}\]</span></p>
            <p><strong>Result</strong>: Range [0, 1]</p>
            <p><img src="figures/preprocessing_before_after.png"
            alt="Data Preprocessing" /> <em>Figure: Why preprocessing
            matters. Left: Raw features at vastly different scales ‚Äî
            salary dominates (10,000√ó larger than height). Middle: After
            standardization (z-score), all features have mean=0 and
            std=1. Right: After min-max normalization, all features are
            in [0, 1]. Without normalization, gradient descent would be
            dominated by large-scale features.</em></p>
            <h3 id="when-to-use-which">When to Use Which?</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Standardization</strong></td>
            <td>Most cases; neural networks; data has outliers</td>
            </tr>
            <tr>
            <td><strong>Min-Max</strong></td>
            <td>Need bounded range; image pixels (0-255 ‚Üí 0-1)</td>
            </tr>
            <tr>
            <td><strong>No preprocessing</strong></td>
            <td>Tree-based models (don‚Äôt need it)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="image-preprocessing">Image Preprocessing</h3>
            <div class="sourceCode" id="cb20"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Typical ImageNet preprocessing</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    transforms.Resize(<span class="dv">256</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    transforms.CenterCrop(<span class="dv">224</span>),</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),  <span class="co"># Converts to [0, 1]</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],  <span class="co"># ImageNet stats</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div>
            <h3 id="text-preprocessing">Text Preprocessing</h3>
            <ol type="1">
            <li><strong>Tokenization</strong>: ‚ÄúHello world‚Äù ‚Üí [‚ÄúHello‚Äù,
            ‚Äúworld‚Äù] ‚Üí [15496, 995]</li>
            <li><strong>Padding</strong>: Make all sequences same
            length</li>
            <li><strong>Embedding</strong>: Token IDs ‚Üí dense
            vectors</li>
            </ol>
            <hr />
            <h3 id="handling-missing-values">Handling Missing
            Values</h3>
            <p>Real-world datasets often have missing values.
            Understanding why data is missing guides how to handle
            it.</p>
            <p><strong>Types of Missingness</strong>:</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 15%" />
            <col style="width: 36%" />
            <col style="width: 23%" />
            <col style="width: 23%" />
            </colgroup>
            <thead>
            <tr>
            <th>Type</th>
            <th>Abbreviation</th>
            <th>Meaning</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Missing Completely at Random</strong></td>
            <td>MCAR</td>
            <td>Missingness is unrelated to any variable</td>
            <td>Survey respondent accidentally skipped a question</td>
            </tr>
            <tr>
            <td><strong>Missing at Random</strong></td>
            <td>MAR</td>
            <td>Missingness depends on observed variables</td>
            <td>Younger people less likely to report income (but we know
            their age)</td>
            </tr>
            <tr>
            <td><strong>Missing Not at Random</strong></td>
            <td>MNAR</td>
            <td>Missingness depends on the missing value itself</td>
            <td>High earners don‚Äôt report income <em>because</em> it‚Äôs
            high</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why It Matters</strong>: MCAR is ‚Äúsafe‚Äù ‚Äî any
            handling method works. MAR can be handled with imputation if
            you model the missingness. MNAR is problematic ‚Äî you can‚Äôt
            fully correct for it without additional information.</p>
            <p><strong>Handling Strategies</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 24%" />
            <col style="width: 19%" />
            <col style="width: 31%" />
            <col style="width: 24%" />
            </colgroup>
            <thead>
            <tr>
            <th>Strategy</th>
            <th>Method</th>
            <th>When to Use</th>
            <th>Drawback</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Deletion</strong></td>
            <td>Drop rows with missing values</td>
            <td>MCAR + few missing values (&lt;5%)</td>
            <td>Loses data, can bias if not MCAR</td>
            </tr>
            <tr>
            <td><strong>Mean/Median Imputation</strong></td>
            <td>Replace with column mean/median</td>
            <td>Numerical features, MCAR</td>
            <td>Reduces variance, ignores relationships</td>
            </tr>
            <tr>
            <td><strong>Mode Imputation</strong></td>
            <td>Replace with most frequent value</td>
            <td>Categorical features</td>
            <td>Over-represents common values</td>
            </tr>
            <tr>
            <td><strong>KNN Imputation</strong></td>
            <td>Use K-nearest neighbors to estimate</td>
            <td>MAR, when features are correlated</td>
            <td>Computationally expensive</td>
            </tr>
            <tr>
            <td><strong>Model-based</strong></td>
            <td>Train model to predict missing values</td>
            <td>MAR, large datasets</td>
            <td>Can propagate errors</td>
            </tr>
            <tr>
            <td><strong>Indicator Variable</strong></td>
            <td>Add binary ‚Äúwas_missing‚Äù column</td>
            <td>When missingness itself is informative</td>
            <td>Increases dimensionality</td>
            </tr>
            </tbody>
            </table>
            <blockquote>
            <p><strong>‚ö†Ô∏è Important: Imputation
            Considerations</strong></p>
            <p><strong>Imputation</strong> means filling in missing
            values with estimated values. Key points:</p>
            <ol type="1">
            <li><strong>Imputation introduces bias</strong>: The imputed
            values are estimates, not real data. They reduce variance
            and can make relationships appear stronger than they
            are.</li>
            <li><strong>Never impute the target variable</strong>: If
            your label/outcome is missing, that sample should typically
            be excluded, not imputed.</li>
            <li><strong>Fit imputer on training data only</strong>: Just
            like with normalization, compute imputation statistics
            (mean, median, etc.) on training data, then apply to
            validation/test sets. This prevents <strong>data
            leakage</strong>.</li>
            <li><strong>Consider multiple imputation</strong>: For
            statistical inference, advanced techniques like MICE
            (Multiple Imputation by Chained Equations) account for
            uncertainty in imputed values.</li>
            <li><strong>Document your approach</strong>: Always report
            what percentage of data was missing and how you handled
            it‚Äîthis affects reproducibility.</li>
            </ol>
            </blockquote>
            <p><strong>Python Examples</strong>:</p>
            <div class="sourceCode" id="cb21"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer, KNNImputer</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data with missing values</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;age&#39;</span>: [<span class="dv">25</span>, <span class="dv">30</span>, np.nan, <span class="dv">45</span>, <span class="dv">50</span>],</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;income&#39;</span>: [<span class="dv">50000</span>, np.nan, <span class="dv">70000</span>, <span class="dv">80000</span>, np.nan],</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;category&#39;</span>: [<span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>, np.nan, <span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 1: Drop rows with any missing values</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>df_dropped <span class="op">=</span> df.dropna()  <span class="co"># 2 rows remain</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 2: Mean imputation for numerical columns</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>mean_imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">&#39;mean&#39;</span>)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;age_imputed&#39;</span>] <span class="op">=</span> mean_imputer.fit_transform(df[[<span class="st">&#39;age&#39;</span>]])</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 3: Median imputation (robust to outliers)</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>median_imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">&#39;median&#39;</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;income_imputed&#39;</span>] <span class="op">=</span> median_imputer.fit_transform(df[[<span class="st">&#39;income&#39;</span>]])</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 4: Mode imputation for categorical</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>mode_imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">&#39;most_frequent&#39;</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;category_imputed&#39;</span>] <span class="op">=</span> mode_imputer.fit_transform(df[[<span class="st">&#39;category&#39;</span>]])</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 5: KNN imputation (considers feature relationships)</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>knn_imputer <span class="op">=</span> KNNImputer(n_neighbors<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>numerical_cols <span class="op">=</span> df[[<span class="st">&#39;age&#39;</span>, <span class="st">&#39;income&#39;</span>]].values</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>df_knn <span class="op">=</span> pd.DataFrame(knn_imputer.fit_transform(numerical_cols),</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>                      columns<span class="op">=</span>[<span class="st">&#39;age_knn&#39;</span>, <span class="st">&#39;income_knn&#39;</span>])</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Method 6: Add missingness indicator</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;income_was_missing&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;income&#39;</span>].isna().astype(<span class="bu">int</span>)</span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúHow would you handle
            missing values in a dataset?‚Äù</p>
            <p><strong>A</strong>: First, I‚Äôd analyze the missingness
            pattern to determine if it‚Äôs MCAR, MAR, or MNAR ‚Äî this
            guides the approach. For MCAR with few missing values
            (&lt;5%), simple deletion may work. For numerical features,
            I‚Äôd use median imputation (robust to outliers) or KNN
            imputation if features are correlated. For categorical
            features, mode imputation or a separate ‚ÄúUnknown‚Äù category.
            If missingness itself might be informative (e.g., people
            skip income questions intentionally), I‚Äôd add an indicator
            variable. For MAR in large datasets, model-based imputation
            (like using Random Forest to predict missing values) can
            capture complex relationships. I‚Äôd always validate by
            comparing model performance with different imputation
            strategies.</p>
            <hr />
            <h3 id="outlier-detection-and-handling">Outlier Detection
            and Handling</h3>
            <p><strong>Interview Q</strong>: ‚ÄúHow do you detect and
            handle outliers in your data? How do you make models robust
            to outliers?‚Äù</p>
            <p>Outliers are data points that are significantly different
            from other observations. They can be:</p>
            <ul>
            <li><strong>True outliers</strong>: Rare but valid data
            points (e.g., a billionaire in income data)</li>
            <li><strong>Errors</strong>: Data entry mistakes, sensor
            malfunctions, or corruption</li>
            </ul>
            <h4 id="detection-methods">Detection Methods</h4>
            <p><strong>1. Interquartile Range (IQR) Method
            (Box-Plot/Tukey‚Äôs Fences)</strong></p>
            <p>The most common statistical method, based on the
            interquartile range:</p>
            <pre><code>IQR = Q3 - Q1

Lower bound = Q1 - 1.5 √ó IQR
Upper bound = Q3 + 1.5 √ó IQR

Points outside these bounds ‚Üí Outliers</code></pre>
            <p><img src="figures/boxplot_anatomy.png"
            alt="Box-Plot Anatomy" /> <em>Figure: Anatomy of a box-plot
            (left) showing Q1, median, Q3, IQR, and whiskers. The IQR
            method (right) detects outliers as points beyond Q1 -
            1.5√óIQR or Q3 + 1.5√óIQR.</em></p>
            <div class="sourceCode" id="cb23"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_outliers_iqr(data):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Detect outliers using IQR method (1.5√óIQR rule)&quot;&quot;&quot;</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    Q1 <span class="op">=</span> np.percentile(data, <span class="dv">25</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    Q3 <span class="op">=</span> np.percentile(data, <span class="dv">75</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    IQR <span class="op">=</span> Q3 <span class="op">-</span> Q1</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    lower_bound <span class="op">=</span> Q1 <span class="op">-</span> <span class="fl">1.5</span> <span class="op">*</span> IQR</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    upper_bound <span class="op">=</span> Q3 <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> IQR</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    outliers <span class="op">=</span> (data <span class="op">&lt;</span> lower_bound) <span class="op">|</span> (data <span class="op">&gt;</span> upper_bound)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outliers, lower_bound, upper_bound</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">100</span>])  <span class="co"># 100 is outlier</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>is_outlier, lb, ub <span class="op">=</span> detect_outliers_iqr(data)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Outliers: </span><span class="sc">{</span>data[is_outlier]<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># [100]</span></span></code></pre></div>
            <p><strong>2. Z-Score Method</strong></p>
            <p>For normally distributed data, points far from the mean
            (typically &gt; 3œÉ) are outliers:</p>
            <p><span class="math display">\[z = \frac{x -
            \mu}{\sigma}\]</span></p>
            <div class="sourceCode" id="cb24"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_outliers_zscore(data, threshold<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Detect outliers using Z-score (points &gt; threshold std from mean)&quot;&quot;&quot;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> np.mean(data)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> np.std(data)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    z_scores <span class="op">=</span> np.<span class="bu">abs</span>((data <span class="op">-</span> mean) <span class="op">/</span> std)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z_scores <span class="op">&gt;</span> threshold</span></code></pre></div>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Best For</th>
            <th>Assumption</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>IQR</strong></td>
            <td>Any distribution, robust</td>
            <td>None (non-parametric)</td>
            </tr>
            <tr>
            <td><strong>Z-Score</strong></td>
            <td>Normal distribution</td>
            <td>Gaussian data</td>
            </tr>
            <tr>
            <td><strong>Modified Z-Score</strong></td>
            <td>Skewed data</td>
            <td>Uses median instead of mean</td>
            </tr>
            </tbody>
            </table>
            <p><strong>3. ML-Based Methods (High-Dimensional
            Data)</strong></p>
            <table>
            <colgroup>
            <col style="width: 22%" />
            <col style="width: 40%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>How It Works</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Isolation Forest</strong></td>
            <td>Isolates outliers with random splits</td>
            <td>High-dimensional, mixed features</td>
            </tr>
            <tr>
            <td><strong>DBSCAN</strong></td>
            <td>Points not in any cluster are outliers</td>
            <td>Spatial data, unknown # of clusters</td>
            </tr>
            <tr>
            <td><strong>LOF (Local Outlier Factor)</strong></td>
            <td>Compares local density to neighbors</td>
            <td>Varying density regions</td>
            </tr>
            <tr>
            <td><strong>Mahalanobis Distance</strong></td>
            <td>Accounts for feature correlations</td>
            <td>Multivariate, correlated features</td>
            </tr>
            <tr>
            <td><strong>Autoencoders</strong></td>
            <td>High reconstruction error = outlier</td>
            <td>Complex patterns, deep learning</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb25"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> IsolationForest</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Isolation Forest for high-dimensional data</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> IsolationForest(contamination<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>outlier_labels <span class="op">=</span> clf.fit_predict(X)  <span class="co"># -1 for outliers, 1 for inliers</span></span></code></pre></div>
            <h4 id="handling-strategies">Handling Strategies</h4>
            <p>Once outliers are detected, you have several options:</p>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 46%" />
            <col style="width: 17%" />
            </colgroup>
            <thead>
            <tr>
            <th>Strategy</th>
            <th>When to Use</th>
            <th>How</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Remove</strong></td>
            <td>True errors, data corruption</td>
            <td>Delete outlier rows</td>
            </tr>
            <tr>
            <td><strong>Cap/Winsorize</strong></td>
            <td>Keep data, limit influence</td>
            <td>Clip to percentiles (e.g., 1st/99th)</td>
            </tr>
            <tr>
            <td><strong>Transform</strong></td>
            <td>Reduce skewness</td>
            <td>Apply log, sqrt, Box-Cox</td>
            </tr>
            <tr>
            <td><strong>Impute</strong></td>
            <td>Treat as missing</td>
            <td>Replace with median/mode</td>
            </tr>
            <tr>
            <td><strong>Keep</strong></td>
            <td>True rare events</td>
            <td>Use robust methods</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb26"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Winsorization: Cap at percentiles</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> mstats</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> winsorize(data, limits<span class="op">=</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>)):</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Cap outliers at 1st and 99th percentiles&quot;&quot;&quot;</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mstats.winsorize(data, limits<span class="op">=</span>limits)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Log transform: Reduce right skew</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_transform(data):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.log1p(data)  <span class="co"># log(1+x) handles zeros</span></span></code></pre></div>
            <h4 id="making-models-robust-to-outliers">Making Models
            Robust to Outliers</h4>
            <p><strong>1. Use Robust Loss Functions</strong></p>
            <table>
            <thead>
            <tr>
            <th>Loss</th>
            <th>Robustness</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MSE</strong></td>
            <td>‚ùå Not robust</td>
            <td>Clean data, Gaussian errors</td>
            </tr>
            <tr>
            <td><strong>MAE</strong></td>
            <td>‚úÖ Robust</td>
            <td>Some outliers expected</td>
            </tr>
            <tr>
            <td><strong>Huber</strong></td>
            <td>‚úÖ Hybrid</td>
            <td>MSE for small errors, MAE for large</td>
            </tr>
            <tr>
            <td><strong>Quantile</strong></td>
            <td>‚úÖ Very robust</td>
            <td>Regression with heavy-tailed errors</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb27"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Huber loss: MSE for small errors, MAE for large</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.HuberLoss(delta<span class="op">=</span><span class="fl">1.0</span>)  <span class="co"># delta controls transition point</span></span></code></pre></div>
            <p><strong>2. Use Robust Scaling</strong></p>
            <div class="sourceCode" id="cb28"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> RobustScaler</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Uses median and IQR instead of mean and std</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Much less affected by outliers than StandardScaler</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> RobustScaler()  <span class="co"># (x - median) / IQR</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span></code></pre></div>
            <p><strong>3. Choose Robust Algorithms</strong></p>
            <table>
            <thead>
            <tr>
            <th>Robust</th>
            <th>Not Robust</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Tree-based (Random Forest, XGBoost)</td>
            <td>Linear Regression</td>
            </tr>
            <tr>
            <td>Median-based methods</td>
            <td>Mean-based methods</td>
            </tr>
            <tr>
            <td>Huber Regression</td>
            <td>OLS Regression</td>
            </tr>
            <tr>
            <td>k-Medoids</td>
            <td>k-Means</td>
            </tr>
            </tbody>
            </table>
            <p><strong>4. Regularization Helps</strong></p>
            <p>Regularization reduces variance and makes models less
            sensitive to individual points:</p>
            <ul>
            <li><strong>L2 (Ridge)</strong>: Shrinks weights, reduces
            influence of any single feature</li>
            <li><strong>Dropout</strong>: Random neuron dropping
            prevents over-reliance on specific patterns</li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúWhy are tree-based models
            robust to outliers?‚Äù</p>
            <p><strong>A</strong>: Tree-based models (Random Forest,
            XGBoost) split data based on <strong>thresholds</strong>,
            not magnitudes. A value of 100 vs 1,000,000 might fall in
            the same leaf node if they‚Äôre both above the split
            threshold. The prediction depends on which leaf the point
            lands in, not the exact value. This makes trees naturally
            robust to outliers‚Äîunlike linear models where outliers
            directly pull the regression line.</p>
            <h4 id="quick-decision-guide">Quick Decision Guide</h4>
            <pre><code>Outlier detected?
       ‚îÇ
       ‚îú‚îÄ‚îÄ Is it a data error?
       ‚îÇ       ‚îÇ
       ‚îÇ       ‚îú‚îÄ‚îÄ YES ‚Üí Remove or impute
       ‚îÇ       ‚îÇ
       ‚îÇ       ‚îî‚îÄ‚îÄ NO (rare but valid)
       ‚îÇ               ‚îÇ
       ‚îÇ               ‚îú‚îÄ‚îÄ Use robust methods (MAE, Huber, trees)
       ‚îÇ               ‚îî‚îÄ‚îÄ Or winsorize/cap if you must use non-robust methods
       ‚îÇ
       ‚îî‚îÄ‚îÄ Not sure?
               ‚îÇ
               ‚îî‚îÄ‚îÄ 1. Investigate the data point
                   2. Try both with/without
                   3. Use cross-validation to decide</code></pre>
            <hr />
            <h2 id="loss-function-comparison">1.10 Loss Function
            Comparison</h2>
            <p>The <strong>loss function</strong> (also called cost
            function or objective function) is arguably the most
            important design choice in machine learning‚Äîit defines
            exactly what ‚Äúgood‚Äù means for your model. During training,
            the optimizer‚Äôs sole job is to minimize this function, so
            your model will learn whatever behavior the loss function
            rewards.</p>
            <p><strong>Why does loss function choice matter so
            much?</strong></p>
            <ol type="1">
            <li><p><strong>Defines the learning signal</strong>: The
            gradients that update your weights come from the loss.
            Choose the wrong loss, and your model receives misleading
            gradients that don‚Äôt guide it toward the right
            solution.</p></li>
            <li><p><strong>Must match your task</strong>:</p>
            <ul>
            <li><strong>Regression</strong> (predict continuous values):
            Use MSE, MAE, or Huber</li>
            <li><strong>Classification</strong> (predict categories):
            Use Cross-Entropy (binary or categorical)</li>
            <li>Using MSE for classification can fail spectacularly‚Äîsee
            below!</li>
            </ul></li>
            <li><p><strong>Affects optimization dynamics</strong>: Some
            losses have better gradient properties than others.
            Cross-entropy gives clean gradients for classification; MSE
            with sigmoid can cause vanishing gradients.</p></li>
            </ol>
            <p><strong>How to think about it</strong>: The loss function
            is a contract between you and the optimizer. You specify
            ‚Äúminimize this number,‚Äù and the optimizer will find weights
            that do exactly that‚Äîeven if that‚Äôs not what you actually
            wanted. Choosing the right loss ensures that minimizing the
            number also means solving your actual problem.</p>
            <h3 id="quick-reference-table">Quick Reference Table</h3>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 12%" />
            <col style="width: 38%" />
            <col style="width: 18%" />
            </colgroup>
            <thead>
            <tr>
            <th>Loss Function</th>
            <th>Task</th>
            <th>Output Activation</th>
            <th>Formula</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MSE</strong></td>
            <td>Regression</td>
            <td>None (linear)</td>
            <td><span class="math inline">\(\frac{1}{N}\sum(y -
            \hat{y})^2\)</span></td>
            </tr>
            <tr>
            <td><strong>MAE</strong></td>
            <td>Regression</td>
            <td>None</td>
            <td><span class="math inline">\(\frac{1}{N}\sum|y -
            \hat{y}|\)</span></td>
            </tr>
            <tr>
            <td><strong>Binary CE</strong></td>
            <td>Binary classification</td>
            <td>Sigmoid</td>
            <td><span class="math inline">\(-[y\log\hat{y} +
            (1-y)\log(1-\hat{y})]\)</span></td>
            </tr>
            <tr>
            <td><strong>Categorical CE</strong></td>
            <td>Multi-class</td>
            <td>Softmax</td>
            <td><span class="math inline">\(-\sum_k y_k \log
            \hat{y}_k\)</span></td>
            </tr>
            <tr>
            <td><strong>Hinge</strong></td>
            <td>Binary (SVM)</td>
            <td>None</td>
            <td><span class="math inline">\(\max(0, 1 - y \cdot
            \hat{y})\)</span></td>
            </tr>
            </tbody>
            </table>
            <h3 id="mse-vs-cross-entropy-for-classification">MSE vs
            Cross-Entropy for Classification</h3>
            <p><strong>Why not MSE for classification?</strong></p>
            <p><img src="figures/cross_entropy_vs_mse.png"
            alt="Cross-Entropy vs MSE Loss" /> <em>Figure: Comparison of
            Cross-Entropy and MSE loss for classification. Cross-entropy
            penalizes confident wrong predictions much more
            severely.</em></p>
            <pre><code>True label: y = 1
Prediction: ≈∑ = 0.99 (very confident, correct)

MSE: (1 - 0.99)¬≤ = 0.0001
CE:  -log(0.99) = 0.01

Prediction: ≈∑ = 0.01 (very confident, WRONG!)

MSE: (1 - 0.01)¬≤ = 0.98
CE:  -log(0.01) = 4.6  ‚Üê Much stronger penalty!</code></pre>
            <p>Cross-entropy penalizes confident wrong predictions much
            more heavily!</p>
            <h3 id="mse-gradient-problem">MSE Gradient Problem</h3>
            <p>For sigmoid output with MSE: <span
            class="math display">\[\frac{\partial \text{MSE}}{\partial
            z} \propto \sigma(z)(1-\sigma(z))\]</span></p>
            <p>When <span class="math inline">\(\sigma(z) \approx
            0\)</span> or <span class="math inline">\(\sigma(z) \approx
            1\)</span>: gradient vanishes!</p>
            <p>For cross-entropy: <span
            class="math display">\[\frac{\partial \text{CE}}{\partial z}
            = \hat{y} - y\]</span></p>
            <p>No vanishing gradient! Clean, constant-scale updates.</p>
            <h3 id="huber-loss-best-of-both-worlds">Huber Loss: Best of
            Both Worlds</h3>
            <p><span class="math display">\[L_\delta(y, \hat{y}) =
            \begin{cases} \frac{1}{2}(y - \hat{y})^2 &amp; |y - \hat{y}|
            \leq \delta \\ \delta|y - \hat{y}| - \frac{1}{2}\delta^2
            &amp; |y - \hat{y}| &gt; \delta \end{cases}\]</span></p>
            <ul>
            <li>MSE for small errors (smooth)</li>
            <li>MAE for large errors (robust to outliers)</li>
            </ul>
            <hr />
            <h2 id="convolutional-neural-networks-cnns">1.11
            Convolutional Neural Networks (CNNs)</h2>
            <h3 id="the-problem-with-mlps-for-images">The Problem with
            MLPs for Images</h3>
            <p>A 224√ó224 RGB image has 224 √ó 224 √ó 3 = <strong>150,528
            input features</strong>.</p>
            <p>Fully connected layer with 1000 hidden units:</p>
            <ul>
            <li>150,528 √ó 1000 = <strong>150 million parameters</strong>
            in first layer alone!</li>
            <li>Doesn‚Äôt exploit spatial structure</li>
            <li>No translation invariance</li>
            </ul>
            <h3 id="the-convolution-operation">The Convolution
            Operation</h3>
            <p>A <strong>filter</strong> (kernel) slides across the
            image:</p>
            <pre><code>Input (5√ó5):              Filter (3√ó3):           Output (3√ó3):
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 0 ‚îÇ 1 ‚îÇ     ‚îÇ 1 ‚îÇ 0 ‚îÇ-1 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§     ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ 3 ‚îÇ 2 ‚îÇ     ‚îÇ 1 ‚îÇ 0 ‚îÇ-1 ‚îÇ         Slide filter,
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§  *  ‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§    ‚Üí    compute dot product
‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ     ‚îÇ 1 ‚îÇ 0 ‚îÇ-1 ‚îÇ         at each position
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§     ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
‚îÇ 2 ‚îÇ 1 ‚îÇ 0 ‚îÇ 1 ‚îÇ 2 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1 ‚îÇ 0 ‚îÇ 2 ‚îÇ 1 ‚îÇ 0 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

Example computation (top-left):
1√ó1 + 2√ó0 + 3√ó(-1) + 0√ó1 + 1√ó0 + 2√ó(-1) + 1√ó1 + 0√ó0 + 1√ó(-1) = 1-3-2+1-1 = -4</code></pre>
            <h3 id="why-convolutions-work">Why Convolutions Work</h3>
            <ol type="1">
            <li><strong>Parameter sharing</strong>: Same filter used
            everywhere ‚Üí fewer parameters</li>
            <li><strong>Local connectivity</strong>: Each output depends
            on small local region</li>
            <li><strong>Translation equivariance</strong>: Cat in corner
            detected same as cat in center</li>
            </ol>
            <h3 id="key-cnn-concepts">Key CNN Concepts</h3>
            <p><strong>Stride</strong>: How many pixels to move the
            filter each step</p>
            <ul>
            <li>Stride 1: Output ‚âà input size</li>
            <li>Stride 2: Output ‚âà half input size</li>
            </ul>
            <p><strong>Padding</strong>: Add zeros around input to
            preserve dimensions</p>
            <ul>
            <li>‚ÄúSame‚Äù padding: Output = Input size</li>
            <li>‚ÄúValid‚Äù padding: No padding, output smaller</li>
            </ul>
            <p><strong>Pooling</strong>: Downsample by taking
            max/average over regions</p>
            <pre><code>2√ó2 Max Pool:
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1 ‚îÇ 3 ‚îÇ     ‚îÇ 4 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§  ‚Üí  ‚îî‚îÄ‚îÄ‚îÄ‚îò
‚îÇ 2 ‚îÇ 4 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="receptive-fields-what-each-neuron-sees">Receptive
            Fields: What Each Neuron ‚ÄúSees‚Äù</h3>
            <p>The <strong>receptive field</strong> of a neuron is the
            region of the input image that can influence its output.
            Understanding receptive fields is crucial for CNN
            design.</p>
            <p><img src="figures/receptive_field.png"
            alt="Receptive Field Growth" /> <em>Figure: Receptive field
            growth in a CNN. Each 3√ó3 convolution increases the
            receptive field by 2 pixels per side. After 2 layers with
            3√ó3 kernels, a single output neuron ‚Äúsees‚Äù a 5√ó5 region of
            the input.</em></p>
            <p><strong>Receptive field formula</strong> (for stride-1
            convolutions):</p>
            <p><span class="math display">\[RF_{out} = RF_{in} + (k - 1)
            \times \prod_{i=1}^{l-1} s_i\]</span></p>
            <p>Where: - <span class="math inline">\(RF\)</span> =
            receptive field size - <span
            class="math inline">\(k\)</span> = kernel size - <span
            class="math inline">\(s_i\)</span> = stride at layer <span
            class="math inline">\(i\)</span></p>
            <p><strong>Simplified rule</strong> for 3√ó3 kernels with
            stride 1: <span class="math display">\[RF_l = RF_{l-1} +
            2\]</span></p>
            <p>Starting from <span class="math inline">\(RF_0 =
            1\)</span> (single pixel): - After Conv1 (3√ó3): RF = 3 -
            After Conv2 (3√ó3): RF = 5 - After Conv3 (3√ó3): RF = 7 -
            After Conv4 (3√ó3): RF = 9</p>
            <p><strong>Why receptive fields matter</strong>:</p>
            <ol type="1">
            <li><strong>Feature hierarchy</strong>: Early layers have
            small RFs ‚Üí detect edges, textures. Deep layers have large
            RFs ‚Üí detect objects, scenes</li>
            <li><strong>Network depth</strong>: Deeper networks = larger
            RFs = can capture more global information</li>
            <li><strong>Design tradeoff</strong>: Larger kernels (5√ó5,
            7√ó7) increase RF faster but add more parameters</li>
            </ol>
            <p><strong>Interview Q</strong>: ‚ÄúWhy do modern CNNs use
            stacked 3√ó3 convolutions instead of larger kernels?‚Äù</p>
            <p><strong>A</strong>: Two 3√ó3 convolutions have the same
            receptive field as one 5√ó5 (RF = 5), but with: - Fewer
            parameters: <span class="math inline">\(2 \times (3^2) =
            18\)</span> vs <span class="math inline">\(5^2 = 25\)</span>
            - More non-linearity: Two ReLU activations vs one - More
            representational power</p>
            <h3 id="a-simple-cnn-architecture">A Simple CNN
            Architecture</h3>
            <pre><code>Input: 32√ó32√ó3 (e.g., CIFAR-10 image)
  ‚Üì
Conv: 32 filters, 3√ó3 ‚Üí 32√ó32√ó32
ReLU
MaxPool 2√ó2 ‚Üí 16√ó16√ó32
  ‚Üì
Conv: 64 filters, 3√ó3 ‚Üí 16√ó16√ó64
ReLU
MaxPool 2√ó2 ‚Üí 8√ó8√ó64
  ‚Üì
Flatten ‚Üí 4096
  ‚Üì
FC ‚Üí 256
ReLU
  ‚Üì
FC ‚Üí 10 (classes)
Softmax</code></pre>
            <h3 id="pytorch-cnn">PyTorch CNN</h3>
            <div class="sourceCode" id="cb34"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleCNN(nn.Module):</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">64</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">256</span>)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))  <span class="co"># 32√ó32 ‚Üí 16√ó16</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))  <span class="co"># 16√ó16 ‚Üí 8√ó8</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">64</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>)            <span class="co"># Flatten</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
            <hr />
            <h2 id="word-embeddings">1.12 Word Embeddings</h2>
            <h3 id="the-problem-how-to-represent-words-as-numbers">The
            Problem: How to Represent Words as Numbers?</h3>
            <p>Neural networks need numerical inputs. How do we convert
            words to numbers?</p>
            <h3 id="approach-1-one-hot-encoding">Approach 1: One-Hot
            Encoding</h3>
            <pre><code>&quot;cat&quot;  ‚Üí [1, 0, 0, 0]
&quot;dog&quot;  ‚Üí [0, 1, 0, 0]
&quot;bird&quot; ‚Üí [0, 0, 1, 0]
&quot;fish&quot; ‚Üí [0, 0, 0, 1]</code></pre>
            <p><strong>Problems</strong>:</p>
            <ol type="1">
            <li><strong>Sparse</strong>: V = 50,000 ‚Üí vectors with
            49,999 zeros</li>
            <li><strong>No similarity</strong>: cat¬∑dog = 0
            (orthogonal), even though semantically related</li>
            <li><strong>Memory</strong>: Large vocabulary = huge
            vectors</li>
            </ol>
            <h3 id="approach-2-dense-embeddings">Approach 2: Dense
            Embeddings</h3>
            <p>Map each word to a <strong>dense, low-dimensional
            vector</strong>:</p>
            <pre><code>&quot;cat&quot;  ‚Üí [0.2, -0.4, 0.1, 0.8, ...]   (d = 300 dimensions)
&quot;dog&quot;  ‚Üí [0.3, -0.3, 0.0, 0.7, ...]   (similar to cat!)
&quot;fish&quot; ‚Üí [-0.5, 0.2, 0.9, -0.1, ...]  (different)</code></pre>
            <h3 id="why-embeddings-work">Why Embeddings Work</h3>
            <p><strong>Distributional hypothesis</strong>: Words that
            appear in similar contexts have similar meanings.</p>
            <p>‚ÄúThe <strong>cat</strong> sat on the mat‚Äù ‚ÄúThe
            <strong>dog</strong> sat on the mat‚Äù</p>
            <p>cat and dog appear in similar contexts ‚Üí similar
            embeddings!</p>
            <h3 id="embedding-layer-in-pytorch">Embedding Layer in
            PyTorch</h3>
            <div class="sourceCode" id="cb37"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Vocabulary of 10,000 words, embedding dimension 300</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span><span class="dv">10000</span>, embedding_dim<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert word indices to embeddings</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>word_indices <span class="op">=</span> torch.tensor([<span class="dv">42</span>, <span class="dv">1337</span>, <span class="dv">99</span>])  <span class="co"># 3 words</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>word_vectors <span class="op">=</span> embedding(word_indices)        <span class="co"># Shape: (3, 300)</span></span></code></pre></div>
            <h3 id="pretrained-embeddings">Pretrained Embeddings</h3>
            <p><strong>Word2Vec</strong>, <strong>GloVe</strong>:
            Trained on billions of words</p>
            <ul>
            <li>Capture semantic relationships</li>
            <li>‚Äúking‚Äù - ‚Äúman‚Äù + ‚Äúwoman‚Äù ‚âà ‚Äúqueen‚Äù</li>
            </ul>
            <div class="sourceCode" id="cb38"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using pretrained GloVe</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> GloVe</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>glove <span class="op">=</span> GloVe(name<span class="op">=</span><span class="st">&#39;6B&#39;</span>, dim<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>cat_embedding <span class="op">=</span> glove[<span class="st">&#39;cat&#39;</span>]  <span class="co"># 300-dimensional vector</span></span></code></pre></div>
            <h3 id="how-word2vec-learns-skip-gram-training">How Word2Vec
            Learns: Skip-gram Training</h3>
            <p>Understanding how Word2Vec learns embeddings provides
            deep insight into self-supervised learning‚Äîthe foundation of
            modern LLMs.</p>
            <p><img src="figures/word2vec_architecture.png"
            alt="Word2Vec Architecture" /> <em>Figure: Skip-gram and
            CBOW architectures. Skip-gram predicts context words from
            the center word; CBOW predicts the center word from
            context.</em></p>
            <p><strong>The Skip-gram Objective</strong></p>
            <p>Given a center word, predict its surrounding context
            words:</p>
            <p><span class="math display">\[\text{maximize} \quad
            \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log
            P(w_{t+j} | w_t)\]</span></p>
            <p>Where <span class="math inline">\(c\)</span> is the
            context window size.</p>
            <p><strong>The probability model</strong>:</p>
            <p><span class="math display">\[P(w_O | w_I) =
            \frac{\exp(v&#39;_{w_O} \cdot v_{w_I})}{\sum_{w=1}^{V}
            \exp(v&#39;_w \cdot v_{w_I})}\]</span></p>
            <p>This is just a softmax! The dot product measures
            similarity between: - <span
            class="math inline">\(v_{w_I}\)</span>: Input embedding of
            the center word - <span
            class="math inline">\(v&#39;_{w_O}\)</span>: Output
            embedding of the context word</p>
            <p><strong>Why this learns semantic similarity</strong>:</p>
            <p>The training signal ‚Äúpredict context from word‚Äù forces
            words appearing in similar contexts to have similar
            embeddings. Consider:</p>
            <pre><code>&quot;The cat sat on the mat&quot;
&quot;The dog sat on the mat&quot;</code></pre>
            <p>Both ‚Äúcat‚Äù and ‚Äúdog‚Äù must predict the same context words
            (‚Äúthe‚Äù, ‚Äúsat‚Äù, ‚Äúon‚Äù, ‚Äúmat‚Äù), so they‚Äôre pushed to have
            similar embeddings!</p>
            <p><strong>Negative Sampling (Practical
            Training)</strong></p>
            <p>The full softmax over vocabulary is expensive (V can be
            100,000+). <strong>Negative sampling</strong> approximates
            it:</p>
            <p>Instead of computing the full softmax, contrast the
            positive (real) context word against random ‚Äúnegative‚Äù
            words:</p>
            <p><span class="math display">\[\mathcal{L} = \log
            \sigma(v&#39;_{w_O} \cdot v_{w_I}) + \sum_{i=1}^{k}
            \mathbb{E}_{w_i \sim P_n(w)} [\log \sigma(-v&#39;_{w_i}
            \cdot v_{w_I})]\]</span></p>
            <ul>
            <li>First term: Push the real context word embedding closer
            to center word</li>
            <li>Second term: Push random noise words away from center
            word</li>
            <li>Typically <span class="math inline">\(k = 5\)</span>-20
            negative samples</li>
            </ul>
            <p><strong>This is contrastive learning!</strong> The same
            principle underlies modern self-supervised methods like
            SimCLR and CLIP.</p>
            <h3 id="from-word-embeddings-to-transformers">From Word
            Embeddings to Transformers</h3>
            <p>Modern LLMs don‚Äôt use fixed word embeddings:</p>
            <ol type="1">
            <li><strong>Subword tokenization</strong>: ‚Äúunhappiness‚Äù ‚Üí
            [‚Äúun‚Äù, ‚Äúhappiness‚Äù]</li>
            <li><strong>Contextual embeddings</strong>: Same word,
            different meaning in context</li>
            <li><strong>Learned during pretraining</strong>: Not
            pretrained separately</li>
            </ol>
            <pre><code>Static embedding (Word2Vec):
  &quot;bank&quot; ‚Üí same vector always

Contextual embedding (BERT, GPT):
  &quot;river bank&quot; ‚Üí one vector
  &quot;bank account&quot; ‚Üí different vector!</code></pre>
            <hr />
            <h2 id="the-xor-problem-a-complete-mlp-example">1.13 The XOR
            Problem: A Complete MLP Example</h2>
            <h3 id="why-xor">Why XOR?</h3>
            <p>XOR is the classic example showing why we need hidden
            layers:</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(x_1\)</span></th>
            <th><span class="math inline">\(x_2\)</span></th>
            <th><span class="math inline">\(y\)</span> (XOR)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            </tr>
            <tr>
            <td>0</td>
            <td>1</td>
            <td>1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0</td>
            <td>1</td>
            </tr>
            <tr>
            <td>1</td>
            <td>1</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p><strong>No single line can separate the classes!</strong>
            (It‚Äôs not linearly separable)</p>
            <pre><code>  x‚ÇÇ
  ‚Üë
1 ‚îÇ  ‚óè        ‚óã      ‚Üê Can&#39;t draw one line
  ‚îÇ                     to separate ‚óè from ‚óã
0 ‚îÇ  ‚óã        ‚óè
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí x‚ÇÅ
     0        1</code></pre>
            <h3 id="the-network-architecture">The Network
            Architecture</h3>
            <pre><code>Input Layer      Hidden Layer (2 neurons)     Output Layer
    x‚ÇÅ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
             ‚îú‚îÄ‚îÄ‚îÄ‚Üí h‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    x‚ÇÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îú‚îÄ‚îÄ‚îÄ‚Üí y
             ‚îú‚îÄ‚îÄ‚îÄ‚Üí h‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    1 (bias)‚îÄ‚îò</code></pre>
            <p><strong>Dimensions</strong>:</p>
            <ul>
            <li>Input: 2 features</li>
            <li>Hidden: 2 neurons (with ReLU)</li>
            <li>Output: 1 neuron (with sigmoid)</li>
            </ul>
            <h3 id="step-1-initialize-weights">Step 1: Initialize
            Weights</h3>
            <p>Let‚Äôs use specific weights that solve XOR:</p>
            <p><strong>Hidden layer weights</strong> <span
            class="math inline">\(W^{(1)}\)</span> and biases <span
            class="math inline">\(b^{(1)}\)</span>: <span
            class="math display">\[W^{(1)} = \begin{bmatrix} 1 &amp; 1
            \\ 1 &amp; 1 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix}
            0 \\ -1 \end{bmatrix}\]</span></p>
            <p><strong>Output layer weights</strong> <span
            class="math inline">\(W^{(2)}\)</span> and bias <span
            class="math inline">\(b^{(2)}\)</span>: <span
            class="math display">\[W^{(2)} = \begin{bmatrix} 1 \\ -2
            \end{bmatrix}, \quad b^{(2)} = 0\]</span></p>
            <h3 id="step-2-forward-pass-for-input-1-1">Step 2: Forward
            Pass (for input [1, 1])</h3>
            <p><strong>Hidden layer pre-activation</strong>: <span
            class="math display">\[z^{(1)} = W^{(1)} \begin{bmatrix} 1
            \\ 1 \end{bmatrix} + b^{(1)} = \begin{bmatrix} 1 \cdot 1 + 1
            \cdot 1 \\ 1 \cdot 1 + 1 \cdot 1 \end{bmatrix} +
            \begin{bmatrix} 0 \\ -1 \end{bmatrix} = \begin{bmatrix} 2 \\
            1 \end{bmatrix}\]</span></p>
            <p><strong>Hidden layer activation (ReLU)</strong>: <span
            class="math display">\[h = \text{ReLU}(z^{(1)}) =
            \begin{bmatrix} \max(0, 2) \\ \max(0, 1) \end{bmatrix} =
            \begin{bmatrix} 2 \\ 1 \end{bmatrix}\]</span></p>
            <p><strong>Output layer pre-activation</strong>: <span
            class="math display">\[z^{(2)} = W^{(2)T} h + b^{(2)} = 1
            \cdot 2 + (-2) \cdot 1 + 0 = 0\]</span></p>
            <p><strong>Output (sigmoid)</strong>: <span
            class="math display">\[\hat{y} = \sigma(0) = \frac{1}{1 +
            e^0} = 0.5\]</span></p>
            <h3 id="step-3-all-four-inputs">Step 3: All Four Inputs</h3>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 14%" />
            <col style="width: 15%" />
            <col style="width: 14%" />
            <col style="width: 14%" />
            <col style="width: 15%" />
            </colgroup>
            <thead>
            <tr>
            <th>Input <span class="math inline">\((x_1,
            x_2)\)</span></th>
            <th><span class="math inline">\(z^{(1)}\)</span></th>
            <th><span class="math inline">\(h\)</span> (ReLU)</th>
            <th><span class="math inline">\(z^{(2)}\)</span></th>
            <th><span class="math inline">\(\hat{y}\)</span></th>
            <th>Target <span class="math inline">\(y\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>(0, 0)</td>
            <td>(0, -1)</td>
            <td>(0, 0)</td>
            <td>0</td>
            <td>0.5</td>
            <td>0</td>
            </tr>
            <tr>
            <td>(0, 1)</td>
            <td>(1, 0)</td>
            <td>(1, 0)</td>
            <td>1</td>
            <td>0.73</td>
            <td>1</td>
            </tr>
            <tr>
            <td>(1, 0)</td>
            <td>(1, 0)</td>
            <td>(1, 0)</td>
            <td>1</td>
            <td>0.73</td>
            <td>1</td>
            </tr>
            <tr>
            <td>(1, 1)</td>
            <td>(2, 1)</td>
            <td>(2, 1)</td>
            <td>0</td>
            <td>0.5</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <h3 id="better-tuned-weights-for-xor">Better-Tuned Weights
            for XOR</h3>
            <p>The weights above give outputs at 0.5 for (0,0) and (1,1)
            ‚Äî not ideal. Here are <strong>better weights</strong> that
            give outputs closer to 0 and 1:</p>
            <p><strong>Optimal hidden layer weights</strong> <span
            class="math inline">\(W^{(1)}\)</span> and biases <span
            class="math inline">\(b^{(1)}\)</span>: <span
            class="math display">\[W^{(1)} = \begin{bmatrix} 20 &amp; 20
            \\ 20 &amp; 20 \end{bmatrix}, \quad b^{(1)} =
            \begin{bmatrix} -10 \\ -30 \end{bmatrix}\]</span></p>
            <p><strong>Optimal output layer weights</strong> <span
            class="math inline">\(W^{(2)}\)</span> and bias <span
            class="math inline">\(b^{(2)}\)</span>: <span
            class="math display">\[W^{(2)} = \begin{bmatrix} 20 \\ -20
            \end{bmatrix}, \quad b^{(2)} = -10\]</span></p>
            <p>With these weights:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 14%" />
            <col style="width: 15%" />
            <col style="width: 14%" />
            <col style="width: 14%" />
            <col style="width: 15%" />
            </colgroup>
            <thead>
            <tr>
            <th>Input <span class="math inline">\((x_1,
            x_2)\)</span></th>
            <th><span class="math inline">\(z^{(1)}\)</span></th>
            <th><span class="math inline">\(h\)</span> (ReLU)</th>
            <th><span class="math inline">\(z^{(2)}\)</span></th>
            <th><span class="math inline">\(\hat{y}\)</span></th>
            <th>Target <span class="math inline">\(y\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>(0, 0)</td>
            <td>(-10, -30)</td>
            <td>(0, 0)</td>
            <td>-10</td>
            <td>0.00005 ‚âà <strong>0</strong></td>
            <td>0 ‚úì</td>
            </tr>
            <tr>
            <td>(0, 1)</td>
            <td>(10, -10)</td>
            <td>(10, 0)</td>
            <td>190</td>
            <td>1.0 ‚âà <strong>1</strong></td>
            <td>1 ‚úì</td>
            </tr>
            <tr>
            <td>(1, 0)</td>
            <td>(10, -10)</td>
            <td>(10, 0)</td>
            <td>190</td>
            <td>1.0 ‚âà <strong>1</strong></td>
            <td>1 ‚úì</td>
            </tr>
            <tr>
            <td>(1, 1)</td>
            <td>(30, 10)</td>
            <td>(30, 10)</td>
            <td>390</td>
            <td>0.00005 ‚âà <strong>0</strong></td>
            <td>0 ‚úì</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Intuition behind these weights</strong>: - Hidden
            neuron 1: <span class="math inline">\(h_1 =
            \text{ReLU}(20x_1 + 20x_2 - 10)\)</span> fires when at least
            one input is 1 - Hidden neuron 2: <span
            class="math inline">\(h_2 = \text{ReLU}(20x_1 + 20x_2 -
            30)\)</span> fires only when both inputs are 1 - Output:
            <span class="math inline">\(20h_1 - 20h_2 - 10\)</span> is
            large positive only when <span class="math inline">\(h_1
            &gt; 0\)</span> and <span class="math inline">\(h_2 =
            0\)</span></p>
            <h3 id="step-4-compute-loss-binary-cross-entropy">Step 4:
            Compute Loss (Binary Cross-Entropy)</h3>
            <p>For input (1, 1) with <span class="math inline">\(\hat{y}
            = 0.5\)</span>, target <span class="math inline">\(y =
            0\)</span>:</p>
            <p><span class="math display">\[\mathcal{L} = -[y
            \log(\hat{y}) + (1-y) \log(1-\hat{y})]\]</span></p>
            <p><span class="math display">\[= -[0 \cdot \log(0.5) + 1
            \cdot \log(0.5)]\]</span></p>
            <p><span class="math display">\[= -\log(0.5) =
            0.693\]</span></p>
            <h3 id="step-5-backward-pass">Step 5: Backward Pass</h3>
            <p><strong>Output layer gradient</strong>: <span
            class="math display">\[\frac{\partial \mathcal{L}}{\partial
            z^{(2)}} = \hat{y} - y = 0.5 - 0 = 0.5\]</span></p>
            <p><strong>Gradient w.r.t. output weights</strong>: <span
            class="math display">\[\frac{\partial \mathcal{L}}{\partial
            W^{(2)}} = h \cdot \frac{\partial \mathcal{L}}{\partial
            z^{(2)}} = \begin{bmatrix} 2 \\ 1 \end{bmatrix} \cdot 0.5 =
            \begin{bmatrix} 1.0 \\ 0.5 \end{bmatrix}\]</span></p>
            <p><strong>Gradient flowing to hidden layer</strong>: <span
            class="math display">\[\frac{\partial \mathcal{L}}{\partial
            h} = W^{(2)} \cdot \frac{\partial \mathcal{L}}{\partial
            z^{(2)}} = \begin{bmatrix} 1 \\ -2 \end{bmatrix} \cdot 0.5 =
            \begin{bmatrix} 0.5 \\ -1.0 \end{bmatrix}\]</span></p>
            <p><strong>Through ReLU</strong> (gradient is 1 where input
            &gt; 0, else 0): <span class="math display">\[\frac{\partial
            \mathcal{L}}{\partial z^{(1)}} = \frac{\partial
            \mathcal{L}}{\partial h} \odot \mathbf{1}_{z^{(1)} &gt; 0} =
            \begin{bmatrix} 0.5 \\ -1.0 \end{bmatrix} \odot
            \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 0.5
            \\ -1.0 \end{bmatrix}\]</span></p>
            <p><strong>Gradient w.r.t. hidden weights</strong>: <span
            class="math display">\[\frac{\partial \mathcal{L}}{\partial
            W^{(1)}} = \frac{\partial \mathcal{L}}{\partial z^{(1)}}
            \cdot x^T = \begin{bmatrix} 0.5 \\ -1.0 \end{bmatrix} \cdot
            \begin{bmatrix} 1 &amp; 1 \end{bmatrix} = \begin{bmatrix}
            0.5 &amp; 0.5 \\ -1.0 &amp; -1.0 \end{bmatrix}\]</span></p>
            <h3 id="pytorch-implementation-1">PyTorch
            Implementation</h3>
            <div class="sourceCode" id="cb43"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># XOR data</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]], dtype<span class="op">=</span>torch.float32)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Network</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid()</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss()</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X)</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(y_pred, y)</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Predictions:&quot;</span>)</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model(X).detach().<span class="bu">round</span>())  <span class="co"># Should be [[0], [1], [1], [0]]</span></span></code></pre></div>
            <h3 id="key-insights">Key Insights</h3>
            <ol type="1">
            <li><strong>Hidden layer creates new
            representation</strong>: The hidden layer transforms the
            space so that XOR becomes linearly separable</li>
            <li><strong>Non-linearity is essential</strong>: Without
            ReLU, two linear layers collapse to one</li>
            <li><strong>Backprop chain rule</strong>: Gradients flow
            backward through each layer</li>
            </ol>
            <hr />
            <h2 id="weight-initialization">1.14 Weight
            Initialization</h2>
            <h3 id="why-does-initialization-matter">Why Does
            Initialization Matter?</h3>
            <p><strong>Bad initialization leads to</strong>:</p>
            <ul>
            <li><strong>All zeros</strong>: All neurons compute the same
            thing, learn the same features (symmetry problem)</li>
            <li><strong>Too small</strong>: Activations shrink to zero
            layer by layer (vanishing signals)</li>
            <li><strong>Too large</strong>: Activations explode,
            gradients explode</li>
            </ul>
            <h3 id="the-goal">The Goal</h3>
            <p>Keep <strong>activations</strong> and
            <strong>gradients</strong> at reasonable scale throughout
            the network.</p>
            <h3 id="xavierglorot-initialization-2010">Xavier/Glorot
            Initialization (2010)</h3>
            <p><strong>For tanh/sigmoid activations</strong>:</p>
            <p><span class="math display">\[W \sim \mathcal{N}\left(0,
            \frac{2}{n_{\text{in}} + n_{\text{out}}}\right) \quad
            \text{or} \quad W \sim
            \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} +
            n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} +
            n_{\text{out}}}}\right)\]</span></p>
            <p><strong>Intuition</strong>: Balance the variance of
            inputs and outputs so signal doesn‚Äôt explode or vanish.</p>
            <div class="sourceCode" id="cb44"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>nn.init.xavier_uniform_(layer.weight)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>nn.init.xavier_normal_(layer.weight)</span></code></pre></div>
            <h3 id="hekaiming-initialization-2015">He/Kaiming
            Initialization (2015)</h3>
            <p><strong>For ReLU activations</strong> (ReLU kills half
            the signal, so we compensate):</p>
            <p><span class="math display">\[W \sim \mathcal{N}\left(0,
            \frac{2}{n_{\text{in}}}\right)\]</span></p>
            <p><strong>Why factor of 2?</strong> ReLU zeroes out
            negative half, so variance is halved. We double the initial
            variance to compensate.</p>
            <h3
            id="deriving-he-initialization-variance-propagation">Deriving
            He Initialization: Variance Propagation</h3>
            <p>Let‚Äôs derive why He initialization uses <span
            class="math inline">\(\text{Var}(W) =
            \frac{2}{n_{in}}\)</span> for ReLU networks.</p>
            <p><img src="figures/variance_propagation.png"
            alt="Variance Propagation" /> <em>Figure: How activation
            variance propagates through layers. Without proper
            initialization, variance either explodes or vanishes
            exponentially. The goal: keep Var(y) ‚âà Var(x) at each
            layer.</em></p>
            <p><strong>Setup</strong>: Consider a single layer <span
            class="math inline">\(y = Wx + b\)</span> where: - Input
            <span class="math inline">\(x\)</span> has <span
            class="math inline">\(n_{in}\)</span> components, each with
            variance <span
            class="math inline">\(\text{Var}(x_j)\)</span> - Weights
            <span class="math inline">\(W_{ij} \sim \mathcal{N}(0,
            \sigma_w^2)\)</span> are independent of inputs - We want
            <span class="math inline">\(\text{Var}(y_i) =
            \text{Var}(x_j)\)</span> (preserve variance)</p>
            <p><strong>Step 1: Variance of one output neuron (before
            activation)</strong></p>
            <p><span class="math display">\[y_i = \sum_{j=1}^{n_{in}}
            W_{ij} x_j + b_i\]</span></p>
            <p>For zero-mean <span class="math inline">\(x\)</span> and
            <span class="math inline">\(W\)</span>, assuming
            independence:</p>
            <p><span class="math display">\[\text{Var}(y_i) =
            \sum_{j=1}^{n_{in}} \text{Var}(W_{ij} x_j) =
            \sum_{j=1}^{n_{in}} \text{Var}(W_{ij}) \cdot \text{Var}(x_j)
            = n_{in} \cdot \sigma_w^2 \cdot \text{Var}(x)\]</span></p>
            <p><strong>Step 2: Preserve variance (Xavier
            derivation)</strong></p>
            <p>For <span class="math inline">\(\text{Var}(y) =
            \text{Var}(x)\)</span>, we need: <span
            class="math display">\[n_{in} \cdot \sigma_w^2 = 1 \implies
            \sigma_w^2 = \frac{1}{n_{in}}\]</span></p>
            <p>This is <strong>Xavier initialization</strong> ‚Äî perfect
            for linear layers or tanh (which is approximately linear
            near 0).</p>
            <p><strong>Step 3: Account for ReLU (He
            derivation)</strong></p>
            <p>ReLU zeros out negative values. For zero-mean Gaussian
            input, exactly <strong>half</strong> the values are
            negative:</p>
            <p><span class="math display">\[\text{ReLU}(y) =
            \begin{cases} y &amp; \text{if } y &gt; 0 \\ 0 &amp;
            \text{if } y \leq 0 \end{cases}\]</span></p>
            <p>The variance after ReLU is halved: <span
            class="math display">\[\text{Var}(\text{ReLU}(y)) =
            \frac{1}{2} \text{Var}(y)\]</span></p>
            <p>To compensate, we need <strong>twice the initial
            variance</strong>:</p>
            <p><span class="math display">\[\sigma_w^2 =
            \frac{2}{n_{in}}\]</span></p>
            <p>This is <strong>He/Kaiming initialization</strong>!</p>
            <p><strong>Why this matters</strong>: Without this factor of
            2, variance shrinks by half at each layer. After 10 layers:
            <span class="math inline">\(0.5^{10} \approx 0.001\)</span>
            ‚Äî activations become tiny, gradients vanish.</p>
            <div class="sourceCode" id="cb45"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch (default for nn.Linear with ReLU)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>nn.init.kaiming_uniform_(layer.weight, nonlinearity<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>nn.init.kaiming_normal_(layer.weight, nonlinearity<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span></code></pre></div>
            <h3 id="quick-reference">Quick Reference</h3>
            <table>
            <thead>
            <tr>
            <th>Activation</th>
            <th>Initialization</th>
            <th>Variance</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Sigmoid/Tanh</td>
            <td>Xavier</td>
            <td><span class="math inline">\(\frac{2}{n_{in} +
            n_{out}}\)</span></td>
            </tr>
            <tr>
            <td>ReLU</td>
            <td>He/Kaiming</td>
            <td><span
            class="math inline">\(\frac{2}{n_{in}}\)</span></td>
            </tr>
            <tr>
            <td>Linear (no activation)</td>
            <td>Xavier</td>
            <td><span class="math inline">\(\frac{2}{n_{in} +
            n_{out}}\)</span></td>
            </tr>
            </tbody>
            </table>
            <h3 id="what-about-biases">What About Biases?</h3>
            <p>Almost always initialize to <strong>zero</strong>:</p>
            <div class="sourceCode" id="cb46"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>nn.init.zeros_(layer.bias)</span></code></pre></div>
            <p>Exception: LSTM forget gate biases often initialized to 1
            to encourage remembering.</p>
            <h3 id="example-why-bad-init-fails">Example: Why Bad Init
            Fails</h3>
            <div class="sourceCode" id="cb47"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: All zeros</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.modules():</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">&#39;weight&#39;</span>):</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        layer.weight.data.fill_(<span class="dv">0</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: All neurons output the same thing, all gradients identical</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Too large</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.modules():</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">&#39;weight&#39;</span>):</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        layer.weight.data.normal_(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Activations explode, NaN losses</span></span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD: He initialization for ReLU network</span></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.modules():</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(layer, nn.Linear):</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_normal_(layer.weight, nonlinearity<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span></code></pre></div>
            <hr />
            <h2 id="dropout">1.15 Dropout</h2>
            <h3 id="what-is-dropout">What is Dropout?</h3>
            <p>During training, <strong>randomly set neurons to
            zero</strong> with probability <span
            class="math inline">\(p\)</span>:</p>
            <pre><code>Without dropout:      With dropout (p=0.5):

[0.5] ‚Üí [0.3]        [0.5] ‚Üí [0.0]  ‚Üê dropped!
[0.8] ‚Üí [0.2]        [0.8] ‚Üí [0.4]  ‚Üê scaled by 1/(1-p)
[0.1] ‚Üí [0.7]        [0.0] ‚Üí [0.0]  ‚Üê dropped!
[0.9] ‚Üí [0.5]        [0.9] ‚Üí [1.0]  ‚Üê scaled</code></pre>
            <h3 id="why-does-it-work">Why Does It Work?</h3>
            <ol type="1">
            <li><strong>Prevents co-adaptation</strong>: Neurons can‚Äôt
            rely on specific other neurons being present</li>
            <li><strong>Ensemble effect</strong>: Each forward pass uses
            a different ‚Äúsub-network‚Äù</li>
            <li><strong>Implicit regularization</strong>: Similar effect
            to training many models and averaging</li>
            </ol>
            <p><img src="figures/dropout_ensemble.png"
            alt="Dropout Ensemble Effect" /> <em>Figure: Dropout creates
            an implicit ensemble of exponentially many sub-networks.
            During training, each forward pass uses a different random
            subset of neurons (shown as different colored sub-networks).
            At inference, we use the full network with scaled weights,
            which approximates the average prediction of all
            sub-networks.</em></p>
            <h3 id="training-vs-inference">Training vs Inference</h3>
            <p><strong>Training</strong>: Drop neurons with probability
            <span class="math inline">\(p\)</span>, scale remaining by
            <span class="math inline">\(\frac{1}{1-p}\)</span></p>
            <p><strong>Inference</strong>: Use all neurons (no
            dropping)</p>
            <div class="sourceCode" id="cb49"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Dropout(nn.Module):</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:  <span class="co"># Training mode</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> (torch.rand_like(x) <span class="op">&gt;</span> <span class="va">self</span>.p).<span class="bu">float</span>()</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x <span class="op">*</span> mask <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.p)  <span class="co"># Scale to maintain expected value</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:  <span class="co"># Inference mode</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x  <span class="co"># No change</span></span></code></pre></div>
            <h3 id="pytorch-usage">PyTorch Usage</h3>
            <div class="sourceCode" id="cb50"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>),  <span class="co"># 50% dropout</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>),</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="co"># CRITICAL: Set mode correctly!</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>model.train()  <span class="co"># Enable dropout</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()   <span class="co"># Disable dropout</span></span></code></pre></div>
            <h3 id="typical-dropout-rates">Typical Dropout Rates</h3>
            <table>
            <thead>
            <tr>
            <th>Layer Type</th>
            <th>Typical <span class="math inline">\(p\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Input layer</td>
            <td>0.1-0.2 (light)</td>
            </tr>
            <tr>
            <td>Hidden layers</td>
            <td>0.3-0.5</td>
            </tr>
            <tr>
            <td>Before output</td>
            <td>0.0-0.3</td>
            </tr>
            <tr>
            <td>CNNs</td>
            <td>0.25-0.5</td>
            </tr>
            <tr>
            <td>Transformers</td>
            <td>0.1</td>
            </tr>
            </tbody>
            </table>
            <h3 id="dropout-variants">Dropout Variants</h3>
            <table>
            <thead>
            <tr>
            <th>Variant</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Standard</strong></td>
            <td>Drop individual neurons</td>
            </tr>
            <tr>
            <td><strong>Spatial</strong> (CNNs)</td>
            <td>Drop entire channels</td>
            </tr>
            <tr>
            <td><strong>DropConnect</strong></td>
            <td>Drop individual weights</td>
            </tr>
            <tr>
            <td><strong>DropBlock</strong></td>
            <td>Drop contiguous regions</td>
            </tr>
            </tbody>
            </table>
            <h3 id="interview-q-why-use-dropout-instead-of-l2">Interview
            Q: ‚ÄúWhy use dropout instead of L2?‚Äù</h3>
            <p><strong>A</strong>: Different mechanisms:</p>
            <ul>
            <li>L2 shrinks all weights smoothly</li>
            <li>Dropout forces redundancy and prevents
            co-adaptation</li>
            <li>Dropout is stochastic (different network each pass)</li>
            <li>Can use both together!</li>
            </ul>
            <hr />
            <h2 id="overfitting-regularization-preview">1.16 Overfitting
            &amp; Regularization Preview</h2>
            <p><strong>Overfitting</strong> is perhaps the most
            fundamental challenge in machine learning. It occurs when a
            model learns the training data <em>too well</em> ‚Äî including
            its noise and idiosyncrasies ‚Äî rather than the underlying
            patterns that generalize to new data. Understanding and
            preventing overfitting is what separates models that work in
            practice from those that only work on paper.</p>
            <h3 id="the-central-problem">The Central Problem</h3>
            <p>The core tension in machine learning is this: we want our
            model to perform well on data it has never seen before, but
            we can only train it on data we have. This creates a
            fundamental dilemma.</p>
            <pre><code>                    Training Data
                         ‚Üì
                    [Your Model]
                         ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                                 ‚Üì
   Fits training                    Generalizes to
   data well                        new data?
   
   EASY                             THE HARD PART</code></pre>
            <p>A sufficiently complex model can memorize any training
            set ‚Äî given enough parameters, it can simply store each
            training example and its label. Such a model achieves zero
            training error but learns nothing useful. When shown a new
            example, it has no idea what to do because it never learned
            the underlying pattern; it only memorized specific
            instances.</p>
            <p><strong>The goal of regularization</strong> is to prevent
            this memorization by encouraging the model to learn simpler,
            more generalizable patterns. The intuition is that the true
            underlying relationship is usually simpler than the
            noise-contaminated training data suggests.</p>
            <h3 id="visual-the-overfitting-spectrum">Visual: The
            Overfitting Spectrum</h3>
            <figure>
            <img src="figures/overfitting_spectrum.png"
            alt="The Overfitting Spectrum" />
            <figcaption aria-hidden="true">The Overfitting
            Spectrum</figcaption>
            </figure>
            <p>The figure above illustrates the spectrum from
            underfitting to overfitting:</p>
            <ul>
            <li><p><strong>Underfitting (High Bias)</strong>: The model
            is too simple to capture the underlying pattern. A linear
            model trying to fit a quadratic relationship, for example.
            Both training and test error are high.</p></li>
            <li><p><strong>Good Fit</strong>: The model captures the
            true underlying pattern without memorizing noise. Training
            error is low, and test error is similarly low.</p></li>
            <li><p><strong>Overfitting (High Variance)</strong>: The
            model is too complex and has memorized training-specific
            noise. Training error is very low (often near zero), but
            test error is high because the ‚Äúlearned‚Äù noise doesn‚Äôt exist
            in new data.</p></li>
            </ul>
            <h3 id="learning-curves-your-diagnostic-tool">Learning
            Curves: Your Diagnostic Tool</h3>
            <figure>
            <img src="figures/learning_curves_diagnostic.png"
            alt="Learning Curves Diagnostic" />
            <figcaption aria-hidden="true">Learning Curves
            Diagnostic</figcaption>
            </figure>
            <p><strong>Learning curves</strong> are your most powerful
            tool for diagnosing training problems. They plot training
            and validation loss (or accuracy) against training progress
            (epochs or iterations).</p>
            <p><strong>How to read learning curves:</strong></p>
            <ol type="1">
            <li><p><strong>Healthy training</strong>: Both curves
            decrease together and converge to similar values. The gap
            between them is small and stable.</p></li>
            <li><p><strong>Overfitting signature</strong>: Training loss
            continues to decrease, but validation loss stops improving
            or starts increasing. The gap between them grows over time.
            This tells you the model is memorizing training data rather
            than learning generalizable patterns.</p></li>
            <li><p><strong>Underfitting signature</strong>: Both
            training and validation loss remain high and plateau early.
            Neither improves much with more training. This tells you the
            model lacks capacity to learn the pattern, or there‚Äôs a
            fundamental problem with the setup.</p></li>
            <li><p><strong>When to stop</strong>: The optimal stopping
            point is typically where validation loss is lowest ‚Äî just
            before the gap starts widening. This is why <strong>early
            stopping</strong> is so effective.</p></li>
            </ol>
            <h3 id="the-regularization-toolbox">The Regularization
            Toolbox</h3>
            <p>Regularization encompasses any technique that helps your
            model generalize better, typically by constraining its
            complexity in some way. Here are the main tools:</p>
            <table>
            <thead>
            <tr>
            <th>Technique</th>
            <th>How It Helps</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>L2 (Weight Decay)</strong></td>
            <td>Shrinks weights</td>
            <td>Almost always</td>
            </tr>
            <tr>
            <td><strong>L1</strong></td>
            <td>Zeros weights (sparsity)</td>
            <td>Feature selection</td>
            </tr>
            <tr>
            <td><strong>Dropout</strong></td>
            <td>Forces redundancy</td>
            <td>Deep networks</td>
            </tr>
            <tr>
            <td><strong>Early Stopping</strong></td>
            <td>Stop before overfitting</td>
            <td>Always monitor!</td>
            </tr>
            <tr>
            <td><strong>Data Augmentation</strong></td>
            <td>More effective data</td>
            <td>Images, text</td>
            </tr>
            <tr>
            <td><strong>Batch Normalization</strong></td>
            <td>Stabilizes + regularizes</td>
            <td>Deep networks</td>
            </tr>
            </tbody>
            </table>
            <h3 id="l2-regularization-weight-decay">L2 Regularization
            (Weight Decay)</h3>
            <p><span class="math display">\[\text{Total Loss} =
            \underbrace{\text{Data Loss}}_{\text{fit the data}} +
            \underbrace{\lambda \sum_j w_j^2}_{\text{keep weights
            small}}\]</span></p>
            <p><strong>Effect</strong>: Larger <span
            class="math inline">\(\lambda\)</span> ‚Üí smaller weights ‚Üí
            simpler model ‚Üí less overfitting</p>
            <p><strong>Key property</strong>: Shrinks all weights toward
            zero, but rarely makes them exactly zero.</p>
            <h3 id="l1-regularization-lasso">L1 Regularization
            (Lasso)</h3>
            <p><span class="math display">\[\text{Total Loss} =
            \underbrace{\text{Data Loss}}_{\text{fit the data}} +
            \underbrace{\lambda \sum_j |w_j|}_{\text{push weights to
            zero}}\]</span></p>
            <p><strong>Effect</strong>: Larger <span
            class="math inline">\(\lambda\)</span> ‚Üí more weights become
            <strong>exactly zero</strong> ‚Üí automatic feature
            selection</p>
            <p><strong>Key property</strong>: Creates
            <strong>sparse</strong> models ‚Äî some features are
            completely ignored.</p>
            <h3 id="l1-vs-l2-when-to-use-which">L1 vs L2: When to Use
            Which?</h3>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 35%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>L1 (Lasso)</th>
            <th>L2 (Ridge)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Sparsity</strong></td>
            <td>Yes (exact zeros)</td>
            <td>No (small but non-zero)</td>
            </tr>
            <tr>
            <td><strong>Feature selection</strong></td>
            <td>Automatic</td>
            <td>No</td>
            </tr>
            <tr>
            <td><strong>Multiple correlated features</strong></td>
            <td>Picks one arbitrarily</td>
            <td>Shrinks all equally</td>
            </tr>
            <tr>
            <td><strong>Computation</strong></td>
            <td>Non-differentiable at 0</td>
            <td>Smooth everywhere</td>
            </tr>
            <tr>
            <td><strong>When to use</strong></td>
            <td>Many irrelevant features</td>
            <td>All features matter</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Visual intuition</strong> (in 2D weight
            space):</p>
            <pre><code>L2 Constraint (circle):        L1 Constraint (diamond):
       
      ___                            /\
    /     \                         /  \
   |   ‚óè   |   ‚Üê Solution          /    \
    \     /                       /  ‚óè   \   ‚Üê Solution hits corner
      ‚Äæ‚Äæ‚Äæ                         \      /     (one weight = 0!)
                                   \    /
                                    \  /
                                     \/</code></pre>
            <p>The L1 diamond has corners on the axes ‚Äî the optimal
            point often lands on a corner where one or more weights are
            exactly zero.</p>
            <h3 id="quick-decision-tree">Quick Decision Tree</h3>
            <pre><code>Model overfitting?
       ‚îÇ
       ‚îú‚îÄ‚îÄ YES ‚Üí 1. Add/increase dropout
       ‚îÇ         2. Add/increase L2 (weight decay)
       ‚îÇ         3. Get more data / data augmentation
       ‚îÇ         4. Reduce model size (last resort)
       ‚îÇ
       ‚îî‚îÄ‚îÄ NO (underfitting) ‚Üí 1. Bigger model
                               2. More features
                               3. Train longer
                               4. Less regularization</code></pre>
            <h3 id="the-bias-variance-tradeoff-preview">The
            Bias-Variance Tradeoff (Preview)</h3>
            <p><span class="math display">\[\text{Expected Error} =
            \text{Bias}^2 + \text{Variance} + \text{Noise}\]</span></p>
            <table>
            <colgroup>
            <col style="width: 10%" />
            <col style="width: 37%" />
            <col style="width: 51%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th>High Bias</th>
            <th>High Variance</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Symptom</td>
            <td>Train error ‚âà Test error (both high)</td>
            <td>Train error &lt;&lt; Test error</td>
            </tr>
            <tr>
            <td>Cause</td>
            <td>Model too simple</td>
            <td>Model too complex</td>
            </tr>
            <tr>
            <td>Fix</td>
            <td>More complexity</td>
            <td>More regularization</td>
            </tr>
            </tbody>
            </table>
            <blockquote>
            <p>üí° <strong>See Part 2 for full treatment of bias-variance
            and Part 3 for detailed regularization math!</strong></p>
            </blockquote>
            <hr />
            <h2
            id="putting-it-all-together-complete-neural-network-setup">1.17
            Putting It All Together: Complete Neural Network Setup</h2>
            <p>This section consolidates everything from Part 1 into a
            single, practical code example showing how to configure each
            component of a neural network.</p>
            <h3 id="complete-keras-example">Complete Keras Example</h3>
            <div class="sourceCode" id="cb54"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, regularizers</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co"># MODEL DEFINITION</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer (optional explicit definition)</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    layers.Input(shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hidden Layer 1</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>    layers.Dense(</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>        units<span class="op">=</span><span class="dv">256</span>,                           <span class="co"># Number of neurons</span></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,                   <span class="co"># Activation function</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>        kernel_initializer<span class="op">=</span><span class="st">&#39;he_normal&#39;</span>,      <span class="co"># Weight initialization  </span></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>regularizers.l2(<span class="fl">0.01</span>)  <span class="co"># L2 regularization (weight decay)</span></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    layers.BatchNormalization(),             <span class="co"># Normalize activations</span></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>    layers.Dropout(<span class="fl">0.5</span>),                     <span class="co"># Regularization (50% dropout)</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hidden Layer 2</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    layers.Dense(</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>        units<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>        kernel_initializer<span class="op">=</span><span class="st">&#39;he_normal&#39;</span>,</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>regularizers.l2(<span class="fl">0.01</span>)</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>    layers.BatchNormalization(),</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>    layers.Dropout(<span class="fl">0.3</span>),</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output Layer (10-class classification)</span></span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a>    layers.Dense(</span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a>        units<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>,                <span class="co"># Softmax for multi-class</span></span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a>        kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>  <span class="co"># Xavier for output layer</span></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a><span class="co"># COMPILE: OPTIMIZER + LOSS + METRICS</span></span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>),</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>,  <span class="co"># For integer labels [0, 1, 2, ...]</span></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>]</span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a><span class="co"># TRAINING</span></span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a>    X_train, y_train,</span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>        keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a>        keras.callbacks.ReduceLROnPlateau(factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a><span class="co"># EVALUATION</span></span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê</span></span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> model.evaluate(X_test, y_test)</span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(X_test)</span></code></pre></div>
            <h3 id="quick-reference-component-options">Quick Reference:
            Component Options</h3>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 27%" />
            <col style="width: 39%" />
            </colgroup>
            <thead>
            <tr>
            <th>Component</th>
            <th>Options</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Activation</strong></td>
            <td><code>'relu'</code></td>
            <td>Hidden layers (default)</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'gelu'</code></td>
            <td>Transformers, modern architectures</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'sigmoid'</code></td>
            <td>Binary classification output</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'tanh'</code></td>
            <td>RNNs, bounded outputs</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'softmax'</code></td>
            <td>Multi-class classification output</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'linear'</code> / <code>None</code></td>
            <td>Regression output</td>
            </tr>
            <tr>
            <td><strong>Initializer</strong></td>
            <td><code>'glorot_uniform'</code> (Xavier)</td>
            <td>Sigmoid/Tanh activations</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'he_normal'</code> (Kaiming)</td>
            <td>ReLU/GELU activations</td>
            </tr>
            <tr>
            <td></td>
            <td><code>'zeros'</code></td>
            <td>Biases only</td>
            </tr>
            <tr>
            <td><strong>Regularizer</strong></td>
            <td><code>l2(Œª)</code></td>
            <td>Almost always (weight decay)</td>
            </tr>
            <tr>
            <td></td>
            <td><code>l1(Œª)</code></td>
            <td>When you want sparsity</td>
            </tr>
            <tr>
            <td></td>
            <td><code>l1_l2(l1, l2)</code></td>
            <td>Elastic net (both)</td>
            </tr>
            <tr>
            <td><strong>Optimizer</strong></td>
            <td><code>SGD(lr, momentum)</code></td>
            <td>Simple, good generalization</td>
            </tr>
            <tr>
            <td></td>
            <td><code>Adam(lr)</code></td>
            <td>Fast convergence (default choice)</td>
            </tr>
            <tr>
            <td></td>
            <td><code>AdamW(lr, weight_decay)</code></td>
            <td>Transformers, large models</td>
            </tr>
            <tr>
            <td><strong>Dropout</strong></td>
            <td><code>0.1-0.3</code></td>
            <td>Light regularization</td>
            </tr>
            <tr>
            <td></td>
            <td><code>0.5</code></td>
            <td>Standard for dense layers</td>
            </tr>
            <tr>
            <td></td>
            <td><code>0.0</code></td>
            <td>When using strong L2</td>
            </tr>
            </tbody>
            </table>
            <h3 id="task-specific-configurations">Task-Specific
            Configurations</h3>
            <table>
            <colgroup>
            <col style="width: 11%" />
            <col style="width: 35%" />
            <col style="width: 27%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Task</th>
            <th>Output Activation</th>
            <th>Loss Function</th>
            <th>Output Units</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Binary Classification</strong></td>
            <td><code>'sigmoid'</code></td>
            <td><code>'binary_crossentropy'</code></td>
            <td>1</td>
            </tr>
            <tr>
            <td><strong>Multi-class (K classes)</strong></td>
            <td><code>'softmax'</code></td>
            <td><code>'sparse_categorical_crossentropy'</code></td>
            <td>K</td>
            </tr>
            <tr>
            <td><strong>Multi-label</strong></td>
            <td><code>'sigmoid'</code></td>
            <td><code>'binary_crossentropy'</code></td>
            <td>K (one per label)</td>
            </tr>
            <tr>
            <td><strong>Regression</strong></td>
            <td><code>None</code> (linear)</td>
            <td><code>'mse'</code> or <code>'mae'</code></td>
            <td>1 (or N outputs)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="common-mistakes-to-avoid">Common Mistakes to
            Avoid</h3>
            <div class="sourceCode" id="cb55"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ùå WRONG: Softmax + sparse_categorical expects integer labels</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)  <span class="co"># Use with one-hot labels</span></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)  <span class="co"># where y = [0, 1, 2] ‚Üê integers</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚úÖ CORRECT: Match loss to label format</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>)  <span class="co"># For integer labels</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>)  <span class="co"># For one-hot labels</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ùå WRONG: Using sigmoid activation with CrossEntropyLoss</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)  <span class="co"># DON&#39;T do this for multi-class!</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚úÖ CORRECT: Softmax for multi-class</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚ùå WRONG: He init with sigmoid/tanh</span></span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>, kernel_initializer<span class="op">=</span><span class="st">&#39;he_normal&#39;</span>)</span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚úÖ CORRECT: Match initializer to activation</span></span>
<span id="cb55-19"><a href="#cb55-19" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>, kernel_initializer<span class="op">=</span><span class="st">&#39;glorot_uniform&#39;</span>)</span>
<span id="cb55-20"><a href="#cb55-20" aria-hidden="true" tabindex="-1"></a>layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, kernel_initializer<span class="op">=</span><span class="st">&#39;he_normal&#39;</span>)</span></code></pre></div>
            <h3 id="pytorch-equivalent-for-reference">PyTorch Equivalent
            (for reference)</h3>
            <div class="sourceCode" id="cb56"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">256</span>),</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.5</span>),</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm1d(<span class="dv">128</span>),</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(<span class="fl">0.3</span>),</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)  <span class="co"># No softmax! CrossEntropyLoss includes it</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>):</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.modules():</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Linear):</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>                nn.init.kaiming_normal_(m.weight, nonlinearity<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>                nn.init.zeros_(m.bias)</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Training uses:</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)</span></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a><span class="co"># criterion = nn.CrossEntropyLoss()  # Combines LogSoftmax + NLLLoss</span></span></code></pre></div>
            <hr />
            <h2 id="batch-normalization">1.18 Batch Normalization</h2>
            <h3 id="the-problem-internal-covariate-shift">The Problem:
            Internal Covariate Shift</h3>
            <p><strong>Internal Covariate Shift (ICS)</strong> refers to
            the phenomenon where the distribution of inputs to each
            layer changes during training because the weights of
            previous layers are constantly being updated. Let‚Äôs
            understand why this is problematic.</p>
            <p><strong>What does ‚Äúdistribution shift‚Äù actually
            mean?</strong></p>
            <p>Consider a hidden layer that receives activations from
            the previous layer. At the start of training:</p>
            <ul>
            <li>The input activations might have mean ‚âà 0.5, std ‚âà
            0.3</li>
            <li>The layer learns weights optimized for this
            distribution</li>
            </ul>
            <p>After a few gradient updates to earlier layers:</p>
            <ul>
            <li>The same inputs now produce activations with mean ‚âà 0.8,
            std ‚âà 0.1</li>
            <li>The layer‚Äôs weights are now suboptimal for this new
            distribution</li>
            <li>It must re-adapt, but by the time it does, the
            distribution shifts again!</li>
            </ul>
            <p><img src="figures/internal_covariate_shift.png"
            alt="Internal Covariate Shift" /> <em>Figure: Internal
            Covariate Shift visualization. At training step 1, Layer 3
            receives inputs with mean=0.5, std=0.3 and learns weights
            for that distribution. By step 100, the distribution has
            shifted (mean=0.8, std=0.1), making the learned weights
            suboptimal.</em></p>
            <p><strong>Why is this problematic?</strong></p>
            <ol type="1">
            <li><strong>Chasing a moving target</strong>: Each layer
            tries to learn, but its optimal weights depend on the input
            distribution, which keeps changing</li>
            <li><strong>Requires small learning rates</strong>: Large
            updates cause dramatic distribution shifts, destabilizing
            training</li>
            <li><strong>Slower convergence</strong>: Layers waste
            capacity constantly re-adapting instead of learning useful
            features</li>
            <li><strong>Saturated activations</strong>: If distributions
            shift into saturation regions of sigmoid/tanh, gradients
            vanish</li>
            </ol>
            <p><strong>A concrete example</strong>:</p>
            <div class="sourceCode" id="cb57"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Without BatchNorm: distributions shift wildly</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer 1 output at step 0:    mean=-0.02, std=0.98  </span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer 1 output at step 100:  mean=2.31,  std=3.45   ‚Üê 100x shift!</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer 1 output at step 1000: mean=-0.87, std=0.23  ‚Üê shifted again!</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="co"># With BatchNorm: distributions stay normalized</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer 1 output (after BN) at any step: mean‚âà0, std‚âà1 (before Œ≥,Œ≤)</span></span></code></pre></div>
            <h3 id="modern-perspective-beyond-ics">Modern Perspective:
            Beyond ICS</h3>
            <p>While the ICS explanation is intuitive, recent research
            suggests BatchNorm may help primarily through other
            mechanisms:</p>
            <ol type="1">
            <li><strong>Smoother loss landscape</strong>: BN makes the
            optimization surface more well-behaved with fewer sharp
            cliffs</li>
            <li><strong>Gradient flow</strong>: Normalized activations
            have more stable gradient magnitudes</li>
            <li><strong>Implicit learning rate adaptation</strong>: BN
            effectively adjusts the learning rate for each layer</li>
            </ol>
            <p>Regardless of the exact mechanism, the empirical benefits
            are clear: BatchNorm enables faster, more stable
            training.</p>
            <h3 id="the-solution-normalize-each-layer">The Solution:
            Normalize Each Layer</h3>
            <p><strong>Batch Normalization</strong> (Ioffe &amp;
            Szegedy, 2015) normalizes activations within each
            mini-batch:</p>
            <p><span class="math display">\[\hat{x}_i = \frac{x_i -
            \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(\mu_B =
            \frac{1}{B}\sum_{i=1}^{B} x_i\)</span> (batch mean)</li>
            <li><span class="math inline">\(\sigma_B^2 =
            \frac{1}{B}\sum_{i=1}^{B} (x_i - \mu_B)^2\)</span> (batch
            variance)</li>
            <li><span class="math inline">\(\epsilon\)</span> ‚âà <span
            class="math inline">\(10^{-5}\)</span> (for numerical
            stability)</li>
            </ul>
            <h3 id="learnable-parameters">Learnable Parameters</h3>
            <p>After normalizing, we add <strong>learnable scale and
            shift</strong> parameters:</p>
            <p><span class="math display">\[y_i = \gamma \hat{x}_i +
            \beta\]</span></p>
            <ul>
            <li><span class="math inline">\(\gamma\)</span> (gamma):
            learned scale parameter</li>
            <li><span class="math inline">\(\beta\)</span> (beta):
            learned shift parameter</li>
            </ul>
            <p><strong>Why?</strong> The network might need non-zero
            mean or non-unit variance for some layers. These parameters
            let it learn the optimal distribution.</p>
            <h3 id="training-vs-inference-1">Training vs Inference</h3>
            <p><strong>Training</strong>: Use batch statistics (<span
            class="math inline">\(\mu_B\)</span>, <span
            class="math inline">\(\sigma_B^2\)</span>)</p>
            <p><strong>Inference</strong>: Use <strong>running
            averages</strong> of statistics accumulated during
            training</p>
            <div class="sourceCode" id="cb58"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># During training, PyTorch automatically tracks:</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="co"># running_mean = momentum * running_mean + (1 - momentum) * batch_mean</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co"># running_var = momentum * running_var + (1 - momentum) * batch_var</span></span></code></pre></div>
            <p>‚ö†Ô∏è <strong>Critical</strong>: Always set
            <code>model.eval()</code> before inference to use running
            statistics!</p>
            <h3 id="where-to-place-batchnorm">Where to Place
            BatchNorm?</h3>
            <div class="sourceCode" id="cb59"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 1: After linear, before activation (original paper)</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>nn.BatchNorm1d(<span class="dv">128</span>),</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>nn.ReLU(),</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 2: After activation (sometimes used)</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>nn.ReLU(),</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>nn.BatchNorm1d(<span class="dv">128</span>),</span></code></pre></div>
            <p>Both work in practice. Option 1 is more common.</p>
            <h3 id="benefits-of-batch-normalization">Benefits of Batch
            Normalization</h3>
            <ol type="1">
            <li><strong>Enables higher learning rates</strong>:
            Normalized activations are more stable</li>
            <li><strong>Regularization effect</strong>: Batch statistics
            add noise (like dropout)</li>
            <li><strong>Reduces sensitivity to initialization</strong>:
            Normalization handles bad init</li>
            <li><strong>Smoother loss landscape</strong>: Easier
            optimization</li>
            </ol>
            <h3 id="pytorch-implementation-2">PyTorch
            Implementation</h3>
            <div class="sourceCode" id="cb60"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For 1D data (fully connected layers): BatchNorm1d</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">784</span>, <span class="dv">256</span>),</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    nn.BatchNorm1d(<span class="dv">256</span>),  <span class="co"># Normalize 256 features</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">256</span>, <span class="dv">10</span>)</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co"># For 2D data (CNNs): BatchNorm2d</span></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> nn.Sequential(</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    nn.BatchNorm2d(<span class="dv">64</span>),  <span class="co"># Normalize 64 channels</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="co"># CRITICAL: Set mode correctly!</span></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>model.train()  <span class="co"># Use batch statistics</span></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()   <span class="co"># Use running statistics</span></span></code></pre></div>
            <h3 id="layer-normalization-alternative">Layer Normalization
            (Alternative)</h3>
            <p>For transformers and RNNs, <strong>Layer
            Normalization</strong> is preferred:</p>
            <table>
            <thead>
            <tr>
            <th>Feature</th>
            <th>BatchNorm</th>
            <th>LayerNorm</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Normalizes across</td>
            <td>Batch dimension</td>
            <td>Feature dimension</td>
            </tr>
            <tr>
            <td>Depends on batch size</td>
            <td>Yes</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Works with batch=1</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Used in</td>
            <td>CNNs</td>
            <td>Transformers, RNNs</td>
            </tr>
            </tbody>
            </table>
            <p><img src="figures/batch_layer_norm.png"
            alt="Batch Norm vs Layer Norm" /> <em>Figure: Batch
            Normalization vs Layer Normalization. BatchNorm (left)
            computes mean and variance across the batch dimension for
            each feature (highlighted column). LayerNorm (right)
            computes statistics across the feature dimension for each
            sample independently (highlighted row). This makes LayerNorm
            batch-independent and suitable for variable-length
            sequences.</em></p>
            <div class="sourceCode" id="cb61"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LayerNorm normalizes across features, not batch</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co"># For a tensor of shape (batch, seq_len, hidden), normalizes across hidden</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>nn.LayerNorm(hidden_size)</span></code></pre></div>
            <h3 id="rmsnorm-the-modern-llm-standard">RMSNorm: The Modern
            LLM Standard</h3>
            <p><strong>RMSNorm</strong> (Root Mean Square Layer
            Normalization) has become the standard for modern LLMs like
            LLaMA, Gemma, Qwen, and Mistral. It simplifies LayerNorm by
            removing the mean-centering step.</p>
            <p><strong>LayerNorm formula:</strong> <span
            class="math display">\[\text{LayerNorm}(x) = \gamma \cdot
            \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} +
            \beta\]</span></p>
            <p><strong>RMSNorm formula:</strong> <span
            class="math display">\[\text{RMSNorm}(x) = \gamma \cdot
            \frac{x}{\text{RMS}(x) + \epsilon} = \gamma \cdot
            \frac{x}{\sqrt{\frac{1}{n}\sum_i x_i^2 +
            \epsilon}}\]</span></p>
            <p><strong>Key differences:</strong></p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 36%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>LayerNorm</th>
            <th>RMSNorm</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Mean centering</strong></td>
            <td>Yes (subtracts Œº)</td>
            <td>No</td>
            </tr>
            <tr>
            <td><strong>Normalization by</strong></td>
            <td>Standard deviation œÉ</td>
            <td>RMS</td>
            </tr>
            <tr>
            <td><strong>Learnable params</strong></td>
            <td>Œ≥ (scale) and Œ≤ (shift)</td>
            <td>Œ≥ (scale) only</td>
            </tr>
            <tr>
            <td><strong>Computation</strong></td>
            <td>~15-20% slower</td>
            <td>Faster</td>
            </tr>
            <tr>
            <td><strong>Used in</strong></td>
            <td>BERT, GPT-2, original Transformer</td>
            <td>LLaMA, Gemma, Qwen, Mistral</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why does RMSNorm work without mean
            centering?</strong></p>
            <p>The key insight is that for well-behaved activations
            (especially after pre-norm architecture), the mean is often
            close to zero anyway. The primary purpose of normalization
            is to <strong>control the scale</strong> of activations, and
            RMS captures scale just as well as standard deviation:</p>
            <div class="sourceCode" id="cb62"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For zero-mean data: RMS ‚âà std</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>)  <span class="co"># zero-mean by construction</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;std: </span><span class="sc">{</span>x<span class="sc">.</span>std()<span class="sc">:.4f}</span><span class="ss">, RMS: </span><span class="sc">{</span>(x<span class="op">**</span><span class="dv">2</span>)<span class="sc">.</span>mean()<span class="sc">.</span>sqrt()<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="co"># std: 1.0012, RMS: 0.9987  ‚Üê Nearly identical!</span></span></code></pre></div>
            <p><strong>Implementation:</strong></p>
            <div class="sourceCode" id="cb63"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSNorm(nn.Module):</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim: <span class="bu">int</span>, eps: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(dim))  <span class="co"># Œ≥ (scale only, no Œ≤)</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># RMS = sqrt(mean(x^2))</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>        rms <span class="op">=</span> torch.sqrt(torch.mean(x <span class="op">**</span> <span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.weight <span class="op">*</span> (x <span class="op">/</span> rms)</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage in a transformer block (Pre-LN architecture)</span></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, n_heads):</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> RMSNorm(dim)</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> MultiHeadAttention(dim, n_heads)</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> RMSNorm(dim)</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FeedForward(dim)</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.norm1(x))  <span class="co"># Pre-norm: normalize BEFORE attention</span></span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.norm2(x))   <span class="co"># Pre-norm: normalize BEFORE FFN</span></span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
            <p><strong>Performance advantage:</strong></p>
            <p>The speedup comes from fewer operations: - LayerNorm:
            compute mean, subtract mean, compute variance, divide by std
            - RMSNorm: compute mean of squares, divide by RMS</p>
            <p>For large hidden dimensions (4096+) common in LLMs, this
            ~15-20% speedup in normalization adds up across billions of
            tokens.</p>
            <h3 id="normalization-comparison-summary">Normalization
            Comparison Summary</h3>
            <table>
            <colgroup>
            <col style="width: 28%" />
            <col style="width: 16%" />
            <col style="width: 35%" />
            <col style="width: 18%" />
            </colgroup>
            <thead>
            <tr>
            <th>Normalization</th>
            <th>Formula</th>
            <th>Batch-Independent?</th>
            <th>Best For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>BatchNorm</strong></td>
            <td><span class="math inline">\((x - \mu_B) /
            \sigma_B\)</span></td>
            <td>‚ùå No</td>
            <td>CNNs, vision</td>
            </tr>
            <tr>
            <td><strong>LayerNorm</strong></td>
            <td><span class="math inline">\((x - \mu_L) /
            \sigma_L\)</span></td>
            <td>‚úÖ Yes</td>
            <td>Transformers (original)</td>
            </tr>
            <tr>
            <td><strong>RMSNorm</strong></td>
            <td><span class="math inline">\(x /
            \text{RMS}(x)\)</span></td>
            <td>‚úÖ Yes</td>
            <td>Modern LLMs</td>
            </tr>
            <tr>
            <td><strong>GroupNorm</strong></td>
            <td>Groups of channels</td>
            <td>‚úÖ Yes</td>
            <td>Small batch CNNs</td>
            </tr>
            <tr>
            <td><strong>InstanceNorm</strong></td>
            <td>Per-sample, per-channel</td>
            <td>‚úÖ Yes</td>
            <td>Style transfer</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="vanishing-and-exploding-gradients">1.19 Vanishing
            and Exploding Gradients</h2>
            <h3 id="the-problem-1">The Problem</h3>
            <p>In deep networks, gradients must flow through many layers
            during backpropagation. At each layer, the gradient is
            multiplied by the layer‚Äôs weights:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            W_1} = \frac{\partial L}{\partial z_n} \cdot W_n \cdot
            W_{n-1} \cdots W_2\]</span></p>
            <h3 id="vanishing-gradients">Vanishing Gradients</h3>
            <p>If weights are small (&lt; 1), repeated multiplication
            makes gradients <strong>exponentially smaller</strong>:</p>
            <pre><code>10 layers with gradient factor 0.5 each:
0.5^10 = 0.001

Gradient at layer 1 is 1000x smaller than at layer 10!
‚Üí Early layers learn extremely slowly</code></pre>
            <p><strong>Causes</strong>: - Sigmoid/Tanh activations
            (derivatives &lt; 1 in most regions) - Small weight
            initialization - Very deep networks</p>
            <p><strong>Symptoms</strong>: - Early layers don‚Äôt learn -
            Loss decreases very slowly - Weights near input barely
            change</p>
            <h3 id="exploding-gradients">Exploding Gradients</h3>
            <p>If weights are large (&gt; 1), gradients grow
            <strong>exponentially</strong>:</p>
            <pre><code>10 layers with gradient factor 2 each:
2^10 = 1024

Gradient at layer 1 is 1024x larger than at layer 10!
‚Üí Huge, unstable weight updates</code></pre>
            <p><strong>Causes</strong>: - Large weight initialization -
            Unstable architecture - No normalization</p>
            <p><strong>Symptoms</strong>: - Loss becomes NaN or Inf -
            Weights explode to very large values - Training completely
            fails</p>
            <h3 id="solutions">Solutions</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 55%" />
            </colgroup>
            <thead>
            <tr>
            <th>Problem</th>
            <th>Solutions</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Vanishing</strong></td>
            <td>ReLU activation (gradient = 1 for positive), skip
            connections (ResNet), LSTM/GRU gates, proper
            initialization</td>
            </tr>
            <tr>
            <td><strong>Exploding</strong></td>
            <td>Gradient clipping, proper initialization, batch
            normalization, weight regularization</td>
            </tr>
            </tbody>
            </table>
            <h3 id="gradient-clipping">Gradient Clipping</h3>
            <p>Cap the gradient norm to prevent explosion:</p>
            <div class="sourceCode" id="cb66"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clip gradient norm to max value of 1.0</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co"># In training loop:</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)  <span class="co"># Add this!</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span></code></pre></div>
            <h3 id="skip-connections-resnet">Skip Connections
            (ResNet)</h3>
            <p>Add the input directly to the output, creating a
            ‚Äúgradient highway‚Äù:</p>
            <p><span class="math display">\[y = F(x) + x\]</span></p>
            <div class="sourceCode" id="cb67"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResidualBlock(nn.Module):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim):</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, dim),</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim, dim)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.layers(x)  <span class="co"># Skip connection!</span></span></code></pre></div>
            <p><strong>Why it helps</strong>: Gradients can flow
            directly through the skip connection, bypassing problematic
            layers.</p>
            <hr />
            <h2 id="numerical-stability">1.20 Numerical Stability</h2>
            <h3 id="what-is-numerical-stability">What is Numerical
            Stability?</h3>
            <p><strong>Numerical stability</strong> in neural networks
            refers to the ability of computations to produce consistent,
            finite, and meaningful results despite the inherent
            limitations of floating-point arithmetic. A numerically
            stable implementation:</p>
            <ol type="1">
            <li><strong>Keeps values in representable ranges</strong>:
            Prevents overflow (values too large ‚Üí <code>inf</code>) and
            underflow (values too small ‚Üí <code>0</code>) as activations
            pass through many layers</li>
            <li><strong>Minimizes accumulation of rounding
            errors</strong>: Ensures small errors don‚Äôt compound into
            large errors over many operations</li>
            <li><strong>Produces consistent results</strong>: Given the
            same inputs, returns the same outputs (determinism)</li>
            </ol>
            <p><strong>Why it matters for training</strong>: When values
            become very large or very small, gradients can explode or
            vanish, loss becomes <code>NaN</code>, and training fails.
            Normalization techniques (BatchNorm, LayerNorm, RMSNorm)
            help keep activations in a ‚Äúnumerically healthy‚Äù range where
            floating-point precision is good and gradients flow
            properly.</p>
            <h3
            id="the-original-sin-floating-point-non-associativity">The
            Original Sin: Floating-Point Non-Associativity</h3>
            <p>The fundamental source of numerical variation in neural
            networks is <strong>floating-point
            non-associativity</strong>:</p>
            <p><span class="math display">\[
            (a + b) + c \neq a + (b + c) \quad \text{in floating-point!}
            \]</span></p>
            <div class="sourceCode" id="cb68"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is NOT a bug - it&#39;s how floating-point works!</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> (<span class="fl">0.1</span> <span class="op">+</span> <span class="fl">1e20</span>) <span class="op">-</span> <span class="fl">1e20</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="fl">0.1</span> <span class="op">+</span> (<span class="fl">1e20</span> <span class="op">-</span> <span class="fl">1e20</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="fl">0.1</span></span></code></pre></div>
            <p><strong>Why does this happen?</strong></p>
            <p>Floating-point numbers use a format like <span
            class="math inline">\(\text{mantissa} \times
            10^{\text{exponent}}\)</span> (in base 2, but the principle
            is the same). This allows representing both very small and
            very large values with limited precision.</p>
            <p>When adding two numbers with <strong>different
            scales</strong> (different exponents), information is
            lost:</p>
            <pre><code>  1.23 √ó 10¬≥   (1230)
+ 2.34 √ó 10¬π   (23.4)
= 1.2534 √ó 10¬≥ (exact: 1253.4)

But with 3 digits of precision, we can only store: 1.25 √ó 10¬≥ (1250)
The &quot;34&quot; is lost!</code></pre>
            <p><strong>Every time we add floating-point numbers in a
            different order, we can get different results:</strong></p>
            <div class="sourceCode" id="cb70"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> [<span class="fl">1e-10</span>, <span class="fl">1e-5</span>, <span class="fl">1e-2</span>, <span class="dv">1</span>, <span class="op">-</span><span class="fl">1e-10</span>, <span class="op">-</span><span class="fl">1e-5</span>, <span class="op">-</span><span class="fl">1e-2</span>, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>):</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    random.shuffle(vals)</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>    results.add(<span class="bu">sum</span>(vals))</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Summing 8 values in different orders gives </span><span class="sc">{</span><span class="bu">len</span>(results)<span class="sc">}</span><span class="ss"> unique results!&quot;</span>)</span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: 102 unique results!</span></span></code></pre></div>
            <p><strong>Key insight</strong>: The mathematically
            ‚Äúcorrect‚Äù answer depends on <em>which order</em> you perform
            the additions. There is no single ‚Äúright‚Äù answer‚Äîjust
            different answers for different orderings.</p>
            <h3
            id="determinism-vs-batch-invariance-advanced">Determinism vs
            Batch Invariance (Advanced)</h3>
            <p>A common misconception is that GPU nondeterminism comes
            from ‚Äúconcurrent threads finishing in random order using
            atomic adds.‚Äù While this <em>can</em> cause nondeterminism,
            the <strong>real culprit in modern LLM inference is lack of
            batch invariance</strong>.</p>
            <p><strong>Run-to-run determinism vs user-observable
            determinism:</strong></p>
            <table>
            <colgroup>
            <col style="width: 18%" />
            <col style="width: 22%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>Definition</th>
            <th>Do modern LLM kernels have it?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Run-to-run determinism</strong></td>
            <td>Same inputs ‚Üí same outputs</td>
            <td>‚úÖ Yes (for forward pass)</td>
            </tr>
            <tr>
            <td><strong>Batch invariance</strong></td>
            <td>Same element gives same result regardless of batch
            size</td>
            <td>‚ùå Often no</td>
            </tr>
            <tr>
            <td><strong>User-observable determinism</strong></td>
            <td>Same query ‚Üí same response</td>
            <td>‚ùå No (because batch size varies)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The batch invariance problem:</strong></p>
            <div class="sourceCode" id="cb71"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>torch.set_default_device(<span class="st">&#39;cuda&#39;</span>)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix-vector multiply (batch size = 1)</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1000</span>, <span class="dv">1000</span>, <span class="dv">2048</span><span class="op">*</span><span class="dv">4096</span>).reshape(<span class="dv">2048</span>, <span class="dv">4096</span>)</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">1000</span>, <span class="dv">1000</span>, <span class="dv">4096</span><span class="op">*</span><span class="dv">4096</span>).reshape(<span class="dv">4096</span>, <span class="dv">4096</span>)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>out1 <span class="op">=</span> torch.mm(a[:<span class="dv">1</span>], b)         <span class="co"># Batch size 1</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>out2 <span class="op">=</span> torch.mm(a, b)[:<span class="dv">1</span>]         <span class="co"># Same element, but batch size 2048</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((out1 <span class="op">-</span> out2).<span class="bu">abs</span>().<span class="bu">max</span>())  <span class="co"># tensor(1669.25) - NOT zero!</span></span></code></pre></div>
            <p>The same element gives different results depending on
            batch size! When server load varies, batch size varies, and
            individual requests become nondeterministic‚Äîeven though the
            kernel itself is ‚Äúdeterministic.‚Äù</p>
            <p><strong>Why this happens:</strong> Different batch sizes
            may trigger different: - Parallelization strategies
            (data-parallel vs split-k) - Tile sizes in matrix
            multiplication<br />
            - Reduction orderings in attention</p>
            <p>To achieve true determinism, kernels must be
            <strong>batch-invariant</strong>: using fixed reduction
            strategies regardless of batch size. This is an active area
            of research for reproducible LLM inference.</p>
            <h3 id="the-log-sum-exp-trick">The Log-Sum-Exp Trick</h3>
            <p>Computing softmax naively can overflow or underflow:</p>
            <div class="sourceCode" id="cb72"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Can overflow!</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.array([<span class="dv">1000</span>, <span class="dv">1001</span>, <span class="dv">1002</span>])</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>exp_z <span class="op">=</span> np.exp(z)  <span class="co"># [inf, inf, inf] - OVERFLOW!</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Can underflow!</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1000</span>, <span class="op">-</span><span class="dv">1001</span>, <span class="op">-</span><span class="dv">1002</span>])</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>exp_z <span class="op">=</span> np.exp(z)  <span class="co"># [0, 0, 0] - UNDERFLOW!</span></span></code></pre></div>
            <p><strong>Solution</strong>: Subtract the maximum value
            before exponentiating:</p>
            <p><span class="math display">\[\text{softmax}(z_i) =
            \frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{z_i -
            \max(z)}}{\sum_j e^{z_j - \max(z)}}\]</span></p>
            <div class="sourceCode" id="cb73"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD: Numerically stable softmax</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stable_softmax(z):</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    z_shifted <span class="op">=</span> z <span class="op">-</span> np.<span class="bu">max</span>(z)  <span class="co"># Shift to prevent overflow</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    exp_z <span class="op">=</span> np.exp(z_shifted)</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_z <span class="op">/</span> np.<span class="bu">sum</span>(exp_z)</span></code></pre></div>
            <h3 id="cross-entropy-with-logits">Cross-Entropy with
            Logits</h3>
            <p>Never compute cross-entropy from probabilities ‚Äî always
            from logits:</p>
            <div class="sourceCode" id="cb74"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: Numerically unstable</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> softmax(logits)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>np.<span class="bu">sum</span>(y <span class="op">*</span> np.log(probs))  <span class="co"># log(0) = -inf!</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD: Use built-in function that handles stability</span></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()(logits, labels)  <span class="co"># PyTorch handles it!</span></span></code></pre></div>
            <p>PyTorch‚Äôs <code>CrossEntropyLoss</code> combines
            log-softmax with NLL loss in a numerically stable way.</p>
            <h3 id="epsilon-for-logarithms">Epsilon for Logarithms</h3>
            <p>Always add a small epsilon when taking logarithms:</p>
            <div class="sourceCode" id="cb75"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BAD: log(0) = -inf</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>np.mean(y <span class="op">*</span> np.log(y_pred))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="co"># GOOD: Add epsilon</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-15</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>np.mean(y <span class="op">*</span> np.log(y_pred <span class="op">+</span> eps))</span></code></pre></div>
            <h3 id="common-numerical-issues-and-fixes">Common Numerical
            Issues and Fixes</h3>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 42%" />
            <col style="width: 23%" />
            </colgroup>
            <thead>
            <tr>
            <th>Issue</th>
            <th>Symptom</th>
            <th>Fix</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Log of zero</strong></td>
            <td><code>-inf</code> or <code>NaN</code></td>
            <td>Add epsilon: <code>log(x + 1e-15)</code></td>
            </tr>
            <tr>
            <td><strong>Exp overflow</strong></td>
            <td><code>inf</code></td>
            <td>Use log-sum-exp trick</td>
            </tr>
            <tr>
            <td><strong>Division by zero</strong></td>
            <td><code>NaN</code> or <code>inf</code></td>
            <td>Add epsilon to denominator</td>
            </tr>
            <tr>
            <td><strong>Gradient explosion</strong></td>
            <td><code>NaN</code> loss</td>
            <td>Gradient clipping, lower LR</td>
            </tr>
            <tr>
            <td><strong>Float32 precision</strong></td>
            <td>Accumulated errors</td>
            <td>Use <code>float64</code> for sensitive ops</td>
            </tr>
            </tbody>
            </table>
            <blockquote>
            <p><strong>Further Reading</strong>: <a
            href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/">Defeating
            Nondeterminism in LLM Inference</a> ‚Äî Deep dive into
            floating-point non-associativity and batch invariance in
            production LLM systems.</p>
            </blockquote>
            <hr />
            <h2 id="optimizers-sgd-momentum-and-adam">1.21 Optimizers:
            SGD, Momentum, and Adam</h2>
            <h3 id="why-different-optimizers">Why Different
            Optimizers?</h3>
            <p>In Section 1.3, we introduced vanilla gradient descent:
            <span class="math display">\[w \leftarrow w - \alpha \nabla
            L(w)\]</span></p>
            <p>This works but has problems: - <strong>Gets
            stuck</strong> in ravines (oscillates back and forth) -
            <strong>Same learning rate</strong> for all parameters
            (suboptimal) - <strong>Sensitive</strong> to learning rate
            choice</p>
            <p>Different optimizers address these issues.</p>
            <h3 id="sgd-with-momentum">SGD with Momentum</h3>
            <p><strong>The problem</strong>: Vanilla SGD oscillates in
            ravines‚Äîdirections with high curvature see the gradient flip
            sign repeatedly, slowing convergence.</p>
            <p><strong>The solution</strong>: Add ‚Äúmomentum‚Äù like a ball
            rolling downhill. Accumulate velocity from past
            gradients:</p>
            <p><span class="math display">\[v_t = \beta v_{t-1} + \nabla
            L(w_t)\]</span> <span class="math display">\[w_{t+1} = w_t -
            \alpha v_t\]</span></p>
            <p>where <span class="math inline">\(\beta\)</span>
            (typically 0.9) controls how much past gradients matter.</p>
            <p><strong>Why it works</strong>: - Gradients that
            consistently point the same direction
            <strong>accumulate</strong> ‚Üí faster progress - Gradients
            that oscillate <strong>cancel out</strong> ‚Üí dampens
            oscillations</p>
            <div class="sourceCode" id="cb76"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Momentum SGD (simplified)</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> compute_gradient(w)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta <span class="op">*</span> v <span class="op">+</span> grad           <span class="co"># Accumulate velocity</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> v                <span class="co"># Update weights</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code></pre></div>
            <h3 id="adam-adaptive-moment-estimation">Adam: Adaptive
            Moment Estimation</h3>
            <p><strong>Key insight</strong>: Different parameters need
            different learning rates!</p>
            <p>Adam combines two ideas: 1. <strong>Momentum</strong>
            (first moment): Accumulate gradient direction 2.
            <strong>RMSprop</strong> (second moment): Adapt learning
            rate per parameter based on gradient magnitude</p>
            <p><span class="math display">\[m_t = \beta_1 m_{t-1} +
            (1-\beta_1) g_t \quad \text{(momentum/first
            moment)}\]</span> <span class="math display">\[v_t = \beta_2
            v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(squared
            gradients/second moment)}\]</span> <span
            class="math display">\[\hat{m}_t = m_t / (1-\beta_1^t) \quad
            \text{(bias correction)}\]</span> <span
            class="math display">\[\hat{v}_t = v_t / (1-\beta_2^t) \quad
            \text{(bias correction)}\]</span> <span
            class="math display">\[w_{t+1} = w_t - \alpha
            \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\]</span></p>
            <p><strong>Typical values</strong>: <span
            class="math inline">\(\beta_1 = 0.9\)</span>, <span
            class="math inline">\(\beta_2 = 0.999\)</span>, <span
            class="math inline">\(\epsilon = 10^{-8}\)</span></p>
            <p><strong>Why it works</strong>: - Parameters with
            consistently large gradients get smaller effective learning
            rate (denominator is big) - Parameters with small gradients
            get larger effective learning rate (denominator is small) -
            This ‚Äúadapts‚Äù to each parameter‚Äôs scale automatically</p>
            <p><img src="figures/adam_intuition.png"
            alt="Adam Intuition" /> <em>Figure: Adam combines momentum
            (accumulating gradient direction) with adaptive learning
            rates (scaling by inverse gradient magnitude). Parameters
            with large, consistent gradients move faster in the right
            direction. Parameters with noisy gradients get dampened. The
            bias correction ensures early steps aren‚Äôt dominated by
            initialization.</em></p>
            <div class="sourceCode" id="cb77"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span></code></pre></div>
            <h3 id="adamw-decoupled-weight-decay">AdamW: Decoupled
            Weight Decay</h3>
            <p><strong>The problem with Adam + L2</strong>: In Adam, L2
            regularization gets scaled by the adaptive learning rate,
            weakening its effect.</p>
            <p><strong>AdamW</strong> decouples weight decay from the
            gradient-based update:</p>
            <div class="sourceCode" id="cb78"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adam with L2 (problematic):</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>grad_with_l2 <span class="op">=</span> grad <span class="op">+</span> <span class="kw">lambda</span> <span class="op">*</span> w</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ... Adam update using grad_with_l2</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Problem: weight decay is scaled by adaptive LR</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW (correct):</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ... Adam update using grad only</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> <span class="kw">lambda</span> <span class="op">*</span> w  <span class="co"># Weight decay applied separately</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Weight decay applied consistently</span></span></code></pre></div>
            <div class="sourceCode" id="cb79"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div>
            <p><strong>AdamW is the default for modern
            transformers/LLMs.</strong></p>
            <h3 id="quick-comparison">Quick Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Optimizer</th>
            <th>Adaptive LR?</th>
            <th>Momentum?</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>SGD</strong></td>
            <td>‚ùå</td>
            <td>‚ùå</td>
            <td>Simple baselines</td>
            </tr>
            <tr>
            <td><strong>SGD + Momentum</strong></td>
            <td>‚ùå</td>
            <td>‚úÖ</td>
            <td>Vision models, good generalization</td>
            </tr>
            <tr>
            <td><strong>Adam</strong></td>
            <td>‚úÖ</td>
            <td>‚úÖ</td>
            <td>Fast convergence, most tasks</td>
            </tr>
            <tr>
            <td><strong>AdamW</strong></td>
            <td>‚úÖ</td>
            <td>‚úÖ</td>
            <td><strong>Transformers, LLMs</strong> (default)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="when-to-use-which-1">When to Use Which?</h3>
            <table>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Recommended</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>LLMs / Transformers</strong></td>
            <td>AdamW with warmup + cosine decay</td>
            </tr>
            <tr>
            <td><strong>CNNs / Vision</strong></td>
            <td>SGD + Momentum (often generalizes better)</td>
            </tr>
            <tr>
            <td><strong>Quick experimentation</strong></td>
            <td>Adam (fast convergence)</td>
            </tr>
            <tr>
            <td><strong>Reproducibility critical</strong></td>
            <td>SGD (simpler, fewer hyperparameters)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-sgd-vs-adam-generalization-debate">The SGD vs
            Adam Generalization Debate</h3>
            <p><strong>Empirical observation</strong>: SGD with momentum
            often finds solutions that <strong>generalize
            better</strong> than Adam, particularly for vision
            models.</p>
            <p><strong>Hypotheses</strong>: - Adam converges to sharper
            minima (overfit) - SGD‚Äôs noise helps escape bad minima -
            Adam‚Äôs adaptivity can be too aggressive</p>
            <p><strong>In practice</strong>: For LLMs, AdamW wins. For
            vision, try both.</p>
            <blockquote>
            <p><strong>Deep dive</strong>: See Part 2 (Core Theory) and
            Part 5 (Optimization) for detailed mathematical treatment of
            convergence, learning rate schedules, and second-order
            methods.</p>
            </blockquote>
            <hr />
            <h2 id="common-interview-questions-summary">1.22 Common
            Interview Questions Summary</h2>
            <h3 id="architecture-training">Architecture &amp;
            Training</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Question</th>
            <th>Key Points</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Why use ReLU over sigmoid?</strong></td>
            <td>No vanishing gradient for positive values; sparse
            activations; computationally simple</td>
            </tr>
            <tr>
            <td><strong>Why use cross-entropy over MSE for
            classification?</strong></td>
            <td>Stronger penalty for confident wrong predictions; no
            gradient vanishing at extremes; probabilistic
            interpretation</td>
            </tr>
            <tr>
            <td><strong>What does batch normalization do?</strong></td>
            <td>Normalizes activations; enables higher learning rates;
            provides regularization; reduces internal covariate
            shift</td>
            </tr>
            <tr>
            <td><strong>Why use dropout?</strong></td>
            <td>Prevents co-adaptation of neurons; implicit ensemble;
            regularization without changing architecture</td>
            </tr>
            <tr>
            <td><strong>How does backpropagation work?</strong></td>
            <td>Chain rule applied backwards; each layer computes local
            gradient; multiply by upstream gradient; O(n)
            complexity</td>
            </tr>
            </tbody>
            </table>
            <h3 id="optimization">Optimization</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Question</th>
            <th>Key Points</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Why use mini-batches?</strong></td>
            <td>Balance between noise (small batch) and stability (large
            batch); GPU parallelism; unbiased gradient estimate</td>
            </tr>
            <tr>
            <td><strong>What is vanishing gradient?</strong></td>
            <td>Gradients shrink exponentially in deep networks; early
            layers learn slowly; solved by ReLU, skip connections,
            proper init</td>
            </tr>
            <tr>
            <td><strong>Adam vs SGD?</strong></td>
            <td>Adam: adaptive LR, faster convergence. SGD: often better
            generalization, simpler</td>
            </tr>
            <tr>
            <td><strong>What is weight decay?</strong></td>
            <td>L2 regularization on weights; prevents overfitting;
            decoupled from gradient in AdamW</td>
            </tr>
            </tbody>
            </table>
            <h3 id="regularization">Regularization</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Question</th>
            <th>Key Points</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>L1 vs L2 regularization?</strong></td>
            <td>L1: sparsity, feature selection. L2: shrinks all
            weights, no exact zeros</td>
            </tr>
            <tr>
            <td><strong>How to diagnose overfitting?</strong></td>
            <td>Train loss &lt;&lt; validation loss; learning curves
            diverge; validation loss increases</td>
            </tr>
            <tr>
            <td><strong>How to fix overfitting?</strong></td>
            <td>More data, dropout, weight decay, early stopping, data
            augmentation, simpler model</td>
            </tr>
            </tbody>
            </table>
            <h3 id="softmax-classification">Softmax &amp;
            Classification</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Question</th>
            <th>Key Points</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Why exponential in softmax?</strong></td>
            <td>Always positive; clean gradients; temperature control;
            max entropy distribution</td>
            </tr>
            <tr>
            <td><strong>Why CrossEntropyLoss takes logits?</strong></td>
            <td>Numerical stability; combines log-softmax + NLL; avoids
            log(0)</td>
            </tr>
            <tr>
            <td><strong>Multi-label vs multi-class?</strong></td>
            <td>Multi-class: one label (softmax). Multi-label: multiple
            labels (sigmoid per class)</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="debugging-checklist">1.23 Debugging Checklist</h2>
            <p>When training doesn‚Äôt work, check these in order:</p>
            <h3 id="loss-is-nan-or-inf">1. Loss is NaN or Inf</h3>
            <pre><code>‚ñ° Learning rate too high? ‚Üí Try 10x smaller
‚ñ° Numerical instability? ‚Üí Check for log(0), exp overflow
‚ñ° Bad initialization? ‚Üí Use proper init (He/Xavier)
‚ñ° Data issue? ‚Üí Check for NaN/Inf in inputs
‚ñ° Exploding gradients? ‚Üí Add gradient clipping</code></pre>
            <h3 id="loss-doesnt-decrease">2. Loss Doesn‚Äôt Decrease</h3>
            <pre><code>‚ñ° Learning rate too low? ‚Üí Try 10x larger
‚ñ° Learning rate too high? ‚Üí Loss oscillates, try smaller
‚ñ° Bug in loss computation? ‚Üí Verify loss formula manually
‚ñ° Data not shuffled? ‚Üí Enable shuffle in DataLoader
‚ñ° Model in eval mode? ‚Üí Ensure model.train() during training
‚ñ° Gradients are zero? ‚Üí Check gradient flow, dead ReLUs
‚ñ° Optimizer not stepping? ‚Üí Verify optimizer.step() is called</code></pre>
            <h3
            id="training-loss-good-validation-loss-bad-overfitting">3.
            Training Loss Good, Validation Loss Bad (Overfitting)</h3>
            <pre><code>‚ñ° Add/increase dropout
‚ñ° Add/increase weight decay (L2)
‚ñ° Get more data
‚ñ° Data augmentation
‚ñ° Early stopping
‚ñ° Reduce model size
‚ñ° Use regularization (BatchNorm helps too)</code></pre>
            <h3 id="both-losses-high-underfitting">4. Both Losses High
            (Underfitting)</h3>
            <pre><code>‚ñ° Model too simple ‚Üí More layers/neurons
‚ñ° Training long enough? ‚Üí More epochs
‚ñ° Learning rate too low? ‚Üí Increase LR
‚ñ° Too much regularization? ‚Üí Reduce dropout/weight decay
‚ñ° Feature engineering needed? ‚Üí Better input features</code></pre>
            <h3 id="quick-sanity-checks">5. Quick Sanity Checks</h3>
            <div class="sourceCode" id="cb84"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Can model overfit single batch?</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="co"># If not, there&#39;s a bug in your model/training loop</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>small_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> train_step(small_batch)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Should be near 0: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Are gradients flowing?</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: grad norm = </span><span class="sc">{</span>param<span class="sc">.</span>grad<span class="sc">.</span>norm()<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">: NO GRADIENT!&quot;</span>)</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Are weights updating?</span></span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>old_weights <span class="op">=</span> model.fc1.weight.clone()</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>new_weights <span class="op">=</span> model.fc1.weight</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Weight change: </span><span class="sc">{</span>(new_weights <span class="op">-</span> old_weights)<span class="sc">.</span><span class="bu">abs</span>()<span class="sc">.</span>mean()<span class="sc">:.6f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
            <hr />
            <h1 id="part-2-core-theory">Part 2: Core Theory</h1>
            <h2 id="the-core-update-rule">2.1 The Core Update Rule</h2>
            <h3 id="the-question">The Question</h3>
            <p>In gradient descent, the update rule is:</p>
            <p><span class="math display">\[
            w_{t+1} = w_t - \alpha \nabla \ell(w_t)
            \]</span></p>
            <p><strong>Why do we pass <span
            class="math inline">\(w_t\)</span> to the loss function
            while also subtracting from <span
            class="math inline">\(w_t\)</span>?</strong></p>
            <h3 id="short-answer">Short Answer</h3>
            <p>Both uses of <span class="math inline">\(w_t\)</span>
            refer to the <strong>same thing</strong>: your current
            position in parameter space. The update says:</p>
            <blockquote>
            <p>‚ÄúFrom where I currently am (<span
            class="math inline">\(w_t\)</span>), take a step in the
            direction that reduces loss, where that direction is
            determined by the gradient computed at my current location
            (<span class="math inline">\(\nabla
            \ell(w_t)\)</span>).‚Äù</p>
            </blockquote>
            <h3 id="breaking-down-each-component">Breaking Down Each
            Component</h3>
            <ul>
            <li><span class="math inline">\(w_t\)</span> ‚Äî
            <strong>Current parameters</strong>: where we are in weight
            space at step <span class="math inline">\(t\)</span></li>
            <li><span class="math inline">\(\ell(w_t)\)</span> ‚Äî
            <strong>Loss evaluated at current parameters</strong>: how
            bad our model is right now</li>
            <li><span class="math inline">\(\nabla \ell(w_t)\)</span> ‚Äî
            <strong>Gradient of loss at current parameters</strong>:
            direction of steepest <em>increase</em></li>
            <li><span class="math inline">\(-\alpha \nabla
            \ell(w_t)\)</span> ‚Äî <strong>The step we take</strong>: move
            opposite to gradient (toward lower loss)</li>
            <li><span class="math inline">\(w_{t+1}\)</span> ‚Äî
            <strong>New parameters</strong>: where we‚Äôll be after this
            update</li>
            </ul>
            <hr />
            <h2 id="why-gradients-point-the-way">2.2 Why Gradients Point
            the Way</h2>
            <h3 id="intuition-hiking-in-the-fog">Intuition: Hiking in
            the Fog</h3>
            <p>Imagine you‚Äôre hiking on a mountain in dense fog. You
            can‚Äôt see the valley below, but you want to get there. What
            do you do?</p>
            <p><strong>You feel the slope under your feet.</strong> If
            the ground tilts left, going left takes you uphill. So you
            go <em>right</em> ‚Äî the opposite direction. You take a small
            step, feel the slope again, and repeat.</p>
            <p>This is exactly what gradient descent does:</p>
            <ol type="1">
            <li><strong>Feel the slope</strong> ‚Üí compute the gradient
            at current position</li>
            <li><strong>Identify uphill</strong> ‚Üí gradient points
            toward steepest ascent<br />
            </li>
            <li><strong>Walk downhill</strong> ‚Üí move in the
            <em>negative</em> gradient direction</li>
            <li><strong>Repeat</strong> ‚Üí keep stepping until you reach
            flat ground (the minimum)</li>
            </ol>
            <p>The gradient is like your feet sensing the tilt in every
            direction simultaneously. In high dimensions, there are
            millions of ‚Äúdirections‚Äù ‚Äî but the gradient tells you the
            single steepest way up, so you go the opposite way.</p>
            <h3 id="the-gradient-as-a-direction">The Gradient as a
            Direction</h3>
            <p>For a scalar function <span class="math inline">\(f:
            \mathbb{R}^n \to \mathbb{R}\)</span>, the gradient <span
            class="math inline">\(\nabla f(x)\)</span> is a vector
            that:</p>
            <ol type="1">
            <li><strong>Points in the direction of steepest
            increase</strong> of <span
            class="math inline">\(f\)</span></li>
            <li><strong>Has magnitude equal to the rate of
            increase</strong> in that direction</li>
            </ol>
            <p><span class="math display">\[
            \nabla f(x) = \begin{bmatrix} \frac{\partial f}{\partial
            x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\
            \frac{\partial f}{\partial x_n} \end{bmatrix}
            \]</span></p>
            <h3 id="directional-derivatives">Directional
            Derivatives</h3>
            <p>The rate of change of <span
            class="math inline">\(f\)</span> in direction <span
            class="math inline">\(u\)</span> (unit vector) is:</p>
            <p><span class="math display">\[
            D_u f(x) = \nabla f(x) \cdot u = \|\nabla f(x)\|
            \cos(\theta)
            \]</span></p>
            <p>where <span class="math inline">\(\theta\)</span> is the
            angle between <span class="math inline">\(\nabla
            f(x)\)</span> and <span
            class="math inline">\(u\)</span>.</p>
            <table>
            <thead>
            <tr>
            <th>Direction</th>
            <th>Angle <span class="math inline">\(\theta\)</span></th>
            <th><span class="math inline">\(\cos(\theta)\)</span></th>
            <th>Rate of Change</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Along <span class="math inline">\(\nabla f\)</span></td>
            <td><span class="math inline">\(0¬∞\)</span></td>
            <td><span class="math inline">\(+1\)</span></td>
            <td>Maximum increase</td>
            </tr>
            <tr>
            <td>Opposite to <span class="math inline">\(\nabla
            f\)</span></td>
            <td><span class="math inline">\(180¬∞\)</span></td>
            <td><span class="math inline">\(-1\)</span></td>
            <td>Maximum decrease</td>
            </tr>
            <tr>
            <td>Perpendicular</td>
            <td><span class="math inline">\(90¬∞\)</span></td>
            <td><span class="math inline">\(0\)</span></td>
            <td>No change</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Conclusion</strong>: To decrease <span
            class="math inline">\(f\)</span> fastest, move opposite to
            <span class="math inline">\(\nabla f\)</span>.</p>
            <h3 id="taylor-expansion-perspective">Taylor Expansion
            Perspective</h3>
            <p>Expanding loss around current point <span
            class="math inline">\(w_t\)</span>:</p>
            <p><span class="math display">\[
            \ell(w_t + \Delta w) \approx \ell(w_t) + \nabla
            \ell(w_t)^\top \Delta w + \frac{1}{2} \Delta w^\top H \Delta
            w
            \]</span></p>
            <p>where <span class="math inline">\(H = \nabla^2
            \ell(w_t)\)</span> is the Hessian matrix.</p>
            <p><strong>First-order methods</strong> (GD, SGD, Adam) use
            only the gradient term. <strong>Second-order
            methods</strong> (Newton, BFGS) also use curvature from
            <span class="math inline">\(H\)</span>.</p>
            <hr />
            <h2 id="stochastic-gradient-descent-sgd">2.3 Stochastic
            Gradient Descent (SGD)</h2>
            <p><strong>Stochastic Gradient Descent (SGD)</strong> is a
            fundamental optimization algorithm used in machine learning
            and deep learning to minimize a model‚Äôs loss. Unlike
            standard Gradient Descent, which calculates the gradient
            over the <em>entire</em> dataset before updating parameters,
            SGD approximates this by using only a single random data
            point (or a small ‚Äúmini-batch‚Äù) at each step.</p>
            <h3 id="how-sgd-works">How SGD Works</h3>
            <p>The algorithm iteratively adjusts model parameters <span
            class="math inline">\(\theta\)</span> (weights and biases)
            to reach the ‚Äúvalley‚Äù or minimum of the loss function:</p>
            <ol type="1">
            <li><strong>Initialize</strong>: Start with random parameter
            values</li>
            <li><strong>Random Selection</strong>: Pick one random
            sample (or a small batch) from the training data</li>
            <li><strong>Compute Gradient</strong>: Calculate the slope
            (gradient) of the loss function based only on that
            sample</li>
            <li><strong>Update</strong>: Adjust parameters in the
            opposite direction of the gradient using a learning rate
            <span class="math inline">\(\eta\)</span>: <span
            class="math display">\[\theta := \theta - \eta \cdot
            \nabla_\theta \ell(x_i, y_i; \theta)\]</span></li>
            <li><strong>Repeat</strong>: Continue until the loss stops
            decreasing significantly (convergence)</li>
            </ol>
            <h3
            id="concrete-example-linear-regression-with-sgd">Concrete
            Example: Linear Regression with SGD</h3>
            <p>To make this concrete, let‚Äôs apply SGD to <strong>linear
            regression</strong>. Suppose we want to fit a straight
            line:</p>
            <p><span class="math display">\[\hat{y} = w_1 + w_2
            x\]</span></p>
            <p>to a training set with observations <span
            class="math inline">\(\{(x_1, y_1), (x_2, y_2), \ldots,
            (x_n, y_n)\}\)</span>.</p>
            <p><strong>The objective function</strong> (using least
            squares) is:</p>
            <p><span class="math display">\[Q(w) = \sum_{i=1}^n Q_i(w) =
            \sum_{i=1}^n \left(\hat{y}_i - y_i\right)^2 = \sum_{i=1}^n
            \left(w_1 + w_2 x_i - y_i\right)^2\]</span></p>
            <p><strong>The SGD update</strong> for a single randomly
            selected point <span class="math inline">\((x_i,
            y_i)\)</span>:</p>
            <p><span class="math display">\[
            \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \leftarrow
            \begin{bmatrix} w_1 \\ w_2 \end{bmatrix}
            - \eta \begin{bmatrix} \frac{\partial}{\partial w_1} (w_1 +
            w_2 x_i - y_i)^2 \\
              \frac{\partial}{\partial w_2} (w_1 + w_2 x_i - y_i)^2
            \end{bmatrix}
            \]</span></p>
            <p>Computing the partial derivatives:</p>
            <p><span class="math display">\[
            = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix}
            - \eta \begin{bmatrix} 2 (w_1 + w_2 x_i - y_i) \\ 2 x_i(w_1
            + w_2 x_i - y_i) \end{bmatrix}
            \]</span></p>
            <p><strong>Key insight</strong>: In each iteration, the
            gradient is evaluated at only a <strong>single
            point</strong> <span class="math inline">\((x_i,
            y_i)\)</span> ‚Äî not the entire dataset. This is what makes
            it ‚Äústochastic.‚Äù</p>
            <p><strong>Python implementation</strong>:</p>
            <div class="sourceCode" id="cb85"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd_linear_regression(X, y, lr<span class="op">=</span><span class="fl">0.01</span>, epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;SGD for linear regression: y = w1 + w2 * x&quot;&quot;&quot;</span></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>    w1, w2 <span class="op">=</span> <span class="fl">0.0</span>, <span class="fl">0.0</span>  <span class="co"># Initialize weights</span></span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shuffle data for randomness</span></span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.permutation(n)</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> indices:</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Prediction error for single point</span></span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> (w1 <span class="op">+</span> w2 <span class="op">*</span> X[i]) <span class="op">-</span> y[i]</span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update weights based on this ONE point</span></span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a>            w1 <span class="op">=</span> w1 <span class="op">-</span> lr <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> error           <span class="co"># ‚àÇL/‚àÇw1 = 2 * error</span></span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>            w2 <span class="op">=</span> w2 <span class="op">-</span> lr <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> error <span class="op">*</span> X[i]    <span class="co"># ‚àÇL/‚àÇw2 = 2 * error * x</span></span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w1, w2</span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="fl">2.1</span>, <span class="fl">4.0</span>, <span class="fl">5.8</span>, <span class="fl">8.1</span>, <span class="fl">9.9</span>])  <span class="co"># Approximately y = 2x</span></span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a>w1, w2 <span class="op">=</span> sgd_linear_regression(X, y, lr<span class="op">=</span><span class="fl">0.01</span>, epochs<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Fitted line: y = </span><span class="sc">{</span>w1<span class="sc">:.2f}</span><span class="ss"> + </span><span class="sc">{</span>w2<span class="sc">:.2f}</span><span class="ss">x&quot;</span>)</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Output: Fitted line: y = 0.12 + 1.98x (close to y = 0 + 2x)</span></span></code></pre></div>
            <p><img src="figures/sgd_convergence.png"
            alt="SGD Convergence for Linear Regression" /> <em>Figure:
            SGD training dynamics. Left: Loss decreases rapidly then
            stabilizes. Middle: Parameter trajectory from (0,0) to near
            the true values. Right: Fitted line closely matches true
            relationship.</em></p>
            <h3
            id="full-batch-vs-mini-batch-gradient-descent">Full-Batch vs
            Mini-Batch Gradient Descent</h3>
            <p>For a dataset with <span class="math inline">\(N\)</span>
            examples, the true gradient is:</p>
            <p><span class="math display">\[
            \nabla \ell(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla
            \ell_i(\theta)
            \]</span></p>
            <p>Computing this exactly is expensive for large datasets.
            Instead, we estimate it using a <strong>mini-batch</strong>
            <span class="math inline">\(B\)</span>:</p>
            <p><span class="math display">\[
            g_t = \frac{1}{|B|} \sum_{i \in B} \nabla \ell_i(\theta_t)
            \]</span></p>
            <p><strong>Key property</strong>: <span
            class="math inline">\(\mathbb{E}[g_t] = \nabla
            \ell(\theta_t)\)</span> ‚Äî the mini-batch gradient is an
            <strong>unbiased estimator</strong> of the true
            gradient!</p>
            <table>
            <thead>
            <tr>
            <th>Variant</th>
            <th>Samples per Update</th>
            <th>Typical Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Stochastic GD</strong></td>
            <td>1</td>
            <td>Rarely used in practice</td>
            </tr>
            <tr>
            <td><strong>Mini-batch GD</strong></td>
            <td>32-512</td>
            <td><strong>The modern standard</strong></td>
            </tr>
            <tr>
            <td><strong>Batch GD</strong></td>
            <td>All N</td>
            <td>Small datasets only</td>
            </tr>
            </tbody>
            </table>
            <h3 id="why-use-sgd">Why Use SGD?</h3>
            <p><strong>1. Efficiency</strong>: SGD is significantly
            faster for massive datasets because it updates parameters
            immediately after seeing a few samples, rather than waiting
            to process millions of data points.</p>
            <p><strong>2. Memory Friendly</strong>: Only one batch needs
            to be in memory at a time, making it essential for training
            large neural networks that wouldn‚Äôt fit otherwise.</p>
            <p><strong>3. Escaping Local Minima</strong>: The ‚Äúnoise‚Äù
            introduced by random sampling causes the optimization path
            to zig-zag. This randomness can help the algorithm ‚Äújump
            out‚Äù of shallow local minima or saddle points to find better
            solutions.</p>
            <h3 id="why-noise-can-help-implicit-regularization">Why
            Noise Can Help (Implicit Regularization)</h3>
            <p>Here‚Äôs a counterintuitive fact: the ‚Äúnoisiness‚Äù of SGD is
            actually a <strong>feature, not a bug</strong>.</p>
            <p>When you use a small mini-batch, your gradient estimate
            bounces around the true gradient. You might think this would
            lead to worse solutions. But empirically, the opposite often
            happens: <strong>noisy SGD often finds better solutions than
            exact gradient descent</strong>.</p>
            <p>Why? Think about it geometrically. Imagine the loss
            landscape has two valleys:</p>
            <ul>
            <li>A <strong>narrow, sharp valley</strong> ‚Äî training loss
            is very low here, but the slightest change in weights causes
            loss to spike</li>
            <li>A <strong>wide, flat valley</strong> ‚Äî training loss is
            similar, but nearby weights also have low loss</li>
            </ul>
            <p>With exact gradients (large batch), you might descend
            into that sharp valley and stay there. But with noisy SGD,
            random fluctuations can <strong>bounce you out of the sharp
            valley</strong>. The wide, flat valley is more stable ‚Äî
            noise doesn‚Äôt push you out because there‚Äôs more ‚Äúroom.‚Äù</p>
            <p>This matters for <strong>generalization</strong>: test
            data is slightly different from training data. A sharp
            minimum that‚Äôs perfect for training data might be terrible
            for test data (small perturbations ‚Üí big loss increases). A
            flat minimum is <strong>robust</strong> ‚Äî exactly what you
            want.</p>
            <p><strong>Bottom line</strong>: SGD noise provides
            <strong>implicit regularization</strong> that biases
            training toward solutions that generalize better.</p>
            <h3 id="batch-size-tradeoffs-1">Batch Size Tradeoffs</h3>
            <table>
            <colgroup>
            <col style="width: 17%" />
            <col style="width: 14%" />
            <col style="width: 21%" />
            <col style="width: 23%" />
            <col style="width: 23%" />
            </colgroup>
            <thead>
            <tr>
            <th>Batch Size</th>
            <th>Variance</th>
            <th>Updates/Epoch</th>
            <th>Compute/Update</th>
            <th>Generalization</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Small (<span class="math inline">\(b = 32\)</span>)</td>
            <td>High</td>
            <td>Many</td>
            <td>Fast</td>
            <td>Often better</td>
            </tr>
            <tr>
            <td>Large (<span class="math inline">\(b =
            4096\)</span>)</td>
            <td>Low</td>
            <td>Few</td>
            <td>Slow</td>
            <td>May need tuning</td>
            </tr>
            </tbody>
            </table>
            <p>Larger batches give more accurate gradient estimates but
            fewer updates per epoch. Smaller batches are noisier but
            provide more frequent updates and often better
            generalization.</p>
            <h3 id="sgd-vs-full-batch-gradient-descent">SGD vs
            Full-Batch Gradient Descent</h3>
            <table>
            <colgroup>
            <col style="width: 20%" />
            <col style="width: 38%" />
            <col style="width: 41%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Full-Batch GD</th>
            <th>Mini-Batch SGD</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Gradient</td>
            <td>Exact</td>
            <td>Noisy estimate</td>
            </tr>
            <tr>
            <td>Per-step compute</td>
            <td>High (all data)</td>
            <td>Low (subset)</td>
            </tr>
            <tr>
            <td>Updates per epoch</td>
            <td>1</td>
            <td>N / batch_size</td>
            </tr>
            <tr>
            <td>Memory</td>
            <td>Entire dataset</td>
            <td>One batch</td>
            </tr>
            <tr>
            <td>Convergence path</td>
            <td>Smooth</td>
            <td>Zig-zag</td>
            </tr>
            <tr>
            <td>Generalization</td>
            <td>Can overfit to sharp minima</td>
            <td>Often better (noise regularizes)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="key-variations">Key Variations</h3>
            <p><strong>Mini-batch Gradient Descent</strong>: The modern
            standard that uses a small group of samples (e.g., 32, 64,
            128) instead of just one. It balances the speed of SGD with
            more stable gradient estimates.</p>
            <p><strong>Momentum</strong>: Adds a ‚Äúvelocity‚Äù term that
            keeps updates moving in a consistent direction, reducing
            erratic zig-zagging and speeding up convergence. (See
            Section 2.4)</p>
            <p><strong>Adaptive Optimizers</strong>: Methods like Adam,
            RMSprop, and AdaGrad automatically adjust the learning rate
            for each parameter during training. (See Section 2.5)</p>
            <h3 id="pytorch-implementation-3">PyTorch
            Implementation</h3>
            <div class="sourceCode" id="cb86"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic SGD</span></span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true" tabindex="-1"></a><span class="co"># SGD with momentum (recommended)</span></span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()          <span class="co"># Reset gradients</span></span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_loss(batch)     <span class="co"># Forward pass</span></span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true" tabindex="-1"></a>    loss.backward()                <span class="co"># Compute gradients</span></span>
<span id="cb86-16"><a href="#cb86-16" aria-hidden="true" tabindex="-1"></a>    optimizer.step()               <span class="co"># Update parameters</span></span></code></pre></div>
            <hr />
            <h2 id="momentum-methods">2.4 Momentum Methods</h2>
            <h3
            id="the-problem-oscillations-in-ill-conditioned-landscapes">The
            Problem: Oscillations in Ill-Conditioned Landscapes</h3>
            <p>Consider optimizing a loss surface shaped like an
            elongated valley:</p>
            <p><img src="figures/ill_conditioned_landscape.png"
            alt="Ill-Conditioned Landscape with SGD Oscillations" />
            <em>Figure: Left: Vanilla SGD oscillates in ill-conditioned
            landscapes (high condition number). Right: Momentum dampens
            oscillations and converges faster.</em></p>
            <p><strong>The problem</strong>:</p>
            <ul>
            <li>Gradient points toward the steepest descent (across the
            valley)</li>
            <li>But the minimum is along the gentle direction (down the
            valley)</li>
            <li>SGD overshoots across the valley, oscillates back and
            forth</li>
            </ul>
            <h3 id="classical-momentum">Classical Momentum</h3>
            <p>Add a <strong>velocity</strong> term that accumulates
            gradient direction:</p>
            <p><span class="math display">\[
            \begin{aligned}
            v_{t+1} &amp;= \beta v_t + \nabla \ell(w_t) \\
            w_{t+1} &amp;= w_t - \alpha v_{t+1}
            \end{aligned}
            \]</span></p>
            <table>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            <th>Typical Value</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(v_t\)</span></td>
            <td>Velocity (accumulated gradient)</td>
            <td>-</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta\)</span></td>
            <td>Momentum coefficient</td>
            <td><strong>0.9</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\alpha\)</span></td>
            <td>Learning rate</td>
            <td>task-dependent</td>
            </tr>
            </tbody>
            </table>
            <h3 id="intuition-ball-rolling-downhill">Intuition: Ball
            Rolling Downhill</h3>
            <p><img src="figures/momentum_ball.png"
            alt="Momentum Ball Rolling Downhill" /> <em>Figure: Without
            momentum (left), SGD oscillates back and forth. With
            momentum (right), the path is smooth and direct.</em></p>
            <ul>
            <li><strong>Perpendicular oscillations cancel</strong>:
            <span class="math inline">\(+g\)</span> then <span
            class="math inline">\(-g\)</span> averages to 0</li>
            <li><strong>Consistent direction accumulates</strong>:
            gradients pointing the same way add up</li>
            <li>Result: faster progress along the consistent
            direction</li>
            </ul>
            <h3 id="effective-step-size">Effective Step Size</h3>
            <p>With momentum, the effective learning rate in a
            consistent direction is:</p>
            <p><span class="math display">\[
            \alpha_{\text{eff}} = \frac{\alpha}{1 - \beta}
            \]</span></p>
            <p>For <span class="math inline">\(\beta = 0.9\)</span>:
            effective LR is <strong>10√ó larger</strong> in consistent
            directions!</p>
            <h3 id="why-Œ≤-0.9">Why Œ≤ = 0.9?</h3>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(\beta\)</span></th>
            <th>Behavior</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0.0</td>
            <td>No momentum (vanilla SGD)</td>
            </tr>
            <tr>
            <td>0.5</td>
            <td>Short memory, mild smoothing</td>
            </tr>
            <tr>
            <td><strong>0.9</strong></td>
            <td>Good balance (default)</td>
            </tr>
            <tr>
            <td>0.99</td>
            <td>Long memory, slow to change direction</td>
            </tr>
            </tbody>
            </table>
            <h3 id="nesterov-accelerated-gradient-nag">Nesterov
            Accelerated Gradient (NAG)</h3>
            <p><strong>Key idea</strong>: ‚ÄúLook ahead‚Äù before computing
            the gradient.</p>
            <p>Standard momentum computes gradient at current position,
            then moves. Nesterov computes gradient at the
            <strong>predicted next position</strong>:</p>
            <p><span class="math display">\[
            \begin{aligned}
            v_{t+1} &amp;= \beta v_t + \nabla \ell(\underbrace{w_t -
            \alpha \beta v_t}_{\text{look-ahead position}}) \\
            w_{t+1} &amp;= w_t - \alpha v_{t+1}
            \end{aligned}
            \]</span></p>
            <pre><code>Standard Momentum:          Nesterov:
                           
w_t ‚îÄ‚îÄgradient‚îÄ‚îÄ‚Üí w_{t+1}   w_t ‚îÄ‚îÄlookahead‚îÄ‚îÄ‚Üí wÃÉ ‚îÄ‚îÄgradient‚îÄ‚îÄ‚Üí w_{t+1}
                           
                            &quot;Where am I going? Let me check
                             the gradient there first.&quot;</code></pre>
            <h3 id="why-it-helps">Why It Helps</h3>
            <ul>
            <li>Can ‚Äúcorrect‚Äù before overshooting</li>
            <li>Better theoretical convergence rate</li>
            <li>In practice: similar to momentum, sometimes slightly
            better</li>
            </ul>
            <h3 id="pytorch-implementation-4">PyTorch
            Implementation</h3>
            <div class="sourceCode" id="cb88"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard momentum</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Nesterov momentum</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
            <hr />
            <h2 id="adaptive-learning-rate-methods">2.5 Adaptive
            Learning Rate Methods</h2>
            <h3 id="the-problem-with-fixed-learning-rates">The Problem
            with Fixed Learning Rates</h3>
            <p>Imagine you‚Äôre training a language model, and your
            vocabulary includes both ‚Äúthe‚Äù (appears in every sentence)
            and ‚Äúquasar‚Äù (appears once in 10,000 sentences).</p>
            <p>With a fixed learning rate, every weight gets updated
            with the same step size. But the embedding weights for ‚Äúthe‚Äù
            see gradients constantly ‚Äî thousands of updates pushing them
            around. Meanwhile, ‚Äúquasar‚Äù barely gets any signal.</p>
            <p>What happens? Either: - Learning rate is tuned for
            frequent words ‚Üí rare words learn too slowly - Learning rate
            is tuned for rare words ‚Üí frequent words oscillate
            wildly</p>
            <p><strong>The insight</strong>: Different parameters need
            different learning rates. Frequent features need smaller
            updates (they‚Äôve already seen lots of data). Rare features
            need larger updates (every bit of signal is precious).</p>
            <p>This is what <strong>adaptive methods</strong> solve:
            they automatically adjust learning rates for each parameter
            based on the history of gradients that parameter has
            seen.</p>
            <h3 id="the-evolution-adagrad-rmsprop-adam-adamw">The
            Evolution: AdaGrad ‚Üí RMSprop ‚Üí Adam ‚Üí AdamW</h3>
            <p>The story of adaptive optimizers is one of progressive
            refinement, each method solving a problem introduced by the
            previous one:</p>
            <pre><code>AdaGrad (2011)  ‚Üí  RMSprop (2012)  ‚Üí  Adam (2015)  ‚Üí  AdamW (2019)
    ‚Üì                   ‚Üì                 ‚Üì               ‚Üì
 Per-param LR     + Decay factor    + Momentum      + Decoupled
                                                      weight decay</code></pre>
            <hr />
            <h3 id="adagrad-2011-the-first-adaptive-method">AdaGrad
            (2011): The First Adaptive Method</h3>
            <p><strong>The idea</strong>: Keep a running sum of squared
            gradients for each parameter. Divide the learning rate by
            the square root of this sum.</p>
            <p><span class="math display">\[
            \begin{aligned}
            v_{t+1} &amp;= v_t + g_t^2 \\
            w_{t+1} &amp;= w_t - \frac{\alpha}{\sqrt{v_{t+1}} +
            \epsilon} g_t
            \end{aligned}
            \]</span></p>
            <p><strong>What this achieves</strong>: Parameters that have
            seen large gradients (frequent features) accumulate a large
            <span class="math inline">\(v\)</span>, so their effective
            learning rate <span class="math inline">\(\alpha /
            \sqrt{v}\)</span> shrinks. Rare features keep their original
            learning rate because they‚Äôve accumulated little.</p>
            <p><strong>The fatal flaw</strong>: <span
            class="math inline">\(v\)</span> only grows. It never
            shrinks. Over time, <em>all</em> learning rates decay toward
            zero and training stalls. This is fine for convex problems
            (where you want to converge and stop), but disastrous for
            deep learning where you need to keep adapting.</p>
            <hr />
            <h3 id="rmsprop-2012-forgetting-the-past">RMSprop (2012):
            Forgetting the Past</h3>
            <p><strong>The fix</strong>: Instead of accumulating
            <em>all</em> past squared gradients, use an
            <strong>exponential moving average</strong>. This ‚Äúforgets‚Äù
            old gradients, allowing learning rates to recover.</p>
            <p><span class="math display">\[
            \begin{aligned}
            v_{t+1} &amp;= \beta v_t + (1-\beta) g_t^2 \\
            w_{t+1} &amp;= w_t - \frac{\alpha}{\sqrt{v_{t+1}} +
            \epsilon} g_t
            \end{aligned}
            \]</span></p>
            <p>With <span class="math inline">\(\beta = 0.9\)</span>,
            we‚Äôre essentially looking at the average squared gradient
            over roughly the last 10 steps. If gradients suddenly become
            small (e.g., after the loss landscape changes), <span
            class="math inline">\(v\)</span> will decay and learning
            rate can increase again.</p>
            <p><strong>The remaining issue</strong>: RMSprop doesn‚Äôt
            have momentum. We‚Äôre adapting the <em>scale</em> of updates,
            but not the <em>direction</em>. Can we get both?</p>
            <hr />
            <h3 id="adam-2015-the-best-of-both-worlds">Adam (2015): The
            Best of Both Worlds</h3>
            <p><strong>The breakthrough</strong>: Combine RMSprop
            (adaptive learning rates) with momentum (smooth, accelerated
            updates). Add bias correction to handle initialization
            issues.</p>
            <p>Think of Adam as having two ‚Äúmemories‚Äù: - <strong>First
            moment (<span class="math inline">\(m\)</span>)</strong>:
            Exponential average of gradients ‚Äî the momentum term,
            pointing toward the consistent direction - <strong>Second
            moment (<span class="math inline">\(v\)</span>)</strong>:
            Exponential average of squared gradients ‚Äî the RMSprop term,
            scaling by gradient magnitude</p>
            <p><span class="math display">\[
            \begin{aligned}
            m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1) g_t &amp;
            \text{(momentum: which way?)} \\
            v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2) g_t^2 &amp;
            \text{(scale: how big?)} \\
            \hat{m}_t &amp;= m_t / (1 - \beta_1^t) &amp; \text{(bias
            correction)} \\
            \hat{v}_t &amp;= v_t / (1 - \beta_2^t) &amp; \text{(bias
            correction)} \\
            w_t &amp;= w_{t-1} - \alpha
            \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
            \end{aligned}
            \]</span></p>
            <blockquote>
            <p><strong>Note on indexing</strong>: Here <span
            class="math inline">\(t\)</span> starts at 1 (step 1, step
            2, ‚Ä¶). At step <span class="math inline">\(t\)</span>, we
            compute <span class="math inline">\(m_t, v_t\)</span> from
            the gradient <span class="math inline">\(g_t\)</span>, apply
            bias correction using <span
            class="math inline">\(\beta^t\)</span>, and update
            weights.</p>
            </blockquote>
            <p>The update <span
            class="math inline">\(\frac{\hat{m}}{\sqrt{\hat{v}}}\)</span>
            has a beautiful interpretation: we take the smoothed
            gradient direction (<span
            class="math inline">\(\hat{m}\)</span>) and normalize it by
            the smoothed gradient magnitude (<span
            class="math inline">\(\sqrt{\hat{v}}\)</span>). This gives
            us a kind of ‚Äústandardized‚Äù step ‚Äî regardless of whether
            gradients are large or small, the update magnitude stays
            roughly bounded.</p>
            <table>
            <colgroup>
            <col style="width: 37%" />
            <col style="width: 31%" />
            <col style="width: 31%" />
            </colgroup>
            <thead>
            <tr>
            <th>Parameter</th>
            <th>Default</th>
            <th>Purpose</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\beta_1\)</span></td>
            <td>0.9</td>
            <td>Momentum decay (how much past gradient direction
            matters)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta_2\)</span></td>
            <td>0.999</td>
            <td>Squared gradient decay (how much past magnitude
            matters)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\epsilon\)</span></td>
            <td><span class="math inline">\(10^{-8}\)</span></td>
            <td>Numerical stability (prevents division by zero)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why these defaults?</strong> <span
            class="math inline">\(\beta_2 = 0.999\)</span> is much
            closer to 1 than <span class="math inline">\(\beta_1 =
            0.9\)</span>. This means Adam has a ‚Äúlong memory‚Äù for
            gradient magnitudes (scale changes slowly) but a ‚Äúshort
            memory‚Äù for gradient direction (adapts quickly to new
            directions). This makes sense: the scale of gradients in a
            layer tends to be stable, but the direction can change
            rapidly.</p>
            <h3 id="why-bias-correction-matters">Why Bias Correction
            Matters</h3>
            <p>Here‚Äôs a subtle but important issue. Both <span
            class="math inline">\(m\)</span> and <span
            class="math inline">\(v\)</span> are initialized to zero. At
            step 1:</p>
            <p><span class="math display">\[m_1 = 0.9 \times 0 + 0.1
            \times g_1 = 0.1 \cdot g_1\]</span></p>
            <p>We wanted an estimate of the gradient, but we got only
            10% of it! The same happens with <span
            class="math inline">\(v\)</span>. Without correction, early
            updates would be way too small.</p>
            <p>The fix divides by <span class="math inline">\((1 -
            \beta^t)\)</span>, which starts large and approaches 1 as
            <span class="math inline">\(t\)</span> grows:</p>
            <ul>
            <li>Step 1: divide by <span class="math inline">\((1 -
            0.9^1) = 0.1\)</span> ‚Üí multiply by 10 ‚Üí <span
            class="math inline">\(\hat{m}_1 = g_1\)</span> ‚úì</li>
            <li>Step 10: divide by <span class="math inline">\((1 -
            0.9^{10}) \approx 0.65\)</span></li>
            <li>Step 100: divide by <span class="math inline">\((1 -
            0.9^{100}) \approx 1.0\)</span> ‚Üí no correction needed</li>
            </ul>
            <hr />
            <h3 id="adamw-2019-the-interview-question">AdamW (2019): The
            Interview Question!</h3>
            <p>AdamW seems like a minor tweak, but understanding
            <em>why</em> it matters reveals deep insight into how
            adaptive optimizers interact with regularization.</p>
            <h3 id="the-problem-adam-breaks-weight-decay">The Problem:
            Adam Breaks Weight Decay</h3>
            <p>You want to regularize your model with L2 (weight decay).
            The standard approach: add <span
            class="math inline">\(\lambda w\)</span> to the
            gradient.</p>
            <p><span class="math display">\[g_{\text{regularized}} = g +
            \lambda w\]</span></p>
            <p>With SGD, this works perfectly. The update becomes:</p>
            <p><span class="math display">\[w \leftarrow w - \alpha(g +
            \lambda w) = (1 - \alpha\lambda)w - \alpha g\]</span></p>
            <p>Every weight shrinks by the same factor <span
            class="math inline">\((1 - \alpha\lambda)\)</span> at every
            step. Fair and consistent.</p>
            <p><strong>But Adam does something different.</strong> It
            divides the gradient by <span
            class="math inline">\(\sqrt{v}\)</span>:</p>
            <p><span class="math display">\[\text{Update} = \frac{g +
            \lambda w}{\sqrt{v}}\]</span></p>
            <p>Now the regularization term <span
            class="math inline">\(\lambda w\)</span> also gets divided
            by <span class="math inline">\(\sqrt{v}\)</span>! Parameters
            with large historical gradients (large <span
            class="math inline">\(v\)</span>) get <em>less</em>
            regularization. Parameters with small gradients get
            <em>more</em> regularization.</p>
            <p>This is backwards! We want consistent regularization
            across all parameters, but Adam‚Äôs adaptive scaling breaks
            it.</p>
            <h3 id="the-fix-decouple-weight-decay">The Fix: Decouple
            Weight Decay</h3>
            <p>AdamW applies weight decay <strong>outside</strong> the
            adaptive mechanism:</p>
            <p><span class="math display">\[w_{t+1} = w_t - \alpha \cdot
            \underbrace{\frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}} +
            \epsilon}}_{\text{Adam update}} - \underbrace{\alpha \lambda
            w_t}_{\text{separate weight decay}}\]</span></p>
            <p>The key difference:</p>
            <pre><code>Adam + L2:    gradient ‚Üê gradient + Œªw,  then divide by ‚àöv
AdamW:        divide by ‚àöv,              then subtract Œ±Œªw separately</code></pre>
            <p>Now every weight decays by exactly <span
            class="math inline">\(\alpha\lambda\)</span> of its current
            value, regardless of gradient history. Simple, consistent,
            correct.</p>
            <h3 id="why-it-matters-in-practice">Why It Matters in
            Practice</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Effective regularization per parameter</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Adam + L2</td>
            <td>Varies wildly (broken!)</td>
            </tr>
            <tr>
            <td><strong>AdamW</strong></td>
            <td>Same for all (correct)</td>
            </tr>
            </tbody>
            </table>
            <p>This isn‚Äôt just theoretical. Empirically, AdamW
            consistently outperforms Adam + L2 on Transformers and other
            architectures. The BERT and GPT papers all use AdamW.</p>
            <p><strong>Bottom line</strong>: If you‚Äôre using Adam and
            want regularization, use AdamW, not Adam with
            <code>weight_decay</code> parameter (which does L2 the wrong
            way).</p>
            <hr />
            <h3 id="pytorch-implementation-5">PyTorch
            Implementation</h3>
            <div class="sourceCode" id="cb91"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Adam (don&#39;t use weight_decay with Adam!)</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW (correct way to regularize)</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a><span class="co"># SGD with momentum</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, weight_decay<span class="op">=</span><span class="fl">1e-4</span>)</span></code></pre></div>
            <hr />
            <h2 id="learning-rate-schedules">2.6 Learning Rate
            Schedules</h2>
            <h3 id="why-schedules-matter">Why Schedules Matter</h3>
            <p>Think of training a neural network like hiking down into
            a valley, but the terrain changes as you descend.</p>
            <p><strong>Early in training</strong>: The loss landscape is
            rough and you‚Äôre far from any minimum. Big steps are fine ‚Äî
            even if they‚Äôre imprecise, they get you moving in the right
            direction quickly. A large learning rate lets you explore
            broadly and make rapid progress.</p>
            <p><strong>Late in training</strong>: You‚Äôre near a minimum
            now. The same big steps that helped earlier now cause you to
            overshoot and oscillate around the minimum. You need
            smaller, more careful steps to settle into the valley
            floor.</p>
            <p>This is why a fixed learning rate is rarely optimal: -
            <strong>Too high throughout</strong> ‚Üí Can‚Äôt converge
            precisely at the end (oscillates) - <strong>Too low
            throughout</strong> ‚Üí Wastes time early on when you could
            move faster</p>
            <p><strong>The solution</strong>: Start with a high learning
            rate (fast progress through the easy terrain), then decay to
            a low rate (careful fine-tuning near the minimum).</p>
            <hr />
            <h3 id="warmup-why-transformers-need-it">Warmup: Why
            Transformers Need It</h3>
            <h4 id="the-problem-with-cold-starting-adam">The Problem
            with Cold-Starting Adam</h4>
            <p>Adam tracks running estimates of gradient statistics
            (<span class="math inline">\(m\)</span> for direction, <span
            class="math inline">\(v\)</span> for scale). But at step 1,
            these estimates are based on a single gradient ‚Äî they‚Äôre
            extremely noisy and unreliable.</p>
            <p>Even with bias correction, the first few hundred steps
            can be problematic:</p>
            <pre><code>Step 1:  v‚ÇÅ = 0.001 √ó g‚ÇÅ¬≤      ‚Üê Based on ONE gradient
         update = g‚ÇÅ/‚àöv‚ÇÅ       ‚Üê Scaling by noisy estimate ‚Üí unstable!</code></pre>
            <p>For Transformers specifically, there‚Äôs another issue: the
            attention mechanism creates sharp, spiky loss landscapes
            early in training. Large learning rates on spiky terrain ‚Üí
            disaster.</p>
            <h4 id="the-solution-linear-warmup">The Solution: Linear
            Warmup</h4>
            <p>Start with tiny LR, increase linearly:</p>
            <p><span class="math display">\[
            \alpha_t = \alpha_{\max} \cdot \frac{t}{T_{\text{warmup}}}
            \quad \text{for } t &lt; T_{\text{warmup}}
            \]</span></p>
            <p><img src="figures/warmup_schedule.png"
            alt="Linear Warmup Schedule" /> <em>Figure: Linear warmup
            gradually increases learning rate from 0 to max, stabilizing
            early training.</em></p>
            <p><strong>Typical warmup</strong>: 1-5% of total training
            steps (e.g., 2000 steps for 100k total)</p>
            <hr />
            <h3 id="common-decay-schedules">Common Decay Schedules</h3>
            <h4 id="step-decay">Step Decay</h4>
            <p>Drop LR by factor at fixed milestones:</p>
            <p><span class="math display">\[
            \alpha_t = \alpha_0 \cdot \gamma^{\lfloor t / S \rfloor}
            \]</span></p>
            <p><img src="figures/step_decay_schedule.png"
            alt="Step Decay Schedule" /> <em>Figure: Step decay drops LR
            by 10√ó at epochs 30, 60, 90 (typical for CNN
            training).</em></p>
            <p><strong>Typical</strong>: Divide by 10 at epochs 30, 60,
            90 (for 100 epoch training)</p>
            <h4 id="cosine-annealing">Cosine Annealing</h4>
            <p>Smooth decay following cosine curve:</p>
            <p><span class="math display">\[
            \alpha_t = \alpha_{\min} + \frac{1}{2}(\alpha_{\max} -
            \alpha_{\min})\left(1 + \cos\left(\frac{\pi
            t}{T}\right)\right)
            \]</span></p>
            <p><img src="figures/cosine_annealing_schedule.png"
            alt="Cosine Annealing Schedule" /> <em>Figure: Cosine
            annealing provides smooth LR decay, preferred for
            Transformers and LLMs.</em></p>
            <p><strong>Used by</strong>: GPT-3, LLaMA, most modern
            LLMs</p>
            <h4 id="linear-decay">Linear Decay</h4>
            <p>Simple linear decrease:</p>
            <p><span class="math display">\[
            \alpha_t = \alpha_{\max} \cdot \left(1 - \frac{t}{T}\right)
            \]</span></p>
            <h3 id="comparison">Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Schedule</th>
            <th>Shape</th>
            <th>Best For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Step</strong></td>
            <td>Staircase</td>
            <td>CNNs, classic vision</td>
            </tr>
            <tr>
            <td><strong>Cosine</strong></td>
            <td>Smooth curve</td>
            <td>Transformers, LLMs</td>
            </tr>
            <tr>
            <td><strong>Linear</strong></td>
            <td>Straight line</td>
            <td>Fine-tuning</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="warmup-cosine-standard-llm-recipe">Warmup + Cosine
            (Standard LLM Recipe)</h3>
            <p><img src="figures/warmup_cosine_schedule.png"
            alt="Warmup + Cosine Schedule" /> <em>Figure: Standard LLM
            training recipe ‚Äî linear warmup followed by cosine
            decay.</em></p>
            <p><strong>PyTorch Implementation</strong>:</p>
            <div class="sourceCode" id="cb93"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> CosineAnnealingLR, LinearLR, SequentialLR</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Warmup for first 2000 steps</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>warmup <span class="op">=</span> LinearLR(optimizer, start_factor<span class="op">=</span><span class="fl">0.01</span>, total_iters<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Cosine decay for rest of training</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>cosine <span class="op">=</span> CosineAnnealingLR(optimizer, T_max<span class="op">=</span><span class="dv">100000</span> <span class="op">-</span> <span class="dv">2000</span>, eta_min<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine them</span></span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> SequentialLR(optimizer, [warmup, cosine], milestones<span class="op">=</span>[<span class="dv">2000</span>])</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a><span class="co"># In training loop</span></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100000</span>):</span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a>    scheduler.step()  <span class="co"># Update LR</span></span></code></pre></div>
            <hr />
            <h3 id="optimizer-schedule-decision-guide">Optimizer +
            Schedule Decision Guide</h3>
            <table>
            <colgroup>
            <col style="width: 13%" />
            <col style="width: 25%" />
            <col style="width: 22%" />
            <col style="width: 27%" />
            <col style="width: 11%" />
            </colgroup>
            <thead>
            <tr>
            <th>Task</th>
            <th>Optimizer</th>
            <th>Schedule</th>
            <th>Typical LR</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>CNN (ImageNet)</strong></td>
            <td>SGD + momentum</td>
            <td>Step decay</td>
            <td>0.1 ‚Üí 0.001</td>
            <td>Better generalization</td>
            </tr>
            <tr>
            <td><strong>Transformer pretraining</strong></td>
            <td>AdamW</td>
            <td>Warmup + cosine</td>
            <td>1e-4 to 3e-4</td>
            <td>Stable, smooth</td>
            </tr>
            <tr>
            <td><strong>Fine-tuning BERT/GPT</strong></td>
            <td>AdamW</td>
            <td>Linear decay</td>
            <td>1e-5 to 5e-5</td>
            <td>Don‚Äôt move far from pretrained</td>
            </tr>
            <tr>
            <td><strong>RL (PPO)</strong></td>
            <td>Adam</td>
            <td>Constant or linear</td>
            <td>3e-4</td>
            <td>Policy updates are noisy</td>
            </tr>
            <tr>
            <td><strong>GAN training</strong></td>
            <td>Adam</td>
            <td>Constant</td>
            <td>1e-4 to 2e-4</td>
            <td>Delicate equilibrium</td>
            </tr>
            </tbody>
            </table>
            <h3 id="quick-recommendations">Quick Recommendations</h3>
            <p><strong>If unsure, start with</strong>:</p>
            <div class="sourceCode" id="cb94"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For Transformers / NLP</span></span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(lr<span class="op">=</span><span class="fl">1e-4</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> warmup(<span class="dv">2000</span> steps) <span class="op">+</span> cosine(to <span class="fl">1e-5</span>)</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a><span class="co"># For CNNs / Vision  </span></span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, weight_decay<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> step(divide by <span class="dv">10</span> at <span class="dv">30</span><span class="op">%</span>, <span class="dv">60</span><span class="op">%</span>, <span class="dv">90</span><span class="op">%</span> of training)</span></code></pre></div>
            <h3 id="decision-flowchart">Decision Flowchart</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                OPTIMIZER &amp; SCHEDULE DECISION GUIDE              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                 What are you training?
                                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì           ‚Üì           ‚Üì           ‚Üì           ‚Üì
   Transformer     CNN      Fine-tune    RL (PPO)     GAN
        ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ
        ‚Üì           ‚Üì           ‚Üì           ‚Üì           ‚Üì
     AdamW      SGD+Mom      AdamW       Adam        Adam
     warmup     step decay   linear     constant    constant
     cosine     0.1‚Üí0.001    decay       lr=3e-4    lr=1e-4
    lr=1e-4                  lr=2e-5</code></pre>
            <hr />
            <h3 id="complete-training-loop-example">Complete Training
            Loop Example</h3>
            <p>Here‚Äôs a complete, minimal training loop using AdamW with
            warmup + cosine schedule ‚Äî the standard recipe for
            Transformers:</p>
            <div class="sourceCode" id="cb96"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> AdamW</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> CosineAnnealingLR, LinearLR, SequentialLR</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Model Definition</span></span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformerBlock(nn.Module):</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Minimal transformer-style block for demonstration.&quot;&quot;&quot;</span></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model<span class="op">=</span><span class="dv">256</span>, nhead<span class="op">=</span><span class="dv">4</span>, dim_ff<span class="op">=</span><span class="dv">512</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> nn.MultiheadAttention(d_model, nhead, dropout<span class="op">=</span>dropout, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff <span class="op">=</span> nn.Sequential(</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(d_model, dim_ff),</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout),</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(dim_ff, d_model),</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout),</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(d_model)</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-attention with residual</span></span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.attn(x, x, x)</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> attn_out)</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feed-forward with residual</span></span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.ff(x))</span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleModel(nn.Module):</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size<span class="op">=</span><span class="dv">1000</span>, d_model<span class="op">=</span><span class="dv">256</span>, num_layers<span class="op">=</span><span class="dv">2</span>, num_classes<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, d_model)</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.ModuleList([SimpleTransformerBlock(d_model) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)])</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(d_model, num_classes)</span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(x)</span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.blocks:</span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling + classification</span></span>
<span id="cb96-45"><a href="#cb96-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb96-46"><a href="#cb96-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.classifier(x)</span>
<span id="cb96-47"><a href="#cb96-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-48"><a href="#cb96-48" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-49"><a href="#cb96-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Training Configuration</span></span>
<span id="cb96-50"><a href="#cb96-50" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-51"><a href="#cb96-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb96-52"><a href="#cb96-52" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb96-53"><a href="#cb96-53" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb96-54"><a href="#cb96-54" aria-hidden="true" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb96-55"><a href="#cb96-55" aria-hidden="true" tabindex="-1"></a>WARMUP_STEPS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb96-56"><a href="#cb96-56" aria-hidden="true" tabindex="-1"></a>TOTAL_STEPS <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb96-57"><a href="#cb96-57" aria-hidden="true" tabindex="-1"></a>GRAD_CLIP <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb96-58"><a href="#cb96-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-59"><a href="#cb96-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Device</span></span>
<span id="cb96-60"><a href="#cb96-60" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb96-61"><a href="#cb96-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-62"><a href="#cb96-62" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-63"><a href="#cb96-63" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Create Model, Optimizer, Scheduler</span></span>
<span id="cb96-64"><a href="#cb96-64" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-65"><a href="#cb96-65" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleModel().to(device)</span>
<span id="cb96-66"><a href="#cb96-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-67"><a href="#cb96-67" aria-hidden="true" tabindex="-1"></a><span class="co"># AdamW with decoupled weight decay (correct way to regularize)</span></span>
<span id="cb96-68"><a href="#cb96-68" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(</span>
<span id="cb96-69"><a href="#cb96-69" aria-hidden="true" tabindex="-1"></a>    model.parameters(),</span>
<span id="cb96-70"><a href="#cb96-70" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span>LEARNING_RATE,</span>
<span id="cb96-71"><a href="#cb96-71" aria-hidden="true" tabindex="-1"></a>    betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.999</span>),</span>
<span id="cb96-72"><a href="#cb96-72" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span>WEIGHT_DECAY</span>
<span id="cb96-73"><a href="#cb96-73" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-74"><a href="#cb96-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-75"><a href="#cb96-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Warmup + Cosine schedule</span></span>
<span id="cb96-76"><a href="#cb96-76" aria-hidden="true" tabindex="-1"></a>warmup_scheduler <span class="op">=</span> LinearLR(</span>
<span id="cb96-77"><a href="#cb96-77" aria-hidden="true" tabindex="-1"></a>    optimizer, </span>
<span id="cb96-78"><a href="#cb96-78" aria-hidden="true" tabindex="-1"></a>    start_factor<span class="op">=</span><span class="fl">0.01</span>,  <span class="co"># Start at 1% of max LR</span></span>
<span id="cb96-79"><a href="#cb96-79" aria-hidden="true" tabindex="-1"></a>    total_iters<span class="op">=</span>WARMUP_STEPS</span>
<span id="cb96-80"><a href="#cb96-80" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-81"><a href="#cb96-81" aria-hidden="true" tabindex="-1"></a>cosine_scheduler <span class="op">=</span> CosineAnnealingLR(</span>
<span id="cb96-82"><a href="#cb96-82" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb96-83"><a href="#cb96-83" aria-hidden="true" tabindex="-1"></a>    T_max<span class="op">=</span>TOTAL_STEPS <span class="op">-</span> WARMUP_STEPS,</span>
<span id="cb96-84"><a href="#cb96-84" aria-hidden="true" tabindex="-1"></a>    eta_min<span class="op">=</span><span class="fl">1e-6</span>  <span class="co"># Minimum LR</span></span>
<span id="cb96-85"><a href="#cb96-85" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-86"><a href="#cb96-86" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> SequentialLR(</span>
<span id="cb96-87"><a href="#cb96-87" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb96-88"><a href="#cb96-88" aria-hidden="true" tabindex="-1"></a>    schedulers<span class="op">=</span>[warmup_scheduler, cosine_scheduler],</span>
<span id="cb96-89"><a href="#cb96-89" aria-hidden="true" tabindex="-1"></a>    milestones<span class="op">=</span>[WARMUP_STEPS]</span>
<span id="cb96-90"><a href="#cb96-90" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb96-91"><a href="#cb96-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-92"><a href="#cb96-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss function</span></span>
<span id="cb96-93"><a href="#cb96-93" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb96-94"><a href="#cb96-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-95"><a href="#cb96-95" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-96"><a href="#cb96-96" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Training Loop</span></span>
<span id="cb96-97"><a href="#cb96-97" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-98"><a href="#cb96-98" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(model, batch, optimizer, scheduler, criterion, grad_clip):</span>
<span id="cb96-99"><a href="#cb96-99" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Single training step with all best practices.&quot;&quot;&quot;</span></span>
<span id="cb96-100"><a href="#cb96-100" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> batch</span>
<span id="cb96-101"><a href="#cb96-101" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb96-102"><a href="#cb96-102" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-103"><a href="#cb96-103" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb96-104"><a href="#cb96-104" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb96-105"><a href="#cb96-105" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(inputs)</span>
<span id="cb96-106"><a href="#cb96-106" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(outputs, targets)</span>
<span id="cb96-107"><a href="#cb96-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-108"><a href="#cb96-108" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb96-109"><a href="#cb96-109" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb96-110"><a href="#cb96-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-111"><a href="#cb96-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient clipping (prevent exploding gradients)</span></span>
<span id="cb96-112"><a href="#cb96-112" aria-hidden="true" tabindex="-1"></a>    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)</span>
<span id="cb96-113"><a href="#cb96-113" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-114"><a href="#cb96-114" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update weights and learning rate</span></span>
<span id="cb96-115"><a href="#cb96-115" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb96-116"><a href="#cb96-116" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb96-117"><a href="#cb96-117" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-118"><a href="#cb96-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span>
<span id="cb96-119"><a href="#cb96-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-120"><a href="#cb96-120" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, dataloader, optimizer, scheduler, criterion, total_steps, grad_clip<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb96-121"><a href="#cb96-121" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Full training loop.&quot;&quot;&quot;</span></span>
<span id="cb96-122"><a href="#cb96-122" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb96-123"><a href="#cb96-123" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-124"><a href="#cb96-124" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb96-125"><a href="#cb96-125" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-126"><a href="#cb96-126" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> step <span class="op">&lt;</span> total_steps:</span>
<span id="cb96-127"><a href="#cb96-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb96-128"><a href="#cb96-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> step <span class="op">&gt;=</span> total_steps:</span>
<span id="cb96-129"><a href="#cb96-129" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb96-130"><a href="#cb96-130" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb96-131"><a href="#cb96-131" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> train_step(model, batch, optimizer, scheduler, criterion, grad_clip)</span>
<span id="cb96-132"><a href="#cb96-132" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss</span>
<span id="cb96-133"><a href="#cb96-133" aria-hidden="true" tabindex="-1"></a>            step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb96-134"><a href="#cb96-134" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb96-135"><a href="#cb96-135" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Logging</span></span>
<span id="cb96-136"><a href="#cb96-136" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> step <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb96-137"><a href="#cb96-137" aria-hidden="true" tabindex="-1"></a>                avg_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="dv">100</span></span>
<span id="cb96-138"><a href="#cb96-138" aria-hidden="true" tabindex="-1"></a>                current_lr <span class="op">=</span> scheduler.get_last_lr()[<span class="dv">0</span>]</span>
<span id="cb96-139"><a href="#cb96-139" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>total_steps<span class="sc">}</span><span class="ss"> | Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ss"> | LR: </span><span class="sc">{</span>current_lr<span class="sc">:.2e}</span><span class="ss">&quot;</span>)</span>
<span id="cb96-140"><a href="#cb96-140" aria-hidden="true" tabindex="-1"></a>                running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb96-141"><a href="#cb96-141" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-142"><a href="#cb96-142" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb96-143"><a href="#cb96-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-144"><a href="#cb96-144" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-145"><a href="#cb96-145" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Example Usage (with dummy data)</span></span>
<span id="cb96-146"><a href="#cb96-146" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb96-147"><a href="#cb96-147" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb96-148"><a href="#cb96-148" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create dummy dataset</span></span>
<span id="cb96-149"><a href="#cb96-149" aria-hidden="true" tabindex="-1"></a>    dummy_inputs <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">1000</span>, (<span class="dv">320</span>, <span class="dv">16</span>))  <span class="co"># 320 samples, seq_len=16</span></span>
<span id="cb96-150"><a href="#cb96-150" aria-hidden="true" tabindex="-1"></a>    dummy_targets <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">320</span>,))      <span class="co"># 10 classes</span></span>
<span id="cb96-151"><a href="#cb96-151" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> TensorDataset(dummy_inputs, dummy_targets)</span>
<span id="cb96-152"><a href="#cb96-152" aria-hidden="true" tabindex="-1"></a>    dataloader <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb96-153"><a href="#cb96-153" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-154"><a href="#cb96-154" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train</span></span>
<span id="cb96-155"><a href="#cb96-155" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> train(model, dataloader, optimizer, scheduler, criterion, TOTAL_STEPS, GRAD_CLIP)</span>
<span id="cb96-156"><a href="#cb96-156" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Training complete!&quot;</span>)</span></code></pre></div>
            <p><strong>Key best practices demonstrated:</strong> 1.
            <strong>AdamW</strong> with decoupled weight decay (not Adam
            + L2) 2. <strong>Linear warmup</strong> ‚Üí <strong>cosine
            decay</strong> schedule 3. <strong>Gradient
            clipping</strong> to prevent instability 4. <strong>Proper
            device handling</strong> (CPU/GPU) 5. <strong>Learning rate
            logging</strong> for debugging</p>
            <hr />
            <h2 id="convergence-and-loss-landscapes">2.7 Convergence and
            Loss Landscapes</h2>
            <p>The loss function defines a landscape over your parameter
            space ‚Äî a surface with peaks, valleys, saddle points, and
            plateaus. Understanding this landscape is key to
            understanding why optimization works (or doesn‚Äôt).</p>
            <h3 id="what-does-the-loss-landscape-look-like">What Does
            the Loss Landscape Look Like?</h3>
            <p>For neural networks, the loss landscape is incredibly
            high-dimensional ‚Äî millions or billions of dimensions. We
            can‚Äôt visualize it directly, but research has revealed
            several important properties:</p>
            <p><strong>1. Many Local Minima, But They‚Äôre Usually
            Good</strong></p>
            <p>Early fears about neural networks centered on ‚Äúgetting
            stuck in bad local minima.‚Äù In practice, this rarely
            happens. Why?</p>
            <ul>
            <li>In high dimensions, most critical points are
            <strong>saddle points</strong>, not local minima</li>
            <li>True local minima tend to have similar loss values to
            the global minimum</li>
            <li>Different minima often generalize equally well</li>
            </ul>
            <p><strong>2. The Landscape is Non-Convex But Surprisingly
            Navigable</strong></p>
            <p>The loss surface is highly non-convex (unlike linear
            regression, where there‚Äôs one obvious valley). But SGD can
            usually find good solutions because:</p>
            <ul>
            <li>Gradient descent tends to avoid saddle points (noise
            helps escape)</li>
            <li>The ‚Äúconnected valley‚Äù hypothesis: good solutions are
            connected by paths of low loss</li>
            </ul>
            <h3
            id="sharp-vs-flat-minima-why-it-matters-for-generalization">Sharp
            vs Flat Minima: Why It Matters for Generalization</h3>
            <p>Here‚Äôs a key insight that connects optimization to
            generalization:</p>
            <p><img src="figures/sharp_flat_minima.png"
            alt="Sharp vs Flat Minima" /> <em>Figure: Sharp minima
            (left) have large loss changes from small weight
            perturbations. Flat minima (right) are robust and generalize
            better.</em></p>
            <p><strong>Sharp minimum</strong>: The loss increases
            steeply when you move away from the optimal weights. Think
            of a narrow spike in the landscape.</p>
            <p><strong>Flat minimum</strong>: The loss stays low even
            when weights are perturbed slightly. Think of a broad,
            shallow valley.</p>
            <p><strong>Why does this matter?</strong> At test time, your
            data is different from training data ‚Äî it‚Äôs like adding a
            small perturbation to everything. A model at a sharp minimum
            has weights that are ‚Äúbrittle‚Äù ‚Äî small changes cause large
            loss increases. A model at a flat minimum has weights that
            are ‚Äúrobust‚Äù ‚Äî nearby weight values also give low loss.</p>
            <p>This is why <strong>flat minima generalize
            better</strong>: they‚Äôre stable under the perturbations that
            test data introduces.</p>
            <h3 id="how-sgd-finds-flat-minima">How SGD Finds Flat
            Minima</h3>
            <p>Here‚Äôs the beautiful connection: <strong>SGD noise
            naturally biases toward flat minima</strong>.</p>
            <p>In a sharp valley, the gradients are steep and variable ‚Äî
            SGD‚Äôs noise bounces you out. In a flat valley, gradients are
            small and stable ‚Äî you stay put.</p>
            <p>It‚Äôs like rolling a ball with random kicks: - In a narrow
            groove, kicks bounce it out - In a wide basin, kicks just
            slosh it around without escaping</p>
            <p>This is another form of <strong>implicit
            regularization</strong> from SGD (beyond what we discussed
            in section 2.3).</p>
            <h3 id="when-optimization-converges">When Optimization
            ‚ÄúConverges‚Äù</h3>
            <p>What does it mean for training to converge? Several
            things:</p>
            <ol type="1">
            <li><strong>Loss stabilizes</strong>: Stops decreasing
            meaningfully</li>
            <li><strong>Gradients shrink</strong>: <span
            class="math inline">\(\|\nabla L\| \to 0\)</span> (at a
            critical point)</li>
            <li><strong>Weights stabilize</strong>: Updates become
            tiny</li>
            </ol>
            <p><strong>Signs of problems</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 50%" />
            <col style="width: 17%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symptom</th>
            <th>Likely Cause</th>
            <th>Fix</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Loss oscillates, doesn‚Äôt settle</td>
            <td>Learning rate too high</td>
            <td>Reduce LR or add decay</td>
            </tr>
            <tr>
            <td>Loss plateaus high</td>
            <td>Stuck at saddle or bad region</td>
            <td>Increase LR, restart, or add noise</td>
            </tr>
            <tr>
            <td>Loss diverges (‚Üí ‚àû or NaN)</td>
            <td>Learning rate way too high, exploding gradients</td>
            <td>Reduce LR, add gradient clipping</td>
            </tr>
            <tr>
            <td>Train loss drops, val loss rises</td>
            <td>Overfitting</td>
            <td>Add regularization, early stopping</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-effect-of-architecture-on-the-landscape">The
            Effect of Architecture on the Landscape</h3>
            <p>Different architectures create different loss
            landscapes:</p>
            <p><strong>ResNets</strong> (skip connections): Create
            smoother landscapes with more direct gradient paths. Easier
            to optimize.</p>
            <p><strong>Transformers</strong>: Attention creates sharp,
            spiky landscapes early in training. This is why warmup is
            critical.</p>
            <p><strong>Very Deep Networks</strong> (without skip
            connections): Pathological landscapes with vanishing
            gradients in most directions.</p>
            <hr />
            <h2 id="practical-considerations">2.8 Practical
            Considerations</h2>
            <h3 id="gradient-clipping-1">Gradient Clipping</h3>
            <p>Prevent exploding gradients:</p>
            <p><span class="math display">\[
            g \leftarrow \min(1, \frac{\tau}{\|g\|}) \cdot g
            \]</span></p>
            <h3 id="weight-initialization-1">Weight Initialization</h3>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 33%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Formula</th>
            <th>Good For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Xavier</td>
            <td><span class="math inline">\(\mathcal{N}(0,
            \frac{2}{n_{in} + n_{out}})\)</span></td>
            <td>Tanh, Sigmoid</td>
            </tr>
            <tr>
            <td>He</td>
            <td><span class="math inline">\(\mathcal{N}(0,
            \frac{2}{n_{in}})\)</span></td>
            <td>ReLU</td>
            </tr>
            </tbody>
            </table>
            <h3 id="debugging-checklist-1">Debugging Checklist</h3>
            <table>
            <thead>
            <tr>
            <th>Symptom</th>
            <th>Cause</th>
            <th>Fix</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Loss NaN</td>
            <td>Exploding gradients</td>
            <td>Clip gradients, lower LR</td>
            </tr>
            <tr>
            <td>Loss stuck</td>
            <td>LR too low</td>
            <td>Increase LR</td>
            </tr>
            <tr>
            <td>Loss oscillating</td>
            <td>LR too high</td>
            <td>Decrease LR</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="regularization-l1-and-l2">2.9 Regularization: L1 and
            L2</h2>
            <h3 id="the-problem-overfitting">The Problem:
            Overfitting</h3>
            <p>A model that fits training data too well may fail on new
            data:</p>
            <p><img src="figures/overfitting_good_fit.png"
            alt="Overfitting vs Good Fit" /> <em>Figure: Left: Training
            data. Middle: Overfitting (wiggly, fits noise). Right: Good
            fit (smooth, captures pattern).</em></p>
            <p><strong>Overfitting symptoms:</strong></p>
            <ul>
            <li>Low training loss, high validation loss</li>
            <li>Model memorizes noise instead of learning patterns</li>
            <li>Weights become very large</li>
            </ul>
            <h3 id="the-solution-penalize-large-weights">The Solution:
            Penalize Large Weights</h3>
            <p>Add a <strong>regularization term</strong> to the
            loss:</p>
            <p><span class="math display">\[
            \text{Loss}_{\text{reg}} = \text{Loss}_{\text{data}} +
            \lambda \cdot R(w)
            \]</span></p>
            <p>where <span class="math inline">\(\lambda\)</span>
            controls regularization strength and <span
            class="math inline">\(R(w)\)</span> penalizes model
            complexity.</p>
            <hr />
            <h3 id="l2-regularization-ridge-weight-decay">L2
            Regularization (Ridge / Weight Decay)</h3>
            <h4 id="formula">Formula</h4>
            <p><span class="math display">\[
            R_{L2}(w) = \|w\|_2^2 = \sum_i w_i^2
            \]</span></p>
            <p>Full loss: <span class="math display">\[
            \text{Loss} = \frac{1}{N}\sum_{i=1}^{N} L(y_i, \hat{y}_i) +
            \frac{\lambda}{2} \sum_j w_j^2
            \]</span></p>
            <h4 id="effect-on-gradient">Effect on Gradient</h4>
            <p><span class="math display">\[
            \frac{\partial \text{Loss}}{\partial w_j} = \frac{\partial
            L}{\partial w_j} + \lambda w_j
            \]</span></p>
            <p>Update rule: <span class="math display">\[
            w_j \leftarrow w_j - \alpha\left(\frac{\partial L}{\partial
            w_j} + \lambda w_j\right) = (1 - \alpha\lambda)w_j -
            \alpha\frac{\partial L}{\partial w_j}
            \]</span></p>
            <p><strong>Interpretation</strong>: Each update
            <strong>shrinks</strong> weights toward zero by factor <span
            class="math inline">\((1 - \alpha\lambda)\)</span>.</p>
            <h4 id="why-it-works">Why It Works</h4>
            <ul>
            <li><strong>Penalizes large weights</strong>: Forces model
            to use all features moderately instead of relying heavily on
            few</li>
            <li><strong>Smooth penalty</strong>: Differentiable
            everywhere, easy to optimize</li>
            <li><strong>Keeps all features</strong>: Weights shrink but
            rarely become exactly zero</li>
            </ul>
            <h4 id="geometric-intuition">Geometric Intuition</h4>
            <p><img src="figures/l1_l2_balls.png"
            alt="L1 and L2 Constraint Regions" /> <em>Figure: L2 ball
            (circle) vs L1 ball (diamond). Loss contours hit the L1
            diamond at corners (sparse), but touch the L2 circle at
            smooth points (non-sparse).</em></p>
            <p>Optimal solution lies where loss contours touch the
            constraint region ‚Äî L2 produces small but non-zero weights,
            L1 produces exact zeros at corners.</p>
            <hr />
            <h3 id="l1-regularization-lasso-1">L1 Regularization
            (Lasso)</h3>
            <h4 id="formula-1">Formula</h4>
            <p><span class="math display">\[
            R_{L1}(w) = \|w\|_1 = \sum_i |w_i|
            \]</span></p>
            <p>Full loss: <span class="math display">\[
            \text{Loss} = \frac{1}{N}\sum_{i=1}^{N} L(y_i, \hat{y}_i) +
            \lambda \sum_j |w_j|
            \]</span></p>
            <h4 id="effect-on-gradient-1">Effect on Gradient</h4>
            <p>The gradient of <span class="math inline">\(|w|\)</span>
            is: <span class="math display">\[
            \frac{\partial |w_j|}{\partial w_j} = \text{sign}(w_j) =
            \begin{cases} +1 &amp; w_j &gt; 0 \\ -1 &amp; w_j &lt; 0 \\
            0 &amp; w_j = 0 \end{cases}
            \]</span></p>
            <p>Update rule: <span class="math display">\[
            w_j \leftarrow w_j - \alpha\left(\frac{\partial L}{\partial
            w_j} + \lambda \cdot \text{sign}(w_j)\right)
            \]</span></p>
            <p><strong>Interpretation</strong>: Pushes weights toward
            zero by a <strong>constant amount</strong> <span
            class="math inline">\(\alpha\lambda\)</span> each step.</p>
            <h4 id="why-it-works-1">Why It Works</h4>
            <ul>
            <li><strong>Induces sparsity</strong>: Weights actually
            become exactly zero</li>
            <li><strong>Feature selection</strong>: Automatically
            identifies which features matter</li>
            <li><strong>Non-smooth</strong>: Subgradient needed at <span
            class="math inline">\(w = 0\)</span></li>
            </ul>
            <h4 id="geometric-intuition-1">Geometric Intuition</h4>
            <p>L1 constraint region is a <strong>diamond</strong> ‚Äî loss
            contours tend to hit at <strong>corners</strong>, where some
            coordinates are exactly zero. This geometric property
            explains why L1 induces sparsity (see L1/L2 figure
            above).</p>
            <hr />
            <h3 id="l1-vs-l2-comparison">L1 vs L2: Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Property</th>
            <th>L1 (Lasso)</th>
            <th>L2 (Ridge)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Penalty</strong></td>
            <td><span class="math inline">\(\sum |w_i|\)</span></td>
            <td><span class="math inline">\(\sum w_i^2\)</span></td>
            </tr>
            <tr>
            <td><strong>Gradient</strong></td>
            <td>Constant push</td>
            <td>Proportional push</td>
            </tr>
            <tr>
            <td><strong>Sparsity</strong></td>
            <td>Yes (exact zeros)</td>
            <td>No (small but non-zero)</td>
            </tr>
            <tr>
            <td><strong>Feature selection</strong></td>
            <td>Automatic</td>
            <td>No</td>
            </tr>
            <tr>
            <td><strong>When weights large</strong></td>
            <td>Moderate penalty</td>
            <td>Strong penalty</td>
            </tr>
            <tr>
            <td><strong>When weights small</strong></td>
            <td>Same penalty</td>
            <td>Weak penalty</td>
            </tr>
            <tr>
            <td><strong>Solution</strong></td>
            <td>Corners of diamond</td>
            <td>Smooth shrinkage</td>
            </tr>
            </tbody>
            </table>
            <h3 id="when-to-use-which-2">When to Use Which</h3>
            <table>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Recommendation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Many features, few relevant</td>
            <td><strong>L1</strong> (automatic selection)</td>
            </tr>
            <tr>
            <td>All features somewhat useful</td>
            <td><strong>L2</strong> (keep all, shrink)</td>
            </tr>
            <tr>
            <td>Interpretability needed</td>
            <td><strong>L1</strong> (sparse = readable)</td>
            </tr>
            <tr>
            <td>Prediction accuracy focus</td>
            <td><strong>L2</strong> (usually better)</td>
            </tr>
            <tr>
            <td>High correlation between features</td>
            <td><strong>L2</strong> (L1 picks arbitrarily)</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="elastic-net-best-of-both">Elastic Net: Best of
            Both</h3>
            <p>Combine L1 and L2:</p>
            <p><span class="math display">\[
            R_{\text{elastic}}(w) = \alpha \|w\|_1 + \frac{1-\alpha}{2}
            \|w\|_2^2
            \]</span></p>
            <ul>
            <li>Gets sparsity from L1</li>
            <li>Gets stability from L2</li>
            <li>Handles correlated features better than pure L1</li>
            </ul>
            <hr />
            <h3
            id="code-example-linear-regression-with-regularization">Code
            Example: Linear Regression with Regularization</h3>
            <div class="sourceCode" id="cb97"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">10</span>)  <span class="co"># 100 samples, 10 features</span></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>true_w <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>])  <span class="co"># Only 2 features matter</span></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> true_w <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent with L2 regularization</span></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_l2(X, y, lambda_reg, lr<span class="op">=</span><span class="fl">0.01</span>, steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> X <span class="op">@</span> w</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> X.T <span class="op">@</span> (pred <span class="op">-</span> y) <span class="op">/</span> <span class="bu">len</span>(y) <span class="op">+</span> lambda_reg <span class="op">*</span> w  <span class="co"># L2 gradient</span></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> grad</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent with L1 regularization  </span></span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_l1(X, y, lambda_reg, lr<span class="op">=</span><span class="fl">0.01</span>, steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> X <span class="op">@</span> w</span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> X.T <span class="op">@</span> (pred <span class="op">-</span> y) <span class="op">/</span> <span class="bu">len</span>(y) <span class="op">+</span> lambda_reg <span class="op">*</span> np.sign(w)  <span class="co"># L1 gradient</span></span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> grad</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare</span></span>
<span id="cb97-28"><a href="#cb97-28" aria-hidden="true" tabindex="-1"></a>w_l2 <span class="op">=</span> train_l2(X, y, lambda_reg<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb97-29"><a href="#cb97-29" aria-hidden="true" tabindex="-1"></a>w_l1 <span class="op">=</span> train_l1(X, y, lambda_reg<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb97-30"><a href="#cb97-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-31"><a href="#cb97-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;True weights:&quot;</span>, true_w)</span>
<span id="cb97-32"><a href="#cb97-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;L2 weights:  &quot;</span>, np.<span class="bu">round</span>(w_l2, <span class="dv">2</span>))</span>
<span id="cb97-33"><a href="#cb97-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;L1 weights:  &quot;</span>, np.<span class="bu">round</span>(w_l1, <span class="dv">2</span>))</span>
<span id="cb97-34"><a href="#cb97-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-35"><a href="#cb97-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Output:</span></span>
<span id="cb97-36"><a href="#cb97-36" aria-hidden="true" tabindex="-1"></a><span class="co"># True weights: [1 2 0 0 0 0 0 0 0 0]</span></span>
<span id="cb97-37"><a href="#cb97-37" aria-hidden="true" tabindex="-1"></a><span class="co"># L2 weights:   [0.95 1.89 0.02 -0.01 0.03 -0.02 0.01 0.02 -0.01 0.03]  ‚Üê all small but non-zero</span></span>
<span id="cb97-38"><a href="#cb97-38" aria-hidden="true" tabindex="-1"></a><span class="co"># L1 weights:   [0.92 1.85 0.   0.   0.   0.   0.   0.   0.   0.  ]     ‚Üê exact zeros!</span></span></code></pre></div>
            <hr />
            <h3 id="regularization-in-deep-learning">Regularization in
            Deep Learning</h3>
            <h4 id="weight-decay-l2-regularization">Weight Decay = L2
            Regularization</h4>
            <p>In SGD:</p>
            <div class="sourceCode" id="cb98"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Equivalent to L2 regularization</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, weight_decay<span class="op">=</span><span class="fl">0.0001</span>)</span></code></pre></div>
            <h4 id="adamw-decoupled-weight-decay-1">AdamW: Decoupled
            Weight Decay</h4>
            <p>Standard Adam with L2 doesn‚Äôt work well because adaptive
            learning rates interfere with regularization.</p>
            <p><strong>AdamW</strong> applies weight decay
            <strong>directly</strong> to weights:</p>
            <div class="sourceCode" id="cb99"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div>
            <h4 id="dropout-different-kind-of-regularization">Dropout:
            Different Kind of Regularization</h4>
            <p>Randomly zero out neurons during training:</p>
            <ul>
            <li>Forces redundancy</li>
            <li>Prevents co-adaptation</li>
            <li>Approximately like model averaging</li>
            </ul>
            <div class="sourceCode" id="cb100"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">100</span>, <span class="dv">50</span>),</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>    nn.Dropout(p<span class="op">=</span><span class="fl">0.5</span>),  <span class="co"># 50% dropout</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>)</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
            <hr />
            <h3 id="summary-regularization-mental-model">Summary:
            Regularization Mental Model</h3>
            <pre><code>                    Overfitting
                         |
                         ‚Üì
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Penalize model complexity ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    /          \
                   ‚Üì            ‚Üì
              L1 (Lasso)    L2 (Ridge)

              - Sparse      - Smooth
              - |w|         - w¬≤
              - Selection   - Shrinkage</code></pre>
            <p><strong>Key insight</strong>: Regularization trades off
            training accuracy for generalization by preventing weights
            from becoming too large or relying too heavily on any single
            feature.</p>
            <hr />
            <h2
            id="gradient-instability-vanishing-and-exploding-gradients">2.10
            Gradient Instability: Vanishing and Exploding Gradients</h2>
            <h3 id="the-problem-2">The Problem</h3>
            <p>In deep networks, gradients can become <strong>very
            small</strong> (vanishing) or <strong>very large</strong>
            (exploding) as they propagate through layers during
            backpropagation.</p>
            <h3 id="why-it-happens-chain-rule-multiplication">Why It
            Happens: Chain Rule Multiplication</h3>
            <p>For an <span class="math inline">\(L\)</span>-layer
            network:</p>
            <p><span class="math display">\[
            \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial
            z_L} \cdot \frac{\partial z_L}{\partial z_{L-1}} \cdot
            \ldots \cdot \frac{\partial z_2}{\partial z_1} \cdot
            \frac{\partial z_1}{\partial W_1}
            \]</span></p>
            <p>Each term <span class="math inline">\(\frac{\partial
            z_i}{\partial z_{i-1}}\)</span> involves the weight matrix
            and activation derivative. If these are consistently <span
            class="math inline">\(&lt; 1\)</span> or <span
            class="math inline">\(&gt; 1\)</span>:</p>
            <p><span class="math display">\[
            \text{Gradient} \approx c^L \quad \text{where } c =
            \text{typical factor per layer}
            \]</span></p>
            <table>
            <thead>
            <tr>
            <th>Factor <span class="math inline">\(c\)</span></th>
            <th>Depth <span class="math inline">\(L = 50\)</span></th>
            <th>Result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(c = 0.9\)</span></td>
            <td><span class="math inline">\(0.9^{50} \approx
            0.005\)</span></td>
            <td><strong>Vanishing</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">\(c = 1.1\)</span></td>
            <td><span class="math inline">\(1.1^{50} \approx
            117\)</span></td>
            <td><strong>Exploding</strong></td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="vanishing-gradients-1">Vanishing Gradients</h3>
            <h4 id="symptoms">Symptoms</h4>
            <ul>
            <li>Early layers learn very slowly (or not at all)</li>
            <li>Loss decreases, then plateaus</li>
            <li>Gradients approach zero</li>
            </ul>
            <h4 id="causes">Causes</h4>
            <p><strong>1. Sigmoid/Tanh Saturation</strong></p>
            <p><img src="figures/sigmoid_derivative.png"
            alt="Sigmoid Derivative and Saturation" /> <em>Figure: Left:
            Sigmoid function saturates at extremes. Right: Derivative
            œÉ‚Ä≤(z) ‚Üí 0 for |z| &gt; 4 (red regions), causing vanishing
            gradients.</em></p>
            <p>For <span class="math inline">\(|z| &gt; 4\)</span>,
            <span class="math inline">\(\sigma&#39;(z) \approx
            0\)</span> ‚Äî gradients die!</p>
            <p><strong>2. Deep Networks + Poor
            Initialization</strong></p>
            <p>If weights are too small, activations shrink each layer:
            <span class="math display">\[
            h_1 \to h_2 \to h_3 \to \ldots \to \text{tiny}
            \]</span></p>
            <h4 id="solutions-1">Solutions</h4>
            <table>
            <colgroup>
            <col style="width: 41%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th>Solution</th>
            <th>How It Helps</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>ReLU activation</strong></td>
            <td>Gradient = 1 for positive inputs (no saturation)</td>
            </tr>
            <tr>
            <td><strong>Proper initialization</strong></td>
            <td>He/Xavier keeps activation variance stable</td>
            </tr>
            <tr>
            <td><strong>Residual connections</strong></td>
            <td>Gradient can flow directly through skip</td>
            </tr>
            <tr>
            <td><strong>Batch/Layer normalization</strong></td>
            <td>Prevents activation drift</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="exploding-gradients-1">Exploding Gradients</h3>
            <h4 id="symptoms-1">Symptoms</h4>
            <ul>
            <li>Loss becomes NaN or Inf</li>
            <li>Weights grow unbounded</li>
            <li>Training becomes unstable</li>
            </ul>
            <h4 id="causes-1">Causes</h4>
            <p><strong>1. Large Weights</strong></p>
            <p>If weights are too large, activations and gradients
            explode: <span class="math display">\[
            z_i = W_i h_{i-1} \quad \Rightarrow \quad \|z_i\| \approx
            \|W_i\| \cdot \|h_{i-1}\|
            \]</span></p>
            <p><strong>2. RNNs/LSTMs</strong></p>
            <p>Recurrent networks multiply the same weight matrix many
            times: <span class="math display">\[
            \frac{\partial L}{\partial h_0} = \prod_{t=1}^{T}
            W_{hh}^\top \cdot \ldots
            \]</span></p>
            <h4 id="solutions-2">Solutions</h4>
            <table>
            <thead>
            <tr>
            <th>Solution</th>
            <th>How It Helps</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Gradient clipping</strong></td>
            <td>Caps gradient norm before update</td>
            </tr>
            <tr>
            <td><strong>Proper initialization</strong></td>
            <td>Keeps initial gradients bounded</td>
            </tr>
            <tr>
            <td><strong>LSTM/GRU gates</strong></td>
            <td>Learn to regulate gradient flow</td>
            </tr>
            <tr>
            <td><strong>Lower learning rate</strong></td>
            <td>Smaller updates = more stable</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="gradient-clipping-in-detail">Gradient Clipping in
            Detail</h3>
            <p>Prevent updates from being too large:</p>
            <p><strong>By Norm</strong> (most common): <span
            class="math display">\[
            g \leftarrow \begin{cases} g &amp; \text{if } \|g\| \leq
            \tau \\ \tau \cdot \frac{g}{\|g\|} &amp; \text{otherwise}
            \end{cases}
            \]</span></p>
            <p><strong>By Value</strong>: <span class="math display">\[
            g_i \leftarrow \text{clip}(g_i, -\tau, \tau)
            \]</span></p>
            <div class="sourceCode" id="cb102"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow</span></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>tf.clip_by_global_norm(gradients, clip_norm<span class="op">=</span><span class="fl">1.0</span>)</span></code></pre></div>
            <p><strong>Typical values</strong>: <span
            class="math inline">\(\tau = 1.0\)</span> for Transformers,
            <span class="math inline">\(\tau = 5.0\)</span> for RNNs</p>
            <hr />
            <h3 id="residual-connections-the-key-innovation">Residual
            Connections: The Key Innovation</h3>
            <p>ResNet (2015) introduced <strong>skip
            connections</strong>:</p>
            <p><span class="math display">\[
            h_{l+1} = h_l + F(h_l, W_l)
            \]</span></p>
            <pre><code>    h_l ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                    ‚îÇ
     ‚Üì                    ‚îÇ (skip)
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
  ‚îÇ Conv ‚îÇ                ‚îÇ
  ‚îÇ BN   ‚îÇ                ‚îÇ
  ‚îÇ ReLU ‚îÇ                ‚îÇ
  ‚îÇ Conv ‚îÇ                ‚îÇ
  ‚îÇ BN   ‚îÇ                ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
     ‚îÇ                    ‚îÇ
     ‚Üì                    ‚îÇ
    (+) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚Üì
   h_{l+1}</code></pre>
            <p><strong>Why it helps gradient flow</strong>:</p>
            <p><span class="math display">\[
            \frac{\partial h_{l+1}}{\partial h_l} = 1 + \frac{\partial
            F}{\partial h_l}
            \]</span></p>
            <p>Even if <span class="math inline">\(\frac{\partial
            F}{\partial h_l} \approx 0\)</span>, gradient still flows
            through the identity path!</p>
            <hr />
            <h2 id="training-at-scale-memory-and-compute">2.11 Training
            at Scale: Memory and Compute</h2>
            <h3 id="memory-breakdown-for-training">Memory Breakdown for
            Training</h3>
            <p>For a model with <span class="math inline">\(P\)</span>
            parameters, training requires storing:</p>
            <table>
            <thead>
            <tr>
            <th>Component</th>
            <th>Size (bytes)</th>
            <th>For 7B params</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Parameters (FP32)</td>
            <td><span class="math inline">\(4P\)</span></td>
            <td>28 GB</td>
            </tr>
            <tr>
            <td>Gradients (FP32)</td>
            <td><span class="math inline">\(4P\)</span></td>
            <td>28 GB</td>
            </tr>
            <tr>
            <td>Optimizer states (Adam)</td>
            <td><span class="math inline">\(8P\)</span></td>
            <td>56 GB</td>
            </tr>
            <tr>
            <td>Activations</td>
            <td><span class="math inline">\(\propto\)</span> batch √ó seq
            √ó hidden</td>
            <td>~50-200 GB</td>
            </tr>
            <tr>
            <td><strong>Total</strong></td>
            <td><span class="math inline">\(\approx 16P\)</span> +
            activations</td>
            <td><strong>160+ GB</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Key insight</strong>: A 7B parameter model needs
            ~160GB+ just for training state ‚Äî doesn‚Äôt fit on a single
            GPU!</p>
            <hr />
            <h3
            id="gradientactivation-checkpointing">Gradient/Activation
            Checkpointing</h3>
            <h4 id="the-problem-3">The Problem</h4>
            <p>Activations grow with sequence length and batch size:</p>
            <ul>
            <li>GPT-3 175B: ~1TB of activations per batch!</li>
            </ul>
            <h4 id="the-solution-trade-compute-for-memory">The Solution:
            Trade Compute for Memory</h4>
            <p>Instead of storing all activations during forward pass,
            <strong>recompute</strong> them during backward pass.</p>
            <pre><code>Standard backprop:              With checkpointing:
                                
Forward:  Save all activations   Forward:  Save only checkpoints
Backward: Use saved activations  Backward: Recompute from checkpoints
                                
Memory:   O(L)                   Memory:   O(‚àöL)
Compute:  O(L)                   Compute:  O(L) forward + O(L) recompute</code></pre>
            <h4 id="tradeoff">Tradeoff</h4>
            <table>
            <thead>
            <tr>
            <th>Strategy</th>
            <th>Memory</th>
            <th>Compute Overhead</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>No checkpointing</td>
            <td>Full activations</td>
            <td>None</td>
            </tr>
            <tr>
            <td>Checkpoint every layer</td>
            <td>O(1)</td>
            <td>~33% more</td>
            </tr>
            <tr>
            <td>Checkpoint every ‚àöL layers</td>
            <td>O(‚àöL)</td>
            <td>~15% more</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="mixed-precision-training">Mixed-Precision
            Training</h3>
            <h4 id="the-idea">The Idea</h4>
            <p>Use <strong>16-bit floats</strong> for most operations,
            <strong>32-bit</strong> only where needed.</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 19%" />
            <col style="width: 22%" />
            <col style="width: 32%" />
            </colgroup>
            <thead>
            <tr>
            <th>Format</th>
            <th>Size</th>
            <th>Range</th>
            <th>Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>FP32</td>
            <td>4 bytes</td>
            <td><span class="math inline">\(\pm 3.4 \times
            10^{38}\)</span></td>
            <td>Master weights, loss scaling</td>
            </tr>
            <tr>
            <td>FP16</td>
            <td>2 bytes</td>
            <td><span class="math inline">\(\pm 65504\)</span></td>
            <td>Forward/backward, small gradients</td>
            </tr>
            <tr>
            <td>BF16</td>
            <td>2 bytes</td>
            <td><span class="math inline">\(\pm 3.4 \times
            10^{38}\)</span></td>
            <td>Same range as FP32, less precision</td>
            </tr>
            </tbody>
            </table>
            <h4 id="benefits">Benefits</h4>
            <ul>
            <li><strong>2√ó memory reduction</strong> for activations and
            gradients</li>
            <li><strong>2-8√ó speedup</strong> on modern GPUs (Tensor
            Cores)</li>
            </ul>
            <h4 id="bf16-vs-fp16">BF16 vs FP16</h4>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>FP16</th>
            <th>BF16</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Exponent bits</td>
            <td>5</td>
            <td>8</td>
            </tr>
            <tr>
            <td>Mantissa bits</td>
            <td>10</td>
            <td>7</td>
            </tr>
            <tr>
            <td>Range</td>
            <td>Small</td>
            <td>Same as FP32</td>
            </tr>
            <tr>
            <td>Precision</td>
            <td>Higher</td>
            <td>Lower</td>
            </tr>
            <tr>
            <td>Loss scaling needed?</td>
            <td>Yes</td>
            <td>Usually no</td>
            </tr>
            <tr>
            <td>Hardware</td>
            <td>All modern GPUs</td>
            <td>A100+, TPUs</td>
            </tr>
            </tbody>
            </table>
            <p><strong>BF16 is now preferred</strong> for LLM training ‚Äî
            same range as FP32 means less overflow/underflow.</p>
            <hr />
            <h3 id="gradient-accumulation">Gradient Accumulation</h3>
            <h4 id="the-problem-4">The Problem</h4>
            <p>You want to train with an effective batch size of 256,
            but your GPU can only fit batch size 32. What do you do?</p>
            <h4
            id="the-solution-accumulate-gradients-across-mini-batches">The
            Solution: Accumulate Gradients Across Mini-Batches</h4>
            <p>Instead of updating weights after every forward-backward
            pass, <strong>accumulate gradients</strong> over multiple
            mini-batches, then update once:</p>
            <div class="sourceCode" id="cb105"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>accumulation_steps <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Effective batch = 32 √ó 8 = 256</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward + backward (gradients accumulate in .grad)</span></span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch) <span class="op">/</span> accumulation_steps  <span class="co"># Scale loss</span></span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update only every accumulation_steps</span></span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div>
            <p><strong>Key insight</strong>:
            <code>loss.backward()</code> <strong>adds</strong> to
            existing <code>.grad</code> tensors, it doesn‚Äôt replace
            them! So multiple backward passes accumulate gradients.</p>
            <h4 id="why-scale-the-loss">Why Scale the Loss?</h4>
            <p>Without scaling, accumulated gradients would be
            <code>accumulation_steps √ó</code> the normal gradient.
            Dividing loss by <code>accumulation_steps</code> gives the
            correct average gradient.</p>
            <table>
            <thead>
            <tr>
            <th>Approach</th>
            <th>Gradient after 8 steps</th>
            <th>Equivalent to</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>No scaling</td>
            <td>8 √ó normal gradient</td>
            <td>Wrong!</td>
            </tr>
            <tr>
            <td>Scale by 1/8</td>
            <td>1 √ó normal gradient</td>
            <td>Batch size 256 ‚úì</td>
            </tr>
            </tbody>
            </table>
            <h4 id="memory-vs-compute-tradeoff">Memory vs Compute
            Tradeoff</h4>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>GPU Memory</th>
            <th>Compute</th>
            <th>Effective Batch</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Direct large batch</td>
            <td>High</td>
            <td>1√ó</td>
            <td>B</td>
            </tr>
            <tr>
            <td>Gradient accumulation (K steps)</td>
            <td>Low (B/K)</td>
            <td>~1√ó</td>
            <td>B</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Note</strong>: Gradient accumulation uses same
            total compute but lower peak memory.</p>
            <h4 id="common-patterns">Common Patterns</h4>
            <div class="sourceCode" id="cb106"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pattern 1: Simple accumulation</span></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch) <span class="op">/</span> accumulation_steps</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>        scheduler.step()</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Pattern 2: With gradient scaling for mixed precision</span></span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> torch.cuda.amp.GradScaler()</span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.cuda.amp.autocast():</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(batch) <span class="op">/</span> accumulation_steps</span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>        scaler.unscale_(optimizer)</span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizer)</span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div>
            <hr />
            <h3 id="normalization-batchnorm-vs-layernorm">Normalization:
            BatchNorm vs LayerNorm</h3>
            <p>Normalization layers are critical for stable training.
            The two most common types have different behaviors and use
            cases:</p>
            <h4 id="batch-normalization-batchnorm">Batch Normalization
            (BatchNorm)</h4>
            <p>Normalizes across the <strong>batch dimension</strong>
            for each feature:</p>
            <p><span class="math display">\[
            \hat{x}_{i,j} = \frac{x_{i,j} - \mu_j}{\sqrt{\sigma_j^2 +
            \epsilon}}, \quad \mu_j = \frac{1}{B}\sum_{i=1}^{B} x_{i,j}
            \]</span></p>
            <ul>
            <li>Statistics computed <strong>per feature, across
            batch</strong></li>
            <li>Requires sufficiently large batch sizes</li>
            <li>Tracks running statistics for inference</li>
            </ul>
            <h4 id="layer-normalization-layernorm">Layer Normalization
            (LayerNorm)</h4>
            <p>Normalizes across the <strong>feature dimension</strong>
            for each sample:</p>
            <p><span class="math display">\[
            \hat{x}_{i,j} = \frac{x_{i,j} - \mu_i}{\sqrt{\sigma_i^2 +
            \epsilon}}, \quad \mu_i = \frac{1}{D}\sum_{j=1}^{D} x_{i,j}
            \]</span></p>
            <ul>
            <li>Statistics computed <strong>per sample, across
            features</strong></li>
            <li>Independent of batch size</li>
            <li>No running statistics needed</li>
            </ul>
            <h4 id="comparison-table">Comparison Table</h4>
            <table>
            <colgroup>
            <col style="width: 26%" />
            <col style="width: 36%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>BatchNorm</th>
            <th>LayerNorm</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Normalize over</strong></td>
            <td>Batch dimension</td>
            <td>Feature dimension</td>
            </tr>
            <tr>
            <td><strong>Statistics</strong></td>
            <td>Per feature, across samples</td>
            <td>Per sample, across features</td>
            </tr>
            <tr>
            <td><strong>Batch dependency</strong></td>
            <td>Yes (needs large batches)</td>
            <td>No (works with batch=1)</td>
            </tr>
            <tr>
            <td><strong>Running stats</strong></td>
            <td>Yes (mean/var for inference)</td>
            <td>No</td>
            </tr>
            <tr>
            <td><strong>Best for</strong></td>
            <td>CNNs, vision</td>
            <td>Transformers, RNNs, NLP</td>
            </tr>
            <tr>
            <td><strong>Distributed training</strong></td>
            <td>Needs sync across GPUs</td>
            <td>No sync needed</td>
            </tr>
            <tr>
            <td><strong>Variable sequence length</strong></td>
            <td>Problematic</td>
            <td>Handles well</td>
            </tr>
            </tbody>
            </table>
            <h4 id="why-transformers-use-layernorm">Why Transformers Use
            LayerNorm</h4>
            <ol type="1">
            <li><strong>Batch independence</strong>: Attention can
            process variable-length sequences, and batch statistics
            would be noisy</li>
            <li><strong>No synchronization</strong>: In distributed
            training, no need to sync statistics across GPUs</li>
            <li><strong>Consistent at inference</strong>: Same
            normalization behavior during training and inference</li>
            <li><strong>Theoretical</strong>: Each token normalized
            independently matches the parallel nature of attention</li>
            </ol>
            <h4 id="pytorch-usage-1">PyTorch Usage</h4>
            <div class="sourceCode" id="cb107"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="co"># BatchNorm for CNNs (normalizes across batch for each channel)</span></span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>bn <span class="op">=</span> nn.BatchNorm2d(num_features<span class="op">=</span><span class="dv">64</span>)  <span class="co"># 64 channels</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a><span class="co"># LayerNorm for Transformers (normalizes across features for each token)</span></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>ln <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span><span class="dv">768</span>)  <span class="co"># hidden dimension</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a><span class="co"># In a Transformer block</span></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model):</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> nn.MultiheadAttention(d_model, num_heads<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(d_model)  <span class="co"># After attention</span></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff <span class="op">=</span> nn.Sequential(</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>            nn.Linear(d_model, <span class="dv">4</span> <span class="op">*</span> d_model),</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> d_model, d_model),</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(d_model)  <span class="co"># After feed-forward</span></span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln1(x), <span class="va">self</span>.ln1(x), <span class="va">self</span>.ln1(x))[<span class="dv">0</span>]  <span class="co"># Pre-norm</span></span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ff(<span class="va">self</span>.ln2(x))</span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
            <hr />
            <h2 id="distributed-training">2.12 Distributed Training</h2>
            <h3 id="why-distribute">Why Distribute?</h3>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Parameters</th>
            <th>Min. Memory</th>
            <th>Single A100 (80GB)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>GPT-2</td>
            <td>1.5B</td>
            <td>~24 GB</td>
            <td>‚úÖ Fits</td>
            </tr>
            <tr>
            <td>LLaMA-7B</td>
            <td>7B</td>
            <td>~112 GB</td>
            <td>‚ùå Too big</td>
            </tr>
            <tr>
            <td>LLaMA-70B</td>
            <td>70B</td>
            <td>~1.1 TB</td>
            <td>‚ùå Way too big</td>
            </tr>
            <tr>
            <td>GPT-4 (est.)</td>
            <td>1.8T</td>
            <td>~28 TB</td>
            <td>‚ùå Need cluster</td>
            </tr>
            </tbody>
            </table>
            <h3 id="parallelism-strategies">Parallelism Strategies</h3>
            <pre><code>                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ        Distributed Training         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                       ‚îÇ                       ‚îÇ
            ‚Üì                       ‚Üì                       ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Data Parallel ‚îÇ      ‚îÇTensor Parallel‚îÇ      ‚îÇPipeline Parall‚îÇ
    ‚îÇ (DP / DDP)    ‚îÇ      ‚îÇ(TP / Megatron)‚îÇ      ‚îÇ (PP)          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    Split: Batch           Split: Layers          Split: Layers
    Replicate: Model       Split: Weight matrices Sequential stages</code></pre>
            <hr />
            <h3 id="data-parallelism-ddp">Data Parallelism (DDP)</h3>
            <h4 id="how-it-works">How It Works</h4>
            <ol type="1">
            <li><strong>Replicate</strong> entire model on each GPU</li>
            <li><strong>Split</strong> batch across GPUs</li>
            <li>Each GPU computes gradients on its local batch</li>
            <li><strong>All-reduce</strong> gradients (average across
            GPUs)</li>
            <li>Each GPU updates its local model copy</li>
            </ol>
            <pre><code>GPU 0: Model copy‚ÇÄ  ‚îÄ‚îÄ‚Üí  Grad‚ÇÄ  ‚îÄ‚îÄ‚îê
                                  ‚îÇ
GPU 1: Model copy‚ÇÅ  ‚îÄ‚îÄ‚Üí  Grad‚ÇÅ  ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí  AllReduce  ‚îÄ‚îÄ‚Üí  Avg Grad  ‚îÄ‚îÄ‚Üí  Update all
                                  ‚îÇ
GPU 2: Model copy‚ÇÇ  ‚îÄ‚îÄ‚Üí  Grad‚ÇÇ  ‚îÄ‚îÄ‚îò</code></pre>
            <h4 id="scaling-efficiency">Scaling Efficiency</h4>
            <p><span class="math display">\[
            \text{Speedup} = \frac{N}{\text{1 + communication overhead}}
            \]</span></p>
            <p>For large models: ~90-95% efficiency with 8 GPUs, ~80%
            with 64 GPUs.</p>
            <hr />
            <h3 id="zero-zero-redundancy-optimizer">ZeRO: Zero
            Redundancy Optimizer</h3>
            <h4 id="the-problem-with-data-parallelism">The Problem with
            Data Parallelism</h4>
            <p>Each GPU stores full copy of:</p>
            <ul>
            <li>Model parameters</li>
            <li>Gradients<br />
            </li>
            <li>Optimizer states (2√ó for Adam)</li>
            </ul>
            <p>Total: ~16√ó parameters per GPU ‚Üí <strong>massive
            redundancy</strong></p>
            <h4 id="zero-solution-partition-everything">ZeRO Solution:
            Partition Everything</h4>
            <table>
            <thead>
            <tr>
            <th>Stage</th>
            <th>Partition</th>
            <th>Memory Savings</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>ZeRO-1</td>
            <td>Optimizer states</td>
            <td>4√ó</td>
            </tr>
            <tr>
            <td>ZeRO-2</td>
            <td>+ Gradients</td>
            <td>8√ó</td>
            </tr>
            <tr>
            <td>ZeRO-3</td>
            <td>+ Parameters</td>
            <td><span class="math inline">\(N\)</span>√ó (linear in GPU
            count)</td>
            </tr>
            </tbody>
            </table>
            <pre><code>Standard DDP (each GPU):          ZeRO-3 (across 4 GPUs):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Full params    (1√ó)    ‚îÇ        ‚îÇ Params‚ÇÄ  (¬º√ó)  - GPU 0 ‚îÇ
‚îÇ Full grads     (1√ó)    ‚îÇ   ‚Üí    ‚îÇ Params‚ÇÅ  (¬º√ó)  - GPU 1 ‚îÇ
‚îÇ Full opt state (2√ó)    ‚îÇ        ‚îÇ Params‚ÇÇ  (¬º√ó)  - GPU 2 ‚îÇ
‚îÇ Total: 4√ó              ‚îÇ        ‚îÇ Params‚ÇÉ  (¬º√ó)  - GPU 3 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   + All-gather when needed</code></pre>
            <hr />
            <h3 id="summary-which-parallelism-when">Summary: Which
            Parallelism When?</h3>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 59%" />
            </colgroup>
            <thead>
            <tr>
            <th>Situation</th>
            <th>Recommendation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Model fits on 1 GPU</td>
            <td>No parallelism needed</td>
            </tr>
            <tr>
            <td>Model fits, want faster training</td>
            <td><strong>Data Parallel</strong> (DDP)</td>
            </tr>
            <tr>
            <td>Model doesn‚Äôt fit on 1 GPU</td>
            <td><strong>Tensor Parallel</strong> +
            <strong>ZeRO</strong></td>
            </tr>
            <tr>
            <td>Very large model (100B+)</td>
            <td><strong>3D Parallelism</strong></td>
            </tr>
            <tr>
            <td>Limited inter-node bandwidth</td>
            <td><strong>Pipeline Parallel</strong> (less
            communication)</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="summary-comparison-table">2.13 Summary Comparison
            Table</h2>
            <table>
            <thead>
            <tr>
            <th>Optimizer</th>
            <th>Update</th>
            <th>Best For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>SGD</strong></td>
            <td><span class="math inline">\(w - \alpha g\)</span></td>
            <td>CNNs, good generalization</td>
            </tr>
            <tr>
            <td><strong>Momentum</strong></td>
            <td><span class="math inline">\(w - \alpha(\beta v +
            g)\)</span></td>
            <td>Faster convergence</td>
            </tr>
            <tr>
            <td><strong>Adam</strong></td>
            <td>Adaptive per-param</td>
            <td>Fast training, Transformers</td>
            </tr>
            <tr>
            <td><strong>AdamW</strong></td>
            <td>Adam + weight decay</td>
            <td><strong>Default for LLMs</strong></td>
            </tr>
            </tbody>
            </table>
            <h3 id="hyperparameter-starting-points">Hyperparameter
            Starting Points</h3>
            <table>
            <thead>
            <tr>
            <th>Optimizer</th>
            <th>Learning Rate</th>
            <th>Notes</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>SGD</td>
            <td><span class="math inline">\(0.1\)</span></td>
            <td>+ momentum <span class="math inline">\(0.9\)</span></td>
            </tr>
            <tr>
            <td>Adam</td>
            <td><span class="math inline">\(3 \times
            10^{-4}\)</span></td>
            <td>Standard</td>
            </tr>
            <tr>
            <td>AdamW</td>
            <td><span class="math inline">\(10^{-4}\)</span> to <span
            class="math inline">\(10^{-3}\)</span></td>
            <td>+ warmup for Transformers</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="references">References</h2>
            <ul>
            <li>Ruder, S. (2016). ‚ÄúAn overview of gradient descent
            optimization algorithms.‚Äù <a
            href="https://arxiv.org/abs/1609.04747">arXiv:1609.04747</a></li>
            <li>Kingma &amp; Ba (2015). ‚ÄúAdam: A Method for Stochastic
            Optimization.‚Äù <a
            href="https://arxiv.org/abs/1412.6980">arXiv:1412.6980</a></li>
            <li>Loshchilov &amp; Hutter (2019). ‚ÄúDecoupled Weight Decay
            Regularization.‚Äù <a
            href="https://arxiv.org/abs/1711.05101">arXiv:1711.05101</a></li>
            </ul>
            <h1 id="part-3-math-foundations">Part 3: Math
            Foundations</h1>
            <h2 id="linear-algebra">3.1 Linear Algebra</h2>
            <h3 id="vectors-and-matrices">Vectors and Matrices</h3>
            <p><strong>Vector</strong>: An ordered list of numbers <span
            class="math display">\[\mathbf{x} = \begin{bmatrix} x_1 \\
            x_2 \\ \vdots \\ x_n \end{bmatrix} \in
            \mathbb{R}^n\]</span></p>
            <p><strong>Matrix</strong>: A 2D array of numbers <span
            class="math display">\[\mathbf{A} = \begin{bmatrix} a_{11}
            &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp;
            a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots
            &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp;
            \cdots &amp; a_{mn} \end{bmatrix} \in \mathbb{R}^{m \times
            n}\]</span></p>
            <blockquote>
            <p><strong>üìê Shape Convention: (rows, columns)</strong></p>
            <p>For shape <code>(m, n)</code> or <span
            class="math inline">\(m \times n\)</span>: <strong>m = rows,
            n = columns</strong></p>
            <p>This is the <strong>row-major convention</strong> used in
            NumPy, PyTorch, and most ML frameworks. The first axis is
            always rows.</p>
            <table>
            <thead>
            <tr>
            <th>Shape</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>(3, 4)</code></td>
            <td>3 rows √ó 4 columns</td>
            </tr>
            <tr>
            <td><code>(1, n)</code></td>
            <td>Row vector (1 row, n columns)</td>
            </tr>
            <tr>
            <td><code>(n, 1)</code></td>
            <td>Column vector (n rows, 1 column)</td>
            </tr>
            </tbody>
            </table>
            </blockquote>
            <h3 id="tensors">Tensors</h3>
            <p>A <strong>tensor</strong> is a generalization of vectors
            and matrices to higher dimensions:</p>
            <table>
            <thead>
            <tr>
            <th>Rank</th>
            <th>Name</th>
            <th>Example Shape</th>
            <th>ML Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>Scalar</td>
            <td>()</td>
            <td>Loss value</td>
            </tr>
            <tr>
            <td>1</td>
            <td>Vector</td>
            <td>(n,)</td>
            <td>Word embedding</td>
            </tr>
            <tr>
            <td>2</td>
            <td>Matrix</td>
            <td>(m, n)</td>
            <td>Weight matrix</td>
            </tr>
            <tr>
            <td>3</td>
            <td>3D Tensor</td>
            <td>(batch, seq, dim)</td>
            <td>Batch of sequences</td>
            </tr>
            <tr>
            <td>4</td>
            <td>4D Tensor</td>
            <td>(batch, C, H, W)</td>
            <td>Batch of images</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why ‚ÄúTensor‚Äù in ML?</strong></p>
            <p>In physics/math, tensors have specific transformation
            properties. In ML, we use ‚Äútensor‚Äù more loosely to mean
            <strong>multi-dimensional array</strong>. PyTorch and
            TensorFlow are named after this concept because neural
            networks are fundamentally tensor operations.</p>
            <div class="sourceCode" id="cb111"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalar (0D tensor)</span></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>scalar <span class="op">=</span> torch.tensor(<span class="fl">3.14</span>)           <span class="co"># shape: ()</span></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Vector (1D tensor)  </span></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])      <span class="co"># shape: (3,)</span></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix (2D tensor)</span></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)            <span class="co"># shape: (3, 4)</span></span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3D tensor (e.g., batch of sequences)</span></span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>batch_seq <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>, <span class="dv">512</span>) <span class="co"># (batch, seq_len, hidden_dim)</span></span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 4D tensor (e.g., batch of images)</span></span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>batch_img <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># (batch, channels, height, width)</span></span></code></pre></div>
            <p><strong>Common Tensor Operations with
            Examples</strong>:</p>
            <div class="sourceCode" id="cb112"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a sample tensor</span></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="dv">12</span>)  <span class="co"># [0, 1, 2, ..., 11]</span></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Original: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([12])</span></span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a><span class="co"># RESHAPE / VIEW: Change shape without changing data</span></span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.view(<span class="dv">3</span>, <span class="dv">4</span>)      <span class="co"># 12 elements ‚Üí 3 rows √ó 4 cols</span></span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;view(3,4): </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([3, 4])</span></span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a><span class="co"># tensor([[ 0,  1,  2,  3],</span></span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a><span class="co">#         [ 4,  5,  6,  7],</span></span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a><span class="co">#         [ 8,  9, 10, 11]])</span></span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> x.view(<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)     <span class="co"># -1 means &quot;infer this dimension&quot;</span></span>
<span id="cb112-17"><a href="#cb112-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;view(2,-1): </span><span class="sc">{</span>z<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([2, 6])</span></span>
<span id="cb112-18"><a href="#cb112-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-19"><a href="#cb112-19" aria-hidden="true" tabindex="-1"></a><span class="co"># reshape() is safer than view() after non-contiguous operations</span></span>
<span id="cb112-20"><a href="#cb112-20" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> x.reshape(<span class="dv">3</span>, <span class="dv">4</span>)   <span class="co"># Same result, but works on non-contiguous tensors</span></span>
<span id="cb112-21"><a href="#cb112-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-22"><a href="#cb112-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-23"><a href="#cb112-23" aria-hidden="true" tabindex="-1"></a><span class="co"># TRANSPOSE: Swap dimensions</span></span>
<span id="cb112-24"><a href="#cb112-24" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-25"><a href="#cb112-25" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb112-26"><a href="#cb112-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;A: </span><span class="sc">{</span>A<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)           <span class="co"># torch.Size([3, 4])</span></span>
<span id="cb112-27"><a href="#cb112-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;A.T: </span><span class="sc">{</span>A<span class="sc">.</span>T<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)       <span class="co"># torch.Size([4, 3])</span></span>
<span id="cb112-28"><a href="#cb112-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;transpose(0,1): </span><span class="sc">{</span>A<span class="sc">.</span>transpose(<span class="dv">0</span>, <span class="dv">1</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([4, 3])</span></span>
<span id="cb112-29"><a href="#cb112-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-30"><a href="#cb112-30" aria-hidden="true" tabindex="-1"></a><span class="co"># For higher dimensions, specify which dims to swap</span></span>
<span id="cb112-31"><a href="#cb112-31" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)  <span class="co"># (batch, seq, hidden)</span></span>
<span id="cb112-32"><a href="#cb112-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;B: </span><span class="sc">{</span>B<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)                        <span class="co"># torch.Size([2, 3, 4])</span></span>
<span id="cb112-33"><a href="#cb112-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;B.transpose(1,2): </span><span class="sc">{</span>B<span class="sc">.</span>transpose(<span class="dv">1</span>,<span class="dv">2</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([2, 4, 3])</span></span>
<span id="cb112-34"><a href="#cb112-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-35"><a href="#cb112-35" aria-hidden="true" tabindex="-1"></a><span class="co"># permute() for arbitrary reordering</span></span>
<span id="cb112-36"><a href="#cb112-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;B.permute(2,0,1): </span><span class="sc">{</span>B<span class="sc">.</span>permute(<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([4, 2, 3])</span></span>
<span id="cb112-37"><a href="#cb112-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-38"><a href="#cb112-38" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-39"><a href="#cb112-39" aria-hidden="true" tabindex="-1"></a><span class="co"># SQUEEZE / UNSQUEEZE: Add or remove dimensions of size 1</span></span>
<span id="cb112-40"><a href="#cb112-40" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-41"><a href="#cb112-41" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb112-42"><a href="#cb112-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Original: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)           <span class="co"># torch.Size([3, 1, 4])</span></span>
<span id="cb112-43"><a href="#cb112-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;squeeze(): </span><span class="sc">{</span>x<span class="sc">.</span>squeeze()<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)     <span class="co"># torch.Size([3, 4]) - removes ALL size-1 dims</span></span>
<span id="cb112-44"><a href="#cb112-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;squeeze(1): </span><span class="sc">{</span>x<span class="sc">.</span>squeeze(<span class="dv">1</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># torch.Size([3, 4]) - removes dim 1 only</span></span>
<span id="cb112-45"><a href="#cb112-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-46"><a href="#cb112-46" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb112-47"><a href="#cb112-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Original: </span><span class="sc">{</span>y<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)                <span class="co"># torch.Size([3, 4])</span></span>
<span id="cb112-48"><a href="#cb112-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;unsqueeze(0): </span><span class="sc">{</span>y<span class="sc">.</span>unsqueeze(<span class="dv">0</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># torch.Size([1, 3, 4]) - add batch dim</span></span>
<span id="cb112-49"><a href="#cb112-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;unsqueeze(2): </span><span class="sc">{</span>y<span class="sc">.</span>unsqueeze(<span class="dv">2</span>)<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)   <span class="co"># torch.Size([3, 4, 1]) - add trailing dim</span></span>
<span id="cb112-50"><a href="#cb112-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;y[None]: </span><span class="sc">{</span>y[<span class="va">None</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)               <span class="co"># torch.Size([1, 3, 4]) - same as unsqueeze(0)</span></span>
<span id="cb112-51"><a href="#cb112-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-52"><a href="#cb112-52" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-53"><a href="#cb112-53" aria-hidden="true" tabindex="-1"></a><span class="co"># BROADCASTING: Auto-expand dimensions for element-wise ops</span></span>
<span id="cb112-54"><a href="#cb112-54" aria-hidden="true" tabindex="-1"></a><span class="co"># ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span></span>
<span id="cb112-55"><a href="#cb112-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Rule: Dimensions are compared right-to-left; must be equal or one must be 1</span></span>
<span id="cb112-56"><a href="#cb112-56" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">4</span>)       <span class="co"># (3, 4)</span></span>
<span id="cb112-57"><a href="#cb112-57" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">4</span>)          <span class="co"># (4,) ‚Üí broadcasts to (3, 4)</span></span>
<span id="cb112-58"><a href="#cb112-58" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">+</span> b                   <span class="co"># Works! b is broadcast along dim 0</span></span>
<span id="cb112-59"><a href="#cb112-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;(3,4) + (4,): </span><span class="sc">{</span>c<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([3, 4])</span></span>
<span id="cb112-60"><a href="#cb112-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-61"><a href="#cb112-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding a scalar (broadcasts to everything)</span></span>
<span id="cb112-62"><a href="#cb112-62" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a <span class="op">+</span> <span class="dv">5</span>                   <span class="co"># 5 broadcasts to (3, 4)</span></span>
<span id="cb112-63"><a href="#cb112-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-64"><a href="#cb112-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch + single example</span></span>
<span id="cb112-65"><a href="#cb112-65" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># (B, C, H, W)</span></span>
<span id="cb112-66"><a href="#cb112-66" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> torch.tensor([<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>]).view(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># (1, 3, 1, 1)</span></span>
<span id="cb112-67"><a href="#cb112-67" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> batch <span class="op">-</span> mean   <span class="co"># mean broadcasts to (32, 3, 224, 224)</span></span>
<span id="cb112-68"><a href="#cb112-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;After broadcast: </span><span class="sc">{</span>normalized<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># torch.Size([32, 3, 224, 224])</span></span>
<span id="cb112-69"><a href="#cb112-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-70"><a href="#cb112-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Common broadcasting pattern: outer product</span></span>
<span id="cb112-71"><a href="#cb112-71" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])          <span class="co"># (3,)</span></span>
<span id="cb112-72"><a href="#cb112-72" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="dv">10</span>, <span class="dv">20</span>])           <span class="co"># (2,)</span></span>
<span id="cb112-73"><a href="#cb112-73" aria-hidden="true" tabindex="-1"></a>outer <span class="op">=</span> x.unsqueeze(<span class="dv">1</span>) <span class="op">*</span> y.unsqueeze(<span class="dv">0</span>)  <span class="co"># (3,1) * (1,2) ‚Üí (3,2)</span></span>
<span id="cb112-74"><a href="#cb112-74" aria-hidden="true" tabindex="-1"></a><span class="co"># tensor([[10, 20],</span></span>
<span id="cb112-75"><a href="#cb112-75" aria-hidden="true" tabindex="-1"></a><span class="co">#         [20, 40],</span></span>
<span id="cb112-76"><a href="#cb112-76" aria-hidden="true" tabindex="-1"></a><span class="co">#         [30, 60]])</span></span></code></pre></div>
            <p><strong>Common Pitfalls</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 36%" />
            <col style="width: 30%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>Operation</th>
            <th>Pitfall</th>
            <th>Solution</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>view()</code></td>
            <td>Fails on non-contiguous tensors</td>
            <td>Use <code>reshape()</code> or call
            <code>.contiguous()</code> first</td>
            </tr>
            <tr>
            <td>Transpose</td>
            <td><code>x.T</code> only works on 2D tensors</td>
            <td>Use <code>transpose(dim0, dim1)</code> for higher
            dims</td>
            </tr>
            <tr>
            <td>Broadcasting</td>
            <td>Silent shape mismatches</td>
            <td>Always print shapes when debugging!</td>
            </tr>
            <tr>
            <td>In-place ops</td>
            <td><code>x.view_()</code> doesn‚Äôt exist</td>
            <td>Use <code>x = x.view(...)</code> (returns new
            tensor)</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3
            id="shape-notation-1d-tensors-vs-rowcolumn-vectors">Shape
            Notation: 1D Tensors vs Row/Column Vectors</h3>
            <p>A common source of confusion is the difference between 1D
            tensors and 2D row/column vectors:</p>
            <table>
            <colgroup>
            <col style="width: 17%" />
            <col style="width: 14%" />
            <col style="width: 31%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>Shape</th>
            <th>Type</th>
            <th>Description</th>
            <th>NumPy Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>(784,)</code></td>
            <td>1D tensor</td>
            <td>Flat array, <strong>not</strong> a matrix</td>
            <td><code>np.zeros(784)</code></td>
            </tr>
            <tr>
            <td><code>(1, 784)</code></td>
            <td>Row vector</td>
            <td>2D: 1 row √ó 784 columns</td>
            <td><code>np.zeros((1, 784))</code></td>
            </tr>
            <tr>
            <td><code>(784, 1)</code></td>
            <td>Column vector</td>
            <td>2D: 784 rows √ó 1 column</td>
            <td><code>np.zeros((784, 1))</code></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why This Matters</strong>: Matrix multiplication
            behaves differently!</p>
            <div class="sourceCode" id="cb113"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create different shapes</span></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.zeros(<span class="dv">784</span>)           <span class="co"># shape (784,)   - 1D tensor</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.zeros((<span class="dv">1</span>, <span class="dv">784</span>))      <span class="co"># shape (1, 784) - row vector  </span></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.zeros((<span class="dv">784</span>, <span class="dv">1</span>))      <span class="co"># shape (784, 1) - column vector</span></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix multiplication behavior differs:</span></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>a <span class="op">@</span> a      <span class="co"># Dot product ‚Üí scalar (1D @ 1D)</span></span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>b <span class="op">@</span> c      <span class="co"># (1, 784) @ (784, 1) ‚Üí (1, 1) matrix (inner product)</span></span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>c <span class="op">@</span> b      <span class="co"># (784, 1) @ (1, 784) ‚Üí (784, 784) matrix (outer product!)</span></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape results:</span></span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((a <span class="op">@</span> a).shape)        <span class="co"># () - scalar</span></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((b <span class="op">@</span> c).shape)        <span class="co"># (1, 1)</span></span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>((c <span class="op">@</span> b).shape)        <span class="co"># (784, 784) - outer product!</span></span></code></pre></div>
            <p><strong>In Keras/TensorFlow</strong>:</p>
            <div class="sourceCode" id="cb114"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input shape specifies per-sample shape (batch dimension is implicit)</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>layers.Input(shape<span class="op">=</span>(<span class="dv">784</span>,))   <span class="co"># Each sample is a 1D tensor of length 784</span></span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>                             <span class="co"># Actual tensor shape: (batch_size, 784)</span></span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the difference
            between <code>(784,)</code> and <code>(784, 1)</code>?‚Äù</p>
            <p><strong>A</strong>: <code>(784,)</code> is a 1D tensor
            (rank 1) ‚Äî a flat array with 784 elements.
            <code>(784, 1)</code> is a 2D tensor (rank 2) ‚Äî a matrix
            with 784 rows and 1 column (column vector). The distinction
            matters for matrix multiplication:
            <code>(784,) @ (784,)</code> gives a scalar (dot product),
            while <code>(784, 1) @ (1, 784)</code> gives a
            <code>(784, 784)</code> matrix (outer product).</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is a tensor in the
            context of deep learning?‚Äù</p>
            <p><strong>A</strong>: In deep learning, a tensor is a
            multi-dimensional array ‚Äî a generalization of scalars (0D),
            vectors (1D), and matrices (2D) to arbitrary dimensions. A
            batch of RGB images is a 4D tensor with shape (batch_size,
            channels, height, width). Tensors are the fundamental data
            structure in frameworks like PyTorch and TensorFlow because
            neural network operations (convolutions, matrix
            multiplications, etc.) are naturally expressed as tensor
            operations.</p>
            <hr />
            <h3 id="tensor-shape-management-in-deep-learning">Tensor
            Shape Management in Deep Learning</h3>
            <p>Correctly manipulating tensor shapes is a <strong>rite of
            passage</strong> in ML implementations. The primary failure
            mode in many coding interviews and real-world bugs is
            <strong>tensor shape management</strong>. Here‚Äôs what you
            need to know:</p>
            <p><strong>The Shape Journey in Multi-Head
            Attention</strong> (Example):</p>
            <div class="sourceCode" id="cb115"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Input: (B, S, D) where B=batch, S=seq_len, D=d_model</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: (32, 100, 512) ‚Äî 32 sequences of 100 tokens, 512-dim embeddings</span></span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Project to Q, K, V</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> X <span class="op">@</span> W_Q  <span class="co"># (B, S, D) @ (D, D) ‚Üí (B, S, D)</span></span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> X <span class="op">@</span> W_K  <span class="co"># (B, S, D)</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> X <span class="op">@</span> W_V  <span class="co"># (B, S, D)</span></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Split into heads ‚Äî THIS IS WHERE BUGS HAPPEN</span></span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We need: (B, H, S, D_k) where H=num_heads, D_k=D/H</span></span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.view(B, S, H, D_k)  <span class="co"># (B, S, H, D_k)</span></span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.transpose(<span class="dv">1</span>, <span class="dv">2</span>)      <span class="co"># (B, H, S, D_k) ‚Üê swap S and H</span></span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Attention scores</span></span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Q: (B, H, S, D_k), K: (B, H, S, D_k)</span></span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Need K.T along last dims: (B, H, D_k, S)</span></span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># (B, H, S, S) ‚Üê attention matrix!</span></span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-19"><a href="#cb115-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Apply softmax and multiply by V</span></span>
<span id="cb115-20"><a href="#cb115-20" aria-hidden="true" tabindex="-1"></a>attn <span class="op">=</span> softmax(scores <span class="op">/</span> sqrt(D_k), dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, H, S, S)</span></span>
<span id="cb115-21"><a href="#cb115-21" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> attn <span class="op">@</span> V  <span class="co"># (B, H, S, D_k)</span></span>
<span id="cb115-22"><a href="#cb115-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-23"><a href="#cb115-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Merge heads back ‚Äî ANOTHER </span><span class="al">BUG</span><span class="co">-PRONE STEP</span></span>
<span id="cb115-24"><a href="#cb115-24" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>)           <span class="co"># (B, S, H, D_k)</span></span>
<span id="cb115-25"><a href="#cb115-25" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.contiguous().view(B, S, D)  <span class="co"># (B, S, D) ‚Üê WHY .contiguous()?</span></span></code></pre></div>
            <p><strong>The view() vs reshape() Trap</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 21%" />
            <col style="width: 32%" />
            <col style="width: 21%" />
            </colgroup>
            <thead>
            <tr>
            <th>Operation</th>
            <th>Works on</th>
            <th>When it fails</th>
            <th>Solution</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>view()</code></td>
            <td>Contiguous tensors only</td>
            <td>After transpose/permute</td>
            <td>Use <code>.contiguous().view()</code> or
            <code>reshape()</code></td>
            </tr>
            <tr>
            <td><code>reshape()</code></td>
            <td>Any tensor</td>
            <td>Never (may copy)</td>
            <td>Always works, but may be slower</td>
            </tr>
            <tr>
            <td><code>contiguous()</code></td>
            <td>Returns contiguous copy</td>
            <td>-</td>
            <td>Call before <code>view()</code></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why transpose breaks contiguity</strong>:</p>
            <div class="sourceCode" id="cb116"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)  <span class="co"># Contiguous: elements stored [0,0,0], [0,0,1], [0,0,2]...</span></span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)      <span class="co"># Non-contiguous! Same data, different stride</span></span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>y.view(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># ERROR: RuntimeError: view size is not compatible with input tensor&#39;s size and stride</span></span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Solutions:</span></span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true" tabindex="-1"></a>y.contiguous().view(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># ‚úì Makes a contiguous copy first</span></span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true" tabindex="-1"></a>y.reshape(<span class="op">-</span><span class="dv">1</span>)            <span class="co"># ‚úì Handles non-contiguous automatically</span></span></code></pre></div>
            <p><strong>Common Interview Bugs</strong>:</p>
            <ol type="1">
            <li><strong>Wrong transpose dimension</strong>:</li>
            </ol>
            <div class="sourceCode" id="cb117"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG: K.transpose(0, 1) ‚Äî transposes batch and seq, not for dot product!</span></span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="co"># RIGHT: K.transpose(-2, -1) ‚Äî transposes last two dims (S, D_k) ‚Üí (D_k, S)</span></span></code></pre></div>
            <ol start="2" type="1">
            <li><strong>Forgetting to split/merge heads
            properly</strong>:</li>
            </ol>
            <div class="sourceCode" id="cb118"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG: Just reshaping without transpose</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.view(B, H, S, D_k)  <span class="co"># This interleaves heads incorrectly!</span></span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a><span class="co"># RIGHT: Reshape then transpose (or use einops)</span></span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> Q.view(B, S, H, D_k).transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, H, S, D_k)</span></span></code></pre></div>
            <ol start="3" type="1">
            <li><strong>view() after transpose without
            contiguous()</strong>:</li>
            </ol>
            <div class="sourceCode" id="cb119"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG:</span></span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).view(B, S, D)  <span class="co"># RuntimeError!</span></span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a><span class="co"># RIGHT:</span></span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, S, D)</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a><span class="co"># OR:</span></span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(B, S, D)</span></code></pre></div>
            <ol start="4" type="1">
            <li><strong>Wrong softmax dimension</strong>:</li>
            </ol>
            <div class="sourceCode" id="cb120"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG: softmax(scores, dim=0) ‚Äî softmax across batch!</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="co"># WRONG: softmax(scores, dim=1) ‚Äî softmax across heads!</span></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a><span class="co"># RIGHT: softmax(scores, dim=-1) ‚Äî softmax across keys (last dim)</span></span></code></pre></div>
            <p><strong>Clean Implementation with einops</strong>
            (cleaner, less error-prone):</p>
            <div class="sourceCode" id="cb121"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> einops <span class="im">import</span> rearrange</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multihead_attention(x, W_Q, W_K, W_V, W_O, n_heads):</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a><span class="co">    x: (batch, seq, d_model)</span></span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>    B, S, D <span class="op">=</span> x.shape</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> D <span class="op">//</span> n_heads</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Project</span></span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> x <span class="op">@</span> W_Q  <span class="co"># (B, S, D)</span></span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> x <span class="op">@</span> W_K</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> x <span class="op">@</span> W_V</span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split heads using einops (cleaner, less error-prone)</span></span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> rearrange(Q, <span class="st">&#39;b s (h d) -&gt; b h s d&#39;</span>, h<span class="op">=</span>n_heads)</span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> rearrange(K, <span class="st">&#39;b s (h d) -&gt; b h s d&#39;</span>, h<span class="op">=</span>n_heads)</span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> rearrange(V, <span class="st">&#39;b s (h d) -&gt; b h s d&#39;</span>, h<span class="op">=</span>n_heads)</span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb121-20"><a href="#cb121-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attention</span></span>
<span id="cb121-21"><a href="#cb121-21" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> torch.einsum(<span class="st">&#39;bhqd,bhkd-&gt;bhqk&#39;</span>, Q, K) <span class="op">/</span> (d_k <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb121-22"><a href="#cb121-22" aria-hidden="true" tabindex="-1"></a>    attn <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb121-23"><a href="#cb121-23" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> torch.einsum(<span class="st">&#39;bhqk,bhkd-&gt;bhqd&#39;</span>, attn, V)</span>
<span id="cb121-24"><a href="#cb121-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb121-25"><a href="#cb121-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Merge heads</span></span>
<span id="cb121-26"><a href="#cb121-26" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> rearrange(out, <span class="st">&#39;b h s d -&gt; b s (h d)&#39;</span>)</span>
<span id="cb121-27"><a href="#cb121-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out <span class="op">@</span> W_O</span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the difference
            between view() and reshape() in PyTorch?‚Äù</p>
            <p><strong>A</strong>: <code>view()</code> requires the
            tensor to be contiguous in memory ‚Äî it returns a view of the
            same data with a different shape. <code>reshape()</code>
            works on any tensor ‚Äî if contiguous, it returns a view; if
            not, it makes a copy. After operations like
            <code>transpose()</code> or <code>permute()</code>, the
            tensor is no longer contiguous, so <code>view()</code> will
            fail. You must either call <code>.contiguous().view()</code>
            or use <code>reshape()</code> directly.</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúWhy does transpose make a
            tensor non-contiguous?‚Äù</p>
            <p><strong>A</strong>: Contiguous means elements are stored
            in row-major order (last dimension changes fastest).
            <code>transpose()</code> just changes the stride metadata,
            not the actual memory layout. The elements are now accessed
            in a different order than they‚Äôre stored. For example, a
            (2,3) tensor stored as [a,b,c,d,e,f] becomes a (3,2) tensor
            accessing [a,d,b,e,c,f] ‚Äî the stride pattern no longer
            matches contiguous storage. <code>view()</code> requires
            contiguous storage to work without copying.</p>
            <h3 id="key-matrix-operations">Key Matrix Operations</h3>
            <p><strong>Transpose</strong>: <span
            class="math inline">\((\mathbf{A}^T)_{ij} =
            A_{ji}\)</span></p>
            <p><strong>Matrix Multiplication</strong>: <span
            class="math inline">\((\mathbf{AB})_{ij} = \sum_k A_{ik}
            B_{kj}\)</span></p>
            <ul>
            <li>Requires: columns of <span
            class="math inline">\(\mathbf{A}\)</span> = rows of <span
            class="math inline">\(\mathbf{B}\)</span></li>
            <li>Result: <span class="math inline">\((m \times k) \cdot
            (k \times n) = (m \times n)\)</span></li>
            </ul>
            <p><strong>Dot Product</strong>: <span
            class="math inline">\(\mathbf{x} \cdot \mathbf{y} = \sum_i
            x_i y_i = \|\mathbf{x}\| \|\mathbf{y}\|
            \cos\theta\)</span></p>
            <h3 id="linear-independence-and-dependence">Linear
            Independence and Dependence</h3>
            <p><strong>Linear Independence</strong>: Vectors <span
            class="math inline">\(\{\mathbf{v}_1, \ldots,
            \mathbf{v}_k\}\)</span> are <strong>linearly
            independent</strong> if no vector can be written as a linear
            combination of the others. Formally:</p>
            <p><span class="math display">\[c_1\mathbf{v}_1 +
            c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k = \mathbf{0}
            \implies c_1 = c_2 = \cdots = c_k = 0\]</span></p>
            <p><strong>Linear Dependence</strong>: Vectors are
            <strong>linearly dependent</strong> if at least one can be
            expressed as a combination of others (some <span
            class="math inline">\(c_i \neq 0\)</span> satisfies the
            equation above).</p>
            <p><strong>Example</strong>:</p>
            <ul>
            <li><span class="math inline">\(\{[1,0], [0,1]\}\)</span> ‚Äî
            <strong>independent</strong> (can‚Äôt make one from the
            other)</li>
            <li><span class="math inline">\(\{[1,0], [2,0]\}\)</span> ‚Äî
            <strong>dependent</strong> (<span
            class="math inline">\([2,0] = 2 \cdot [1,0]\)</span>)</li>
            </ul>
            <p><strong>Why It Matters for ML</strong>:</p>
            <ul>
            <li><strong>Rank of weight matrix</strong>: If columns are
            dependent, model has redundant parameters</li>
            <li><strong>Feature engineering</strong>: Dependent features
            don‚Äôt add information</li>
            <li><strong>PCA</strong>: Finds independent directions of
            variance</li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is linear
            independence?‚Äù</p>
            <p><strong>A</strong>: Vectors are linearly independent if
            no vector can be written as a linear combination of the
            others. Equivalently, the only solution to <span
            class="math inline">\(c_1\mathbf{v}_1 + \cdots +
            c_k\mathbf{v}_k = 0\)</span> is all <span
            class="math inline">\(c_i = 0\)</span>. In ML, this matters
            for understanding model capacity ‚Äî a weight matrix with
            linearly dependent columns has redundant parameters and
            lower effective rank.</p>
            <p><strong>Inverse</strong>: <span
            class="math inline">\(\mathbf{A}^{-1}\)</span> such that
            <span class="math inline">\(\mathbf{A}\mathbf{A}^{-1} =
            \mathbf{I}\)</span></p>
            <p><strong>Determinant</strong>: <span
            class="math inline">\(\det(\mathbf{A})\)</span> - scalar
            value, matrix is invertible iff <span
            class="math inline">\(\det(\mathbf{A}) \neq 0\)</span></p>
            <h3 id="how-to-calculate-determinants">How to Calculate
            Determinants</h3>
            <p><strong>2√ó2 Matrix</strong> (memorize this!):</p>
            <p><span class="math display">\[\det\begin{bmatrix} a &amp;
            b \\ c &amp; d \end{bmatrix} = ad - bc\]</span></p>
            <p><strong>Example</strong>: <span
            class="math display">\[\det\begin{bmatrix} 3 &amp; 2 \\ 1
            &amp; 4 \end{bmatrix} = 3 \times 4 - 2 \times 1 = 12 - 2 =
            10\]</span></p>
            <p><strong>3√ó3 Matrix</strong> (expansion along first
            row):</p>
            <p><span class="math display">\[\det\begin{bmatrix} a &amp;
            b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i
            \end{bmatrix} = a \det\begin{bmatrix} e &amp; f \\ h &amp; i
            \end{bmatrix} - b \det\begin{bmatrix} d &amp; f \\ g &amp; i
            \end{bmatrix} + c \det\begin{bmatrix} d &amp; e \\ g &amp; h
            \end{bmatrix}\]</span></p>
            <p><span class="math display">\[= a(ei - fh) - b(di - fg) +
            c(dh - eg)\]</span></p>
            <p><strong>Worked Example</strong>:</p>
            <p><span class="math display">\[\det\begin{bmatrix} 1 &amp;
            2 &amp; 3 \\ 0 &amp; 4 &amp; 5 \\ 1 &amp; 0 &amp; 6
            \end{bmatrix}\]</span></p>
            <p>Expand along first row: <span class="math display">\[= 1
            \cdot \det\begin{bmatrix} 4 &amp; 5 \\ 0 &amp; 6
            \end{bmatrix} - 2 \cdot \det\begin{bmatrix} 0 &amp; 5 \\ 1
            &amp; 6 \end{bmatrix} + 3 \cdot \det\begin{bmatrix} 0 &amp;
            4 \\ 1 &amp; 0 \end{bmatrix}\]</span></p>
            <p><span class="math display">\[= 1 \cdot (4 \times 6 - 5
            \times 0) - 2 \cdot (0 \times 6 - 5 \times 1) + 3 \cdot (0
            \times 0 - 4 \times 1)\]</span></p>
            <p><span class="math display">\[= 1 \cdot 24 - 2 \cdot (-5)
            + 3 \cdot (-4)\]</span></p>
            <p><span class="math display">\[= 24 + 10 - 12 =
            22\]</span></p>
            <p><strong>Pro Tip</strong>: Expand along a row/column with
            the most zeros ‚Äî it reduces computation!</p>
            <p><strong>Geometric Interpretation</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Dimension</th>
            <th>Determinant Measures</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>2D</td>
            <td>Signed area of parallelogram formed by column
            vectors</td>
            </tr>
            <tr>
            <td>3D</td>
            <td>Signed volume of parallelepiped formed by column
            vectors</td>
            </tr>
            </tbody>
            </table>
            <ul>
            <li><span class="math inline">\(\det &gt; 0\)</span>:
            Orientation preserved</li>
            <li><span class="math inline">\(\det &lt; 0\)</span>:
            Orientation flipped (reflection)</li>
            <li><span class="math inline">\(\det = 0\)</span>: Collapsed
            to lower dimension (vectors are linearly dependent)</li>
            </ul>
            <p><strong>Why It Matters for ML</strong>:</p>
            <ul>
            <li><strong>Invertibility check</strong>: <span
            class="math inline">\(\det(\mathbf{A}) = 0\)</span> means
            matrix is singular (can‚Äôt invert)</li>
            <li><strong>Covariance matrix</strong>: <span
            class="math inline">\(\det(\mathbf{\Sigma})\)</span> appears
            in multivariate Gaussian PDF</li>
            <li><strong>Jacobian determinant</strong>: Measures volume
            change in transformations (normalizing flows!)</li>
            <li><strong>Eigenvalue product</strong>: <span
            class="math inline">\(\det(\mathbf{A}) = \prod_i
            \lambda_i\)</span> (product of all eigenvalues)</li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúHow do you calculate a
            determinant?‚Äù</p>
            <p><strong>A</strong>: For a 2√ó2 matrix <span
            class="math inline">\([[a, b], [c, d]]\)</span>, the
            determinant is <span class="math inline">\(ad - bc\)</span>.
            For larger matrices, use cofactor expansion: pick a
            row/column, multiply each element by its cofactor (the
            determinant of the submatrix with that row/column removed,
            with alternating signs), and sum. For efficiency, expand
            along a row/column with many zeros. Geometrically, the
            determinant measures the signed volume scaling factor of the
            linear transformation.</p>
            <hr />
            <h3 id="eigenvalues-and-eigenvectors">Eigenvalues and
            Eigenvectors</h3>
            <p>For a square matrix <span
            class="math inline">\(\mathbf{A}\)</span>, vector <span
            class="math inline">\(\mathbf{v}\)</span> is an
            <strong>eigenvector</strong> if:</p>
            <p><span class="math display">\[\mathbf{A}\mathbf{v} =
            \lambda \mathbf{v}\]</span></p>
            <p>where <span class="math inline">\(\lambda\)</span> is the
            corresponding <strong>eigenvalue</strong>.</p>
            <p><strong>Intuition (What It Means Verbally)</strong>:</p>
            <p>Think of a matrix as a <em>transformation</em> that
            stretches, rotates, or squishes space. Most vectors get both
            rotated AND stretched when you apply the matrix. But
            <strong>eigenvectors are special</strong> ‚Äî they only get
            stretched (or shrunk), not rotated. They point in a
            direction that the transformation ‚Äúrespects.‚Äù</p>
            <p>The eigenvalue <span
            class="math inline">\(\lambda\)</span> tells you <em>how
            much</em> it gets stretched:</p>
            <ul>
            <li><span class="math inline">\(\lambda &gt; 1\)</span>:
            Eigenvector gets longer</li>
            <li><span class="math inline">\(0 &lt; \lambda &lt;
            1\)</span>: Eigenvector gets shorter</li>
            <li><span class="math inline">\(\lambda &lt; 0\)</span>:
            Eigenvector gets flipped and scaled</li>
            <li><span class="math inline">\(\lambda = 1\)</span>:
            Eigenvector unchanged</li>
            </ul>
            <p><strong>Geometric Example</strong>: Imagine a rubber
            sheet being stretched horizontally but compressed
            vertically. Vectors pointing purely horizontal or purely
            vertical are eigenvectors ‚Äî they just scale, don‚Äôt rotate.
            Diagonal vectors would rotate toward the stretching
            direction.</p>
            <h3 id="how-to-calculate-eigenvalues-step-by-step">How to
            Calculate Eigenvalues (Step-by-Step)</h3>
            <p><strong>The Key Equation</strong>:</p>
            <p>Starting from <span
            class="math inline">\(\mathbf{A}\mathbf{v} = \lambda
            \mathbf{v}\)</span>, rearrange:</p>
            <p><span class="math display">\[\mathbf{A}\mathbf{v} -
            \lambda \mathbf{v} = 0\]</span> <span
            class="math display">\[(\mathbf{A} - \lambda
            \mathbf{I})\mathbf{v} = 0\]</span></p>
            <p>For a non-zero <span
            class="math inline">\(\mathbf{v}\)</span> to exist, the
            matrix <span class="math inline">\((\mathbf{A} - \lambda
            \mathbf{I})\)</span> must be <strong>singular</strong> (not
            invertible):</p>
            <p><span class="math display">\[\boxed{\det(\mathbf{A} -
            \lambda \mathbf{I}) = 0}\]</span></p>
            <p>This is called the <strong>characteristic
            equation</strong>. Solving it gives the eigenvalues.</p>
            <h3 id="worked-example-22-matrix">Worked Example: 2√ó2
            Matrix</h3>
            <p>Let‚Äôs find eigenvalues and eigenvectors of:</p>
            <p><span class="math display">\[\mathbf{A} = \begin{bmatrix}
            4 &amp; 2 \\ 1 &amp; 3 \end{bmatrix}\]</span></p>
            <p><strong>Step 1: Set up the characteristic
            equation</strong></p>
            <p><span class="math display">\[\mathbf{A} - \lambda
            \mathbf{I} = \begin{bmatrix} 4-\lambda &amp; 2 \\ 1 &amp;
            3-\lambda \end{bmatrix}\]</span></p>
            <p><span class="math display">\[\det(\mathbf{A} - \lambda
            \mathbf{I}) = (4-\lambda)(3-\lambda) - (2)(1) =
            0\]</span></p>
            <p><strong>Step 2: Expand and solve</strong></p>
            <p><span class="math display">\[12 - 4\lambda - 3\lambda +
            \lambda^2 - 2 = 0\]</span> <span
            class="math display">\[\lambda^2 - 7\lambda + 10 =
            0\]</span> <span class="math display">\[(\lambda -
            5)(\lambda - 2) = 0\]</span></p>
            <p><strong>Eigenvalues</strong>: <span
            class="math inline">\(\lambda_1 = 5\)</span>, <span
            class="math inline">\(\lambda_2 = 2\)</span></p>
            <p><strong>Step 3: Find eigenvectors for each
            eigenvalue</strong></p>
            <p><em>For <span class="math inline">\(\lambda_1 =
            5\)</span>:</em></p>
            <p><span class="math display">\[(\mathbf{A} -
            5\mathbf{I})\mathbf{v} = \begin{bmatrix} -1 &amp; 2 \\ 1
            &amp; -2 \end{bmatrix}\begin{bmatrix} v_1 \\ v_2
            \end{bmatrix} = \begin{bmatrix} 0 \\ 0
            \end{bmatrix}\]</span></p>
            <p>From first row: <span class="math inline">\(-v_1 + 2v_2 =
            0\)</span> ‚Üí <span class="math inline">\(v_1 =
            2v_2\)</span></p>
            <p><strong>Eigenvector</strong>: <span
            class="math inline">\(\mathbf{v}_1 = \begin{bmatrix} 2 \\ 1
            \end{bmatrix}\)</span> (or any scalar multiple)</p>
            <p><em>For <span class="math inline">\(\lambda_2 =
            2\)</span>:</em></p>
            <p><span class="math display">\[(\mathbf{A} -
            2\mathbf{I})\mathbf{v} = \begin{bmatrix} 2 &amp; 2 \\ 1
            &amp; 1 \end{bmatrix}\begin{bmatrix} v_1 \\ v_2
            \end{bmatrix} = \begin{bmatrix} 0 \\ 0
            \end{bmatrix}\]</span></p>
            <p>From first row: <span class="math inline">\(2v_1 + 2v_2 =
            0\)</span> ‚Üí <span class="math inline">\(v_1 =
            -v_2\)</span></p>
            <p><strong>Eigenvector</strong>: <span
            class="math inline">\(\mathbf{v}_2 = \begin{bmatrix} 1 \\ -1
            \end{bmatrix}\)</span></p>
            <p><strong>Verification</strong>: Let‚Äôs check <span
            class="math inline">\(\mathbf{A}\mathbf{v}_1 = \lambda_1
            \mathbf{v}_1\)</span>:</p>
            <p><span class="math display">\[\begin{bmatrix} 4 &amp; 2 \\
            1 &amp; 3 \end{bmatrix}\begin{bmatrix} 2 \\ 1 \end{bmatrix}
            = \begin{bmatrix} 10 \\ 5 \end{bmatrix} = 5 \begin{bmatrix}
            2 \\ 1 \end{bmatrix} \checkmark\]</span></p>
            <hr />
            <h3 id="what-the-eigenvalues-tell-us">What the Eigenvalues
            Tell Us</h3>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 64%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>What to Look For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Positive definite</strong></td>
            <td>All <span class="math inline">\(\lambda_i &gt;
            0\)</span></td>
            </tr>
            <tr>
            <td><strong>Positive semi-definite</strong></td>
            <td>All <span class="math inline">\(\lambda_i \geq
            0\)</span></td>
            </tr>
            <tr>
            <td><strong>Indefinite</strong></td>
            <td>Mixed signs ‚Üí <strong>saddle point</strong> in
            optimization</td>
            </tr>
            <tr>
            <td><strong>Condition number</strong></td>
            <td><span class="math inline">\(\kappa =
            \lambda_{\max}/\lambda_{\min}\)</span> (large =
            ill-conditioned)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúHow do you find
            eigenvalues of a matrix?‚Äù</p>
            <p><strong>A</strong>: You solve the characteristic equation
            <span class="math inline">\(\det(\mathbf{A} - \lambda
            \mathbf{I}) = 0\)</span>. For a 2√ó2 matrix, this gives a
            quadratic equation. For the matrix <span
            class="math inline">\([[4, 2], [1, 3]]\)</span>, expanding
            the determinant gives <span class="math inline">\(\lambda^2
            - 7\lambda + 10 = 0\)</span>, which factors to <span
            class="math inline">\((\lambda - 5)(\lambda - 2) =
            0\)</span>, giving eigenvalues 5 and 2. Then for each
            eigenvalue, you solve <span
            class="math inline">\((\mathbf{A} - \lambda
            \mathbf{I})\mathbf{v} = 0\)</span> to find the corresponding
            eigenvector.</p>
            <p><img src="figures/eigenvector_transformation.png"
            alt="Eigenvector Transformation" /> <em>Figure: Eigenvectors
            only get scaled (not rotated) when multiplied by the matrix.
            The unit circle (left) becomes an ellipse (right), but
            eigenvectors stay pointing in their original
            directions.</em></p>
            <p><img src="figures/loss_landscape.png"
            alt="Loss Landscape Visualization" /> <em>Figure: 3D loss
            landscape showing the optimization surface and gradient
            descent path.</em></p>
            <p><strong>ML Applications</strong>:</p>
            <ul>
            <li><strong>PCA</strong>: Principal components are
            eigenvectors of covariance matrix</li>
            <li><strong>Spectral clustering</strong>: Uses eigenvectors
            of graph Laplacian</li>
            <li><strong>Matrix condition number</strong>: Ratio of
            largest to smallest eigenvalue (affects optimization)</li>
            </ul>
            <hr />
            <h3 id="singular-value-decomposition-svd">Singular Value
            Decomposition (SVD)</h3>
            <p>Any matrix <span class="math inline">\(\mathbf{A} \in
            \mathbb{R}^{m \times n}\)</span> can be decomposed as:</p>
            <p><span class="math display">\[\mathbf{A} = \mathbf{U}
            \mathbf{\Sigma} \mathbf{V}^T\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(\mathbf{U}\)</span> (<span
            class="math inline">\(m \times m\)</span>): Left singular
            vectors (orthonormal)</li>
            <li><span class="math inline">\(\mathbf{\Sigma}\)</span>
            (<span class="math inline">\(m \times n\)</span>): Diagonal
            matrix of singular values <span
            class="math inline">\(\sigma_1 \geq \sigma_2 \geq \cdots
            \geq 0\)</span></li>
            <li><span class="math inline">\(\mathbf{V}\)</span> (<span
            class="math inline">\(n \times n\)</span>): Right singular
            vectors (orthonormal)</li>
            </ul>
            <p><strong>ML Applications</strong>:</p>
            <ul>
            <li><strong>Dimensionality reduction</strong>: Keep top
            <span class="math inline">\(k\)</span> singular values</li>
            <li><strong>Low-rank approximation</strong>: Compress weight
            matrices</li>
            <li><strong>LoRA</strong>: Low-rank adaptation of LLMs</li>
            </ul>
            <hr />
            <h3 id="principal-component-analysis-pca">Principal
            Component Analysis (PCA)</h3>
            <p><strong>Goal</strong>: Find directions of maximum
            variance in data.</p>
            <p><strong>Algorithm</strong>:</p>
            <ol type="1">
            <li>Center data: <span class="math inline">\(\mathbf{X}
            \leftarrow \mathbf{X} - \bar{\mathbf{X}}\)</span></li>
            <li>Compute covariance matrix: <span
            class="math inline">\(\mathbf{C} =
            \frac{1}{n}\mathbf{X}^T\mathbf{X}\)</span></li>
            <li>Find eigenvectors of <span
            class="math inline">\(\mathbf{C}\)</span></li>
            <li>Project onto top <span class="math inline">\(k\)</span>
            eigenvectors</li>
            </ol>
            <h3 id="pca-worked-example-2d-1d">PCA Worked Example: 2D ‚Üí
            1D</h3>
            <p><strong>Data</strong>: 5 points in 2D</p>
            <table>
            <thead>
            <tr>
            <th>Point</th>
            <th><span class="math inline">\(x_1\)</span></th>
            <th><span class="math inline">\(x_2\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>2.5</td>
            <td>2.4</td>
            </tr>
            <tr>
            <td>2</td>
            <td>0.5</td>
            <td>0.7</td>
            </tr>
            <tr>
            <td>3</td>
            <td>2.2</td>
            <td>2.9</td>
            </tr>
            <tr>
            <td>4</td>
            <td>1.9</td>
            <td>2.2</td>
            </tr>
            <tr>
            <td>5</td>
            <td>3.1</td>
            <td>3.0</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Step 1: Center the data</strong></p>
            <p>Mean: <span class="math inline">\(\bar{x}_1 =
            2.04\)</span>, <span class="math inline">\(\bar{x}_2 =
            2.24\)</span></p>
            <p>Centered data:</p>
            <table>
            <thead>
            <tr>
            <th>Point</th>
            <th><span class="math inline">\(x_1 -
            \bar{x}_1\)</span></th>
            <th><span class="math inline">\(x_2 -
            \bar{x}_2\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>0.46</td>
            <td>0.16</td>
            </tr>
            <tr>
            <td>2</td>
            <td>-1.54</td>
            <td>-1.54</td>
            </tr>
            <tr>
            <td>3</td>
            <td>0.16</td>
            <td>0.66</td>
            </tr>
            <tr>
            <td>4</td>
            <td>-0.14</td>
            <td>-0.04</td>
            </tr>
            <tr>
            <td>5</td>
            <td>1.06</td>
            <td>0.76</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Step 2: Compute covariance matrix</strong></p>
            <p><span class="math display">\[\mathbf{C} =
            \frac{1}{n-1}\mathbf{X}^T\mathbf{X} = \begin{bmatrix} 0.616
            &amp; 0.615 \\ 0.615 &amp; 0.716 \end{bmatrix}\]</span></p>
            <p>(Note: Using <span class="math inline">\(n-1\)</span> for
            sample covariance)</p>
            <p><strong>Step 3: Find eigenvalues and
            eigenvectors</strong></p>
            <p>Characteristic equation: <span
            class="math inline">\(\det(\mathbf{C} - \lambda \mathbf{I})
            = 0\)</span></p>
            <p><span class="math display">\[\lambda^2 - 1.332\lambda +
            0.062 = 0\]</span></p>
            <p>Eigenvalues: <span class="math inline">\(\lambda_1 =
            1.284\)</span>, <span class="math inline">\(\lambda_2 =
            0.049\)</span></p>
            <p>Eigenvectors (normalized):</p>
            <p><span class="math display">\[\mathbf{v}_1 =
            \begin{bmatrix} 0.677 \\ 0.735 \end{bmatrix}, \quad
            \mathbf{v}_2 = \begin{bmatrix} -0.735 \\ 0.677
            \end{bmatrix}\]</span></p>
            <p><strong>Step 4: Variance explained</strong></p>
            <ul>
            <li>PC1 explains: <span
            class="math inline">\(\frac{1.284}{1.284 + 0.049} =
            96.3\%\)</span> of variance</li>
            <li>PC2 explains: <span class="math inline">\(3.7\%\)</span>
            of variance</li>
            </ul>
            <p><strong>Keeping just PC1 captures 96.3% of the
            information!</strong></p>
            <p><strong>Step 5: Project onto PC1 (dimensionality
            reduction)</strong></p>
            <p><span class="math display">\[z_i = \mathbf{v}_1^T
            \mathbf{x}_i\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Original 2D</th>
            <th>Projected 1D</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>(0.46, 0.16)</td>
            <td>0.43</td>
            </tr>
            <tr>
            <td>(-1.54, -1.54)</td>
            <td>-2.17</td>
            </tr>
            <tr>
            <td>(0.16, 0.66)</td>
            <td>0.60</td>
            </tr>
            <tr>
            <td>(-0.14, -0.04)</td>
            <td>-0.12</td>
            </tr>
            <tr>
            <td>(1.06, 0.76)</td>
            <td>1.27</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Visualization intuition</strong>: The first
            principal component points diagonally (roughly 45¬∞) because
            <span class="math inline">\(x_1\)</span> and <span
            class="math inline">\(x_2\)</span> are positively
            correlated. Projecting onto this direction captures most of
            the spread in the data.</p>
            <p><img src="figures/pca_projection.png"
            alt="PCA Projection" /> <em>Figure: PCA finds the direction
            of maximum variance (PC1, red arrow) and projects data onto
            it. The 2D points become 1D values while preserving 97% of
            the variance.</em></p>
            <p><strong>Interview Q</strong>: ‚ÄúWhy do we use eigenvectors
            in PCA?‚Äù</p>
            <p><strong>Answer</strong>: Eigenvectors of the covariance
            matrix represent directions of maximum variance. The first
            eigenvector (largest eigenvalue) is the direction along
            which the data varies most ‚Äî projecting onto it loses the
            least information. In the example above, PC1 captures 96.3%
            of variance because the data is elongated along that
            diagonal direction. The eigenvalue itself tells you how much
            variance is in that direction.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWalk through PCA step by
            step.‚Äù</p>
            <p><strong>A</strong>: (1) Center the data by subtracting
            the mean of each feature. (2) Compute the covariance matrix
            <span class="math inline">\(\mathbf{C} =
            \frac{1}{n-1}\mathbf{X}^T\mathbf{X}\)</span>. (3) Find
            eigenvalues and eigenvectors of <span
            class="math inline">\(\mathbf{C}\)</span>. (4) Sort
            eigenvectors by eigenvalue (largest first) ‚Äî these are the
            principal components. (5) To reduce to <span
            class="math inline">\(k\)</span> dimensions, project data
            onto the top <span class="math inline">\(k\)</span>
            eigenvectors. The eigenvalues tell you how much variance
            each PC captures.</p>
            <hr />
            <h2 id="probability-and-statistics">3.2 Probability and
            Statistics</h2>
            <h3 id="probability-distributions">Probability
            Distributions</h3>
            <h4 id="discrete-distributions">Discrete Distributions</h4>
            <p><strong>Bernoulli</strong>: Single binary outcome <span
            class="math display">\[P(X = 1) = p, \quad P(X = 0) = 1 -
            p\]</span></p>
            <p><span class="math display">\[\mathbb{E}[X] = p, \quad
            \text{Var}(X) = p(1-p)\]</span></p>
            <p><strong>Categorical</strong> (Multinoulli): Single
            outcome from <span class="math inline">\(k\)</span> classes
            <span class="math display">\[P(X = i) = p_i, \quad
            \sum_{i=1}^{k} p_i = 1\]</span></p>
            <p><strong>Binomial</strong>: <span
            class="math inline">\(n\)</span> independent Bernoulli
            trials <span class="math display">\[P(X = k) = \binom{n}{k}
            p^k (1-p)^{n-k}\]</span></p>
            <p><strong>Poisson</strong>: Count of rare events <span
            class="math display">\[P(X = k) = \frac{\lambda^k
            e^{-\lambda}}{k!}\]</span></p>
            <h4 id="continuous-distributions">Continuous
            Distributions</h4>
            <p><strong>Gaussian (Normal)</strong>: <span
            class="math display">\[p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}
            \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
            <p><span class="math display">\[\mathbb{E}[X] = \mu, \quad
            \text{Var}(X) = \sigma^2\]</span></p>
            <p><strong>Multivariate Gaussian</strong>: <span
            class="math display">\[p(\mathbf{x}) =
            \frac{1}{(2\pi)^{d/2}|\mathbf{\Sigma}|^{1/2}}
            \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)\]</span></p>
            <p><strong>Uniform</strong>: <span
            class="math inline">\(p(x) = \frac{1}{b-a}\)</span> for
            <span class="math inline">\(x \in [a, b]\)</span></p>
            <hr />
            <h3 id="bayes-theorem">Bayes‚Äô Theorem</h3>
            <p><span class="math display">\[P(A|B) = \frac{P(B|A)
            P(A)}{P(B)}\]</span></p>
            <p><strong>Terminology</strong>:</p>
            <ul>
            <li><span class="math inline">\(P(A)\)</span>: Prior
            probability</li>
            <li><span class="math inline">\(P(B|A)\)</span>:
            Likelihood</li>
            <li><span class="math inline">\(P(A|B)\)</span>: Posterior
            probability</li>
            <li><span class="math inline">\(P(B)\)</span>: Evidence
            (normalizing constant)</li>
            </ul>
            <hr />
            <h3
            id="what-bayes-theorem-actually-means-verbal-intuition">What
            Bayes‚Äô Theorem Actually Means (Verbal Intuition)</h3>
            <p><strong>The Core Question</strong>: You observed evidence
            <span class="math inline">\(B\)</span>. How should you
            update your belief about hypothesis <span
            class="math inline">\(A\)</span>?</p>
            <p><strong>The Problem</strong>: In the real world, we often
            know <span
            class="math inline">\(P(\text{evidence}|\text{hypothesis})\)</span>
            but want <span
            class="math inline">\(P(\text{hypothesis}|\text{evidence})\)</span>.
            Bayes‚Äô theorem lets us <strong>flip</strong> conditional
            probabilities!</p>
            <p><strong>A Story for Each Term</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 12%" />
            <col style="width: 16%" />
            <col style="width: 18%" />
            <col style="width: 53%" />
            </colgroup>
            <thead>
            <tr>
            <th>Term</th>
            <th>Symbol</th>
            <th>Meaning</th>
            <th>Example (disease testing)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Prior</strong></td>
            <td><span class="math inline">\(P(A)\)</span></td>
            <td>Your belief BEFORE seeing evidence</td>
            <td>1% of people have the disease</td>
            </tr>
            <tr>
            <td><strong>Likelihood</strong></td>
            <td><span class="math inline">\(P(B \mid A)\)</span></td>
            <td>How likely is this evidence IF the hypothesis is
            true?</td>
            <td>99% of sick people test positive</td>
            </tr>
            <tr>
            <td><strong>Posterior</strong></td>
            <td><span class="math inline">\(P(A \mid B)\)</span></td>
            <td>Your updated belief AFTER seeing evidence</td>
            <td>What we want: P(sick given positive test)</td>
            </tr>
            <tr>
            <td><strong>Evidence</strong></td>
            <td><span class="math inline">\(P(B)\)</span></td>
            <td>How common is this evidence overall?</td>
            <td>Total rate of positive tests (sick + false
            positives)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The Key Insight</strong>:</p>
            <blockquote>
            <p><em>‚ÄúDon‚Äôt just ask how well the evidence fits the
            hypothesis ‚Äî also consider how likely the hypothesis was to
            begin with!‚Äù</em></p>
            </blockquote>
            <p>This is why rare diseases remain unlikely even with
            positive tests. If only 1% of people are sick, you need VERY
            strong evidence to conclude someone is probably sick.</p>
            <p><strong>The Bayesian Update Process</strong>:</p>
            <pre><code>Prior belief  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  See evidence  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  Posterior belief
   P(A)                    B                    P(A|B)
   
   &quot;Before&quot;                                     &quot;After&quot;</code></pre>
            <p>The formula tells you HOW to update: multiply prior by
            likelihood, normalize by evidence.</p>
            <p><strong>Example</strong>: Medical diagnosis</p>
            <p>Given:</p>
            <ul>
            <li><span class="math inline">\(P(\text{disease}) =
            0.01\)</span> (1% have disease)</li>
            <li><span
            class="math inline">\(P(\text{positive}|\text{disease}) =
            0.99\)</span> (test is 99% sensitive)</li>
            <li><span class="math inline">\(P(\text{positive}|\text{no
            disease}) = 0.05\)</span> (5% false positive)</li>
            </ul>
            <p>What is <span
            class="math inline">\(P(\text{disease}|\text{positive})\)</span>?</p>
            <p><span
            class="math display">\[P(\text{disease}|\text{positive}) =
            \frac{0.99 \times 0.01}{0.99 \times 0.01 + 0.05 \times 0.99}
            = \frac{0.0099}{0.0099 + 0.0495} \approx 0.167\]</span></p>
            <p>Only 16.7% chance of disease even with positive test!</p>
            <p><strong>Interview Q</strong>: ‚ÄúDerive Bayes‚Äô theorem from
            the definition of conditional probability.‚Äù</p>
            <p><strong>Answer</strong>:</p>
            <ul>
            <li><span class="math inline">\(P(A|B) = \frac{P(A \cap
            B)}{P(B)}\)</span></li>
            <li><span class="math inline">\(P(B|A) = \frac{P(A \cap
            B)}{P(A)}\)</span></li>
            <li>Therefore: <span class="math inline">\(P(A \cap B) =
            P(B|A)P(A)\)</span></li>
            <li>Substituting: <span class="math inline">\(P(A|B) =
            \frac{P(B|A)P(A)}{P(B)}\)</span></li>
            </ul>
            <hr />
            <h3 id="maximum-likelihood-estimation-mle">Maximum
            Likelihood Estimation (MLE)</h3>
            <blockquote>
            <p><strong>üîë The Big Picture: MLE = Parameter Estimation as
            OPTIMIZATION</strong></p>
            <p>MLE is perhaps the most important bridge between
            <strong>probability theory</strong> and
            <strong>optimization</strong>. Instead of ‚Äúguessing‚Äù
            parameters, we:</p>
            <ol type="1">
            <li>Define a <strong>likelihood function</strong> <span
            class="math inline">\(P(\text{data}|\theta)\)</span> ‚Äî
            probability of seeing our data given parameter <span
            class="math inline">\(\theta\)</span></li>
            <li>Use <strong>calculus</strong> or <strong>gradient
            descent</strong> to find <span
            class="math inline">\(\theta\)</span> that maximizes this
            probability</li>
            <li>This is why ML uses gradient-based optimization ‚Äî we‚Äôre
            doing MLE!</li>
            </ol>
            <p><strong>Every time you train a neural network with
            cross-entropy loss, you‚Äôre doing MLE!</strong></p>
            </blockquote>
            <p><strong>Goal</strong>: Find parameters <span
            class="math inline">\(\theta\)</span> that maximize the
            probability of observed data.</p>
            <p><strong>The Key Assumption</strong>: We assume our
            observed data was sampled from some distribution
            parameterized by an unknown ‚Äútrue‚Äù <span
            class="math inline">\(\theta^*\)</span>. MLE tries to find
            the <span class="math inline">\(\hat{\theta}\)</span> that
            best explains the data we actually saw. We don‚Äôt know <span
            class="math inline">\(\theta^*\)</span>, but if our model is
            correct and we have enough data, <span
            class="math inline">\(\hat{\theta}_{MLE} \to
            \theta^*\)</span>.</p>
            <blockquote>
            <p><strong>üìù Terminology: Estimator vs
            Estimate</strong></p>
            <p>An <strong>estimator</strong> <span
            class="math inline">\(\hat{\theta}\)</span> is a
            <strong>function of the data</strong> ‚Äî it‚Äôs a random
            variable because it depends on which data you happen to
            observe. Before you collect data, the estimator is
            uncertain. After you plug in your actual data, you get a
            specific <strong>estimate</strong>.</p>
            <p><span class="math display">\[\hat{\theta} = g(X_1, X_2,
            \ldots, X_n)\]</span></p>
            <p>For MLE of a Bernoulli parameter:</p>
            <ul>
            <li><strong>Estimator</strong>: <span
            class="math inline">\(\hat{p} = \frac{1}{n}\sum_{i=1}^n
            X_i\)</span> (the formula)</li>
            <li><strong>Estimate</strong>: <span
            class="math inline">\(\hat{p} = \frac{7}{10} = 0.7\)</span>
            (the number after seeing data)</li>
            </ul>
            </blockquote>
            <p><strong>The Core Intuition (Three Ways to Understand
            MLE)</strong>:</p>
            <p><strong>1. Reverse Probability</strong>: Normally
            probability asks: ‚ÄúGiven parameters <span
            class="math inline">\(\theta\)</span>, how likely is data
            <span class="math inline">\(D\)</span>?‚Äù ‚Äî that‚Äôs <span
            class="math inline">\(P(D|\theta)\)</span>. MLE
            <strong>reverses the question</strong>: ‚ÄúGiven data <span
            class="math inline">\(D\)</span>, which <span
            class="math inline">\(\theta\)</span> makes <span
            class="math inline">\(D\)</span> most probable?‚Äù We scan
            through all possible <span
            class="math inline">\(\theta\)</span> values, compute <span
            class="math inline">\(P(D|\theta)\)</span> for each, and
            pick the <span class="math inline">\(\theta\)</span> that
            gives the highest probability.</p>
            <p><strong>2. Which World?</strong>: Imagine infinitely many
            parallel worlds, each with a slightly different <span
            class="math inline">\(\theta\)</span>. In one world the coin
            has <span class="math inline">\(p=0.5\)</span>, in another
            <span class="math inline">\(p=0.7\)</span>, etc. You
            observed 7 heads in 10 flips. MLE asks: <strong>‚ÄúIn which
            world was my observation most likely to happen?‚Äù</strong>
            Answer: the world where <span
            class="math inline">\(p=0.7\)</span>.</p>
            <p><strong>3. Concrete Example</strong>: You flip an unknown
            coin 10 times and see 7 heads. Let <span
            class="math inline">\(D\)</span> = ‚Äú7 heads in 10
            flips‚Äù.</p>
            <ul>
            <li>If <span class="math inline">\(p=0.5\)</span>: <span
            class="math inline">\(L(p=0.5|D) = \binom{10}{7} \cdot
            0.5^{10} \approx 0.12\)</span></li>
            <li>If <span class="math inline">\(p=0.7\)</span>: <span
            class="math inline">\(L(p=0.7|D) = \binom{10}{7} \cdot 0.7^7
            \cdot 0.3^3 \approx 0.27\)</span> ‚Üê
            <strong>highest!</strong></li>
            <li>If <span class="math inline">\(p=0.9\)</span>: <span
            class="math inline">\(L(p=0.9|D) = \binom{10}{7} \cdot 0.9^7
            \cdot 0.1^3 \approx 0.06\)</span></li>
            </ul>
            <p><span class="math inline">\(p=0.7\)</span> makes your
            data most likely ‚Üí that‚Äôs the MLE!</p>
            <p><strong>Notation</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 34%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\mathcal{D}\)</span></td>
            <td><strong>Dataset</strong> ‚Äî the collection of all
            observed data points</td>
            <td><span class="math inline">\(\mathcal{D} = \{x_1, x_2,
            \ldots, x_N\}\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(x_i\)</span></td>
            <td><strong>Single observation</strong> ‚Äî the <span
            class="math inline">\(i\)</span>-th data point in our
            dataset</td>
            <td>One image, one coin flip result, one measurement</td>
            </tr>
            <tr>
            <td><span class="math inline">\(N\)</span></td>
            <td><strong>Number of samples</strong> ‚Äî total data points
            in dataset</td>
            <td>1000 training examples</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\theta\)</span></td>
            <td><strong>Parameters</strong> ‚Äî the values we want to
            estimate</td>
            <td>Mean <span class="math inline">\(\mu\)</span>, weights
            <span class="math inline">\(\mathbf{w}\)</span>, probability
            <span class="math inline">\(p\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(P(x_i|\theta)\)</span></td>
            <td><strong>Likelihood of sample <span
            class="math inline">\(i\)</span></strong> ‚Äî probability of
            observing <span class="math inline">\(x_i\)</span> if the
            true parameter is <span
            class="math inline">\(\theta\)</span></td>
            <td><span class="math inline">\(P(\text{heads}|p=0.7) =
            0.7\)</span></td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(P(\mathcal{D}|\theta)\)</span></td>
            <td><strong>Likelihood of entire dataset</strong> ‚Äî
            probability of seeing all our data given <span
            class="math inline">\(\theta\)</span></td>
            <td>Product of individual likelihoods (assuming i.i.d.)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The Math</strong>:</p>
            <p><span class="math display">\[\hat{\theta}_{MLE} =
            \arg\max_{\theta} P(\mathcal{D}|\theta) = \arg\max_{\theta}
            \prod_{i=1}^{N} P(x_i|\theta)\]</span></p>
            <p><strong>Likelihood ‚Üí Log-Likelihood
            Transformation</strong>:</p>
            <p><strong>Likelihood</strong> (product of probabilities):
            <span class="math display">\[L(\theta) = P(x_1|\theta) \cdot
            P(x_2|\theta) \cdot \ldots \cdot P(x_N|\theta) =
            \prod_{i=1}^{N} P(x_i|\theta)\]</span></p>
            <p><strong>Log-Likelihood</strong> (sum of
            log-probabilities): <span
            class="math display">\[\ell(\theta) = \log P(x_1|\theta) +
            \log P(x_2|\theta) + \ldots + \log P(x_N|\theta) =
            \sum_{i=1}^{N} \log P(x_i|\theta)\]</span></p>
            <p>Taking log doesn‚Äôt change the argmax, so: <span
            class="math display">\[\hat{\theta}_{MLE} =
            \arg\max_{\theta} L(\theta) = \arg\max_{\theta}
            \ell(\theta)\]</span></p>
            <hr />
            <h3
            id="from-maximization-to-minimization-why-we-minimize-negative-log-likelihood">From
            Maximization to Minimization: Why We MINIMIZE Negative
            Log-Likelihood</h3>
            <p><strong>The Key Insight</strong>: MLE
            <strong>maximizes</strong> likelihood, but in practice we
            <strong>minimize</strong> the <strong>negative</strong>
            log-likelihood (NLL).</p>
            <table>
            <colgroup>
            <col style="width: 16%" />
            <col style="width: 30%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>Step</th>
            <th>Operation</th>
            <th>Optimization Goal</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1. MLE objective</td>
            <td>Likelihood <span class="math inline">\(P(\mathcal{D}
            \mid \theta)\)</span></td>
            <td><strong>MAXIMIZE</strong></td>
            </tr>
            <tr>
            <td>2. Take log</td>
            <td>Log-likelihood <span class="math inline">\(\log
            P(\mathcal{D} \mid \theta)\)</span></td>
            <td><strong>MAXIMIZE</strong> (log is monotonic)</td>
            </tr>
            <tr>
            <td>3. Negate</td>
            <td>Negative log-likelihood <span
            class="math inline">\(-\log P(\mathcal{D} \mid
            \theta)\)</span></td>
            <td><strong>MINIMIZE</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why negate?</strong> ML frameworks only minimize
            losses ‚Äî gradient descent goes <em>downhill</em>. There‚Äôs no
            <code>optimizer.maximize()</code> in PyTorch! So we flip the
            objective:</p>
            <p><span class="math display">\[\underbrace{\arg\max_\theta
            \log P(\mathcal{D}|\theta)}_{\text{Maximize log-likelihood}}
            = \underbrace{\arg\min_\theta \left[-\log
            P(\mathcal{D}|\theta)\right]}_{\text{Minimize NLL (the
            loss)}}\]</span></p>
            <p><strong>Cross-entropy loss IS negative
            log-likelihood</strong>, so when you
            <code>loss.backward()</code> and
            <code>optimizer.step()</code>, you‚Äôre doing MLE!</p>
            <p><strong>Second Derivative Test Connection</strong>:</p>
            <p>At a critical point (where <span
            class="math inline">\(f&#39;(x) = 0\)</span>), the second
            derivative determines if it‚Äôs a min or max:</p>
            <table>
            <thead>
            <tr>
            <th>Second Derivative</th>
            <th>Shape</th>
            <th>Critical Point</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(f&#39;&#39;(x) &gt;
            0\)</span></td>
            <td>Concave <strong>up</strong> (bowl)</td>
            <td><strong>Minimum</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">\(f&#39;&#39;(x) &lt;
            0\)</span></td>
            <td>Concave <strong>down</strong> (hill)</td>
            <td><strong>Maximum</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why negation flips min ‚ÜîÔ∏é max</strong>: If <span
            class="math inline">\(f&#39;&#39;(x) &lt; 0\)</span> at a
            maximum, then <span class="math inline">\((-f)&#39;&#39;(x)
            = -f&#39;&#39;(x) &gt; 0\)</span> at the same point ‚Äî making
            it a minimum!</p>
            <p>For MLE: the log-likelihood has a maximum (concave down).
            Negating gives a convex function (concave up) with a minimum
            at the same point. This is why minimizing NLL finds the same
            parameters as maximizing log-likelihood.</p>
            <hr />
            <h3 id="why-log-likelihood-5-important-reasons">Why
            Log-Likelihood? (5 Important Reasons)</h3>
            <p>Taking the log of the likelihood is one of the most
            important tricks in ML. Here‚Äôs <strong>why</strong>:</p>
            <p><strong>1. Products ‚Üí Sums (Easier
            Derivatives)</strong></p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>Likelihood</th>
            <th>Log-Likelihood</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(L(\theta) = \prod_i P(x_i
            \mid \theta)\)</span></td>
            <td><span class="math inline">\(\ell(\theta) = \sum_i \log
            P(x_i \mid \theta)\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\frac{d}{d\theta} \prod_i
            f_i = \sum_i \frac{df_i}{d\theta} \prod_{j \neq i}
            f_j\)</span> (messy!)</td>
            <td><span class="math inline">\(\frac{d}{d\theta} \sum_i
            \log f_i = \sum_i \frac{1}{f_i}\frac{df_i}{d\theta}\)</span>
            (clean!)</td>
            </tr>
            </tbody>
            </table>
            <p>The derivative of a sum is the sum of derivatives ‚Äî
            simple. The derivative of a product uses the product rule
            recursively ‚Äî a nightmare with 1000 samples!</p>
            <p><strong>2. Numerical Stability (Prevents
            Underflow)</strong></p>
            <p>Probabilities are often tiny. Multiplying them causes
            <strong>underflow</strong> (rounds to 0):</p>
            <div class="sourceCode" id="cb123"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Imagine 100 samples, each with P(x) = 0.1</span></span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> np.array([<span class="fl">0.1</span>] <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Direct product UNDERFLOWS to 0!</span></span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> np.prod(probs)</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Likelihood: </span><span class="sc">{</span>likelihood<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># 0.0 (underflow!)</span></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-likelihood stays finite</span></span>
<span id="cb123-11"><a href="#cb123-11" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(probs))</span>
<span id="cb123-12"><a href="#cb123-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Log-likelihood: </span><span class="sc">{</span>log_likelihood<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># -230.26 (correct!)</span></span></code></pre></div>
            <p>Even with 100 samples at <span class="math inline">\(P =
            0.1\)</span>, the likelihood <span
            class="math inline">\(0.1^{100} = 10^{-100}\)</span>
            underflows. Log-likelihood is <span
            class="math inline">\(100 \times \log(0.1) \approx
            -230\)</span> ‚Äî perfectly representable!</p>
            <p><strong>3. Monotonicity Preserves the Argmax</strong></p>
            <p>The <span class="math inline">\(\log\)</span> function is
            <strong>strictly monotonically increasing</strong>: if <span
            class="math inline">\(a &gt; b\)</span>, then <span
            class="math inline">\(\log(a) &gt; \log(b)\)</span>.</p>
            <p>This means: <span
            class="math display">\[\arg\max_{\theta} L(\theta) =
            \arg\max_{\theta} \log L(\theta)\]</span></p>
            <p>The parameters that maximize likelihood also maximize
            log-likelihood ‚Äî we lose nothing!</p>
            <p><strong>4. Connection to Information Theory</strong></p>
            <p>Log-likelihood connects beautifully to information
            theory:</p>
            <ul>
            <li><span class="math inline">\(-\log P(x)\)</span> =
            <strong>surprise</strong> or <strong>information
            content</strong> of event <span
            class="math inline">\(x\)</span></li>
            <li>Lower probability ‚Üí higher surprise ‚Üí larger <span
            class="math inline">\(-\log P\)</span></li>
            <li>Average negative log-likelihood =
            <strong>cross-entropy</strong> <span
            class="math inline">\(H(P, Q)\)</span></li>
            </ul>
            <p>This is why minimizing cross-entropy loss (NLL) is
            equivalent to MLE ‚Äî they‚Äôre the same thing viewed through
            different lenses!</p>
            <p><strong>5. Convexity for Exponential Family
            Distributions</strong></p>
            <p>For distributions in the <strong>exponential
            family</strong> (Gaussian, Bernoulli, Poisson, Exponential,
            etc.), the <strong>negative log-likelihood is
            convex</strong>!</p>
            <table>
            <thead>
            <tr>
            <th>Distribution</th>
            <th>NLL Shape</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Bernoulli ‚Üí Logistic Regression</td>
            <td>Convex ‚úì</td>
            </tr>
            <tr>
            <td>Gaussian ‚Üí Linear Regression (MSE)</td>
            <td>Convex ‚úì</td>
            </tr>
            <tr>
            <td>Poisson Regression</td>
            <td>Convex ‚úì</td>
            </tr>
            <tr>
            <td>Multinomial ‚Üí Softmax</td>
            <td>Convex ‚úì</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why does convexity matter?</strong></p>
            <ul>
            <li>Convex functions have a <strong>unique global
            minimum</strong> ‚Äî no local minima!</li>
            <li>Gradient descent is guaranteed to find the optimal
            solution</li>
            <li>This is why logistic regression always converges (unlike
            deep networks)</li>
            </ul>
            <blockquote>
            <p><strong>üîë Summary: Why We Always Use
            Log-Likelihood</strong></p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>Reason</th>
            <th>Benefit</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Products ‚Üí Sums</td>
            <td>Easier derivatives (sum rule vs product rule)</td>
            </tr>
            <tr>
            <td>Numerical Stability</td>
            <td>Avoids underflow when multiplying tiny
            probabilities</td>
            </tr>
            <tr>
            <td>Monotonicity</td>
            <td>Same optimal parameters as likelihood</td>
            </tr>
            <tr>
            <td>Information Theory</td>
            <td>Connects to entropy, cross-entropy, KL divergence</td>
            </tr>
            <tr>
            <td>Convexity</td>
            <td>For exponential family, guarantees global optimum</td>
            </tr>
            </tbody>
            </table>
            </blockquote>
            <hr />
            <p><strong>Interview Q</strong>: ‚ÄúWhy do we use
            log-likelihood instead of likelihood?‚Äù</p>
            <p><strong>A</strong>: Five reasons: (1) <strong>Easier
            optimization</strong> ‚Äî products become sums, so derivatives
            are simpler (sum rule vs product rule). (2)
            <strong>Numerical stability</strong> ‚Äî multiplying many
            small probabilities underflows to 0, but log-probabilities
            remain finite. (3) <strong>Same argmax</strong> ‚Äî log is
            monotonic, so maximizing log-likelihood gives the same
            optimal parameters. (4) <strong>Information theory
            connection</strong> ‚Äî negative log-likelihood is
            cross-entropy, linking MLE to information-theoretic
            concepts. (5) <strong>Convexity</strong> ‚Äî for exponential
            family distributions (Gaussian, Bernoulli, etc.), negative
            log-likelihood is convex, guaranteeing a unique global
            minimum.</p>
            <p><img src="figures/log_function_why.png"
            alt="Why We Use Log-Likelihood" /> <em>Figure: Four reasons
            we use log-likelihood: (Top-left) Log is monotonically
            increasing, preserving argmax. (Top-right) Products become
            sums, simplifying derivatives. (Bottom-left) Raw likelihood
            underflows with many samples; log-likelihood stays finite.
            (Bottom-right) Log compresses large values and expands small
            ones, improving optimization.</em></p>
            <hr />
            <blockquote>
            <p><strong>üìù Probability vs Likelihood: Same Math,
            Different Perspective</strong></p>
            <p>Both involve the same function <span
            class="math inline">\(P(D|\theta)\)</span>, but the
            interpretation depends on <strong>what‚Äôs fixed vs.¬†what
            varies</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th><strong>Probability</strong></th>
            <th><strong>Likelihood</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Fixed</strong></td>
            <td>Parameters <span
            class="math inline">\(\theta\)</span></td>
            <td>Data <span class="math inline">\(D\)</span></td>
            </tr>
            <tr>
            <td><strong>Varies</strong></td>
            <td>Data <span class="math inline">\(D\)</span></td>
            <td>Parameters <span
            class="math inline">\(\theta\)</span></td>
            </tr>
            <tr>
            <td><strong>Question</strong></td>
            <td>‚ÄúGiven this coin (<span
            class="math inline">\(p\)</span>=0.5), what‚Äôs the chance of
            7 heads?‚Äù</td>
            <td>‚ÄúGiven I saw 7 heads, which coin (which <span
            class="math inline">\(p\)</span>) best explains this?‚Äù</td>
            </tr>
            <tr>
            <td><strong>Direction</strong></td>
            <td>Parameters ‚Üí Data</td>
            <td>Data ‚Üí Parameters</td>
            </tr>
            <tr>
            <td><strong>Sums to 1?</strong></td>
            <td>Yes (over all possible data)</td>
            <td><strong>No!</strong> (not a probability distribution
            over <span class="math inline">\(\theta\)</span>)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Notation convention</strong>: We write <span
            class="math inline">\(L(\theta | D)\)</span> to emphasize
            that likelihood is a function <strong>of <span
            class="math inline">\(\theta\)</span></strong>, given fixed
            data. It‚Äôs NOT <span
            class="math inline">\(P(\theta|D)\)</span> ‚Äî that would be
            the posterior in Bayesian inference.</p>
            <p><strong>Why likelihood doesn‚Äôt sum to 1</strong>: If you
            integrate <span class="math inline">\(L(p | D)\)</span> over
            all <span class="math inline">\(p \in [0,1]\)</span>, you
            don‚Äôt get 1. Likelihood tells you <strong>relative
            plausibility</strong> of parameter values, not absolute
            probability.</p>
            </blockquote>
            <hr />
            <h3 id="worked-example-coin-flip-bernoulli-mle">Worked
            Example: Coin Flip (Bernoulli MLE)</h3>
            <p>You flip a coin 10 times and observe: <strong>H H T H T H
            H H T H</strong> (7 heads, 3 tails)</p>
            <p>What‚Äôs the MLE estimate for <span
            class="math inline">\(p\)</span> = probability of heads?</p>
            <p><strong>Step 1: Write the likelihood</strong></p>
            <p>Each flip is Bernoulli: <span class="math inline">\(P(X =
            1) = p\)</span>, <span class="math inline">\(P(X = 0) =
            1-p\)</span></p>
            <p>Likelihood of all observations (assuming independence):
            <span class="math display">\[L(p) = p^7 \cdot
            (1-p)^3\]</span></p>
            <p><strong>Step 2: Take log-likelihood</strong> <span
            class="math display">\[\ell(p) = \log L(p) = 7\log(p) +
            3\log(1-p)\]</span></p>
            <p><strong>Step 3: Take derivative and set to zero</strong>
            <span class="math display">\[\frac{d\ell}{dp} = \frac{7}{p}
            - \frac{3}{1-p} = 0\]</span></p>
            <p><span class="math display">\[\frac{7}{p} =
            \frac{3}{1-p}\]</span> <span class="math display">\[7(1-p) =
            3p\]</span> <span class="math display">\[7 - 7p =
            3p\]</span> <span class="math display">\[7 = 10p\]</span>
            <span class="math display">\[\hat{p}_{MLE} =
            0.7\]</span></p>
            <p><strong>Result</strong>: The MLE estimate is just the
            fraction of heads! This is intuitive ‚Äî if you saw 70% heads,
            your best guess for <span class="math inline">\(p\)</span>
            is 0.7.</p>
            <p><strong>General Bernoulli MLE</strong>: For <span
            class="math inline">\(k\)</span> successes in <span
            class="math inline">\(n\)</span> trials: <span
            class="math display">\[\hat{p}_{MLE} =
            \frac{k}{n}\]</span></p>
            <hr />
            <h3
            id="worked-example-complete-mle-for-normal-distribution-Œº-and-œÉ¬≤">Worked
            Example: Complete MLE for Normal Distribution (Œº and
            œÉ¬≤)</h3>
            <p>This is a classic derivation that demonstrates
            multivariate MLE with partial derivatives. Given N i.i.d.
            samples <span class="math inline">\(x_1, x_2, \ldots,
            x_N\)</span> from <span
            class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>,
            we want to find MLEs for <strong>both</strong> <span
            class="math inline">\(\mu\)</span> and <span
            class="math inline">\(\sigma^2\)</span>.</p>
            <p><strong>Step 1: Write the joint likelihood</strong></p>
            <p>Each sample has PDF: <span class="math display">\[p(x_i
            \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}
            \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\]</span></p>
            <p>Joint likelihood (i.i.d. assumption ‚Üí product): <span
            class="math display">\[L(\mu, \sigma^2) = \prod_{i=1}^{N}
            \frac{1}{\sqrt{2\pi\sigma^2}}
            \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\]</span></p>
            <p><strong>Step 2: Take the log-likelihood</strong></p>
            <p><span class="math display">\[\ell(\mu, \sigma^2) = \log
            L(\mu, \sigma^2) = \sum_{i=1}^{N} \log
            \left[\frac{1}{\sqrt{2\pi\sigma^2}}
            \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)\right]\]</span></p>
            <p>Expanding: <span class="math display">\[\ell(\mu,
            \sigma^2) = \sum_{i=1}^{N} \left[-\frac{1}{2}\log(2\pi) -
            \frac{1}{2}\log(\sigma^2) -
            \frac{(x_i-\mu)^2}{2\sigma^2}\right]\]</span></p>
            <p><span class="math display">\[= -\frac{N}{2}\log(2\pi) -
            \frac{N}{2}\log(\sigma^2) -
            \frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i - \mu)^2\]</span></p>
            <p><strong>Step 3: Take partial derivative w.r.t. Œº and
            solve</strong></p>
            <p><span class="math display">\[\frac{\partial
            \ell}{\partial \mu} = -\frac{1}{2\sigma^2} \cdot 2
            \sum_{i=1}^{N}(x_i - \mu) \cdot (-1) =
            \frac{1}{\sigma^2}\sum_{i=1}^{N}(x_i - \mu)\]</span></p>
            <p>Setting to zero: <span
            class="math display">\[\frac{1}{\sigma^2}\sum_{i=1}^{N}(x_i
            - \mu) = 0\]</span></p>
            <p><span class="math display">\[\sum_{i=1}^{N} x_i - N\mu =
            0\]</span></p>
            <p><span class="math display">\[\boxed{\hat{\mu}_{MLE} =
            \frac{1}{N}\sum_{i=1}^{N} x_i = \bar{x}}\]</span></p>
            <p><strong>Result</strong>: The MLE for the mean is the
            <strong>sample mean</strong>! ‚úì</p>
            <p><strong>Step 4: Take partial derivative w.r.t. œÉ¬≤ and
            solve</strong></p>
            <p>Let <span class="math inline">\(\tau = \sigma^2\)</span>
            for cleaner notation:</p>
            <p><span class="math display">\[\ell(\mu, \tau) =
            -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log(\tau) -
            \frac{1}{2\tau}\sum_{i=1}^{N}(x_i - \mu)^2\]</span></p>
            <p><span class="math display">\[\frac{\partial
            \ell}{\partial \tau} = -\frac{N}{2\tau} +
            \frac{1}{2\tau^2}\sum_{i=1}^{N}(x_i - \mu)^2\]</span></p>
            <p>Setting to zero: <span
            class="math display">\[-\frac{N}{2\tau} +
            \frac{1}{2\tau^2}\sum_{i=1}^{N}(x_i - \mu)^2 =
            0\]</span></p>
            <p>Multiply by <span class="math inline">\(2\tau^2\)</span>:
            <span class="math display">\[-N\tau + \sum_{i=1}^{N}(x_i -
            \mu)^2 = 0\]</span></p>
            <p><span class="math display">\[\tau =
            \frac{1}{N}\sum_{i=1}^{N}(x_i - \mu)^2\]</span></p>
            <p>Substituting <span class="math inline">\(\hat{\mu}_{MLE}
            = \bar{x}\)</span>:</p>
            <p><span class="math display">\[\boxed{\hat{\sigma}^2_{MLE}
            = \frac{1}{N}\sum_{i=1}^{N}(x_i - \bar{x})^2}\]</span></p>
            <p><strong>Result</strong>: The MLE for variance is the
            <strong>sample variance</strong> (but with <span
            class="math inline">\(N\)</span>, not <span
            class="math inline">\(N-1\)</span>)!</p>
            <hr />
            <h3 id="important-mle-variance-is-biased">‚ö†Ô∏è Important: MLE
            Variance is Biased!</h3>
            <p>Notice the MLE uses <span
            class="math inline">\(\frac{1}{N}\)</span>, but textbooks
            often use <span
            class="math inline">\(\frac{1}{N-1}\)</span>:</p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 34%" />
            <col style="width: 23%" />
            </colgroup>
            <thead>
            <tr>
            <th>Estimator</th>
            <th>Formula</th>
            <th>Bias</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MLE</strong></td>
            <td><span class="math inline">\(\hat{\sigma}^2_{MLE} =
            \frac{1}{N}\sum(x_i - \bar{x})^2\)</span></td>
            <td><strong>Biased</strong> ‚Äî underestimates <span
            class="math inline">\(\sigma^2\)</span></td>
            </tr>
            <tr>
            <td><strong>Unbiased</strong></td>
            <td><span class="math inline">\(s^2 = \frac{1}{N-1}\sum(x_i
            - \bar{x})^2\)</span></td>
            <td><strong>Unbiased</strong> ‚Äî <span
            class="math inline">\(\mathbb{E}[s^2] =
            \sigma^2\)</span></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why is MLE biased?</strong></p>
            <p>The MLE uses <span class="math inline">\(\bar{x}\)</span>
            (estimated from data) instead of true <span
            class="math inline">\(\mu\)</span>. Since <span
            class="math inline">\(\bar{x}\)</span> minimizes <span
            class="math inline">\(\sum(x_i - c)^2\)</span> over all
            <span class="math inline">\(c\)</span>, using <span
            class="math inline">\(\bar{x}\)</span> systematically
            underestimates the squared deviations compared to the true
            mean. Dividing by <span class="math inline">\(N-1\)</span>
            (Bessel‚Äôs correction) compensates for this.</p>
            <p><strong>Does bias matter in practice?</strong></p>
            <ul>
            <li>For large <span class="math inline">\(N\)</span>:
            Difference is negligible (<span
            class="math inline">\(\frac{1}{N} \approx
            \frac{1}{N-1}\)</span>)</li>
            <li>For small <span class="math inline">\(N\)</span>:
            Unbiased estimator is preferred for inference</li>
            <li>For ML: We typically have large datasets, so MLE is
            fine</li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúIs the MLE variance
            estimator biased?‚Äù</p>
            <p><strong>A</strong>: Yes! The MLE for variance is <span
            class="math inline">\(\hat{\sigma}^2 = \frac{1}{N}\sum(x_i -
            \bar{x})^2\)</span>, which underestimates the true variance
            because we use the sample mean <span
            class="math inline">\(\bar{x}\)</span> instead of the true
            mean. The expected value is <span
            class="math inline">\(\mathbb{E}[\hat{\sigma}^2_{MLE}] =
            \frac{N-1}{N}\sigma^2\)</span>, so it‚Äôs biased downward.
            Using <span class="math inline">\(\frac{1}{N-1}\)</span>
            (Bessel‚Äôs correction) gives an unbiased estimator. This
            matters more for small samples; for typical ML datasets, the
            bias is negligible.</p>
            <p><strong>Interview Q</strong>: ‚ÄúDerive the MLE for a
            Normal distribution.‚Äù</p>
            <p><strong>A</strong>: Given i.i.d. samples from <span
            class="math inline">\(\mathcal{N}(\mu,
            \sigma^2)\)</span>:</p>
            <ol type="1">
            <li><p><strong>Write log-likelihood</strong>: <span
            class="math inline">\(\ell = -\frac{N}{2}\log(2\pi\sigma^2)
            - \frac{1}{2\sigma^2}\sum(x_i - \mu)^2\)</span></p></li>
            <li><p><strong>Solve for Œº</strong>: Take <span
            class="math inline">\(\frac{\partial \ell}{\partial \mu} =
            \frac{1}{\sigma^2}\sum(x_i - \mu) = 0\)</span>, giving <span
            class="math inline">\(\hat{\mu} = \frac{1}{N}\sum x_i =
            \bar{x}\)</span></p></li>
            <li><p><strong>Solve for œÉ¬≤</strong>: Take <span
            class="math inline">\(\frac{\partial \ell}{\partial
            \sigma^2} = -\frac{N}{2\sigma^2} +
            \frac{1}{2\sigma^4}\sum(x_i-\mu)^2 = 0\)</span>, giving
            <span class="math inline">\(\hat{\sigma}^2 =
            \frac{1}{N}\sum(x_i - \bar{x})^2\)</span></p></li>
            </ol>
            <p>Key insight: The MLE variance uses <span
            class="math inline">\(\frac{1}{N}\)</span>, which is biased.
            The unbiased estimator uses <span
            class="math inline">\(\frac{1}{N-1}\)</span>.</p>
            <hr />
            <h3 id="mle-for-logistic-regression-cross-entropy">MLE for
            Logistic Regression ‚Üí Cross-Entropy</h3>
            <p>This is the <strong>key connection</strong> to deep
            learning!</p>
            <p>For binary classification, model predicts <span
            class="math inline">\(\hat{y} = \sigma(w^T x + b)\)</span> =
            probability of class 1.</p>
            <p><strong>Step 1: Model the labels as Bernoulli random
            variables</strong></p>
            <p>Each label <span class="math inline">\(y \in \{0,
            1\}\)</span> follows a Bernoulli distribution with parameter
            <span class="math inline">\(\hat{y}\)</span> (our predicted
            probability): <span class="math display">\[y \sim
            \text{Bernoulli}(\hat{y})\]</span></p>
            <p><strong>Step 2: Write the likelihood for one
            sample</strong></p>
            <p>The Bernoulli PMF is: <span
            class="math inline">\(P(y|\hat{y}) = \hat{y}^y \cdot
            (1-\hat{y})^{1-y}\)</span></p>
            <p>This compact formula handles both cases:</p>
            <ul>
            <li>If <span class="math inline">\(y=1\)</span>: <span
            class="math inline">\(P(y|x) = \hat{y}^1 \cdot (1-\hat{y})^0
            = \hat{y}\)</span> (want <span
            class="math inline">\(\hat{y}\)</span> high)</li>
            <li>If <span class="math inline">\(y=0\)</span>: <span
            class="math inline">\(P(y|x) = \hat{y}^0 \cdot (1-\hat{y})^1
            = 1-\hat{y}\)</span> (want <span
            class="math inline">\(\hat{y}\)</span> low)</li>
            </ul>
            <p><strong>Step 3: Write likelihood for entire
            dataset</strong> (assuming i.i.d.)</p>
            <p><span class="math display">\[L(\theta) = \prod_{i=1}^{N}
            P(y_i|x_i; \theta) = \prod_{i=1}^{N} \hat{y}_i^{y_i}
            (1-\hat{y}_i)^{1-y_i}\]</span></p>
            <blockquote>
            <p><strong>üîë Why Products? The i.i.d.
            Assumption</strong></p>
            <p><strong>i.i.d. = Independent and Identically
            Distributed</strong></p>
            <ul>
            <li><strong>Independent</strong>: Each data point‚Äôs outcome
            doesn‚Äôt affect others. Whether sample 1 is class 0 or 1
            tells us nothing about sample 2.</li>
            <li><strong>Identically distributed</strong>: All samples
            come from the same underlying distribution (same <span
            class="math inline">\(P(y|x; \theta)\)</span>).</li>
            </ul>
            <p><strong>Why multiply?</strong> For independent events,
            joint probability = product of individual probabilities:
            <span class="math display">\[P(A \text{ and } B) = P(A)
            \times P(B) \quad \text{(if A, B independent)}\]</span></p>
            <p>So the probability of seeing our <em>entire dataset</em>
            is: <span class="math display">\[P(\text{all labels}) =
            P(y_1) \times P(y_2) \times \cdots \times P(y_N) = \prod_i
            P(y_i)\]</span></p>
            <p><strong>If samples were NOT independent</strong> (e.g.,
            time series, correlated data), we‚Äôd need: <span
            class="math display">\[P(y_1, y_2, \ldots, y_N) = P(y_1)
            \cdot P(y_2|y_1) \cdot P(y_3|y_1,y_2) \cdots\]</span> This
            is much harder! The i.i.d. assumption makes MLE
            tractable.</p>
            </blockquote>
            <p><strong>Step 4: Take log-likelihood</strong> (products ‚Üí
            sums)</p>
            <p><span class="math display">\[\ell(\theta) = \log
            L(\theta) = \sum_{i=1}^{N} \left[y_i \log \hat{y}_i +
            (1-y_i)\log(1-\hat{y}_i)\right]\]</span></p>
            <p><strong>Step 5: Negate to get loss</strong> (we minimize,
            not maximize)</p>
            <p><span class="math display">\[-\ell(\theta) =
            -\sum_{i=1}^{N} \left[y_i \log \hat{y}_i +
            (1-y_i)\log(1-\hat{y}_i)\right]\]</span></p>
            <p><strong>This is exactly Binary Cross-Entropy
            Loss!</strong></p>
            <p><span class="math display">\[\mathcal{L}_{BCE} =
            -\frac{1}{N}\sum_i \left[y_i \log \hat{y}_i +
            (1-y_i)\log(1-\hat{y}_i)\right]\]</span></p>
            <p><strong>Why this matters</strong>: The loss function
            isn‚Äôt arbitrary ‚Äî it has deep probabilistic foundations.
            Cross-entropy is the <em>principled</em> choice for
            classification because it directly answers ‚Äúwhat parameters
            make the correct labels most probable?‚Äù</p>
            <hr />
            <h3
            id="cross-entropy-vs-nll-two-derivations-same-formula">Cross-Entropy
            vs NLL: Two Derivations, Same Formula</h3>
            <blockquote>
            <p><strong>üìù This is a common interview question!</strong>
            ‚ÄúWhat‚Äôs the relationship between cross-entropy loss and
            negative log-likelihood?‚Äù Let‚Äôs derive both and see why
            they‚Äôre mathematically identical for classification.</p>
            </blockquote>
            <hr />
            <h4
            id="derivation-1-nll-from-maximum-likelihood-probability-view">Derivation
            1: NLL from Maximum Likelihood (Probability View)</h4>
            <p><strong>Starting question</strong>: ‚ÄúWhat parameters make
            my observed data most likely?‚Äù</p>
            <p><strong>Step 1: Model labels as Categorical random
            variables</strong></p>
            <p>For classification, we model each label as drawn from a
            Categorical distribution: <span class="math display">\[y
            \sim \text{Categorical}(\hat{y}_1, \hat{y}_2, \ldots,
            \hat{y}_K)\]</span></p>
            <p>where <span class="math inline">\(\hat{y}_k = P(y = k |
            x; \theta)\)</span> is the model‚Äôs predicted probability for
            class <span class="math inline">\(k\)</span>.</p>
            <p><strong>Step 2: Write the likelihood for one
            sample</strong></p>
            <p>For a one-hot label where the true class is <span
            class="math inline">\(c\)</span>: <span
            class="math display">\[P(y = c | x) = \hat{y}_c\]</span></p>
            <p>More compactly, using one-hot vector <span
            class="math inline">\(\mathbf{y}\)</span> (with <span
            class="math inline">\(y_c = 1\)</span>, others 0): <span
            class="math display">\[P(\mathbf{y} | x) = \prod_{k=1}^{K}
            \hat{y}_k^{y_k} = \hat{y}_c^1 \cdot \hat{y}_1^0 \cdot \ldots
            = \hat{y}_c\]</span></p>
            <p><strong>Step 3: Write likelihood for entire
            dataset</strong> (i.i.d. assumption) <span
            class="math display">\[L(\theta) = \prod_{i=1}^{N} P(y^{(i)}
            | x^{(i)}; \theta) = \prod_{i=1}^{N}
            \hat{y}_{c_i}\]</span></p>
            <p><strong>Step 4: Take log-likelihood</strong> (products ‚Üí
            sums) <span class="math display">\[\ell(\theta) = \log
            L(\theta) = \sum_{i=1}^{N} \log \hat{y}_{c_i}\]</span></p>
            <p><strong>Step 5: Negate</strong> (maximize ‚Üí minimize)
            <span class="math display">\[\text{NLL} = -\ell(\theta) =
            -\sum_{i=1}^{N} \log \hat{y}_{c_i}\]</span></p>
            <p><strong>Result</strong>: <span
            class="math display">\[\boxed{\text{NLL} = -\sum_{i=1}^{N}
            \log \hat{y}_{c_i}}\]</span> (Sum of negative
            log-probabilities of the correct classes)</p>
            <hr />
            <h4
            id="derivation-2-cross-entropy-from-information-theory">Derivation
            2: Cross-Entropy from Information Theory</h4>
            <p><strong>Starting question</strong>: ‚ÄúHow many bits do we
            waste by using the wrong distribution?‚Äù</p>
            <p><strong>Step 1: Define Cross-Entropy</strong></p>
            <p>Cross-entropy measures the expected bits needed to encode
            samples from <span class="math inline">\(P\)</span> using
            code optimized for <span class="math inline">\(Q\)</span>:
            <span class="math display">\[H(P, Q) = -\sum_x p(x) \log
            q(x) = \mathbb{E}_{x \sim P}[-\log Q(x)]\]</span></p>
            <p><strong>Step 2: Identify the distributions for
            classification</strong></p>
            <ul>
            <li><strong>True distribution <span
            class="math inline">\(P\)</span></strong>: One-hot (all
            probability mass on correct class <span
            class="math inline">\(c\)</span>)
            <ul>
            <li><span class="math inline">\(p(y = c) = 1\)</span></li>
            <li><span class="math inline">\(p(y \neq c) =
            0\)</span></li>
            </ul></li>
            <li><strong>Predicted distribution <span
            class="math inline">\(Q\)</span></strong>: Softmax output
            <span class="math inline">\([\hat{y}_1, \ldots,
            \hat{y}_K]\)</span></li>
            </ul>
            <p><strong>Step 3: Apply the cross-entropy formula</strong>
            <span class="math display">\[H(P, Q) = -\sum_{k=1}^{K} p_k
            \log \hat{y}_k\]</span></p>
            <p><strong>Step 4: Simplify using one-hot
            structure</strong></p>
            <p>Since <span class="math inline">\(p_c = 1\)</span> and
            <span class="math inline">\(p_{k \neq c} = 0\)</span>: <span
            class="math display">\[H(P, Q) = -1 \cdot \log \hat{y}_c - 0
            \cdot \log \hat{y}_1 - \ldots - 0 \cdot \log \hat{y}_K =
            -\log \hat{y}_c\]</span></p>
            <p><strong>Result</strong>: <span
            class="math display">\[\boxed{H(P, Q) = -\log
            \hat{y}_c}\]</span> (Same as NLL for a single sample!)</p>
            <hr />
            <h4 id="why-theyre-identical-the-punchline">Why They‚Äôre
            Identical: The Punchline</h4>
            <p>For the full dataset with one-hot labels:</p>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 24%" />
            <col style="width: 43%" />
            </colgroup>
            <thead>
            <tr>
            <th>Derivation</th>
            <th>Formula</th>
            <th>Starting Point</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>NLL</strong></td>
            <td><span class="math inline">\(-\sum_i \log
            \hat{y}_{c_i}\)</span></td>
            <td>Probability theory (MLE)</td>
            </tr>
            <tr>
            <td><strong>Cross-Entropy</strong></td>
            <td><span class="math inline">\(-\sum_i \sum_k y_k^{(i)}
            \log \hat{y}_k^{(i)}\)</span></td>
            <td>Information theory</td>
            </tr>
            </tbody>
            </table>
            <p>For one-hot labels, the inner sum collapses because only
            one <span class="math inline">\(y_k = 1\)</span>: <span
            class="math display">\[-\sum_k y_k \log \hat{y}_k = -1 \cdot
            \log \hat{y}_c + 0 + \ldots + 0 = -\log
            \hat{y}_c\]</span></p>
            <p><strong>Both derivations give the same answer:</strong>
            <span class="math display">\[\mathcal{L} = -\sum_{i=1}^{N}
            \log \hat{y}_{c_i}\]</span></p>
            <p><strong>Intuitive summary</strong>: - <strong>NLL
            asks</strong>: ‚ÄúHow improbable is my observed data under the
            model?‚Äù - <strong>Cross-Entropy asks</strong>: ‚ÄúHow many
            bits am I wasting by using the wrong distribution?‚Äù -
            <strong>Same question, different language!</strong></p>
            <hr />
            <h4 id="when-they-differ">When They Differ</h4>
            <p>While NLL and cross-entropy are identical for one-hot
            labels, they can differ in other settings:</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 16%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>NLL</th>
            <th>Cross-Entropy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>One-hot labels</strong></td>
            <td><span class="math inline">\(-\log
            \hat{y}_c\)</span></td>
            <td><span class="math inline">\(-\log \hat{y}_c\)</span>
            (identical)</td>
            </tr>
            <tr>
            <td><strong>Label smoothing</strong></td>
            <td>Still uses hard labels</td>
            <td>Uses soft targets <span
            class="math inline">\(\tilde{y}\)</span></td>
            </tr>
            <tr>
            <td><strong>Knowledge distillation</strong></td>
            <td>Teacher is fixed</td>
            <td><span class="math inline">\(H(P_{\text{teacher}},
            Q_{\text{student}})\)</span></td>
            </tr>
            <tr>
            <td><strong>Regression (continuous)</strong></td>
            <td>Gaussian ‚Üí MSE</td>
            <td>Not typically defined</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Label smoothing example</strong>: Instead of
            <span class="math inline">\(\mathbf{y} = [0, 0, 1,
            0]\)</span> (one-hot), use <span
            class="math inline">\(\tilde{\mathbf{y}} = [0.025, 0.025,
            0.925, 0.025]\)</span>. Cross-entropy with <span
            class="math inline">\(\tilde{\mathbf{y}}\)</span> ‚â† NLL.</p>
            <hr />
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the relationship
            between cross-entropy loss and negative log-likelihood?‚Äù</p>
            <p><strong>A</strong>: For classification with one-hot
            labels, they‚Äôre mathematically identical. NLL comes from
            MLE: we model labels as Categorical random variables and
            maximize the likelihood of observed labels. Cross-entropy
            comes from information theory: we measure the ‚Äúwasted bits‚Äù
            when using predicted distribution Q to encode samples from
            true distribution P.</p>
            <p>Both reduce to <span class="math inline">\(-\sum_i \log
            \hat{y}_{c_i}\)</span> ‚Äî the sum of negative
            log-probabilities of correct classes. The key insight is
            that for one-hot P, the cross-entropy formula <span
            class="math inline">\(-\sum_k p_k \log q_k\)</span>
            collapses to <span class="math inline">\(-\log q_c\)</span>
            since only one <span class="math inline">\(p_k = 1\)</span>.
            They differ only when using soft labels (label smoothing,
            knowledge distillation) where P is no longer one-hot.</p>
            <hr />
            <h3 id="mle-for-regression-mse">MLE for Regression ‚Üí
            MSE</h3>
            <p>This is the <strong>regression counterpart</strong> to
            cross-entropy!</p>
            <p>For regression, model predicts <span
            class="math inline">\(\hat{y} = f(x; \theta)\)</span> =
            continuous output (e.g., house price, temperature).</p>
            <p><strong>Step 1: Model the targets as Gaussian random
            variables</strong></p>
            <p>Each target <span class="math inline">\(y\)</span> is
            Gaussian-distributed around our prediction: <span
            class="math display">\[y \sim \mathcal{N}(\hat{y},
            \sigma^2)\]</span></p>
            <p>This means: <span class="math inline">\(y = \hat{y} +
            \epsilon\)</span> where <span class="math inline">\(\epsilon
            \sim \mathcal{N}(0, \sigma^2)\)</span> (additive Gaussian
            noise).</p>
            <p><strong>Intuition</strong>: We‚Äôre saying ‚Äúthe true value
            is our prediction plus some random Gaussian noise.‚Äù</p>
            <p><strong>Step 2: Write the likelihood for one
            sample</strong></p>
            <p>The Gaussian PDF is: <span
            class="math display">\[P(y|\hat{y}) =
            \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y -
            \hat{y})^2}{2\sigma^2}\right)\]</span></p>
            <p><strong>Step 3: Write likelihood for entire
            dataset</strong> (assuming i.i.d.)</p>
            <p><span class="math display">\[L(\theta) = \prod_{i=1}^{N}
            P(y_i|x_i; \theta) = \prod_{i=1}^{N}
            \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i -
            \hat{y}_i)^2}{2\sigma^2}\right)\]</span></p>
            <p><strong>Step 4: Take log-likelihood</strong> (products ‚Üí
            sums)</p>
            <p><span class="math display">\[\ell(\theta) =
            \sum_{i=1}^{N} \left[-\frac{1}{2}\log(2\pi\sigma^2) -
            \frac{(y_i - \hat{y}_i)^2}{2\sigma^2}\right]\]</span></p>
            <p><span class="math display">\[=
            -\frac{N}{2}\log(2\pi\sigma^2) -
            \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i -
            \hat{y}_i)^2\]</span></p>
            <p><strong>Step 5: Negate to get loss</strong> (we minimize,
            not maximize)</p>
            <p><span class="math display">\[-\ell(\theta) =
            \frac{N}{2}\log(2\pi\sigma^2) +
            \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i -
            \hat{y}_i)^2\]</span></p>
            <p><strong>Step 6: Drop constants</strong> (don‚Äôt affect
            optimization)</p>
            <p>Since <span class="math inline">\(\sigma\)</span> is
            typically fixed (not learned), all terms except the sum are
            constant:</p>
            <p><span class="math display">\[-\ell(\theta) \propto
            \sum_{i=1}^{N}(y_i - \hat{y}_i)^2\]</span></p>
            <p><strong>This is exactly MSE Loss!</strong></p>
            <p><span class="math display">\[\mathcal{L}_{MSE} =
            \frac{1}{N}\sum_i (y_i - \hat{y}_i)^2\]</span></p>
            <blockquote>
            <p><strong>üîë Connection to Deep Learning</strong></p>
            <p>This derivation shows that <strong>training a neural
            network with MSE loss IS MLE under Gaussian noise
            assumption</strong>:</p>
            <ul>
            <li>Model predicts <span class="math inline">\(\hat{y} =
            f(x; \theta)\)</span> (continuous value)</li>
            <li>MSE = negative log-likelihood (up to constants)</li>
            <li>Gradient descent minimizes MSE = maximizes
            likelihood</li>
            <li>So backprop + MSE is just gradient-based MLE!</li>
            </ul>
            <p><strong>The parallel to classification is
            exact:</strong></p>
            <table>
            <thead>
            <tr>
            <th>Task</th>
            <th>Distribution</th>
            <th>Likelihood ‚Üí Loss</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Classification</strong></td>
            <td>Bernoulli/Categorical</td>
            <td>Cross-Entropy</td>
            </tr>
            <tr>
            <td><strong>Regression</strong></td>
            <td>Gaussian</td>
            <td>MSE</td>
            </tr>
            </tbody>
            </table>
            </blockquote>
            <p><strong>Why this matters</strong>: MSE isn‚Äôt just
            ‚Äúsquared error‚Äù ‚Äî it‚Äôs the <em>principled</em> choice for
            regression when you assume Gaussian noise. If your noise
            isn‚Äôt Gaussian (e.g., heavy-tailed outliers), you should use
            a different loss (MAE for Laplace, Huber for robust).</p>
            <hr />
            <h3
            id="loss-functions-as-mle-under-different-distributions">Loss
            Functions as MLE Under Different Distributions</h3>
            <p>The connection between loss functions and MLE extends far
            beyond cross-entropy. <strong>Your choice of loss function
            implicitly assumes a noise distribution for your
            data!</strong></p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 21%" />
            <col style="width: 35%" />
            <col style="width: 20%" />
            <col style="width: 22%" />
            </colgroup>
            <thead>
            <tr>
            <th>Loss Function</th>
            <th>Probabilistic Assumption</th>
            <th>Distribution</th>
            <th>MLE Derivation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Binary Cross-Entropy</strong></td>
            <td>Labels are binary</td>
            <td><span class="math inline">\(y \sim
            \text{Bernoulli}(\hat{y})\)</span></td>
            <td><span class="math inline">\(-\log p(y) = -[y\log\hat{y}
            + (1-y)\log(1-\hat{y})]\)</span></td>
            </tr>
            <tr>
            <td><strong>Categorical Cross-Entropy</strong></td>
            <td>Labels are one-hot</td>
            <td><span class="math inline">\(y \sim
            \text{Categorical}(\hat{\mathbf{y}})\)</span></td>
            <td><span class="math inline">\(-\log p(y) = -\sum_k y_k
            \log \hat{y}_k\)</span></td>
            </tr>
            <tr>
            <td><strong>Mean Squared Error (MSE)</strong></td>
            <td>Targets are Gaussian</td>
            <td><span class="math inline">\(y \sim \mathcal{N}(\hat{y},
            \sigma^2)\)</span></td>
            <td><span class="math inline">\(-\log p(y) \propto (y -
            \hat{y})^2\)</span></td>
            </tr>
            <tr>
            <td><strong>Mean Absolute Error (MAE)</strong></td>
            <td>Targets are Laplace</td>
            <td><span class="math inline">\(y \sim
            \text{Laplace}(\hat{y}, b)\)</span></td>
            <td><span class="math inline">\(-\log p(y) \propto |y -
            \hat{y}|\)</span></td>
            </tr>
            <tr>
            <td><strong>Huber Loss</strong></td>
            <td>Hybrid (Gaussian near mean, Laplace in tails)</td>
            <td>Mixture</td>
            <td>Smooth transition</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="mse-from-mle-the-gaussian-derivation">MSE from MLE:
            The Gaussian Derivation</h3>
            <p><strong>Assumption</strong>: The target <span
            class="math inline">\(y\)</span> is Gaussian-distributed
            around our prediction <span
            class="math inline">\(\hat{y}\)</span>: <span
            class="math display">\[y \sim \mathcal{N}(\hat{y},
            \sigma^2)\]</span></p>
            <p>This means: <span class="math inline">\(y = \hat{y} +
            \epsilon\)</span> where <span class="math inline">\(\epsilon
            \sim \mathcal{N}(0, \sigma^2)\)</span> (additive Gaussian
            noise).</p>
            <p><strong>Step 1: Write the Gaussian PDF</strong> <span
            class="math display">\[p(y|\hat{y}) =
            \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y -
            \hat{y})^2}{2\sigma^2}\right)\]</span></p>
            <p><strong>Step 2: Take negative log-likelihood</strong>
            <span class="math display">\[-\log p(y|\hat{y}) = \frac{(y -
            \hat{y})^2}{2\sigma^2} +
            \frac{1}{2}\log(2\pi\sigma^2)\]</span></p>
            <p><strong>Step 3: Drop constants</strong> (don‚Äôt affect
            optimization)</p>
            <p>Since <span class="math inline">\(\sigma\)</span> is
            typically fixed (not learned), the second term is constant.
            We get: <span class="math display">\[-\log p(y|\hat{y})
            \propto (y - \hat{y})^2\]</span></p>
            <p><strong>This is MSE!</strong> Minimizing MSE is
            equivalent to MLE under Gaussian noise assumption.</p>
            <blockquote>
            <p><strong>üîë Key Insight: When to Use MSE vs
            MAE</strong></p>
            <ul>
            <li><strong>MSE assumes Gaussian noise</strong>: Outliers
            get squared, so MSE heavily penalizes them. Good when noise
            is truly Gaussian (symmetric, light-tailed).</li>
            <li><strong>MAE assumes Laplace noise</strong>: Laplace has
            heavier tails, so MAE is more robust to outliers. Use when
            you expect some corrupted data points.</li>
            <li><strong>Huber Loss</strong>: Best of both worlds ‚Äî
            behaves like MSE near the mean, like MAE for outliers.</li>
            </ul>
            </blockquote>
            <hr />
            <h3 id="mae-from-mle-the-laplace-derivation">MAE from MLE:
            The Laplace Derivation</h3>
            <p><strong>Assumption</strong>: The target <span
            class="math inline">\(y\)</span> is Laplace-distributed
            around our prediction: <span class="math display">\[y \sim
            \text{Laplace}(\hat{y}, b)\]</span></p>
            <p><strong>The Laplace PDF</strong>: <span
            class="math display">\[p(y|\hat{y}) = \frac{1}{2b}
            \exp\left(-\frac{|y - \hat{y}|}{b}\right)\]</span></p>
            <p><strong>Negative log-likelihood</strong>: <span
            class="math display">\[-\log p(y|\hat{y}) = \frac{|y -
            \hat{y}|}{b} + \log(2b)\]</span></p>
            <p>Dropping constants: <span class="math display">\[-\log
            p(y|\hat{y}) \propto |y - \hat{y}|\]</span></p>
            <p><strong>This is MAE!</strong> Minimizing MAE is MLE under
            Laplace noise assumption.</p>
            <hr />
            <h3 id="why-this-matters-in-practice">Why This Matters in
            Practice</h3>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 54%" />
            <col style="width: 15%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Recommended Loss</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Clean regression data</strong></td>
            <td>MSE</td>
            <td>Gaussian assumption usually holds</td>
            </tr>
            <tr>
            <td><strong>Data with outliers</strong></td>
            <td>MAE or Huber</td>
            <td>More robust, doesn‚Äôt square large errors</td>
            </tr>
            <tr>
            <td><strong>Classification</strong></td>
            <td>Cross-Entropy</td>
            <td>Matches Bernoulli/Categorical assumption</td>
            </tr>
            <tr>
            <td><strong>Ordinal regression</strong></td>
            <td>Custom (often MSE-like)</td>
            <td>Depends on label semantics</td>
            </tr>
            <tr>
            <td><strong>Probabilistic models</strong></td>
            <td>NLL of chosen distribution</td>
            <td>Explicitly model the distribution you want</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhy do we use MSE for
            regression instead of MAE?‚Äù</p>
            <p><strong>A</strong>: Both are valid ‚Äî they just make
            different assumptions. MSE assumes targets have Gaussian
            noise around the prediction (<span class="math inline">\(y
            \sim \mathcal{N}(\hat{y}, \sigma^2)\)</span>), and
            minimizing MSE is MLE under this assumption. MAE assumes
            Laplace noise, which has heavier tails. MSE penalizes
            outliers more heavily (errors are squared), while MAE is
            more robust to them. Choose based on your noise assumptions:
            MSE for Gaussian-like errors, MAE (or Huber) when outliers
            are expected.</p>
            <hr />
            <p><strong>Interview Q</strong>: ‚ÄúDerive MSE loss from
            maximum likelihood estimation.‚Äù</p>
            <p><strong>A</strong>: Assume <span class="math inline">\(y
            \sim \mathcal{N}(\hat{y}, \sigma^2)\)</span> ‚Äî targets are
            Gaussian-distributed around predictions. The PDF is <span
            class="math inline">\(p(y|\hat{y}) =
            \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(y-\hat{y})^2}{2\sigma^2})\)</span>.
            Taking the negative log: <span class="math inline">\(-\log
            p(y|\hat{y}) = \frac{(y-\hat{y})^2}{2\sigma^2} +
            \text{const}\)</span>. Summing over the dataset and dropping
            the constant gives <span class="math inline">\(\mathcal{L}
            \propto \sum_i (y_i - \hat{y}_i)^2\)</span>, which is MSE.
            So MSE is the MLE loss under Gaussian noise assumption.</p>
            <p><strong>Interview Q</strong>: ‚ÄúDerive the loss function
            for logistic regression from MLE.‚Äù</p>
            <p><strong>A</strong>: We model <span
            class="math inline">\(P(y=1|x) = \sigma(w^Tx + b)\)</span>,
            treating labels as Bernoulli random variables. The
            likelihood for one sample is <span
            class="math inline">\(P(y|x) =
            \hat{y}^y(1-\hat{y})^{1-y}\)</span> ‚Äî this is just the
            Bernoulli PMF. Taking the log gives <span
            class="math inline">\(y\log\hat{y} +
            (1-y)\log(1-\hat{y})\)</span>. Summing over the dataset
            gives the log-likelihood. We negate (because we minimize
            loss, not maximize likelihood) to get Binary Cross-Entropy:
            <span class="math inline">\(\mathcal{L} =
            -\sum_i[y_i\log\hat{y}_i +
            (1-y_i)\log(1-\hat{y}_i)]\)</span>. So cross-entropy loss IS
            negative log-likelihood ‚Äî training with cross-entropy is
            doing MLE.</p>
            <hr />
            <h3 id="maximum-a-posteriori-map-estimation">Maximum A
            Posteriori (MAP) Estimation</h3>
            <blockquote>
            <p><strong>üîë The Big Picture: MAP = MLE + Prior
            Beliefs</strong></p>
            <p>MLE answers: ‚ÄúWhat parameters maximize the likelihood of
            the data?‚Äù MAP answers: ‚ÄúWhat parameters maximize the
            likelihood of the data <strong>AND</strong> are consistent
            with our prior beliefs?‚Äù</p>
            <p>MAP is the <strong>Bayesian</strong> approach to
            parameter estimation ‚Äî it incorporates prior knowledge!</p>
            </blockquote>
            <p><strong>The Core Intuition</strong>:</p>
            <p>MLE can overfit, especially with limited data. If you
            flip a coin 3 times and see 3 heads, MLE says <span
            class="math inline">\(\hat{p} = 1.0\)</span> ‚Äî the coin
            always lands heads! But that seems extreme.</p>
            <p><strong>MAP lets you say</strong>: ‚ÄúI believe coins are
            usually close to fair. Let me combine this belief with the
            evidence.‚Äù</p>
            <hr />
            <p><strong>Deriving MAP from Bayes‚Äô Theorem</strong>:</p>
            <p>We want the most probable parameters given the data:</p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\max_{\theta} P(\theta|\mathcal{D})\]</span></p>
            <p>Apply Bayes‚Äô theorem:</p>
            <p><span class="math display">\[P(\theta|\mathcal{D}) =
            \frac{P(\mathcal{D}|\theta) \cdot
            P(\theta)}{P(\mathcal{D})}\]</span></p>
            <p>Since <span class="math inline">\(P(\mathcal{D})\)</span>
            is constant with respect to <span
            class="math inline">\(\theta\)</span> (it‚Äôs just the
            evidence):</p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\max_{\theta} P(\mathcal{D}|\theta) \cdot
            P(\theta)\]</span></p>
            <p><span class="math display">\[= \arg\max_{\theta}
            \underbrace{P(\mathcal{D}|\theta)}_{\text{likelihood}} \cdot
            \underbrace{P(\theta)}_{\text{prior}}\]</span></p>
            <p><strong>Taking the log</strong> (products ‚Üí sums):</p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\max_{\theta} \left[\log P(\mathcal{D}|\theta) + \log
            P(\theta)\right]\]</span></p>
            <p><span class="math display">\[= \arg\max_{\theta}
            \left[\underbrace{\ell(\theta)}_{\text{log-likelihood}} +
            \underbrace{\log
            P(\theta)}_{\text{log-prior}}\right]\]</span></p>
            <hr />
            <h3 id="the-key-insight-priors-are-regularization">The Key
            Insight: Priors ARE Regularization!</h3>
            <p>Different priors lead to different regularization
            terms:</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 30%" />
            <col style="width: 39%" />
            </colgroup>
            <thead>
            <tr>
            <th>Prior Distribution</th>
            <th>Mathematical Form</th>
            <th>Resulting Regularization</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Gaussian (Normal)</strong></td>
            <td><span class="math inline">\(P(\theta) \propto
            e^{-\frac{\lambda}{2}\|\theta\|^2}\)</span></td>
            <td><strong>L2 (Ridge / Weight Decay)</strong></td>
            </tr>
            <tr>
            <td><strong>Laplace</strong></td>
            <td><span class="math inline">\(P(\theta) \propto
            e^{-\lambda\|\theta\|_1}\)</span></td>
            <td><strong>L1 (Lasso / Sparsity)</strong></td>
            </tr>
            <tr>
            <td><strong>Uniform (flat)</strong></td>
            <td><span class="math inline">\(P(\theta) =
            \text{const}\)</span></td>
            <td><strong>None (reduces to MLE!)</strong></td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3
            id="worked-example-gaussian-prior-l2-regularization">Worked
            Example: Gaussian Prior ‚Üí L2 Regularization</h3>
            <p>Let‚Äôs derive this connection explicitly.</p>
            <p><strong>Setup</strong>: We want to find parameters <span
            class="math inline">\(\theta\)</span> using MAP with a
            Gaussian prior.</p>
            <p><strong>Step 1: Write the Gaussian prior</strong></p>
            <p><span class="math display">\[P(\theta) =
            \frac{1}{\sqrt{2\pi\sigma_0^2}}
            \exp\left(-\frac{\theta^2}{2\sigma_0^2}\right)\]</span></p>
            <p>For simplicity, let <span
            class="math inline">\(\sigma_0^2 =
            \frac{1}{\lambda}\)</span> (parameterize by precision):</p>
            <p><span class="math display">\[P(\theta) \propto
            \exp\left(-\frac{\lambda}{2}\theta^2\right)\]</span></p>
            <p><strong>Step 2: Take the log-prior</strong></p>
            <p><span class="math display">\[\log P(\theta) =
            -\frac{\lambda}{2}\theta^2 + \text{const}\]</span></p>
            <p><strong>Step 3: Write the MAP objective</strong></p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\max_{\theta} \left[\log P(\mathcal{D}|\theta) + \log
            P(\theta)\right]\]</span></p>
            <p><span class="math display">\[= \arg\max_{\theta}
            \left[\ell(\theta) -
            \frac{\lambda}{2}\|\theta\|^2\right]\]</span></p>
            <p><strong>Step 4: Convert to minimization</strong>
            (negate):</p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\min_{\theta} \left[-\ell(\theta) +
            \frac{\lambda}{2}\|\theta\|^2\right]\]</span></p>
            <p><span class="math display">\[= \arg\min_{\theta}
            \left[\underbrace{\mathcal{L}(\theta)}_{\text{loss (e.g.,
            MSE, CE)}} +
            \underbrace{\frac{\lambda}{2}\|\theta\|^2}_{\text{L2
            regularization!}}\right]\]</span></p>
            <p><strong>This is exactly regularized loss!</strong> The L2
            penalty term comes directly from the Gaussian prior.</p>
            <hr />
            <h3
            id="worked-example-laplace-prior-l1-regularization">Worked
            Example: Laplace Prior ‚Üí L1 Regularization</h3>
            <p><strong>The Laplace prior</strong>:</p>
            <p><span class="math display">\[P(\theta) =
            \frac{\lambda}{2} \exp(-\lambda|\theta|)\]</span></p>
            <p><strong>Take the log</strong>:</p>
            <p><span class="math display">\[\log P(\theta) =
            \log\frac{\lambda}{2} - \lambda|\theta|\]</span></p>
            <p><strong>MAP objective</strong>:</p>
            <p><span class="math display">\[\hat{\theta}_{MAP} =
            \arg\min_{\theta} \left[\mathcal{L}(\theta) +
            \lambda\|\theta\|_1\right]\]</span></p>
            <p><strong>This is L1 regularization!</strong> The Laplace
            prior encourages sparsity because it has more mass at zero
            than the Gaussian.</p>
            <hr />
            <h3 id="why-different-priors-lead-to-different-sparsity">Why
            Different Priors Lead to Different Sparsity</h3>
            <table>
            <colgroup>
            <col style="width: 17%" />
            <col style="width: 36%" />
            <col style="width: 46%" />
            </colgroup>
            <thead>
            <tr>
            <th>Prior</th>
            <th>Shape at Zero</th>
            <th>Effect on Weights</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Gaussian</strong></td>
            <td>Smooth, rounded</td>
            <td>Shrinks all weights toward zero, but doesn‚Äôt make them
            exactly zero</td>
            </tr>
            <tr>
            <td><strong>Laplace</strong></td>
            <td>Sharp peak</td>
            <td>Pushes many weights to exactly zero (sparse
            solutions)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Intuition</strong>: The Laplace prior has a
            ‚Äúspike‚Äù at zero ‚Äî it strongly believes many parameters
            should be zero. The Gaussian is ‚Äúsmooth‚Äù at zero ‚Äî it
            believes parameters are small but not necessarily zero.</p>
            <hr />
            <p><img src="figures/laplace_gaussian_prior.png"
            alt="Laplace vs Gaussian Prior" /> <em>Figure: Comparing
            Laplace and Gaussian priors for MAP estimation. Top-left:
            Laplace has a sharp peak at zero (inducing sparsity) while
            Gaussian is smooth (shrinking all weights). Top-right:
            Taking the log reveals why ‚Äî log-Laplace is V-shaped (L1
            penalty) while log-Gaussian is parabolic (L2 penalty).
            Bottom-left: Effect on trained weights ‚Äî L1 produces exact
            zeros (sparse), L2 shrinks all weights (dense).
            Bottom-right: Geometric intuition ‚Äî loss contours hit L1
            diamond at corners (sparse solutions) but touch L2 circle at
            smooth points (non-sparse).</em></p>
            <h3 id="mle-vs-map-comparison">MLE vs MAP Comparison</h3>
            <table>
            <colgroup>
            <col style="width: 44%" />
            <col style="width: 27%" />
            <col style="width: 27%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>MLE</th>
            <th>MAP</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Objective</strong></td>
            <td><span class="math inline">\(\arg\max_\theta
            P(\mathcal{D} \mid \theta)\)</span></td>
            <td><span class="math inline">\(\arg\max_\theta
            P(\mathcal{D} \mid \theta) P(\theta)\)</span></td>
            </tr>
            <tr>
            <td><strong>Prior</strong></td>
            <td>None (or implicit uniform)</td>
            <td>Explicit prior <span
            class="math inline">\(P(\theta)\)</span></td>
            </tr>
            <tr>
            <td><strong>Regularization</strong></td>
            <td>None</td>
            <td>Automatic (from prior)</td>
            </tr>
            <tr>
            <td><strong>Small data</strong></td>
            <td>Prone to overfitting</td>
            <td>More robust (prior regularizes)</td>
            </tr>
            <tr>
            <td><strong>Large data</strong></td>
            <td>Works well</td>
            <td>Converges to MLE (data overwhelms prior)</td>
            </tr>
            <tr>
            <td><strong>Interpretation</strong></td>
            <td>Point estimate maximizing likelihood</td>
            <td>Point estimate maximizing posterior</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="when-to-use-mle-vs-map">When to Use MLE vs MAP</h3>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 51%" />
            <col style="width: 16%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Recommendation</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Large dataset</strong></td>
            <td>MLE (or MAP, doesn‚Äôt matter)</td>
            <td>With lots of data, the likelihood dominates ‚Äî prior has
            negligible effect</td>
            </tr>
            <tr>
            <td><strong>Small dataset</strong></td>
            <td>MAP</td>
            <td>Prior prevents overfitting; incorporates domain
            knowledge</td>
            </tr>
            <tr>
            <td><strong>Strong domain knowledge</strong></td>
            <td>MAP</td>
            <td>Can encode beliefs (e.g., ‚Äúweights should be
            small‚Äù)</td>
            </tr>
            <tr>
            <td><strong>No prior information</strong></td>
            <td>MLE</td>
            <td>Using a prior without justification can bias
            results</td>
            </tr>
            <tr>
            <td><strong>Interpretability</strong></td>
            <td>MLE</td>
            <td>Easier to explain ‚Äî ‚Äúmost likely given the data‚Äù</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The Connection to Deep Learning</strong>:</p>
            <ul>
            <li><strong>Weight decay</strong> = L2 regularization =
            Gaussian prior on weights</li>
            <li><strong>L1 regularization</strong> = Laplace prior on
            weights<br />
            </li>
            <li><strong>Dropout</strong> ‚âà approximate Bayesian
            inference (model averaging)</li>
            </ul>
            <p>When you add <code>weight_decay=0.01</code> in PyTorch,
            you‚Äôre implicitly doing MAP with a Gaussian prior!</p>
            <div class="sourceCode" id="cb124"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># These are equivalent:</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Explicit L2 regularization</span></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy(pred, target) <span class="op">+</span> <span class="fl">0.01</span> <span class="op">*</span> <span class="bu">sum</span>(p.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>() <span class="cf">for</span> p <span class="kw">in</span> model.parameters())</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Weight decay in optimizer (MAP with Gaussian prior)</span></span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), weight_decay<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div>
            <hr />
            <h3 id="concrete-example-coin-flip-with-prior">Concrete
            Example: Coin Flip with Prior</h3>
            <p><strong>Scenario</strong>: You flip a coin 3 times and
            see 3 heads. What‚Äôs <span class="math inline">\(p\)</span>
            (probability of heads)?</p>
            <p><strong>MLE</strong>: <span
            class="math display">\[\hat{p}_{MLE} =
            \frac{\text{heads}}{\text{total}} = \frac{3}{3} =
            1.0\]</span></p>
            <p>MLE says the coin <em>always</em> lands heads! This seems
            overconfident.</p>
            <p><strong>MAP with Beta(2, 2) prior</strong> (belief that
            coins are usually fair-ish):</p>
            <p>The Beta prior is: <span class="math inline">\(P(p)
            \propto p^{\alpha-1}(1-p)^{\beta-1}\)</span> with <span
            class="math inline">\(\alpha=\beta=2\)</span>.</p>
            <p>The posterior is: <span class="math inline">\(P(p|D)
            \propto p^{3}(1-p)^{0} \cdot p^{1}(1-p)^{1} =
            p^{4}(1-p)^{1}\)</span></p>
            <p>This is Beta(5, 2), with mode: <span
            class="math display">\[\hat{p}_{MAP} = \frac{\alpha -
            1}{\alpha + \beta - 2} = \frac{4}{5} = 0.8\]</span></p>
            <p><strong>MAP gives 0.8</strong> ‚Äî still high (we saw 3
            heads!), but tempered by our prior belief that coins aren‚Äôt
            usually extreme.</p>
            <hr />
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the difference
            between MLE and MAP?‚Äù</p>
            <p><strong>A</strong>: MLE maximizes <span
            class="math inline">\(P(\mathcal{D}|\theta)\)</span> ‚Äî the
            likelihood of data given parameters. MAP maximizes <span
            class="math inline">\(P(\theta|\mathcal{D}) \propto
            P(\mathcal{D}|\theta)P(\theta)\)</span> ‚Äî the posterior,
            which is likelihood times prior. MAP incorporates prior
            beliefs about parameters, acting as regularization. With a
            Gaussian prior, MAP = MLE + L2 regularization. With a flat
            (uniform) prior, MAP reduces to MLE. Use MLE for large
            datasets; use MAP when data is limited or you have prior
            knowledge.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhy does L2 regularization
            correspond to a Gaussian prior?‚Äù</p>
            <p><strong>A</strong>: The Gaussian prior is <span
            class="math inline">\(P(\theta) \propto
            \exp(-\frac{\lambda}{2}\|\theta\|^2)\)</span>. Taking the
            log gives <span class="math inline">\(\log P(\theta) =
            -\frac{\lambda}{2}\|\theta\|^2 + \text{const}\)</span>. The
            MAP objective is <span class="math inline">\(\arg\max[\log
            P(\mathcal{D}|\theta) + \log P(\theta)]\)</span>, which
            equals <span
            class="math inline">\(\arg\min[\mathcal{L}(\theta) +
            \frac{\lambda}{2}\|\theta\|^2]\)</span> after negating. The
            <span
            class="math inline">\(\frac{\lambda}{2}\|\theta\|^2\)</span>
            term is exactly L2 regularization. So weight decay in neural
            networks is equivalent to MAP estimation with a Gaussian
            prior on weights.</p>
            <hr />
            <h3 id="markov-chains">Markov Chains</h3>
            <p>A <strong>Markov chain</strong> is a sequence of random
            variables where each state depends only on the previous
            state:</p>
            <p><span class="math display">\[P(X_{t+1}|X_t, X_{t-1},
            \ldots, X_1) = P(X_{t+1}|X_t)\]</span></p>
            <p><strong>The Markov Property (Verbal
            Explanation)</strong>:</p>
            <p>Think of it as ‚Äúmemorylessness‚Äù ‚Äî to predict the future,
            you only need to know the present, not the entire history.
            Yesterday‚Äôs weather doesn‚Äôt help predict tomorrow‚Äôs weather
            if you already know today‚Äôs weather.</p>
            <h3 id="worked-example-weather-markov-chain">Worked Example:
            Weather Markov Chain</h3>
            <p>Consider weather that‚Äôs either <strong>Sunny</strong> (S)
            or <strong>Rainy</strong> (R):</p>
            <pre><code>          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ      0.7            ‚îÇ
          ‚ñº                     ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     0.3      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ      ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ      ‚îÇ
       ‚îÇ Sunny‚îÇ              ‚îÇ Rainy‚îÇ
       ‚îÇ      ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     0.4      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñ≤                     ‚îÇ
          ‚îÇ      0.6            ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Transition Matrix</strong>:</p>
            <p><span class="math display">\[\mathbf{P} = \begin{bmatrix}
            P(S|S) &amp; P(R|S) \\ P(S|R) &amp; P(R|R) \end{bmatrix} =
            \begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6
            \end{bmatrix}\]</span></p>
            <p><strong>Interpretation</strong>: Each row must sum to 1
            (from each state, you go somewhere).</p>
            <p><strong>Multi-Step Transitions</strong>: What‚Äôs the
            probability of sunny in 2 days given sunny today?</p>
            <p><span class="math display">\[P(\text{sunny in 2 days} |
            \text{sunny today}) = [\mathbf{P}^2]_{11}\]</span></p>
            <p><span class="math display">\[\mathbf{P}^2 =
            \begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6 \end{bmatrix}
            \begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6 \end{bmatrix}
            = \begin{bmatrix} 0.61 &amp; 0.39 \\ 0.52 &amp; 0.48
            \end{bmatrix}\]</span></p>
            <p>So 61% chance of sunny in 2 days, given sunny today.</p>
            <p><strong>Stationary Distribution</strong>: The long-run
            equilibrium where the distribution stops changing.</p>
            <p>Solve <span class="math inline">\(\boldsymbol{\pi} =
            \boldsymbol{\pi}\mathbf{P}\)</span> (left eigenvector with
            eigenvalue 1):</p>
            <p><span class="math display">\[[\pi_S, \pi_R] = [\pi_S,
            \pi_R] \begin{bmatrix} 0.7 &amp; 0.3 \\ 0.4 &amp; 0.6
            \end{bmatrix}\]</span></p>
            <p>This gives: - <span class="math inline">\(\pi_S =
            0.7\pi_S + 0.4\pi_R\)</span> - <span
            class="math inline">\(\pi_R = 0.3\pi_S +
            0.6\pi_R\)</span></p>
            <p>From first equation: <span class="math inline">\(0.3\pi_S
            = 0.4\pi_R\)</span> ‚Üí <span class="math inline">\(\pi_S =
            \frac{4}{3}\pi_R\)</span></p>
            <p>With constraint <span class="math inline">\(\pi_S + \pi_R
            = 1\)</span>: <span class="math display">\[\frac{4}{3}\pi_R
            + \pi_R = 1 \implies \pi_R = \frac{3}{7}, \quad \pi_S =
            \frac{4}{7}\]</span></p>
            <p><strong>Result</strong>: In the long run, it‚Äôs sunny ~57%
            of the time and rainy ~43%, regardless of starting
            state!</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is the Markov
            property?‚Äù</p>
            <p><strong>A</strong>: The Markov property states that the
            future is conditionally independent of the past given the
            present: <span class="math inline">\(P(X_{t+1}|X_t, X_{t-1},
            \ldots) = P(X_{t+1}|X_t)\)</span>. Intuitively, the current
            state contains all relevant information for predicting the
            next state ‚Äî the history doesn‚Äôt provide additional
            information. This ‚Äúmemorylessness‚Äù is fundamental to Markov
            chains, MDPs in reinforcement learning, and HMMs.</p>
            <p><strong>ML Applications</strong>:</p>
            <ul>
            <li><strong>Language models</strong>: N-gram models assume
            word depends only on previous n-1 words</li>
            <li><strong>PageRank</strong>: Web pages as states, links as
            transitions, stationary distribution = importance</li>
            <li><strong>MCMC sampling</strong>: Construct chains whose
            stationary distribution is our target</li>
            <li><strong>Reinforcement learning</strong>: MDP is a Markov
            chain with actions</li>
            </ul>
            <hr />
            <h3 id="mcmc-markov-chain-monte-carlo">MCMC: Markov Chain
            Monte Carlo</h3>
            <p><strong>The Problem</strong>: We want to sample from a
            complex distribution <span
            class="math inline">\(p(x)\)</span> that we can‚Äôt directly
            sample from, but we can evaluate (up to a normalizing
            constant).</p>
            <p><strong>Why we need it</strong>: - Bayesian inference:
            posterior <span class="math inline">\(p(\theta|D) \propto
            p(D|\theta)p(\theta)\)</span> - We can compute the
            numerator, but the normalizing constant requires intractable
            integrals</p>
            <p><strong>The Idea</strong>: Construct a Markov chain whose
            stationary distribution is exactly <span
            class="math inline">\(p(x)\)</span>. Run the chain long
            enough, and samples will come from <span
            class="math inline">\(p(x)\)</span>!</p>
            <h3 id="metropolis-hastings-algorithm">Metropolis-Hastings
            Algorithm</h3>
            <p>The most fundamental MCMC algorithm:</p>
            <p><strong>Setup</strong>:</p>
            <ul>
            <li><span class="math inline">\(p(x)\)</span>: Target
            distribution (we want to sample from this)</li>
            <li><span class="math inline">\(q(x&#39;|x)\)</span>:
            Proposal distribution (how we propose new samples)</li>
            </ul>
            <p><strong>Algorithm</strong>:</p>
            <pre><code>1. Start at some x‚ÇÄ
2. For t = 1, 2, 3, ...
   a. Propose: x&#39; ~ q(x&#39;|x‚Çú)
   b. Compute acceptance ratio:
      Œ± = min(1, [p(x&#39;) √ó q(x‚Çú|x&#39;)] / [p(x‚Çú) √ó q(x&#39;|x‚Çú)])
   c. Accept or reject:
      - With probability Œ±: x‚Çú‚Çä‚ÇÅ = x&#39; (accept)
      - Otherwise: x‚Çú‚Çä‚ÇÅ = x‚Çú (reject, stay put)</code></pre>
            <p><strong>Why it works</strong>: The accept/reject step
            ensures ‚Äúdetailed balance‚Äù ‚Äî flow between any two states is
            equal in both directions, guaranteeing <span
            class="math inline">\(p(x)\)</span> is the stationary
            distribution.</p>
            <h3
            id="simple-mcmc-example-sampling-from-a-mixture-of-gaussians">Simple
            MCMC Example: Sampling from a Mixture of Gaussians</h3>
            <p>Target: <span class="math inline">\(p(x) = 0.3 \cdot
            \mathcal{N}(x; -2, 0.5) + 0.7 \cdot \mathcal{N}(x; 2,
            1)\)</span></p>
            <div class="sourceCode" id="cb127"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_pdf(x):</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Mixture of two Gaussians (unnormalized is fine!)&quot;&quot;&quot;</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.3</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">+</span> <span class="dv">2</span>) <span class="op">/</span> <span class="fl">0.5</span>)<span class="op">**</span><span class="dv">2</span>) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a>           <span class="fl">0.7</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> <span class="dv">2</span>) <span class="op">/</span> <span class="fl">1.0</span>)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings(n_samples, proposal_std<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> []</span>
<span id="cb127-10"><a href="#cb127-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="fl">0.0</span>  <span class="co"># Starting point</span></span>
<span id="cb127-11"><a href="#cb127-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb127-12"><a href="#cb127-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb127-13"><a href="#cb127-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Propose new point (symmetric proposal: q(x&#39;|x) = q(x|x&#39;))</span></span>
<span id="cb127-14"><a href="#cb127-14" aria-hidden="true" tabindex="-1"></a>        x_proposed <span class="op">=</span> x <span class="op">+</span> np.random.normal(<span class="dv">0</span>, proposal_std)</span>
<span id="cb127-15"><a href="#cb127-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb127-16"><a href="#cb127-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Acceptance ratio (symmetric proposal simplifies to p(x&#39;)/p(x))</span></span>
<span id="cb127-17"><a href="#cb127-17" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, target_pdf(x_proposed) <span class="op">/</span> target_pdf(x))</span>
<span id="cb127-18"><a href="#cb127-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb127-19"><a href="#cb127-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accept or reject</span></span>
<span id="cb127-20"><a href="#cb127-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.random() <span class="op">&lt;</span> alpha:</span>
<span id="cb127-21"><a href="#cb127-21" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_proposed  <span class="co"># Accept</span></span>
<span id="cb127-22"><a href="#cb127-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># else: stay at current x (reject)</span></span>
<span id="cb127-23"><a href="#cb127-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb127-24"><a href="#cb127-24" aria-hidden="true" tabindex="-1"></a>        samples.append(x)</span>
<span id="cb127-25"><a href="#cb127-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb127-26"><a href="#cb127-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(samples)</span>
<span id="cb127-27"><a href="#cb127-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-28"><a href="#cb127-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MCMC</span></span>
<span id="cb127-29"><a href="#cb127-29" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> metropolis_hastings(<span class="dv">10000</span>)</span>
<span id="cb127-30"><a href="#cb127-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Discard first 1000 as &quot;burn-in&quot; (chain needs time to reach stationary dist)</span></span>
<span id="cb127-31"><a href="#cb127-31" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> samples[<span class="dv">1000</span>:]</span></code></pre></div>
            <p><strong>Key Concepts</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 60%" />
            </colgroup>
            <thead>
            <tr>
            <th>Term</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Burn-in</strong></td>
            <td>Initial samples to discard (chain hasn‚Äôt converged
            yet)</td>
            </tr>
            <tr>
            <td><strong>Mixing</strong></td>
            <td>How quickly chain explores the state space</td>
            </tr>
            <tr>
            <td><strong>Autocorrelation</strong></td>
            <td>Correlation between successive samples</td>
            </tr>
            <tr>
            <td><strong>Effective sample size</strong></td>
            <td>Accounts for correlation ‚Äî often much smaller than
            actual samples</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is MCMC and why do we
            need it?‚Äù</p>
            <p><strong>A</strong>: MCMC (Markov Chain Monte Carlo)
            constructs a Markov chain whose stationary distribution
            equals our target distribution <span
            class="math inline">\(p(x)\)</span>. We need it when we can
            evaluate <span class="math inline">\(p(x)\)</span> (up to a
            constant) but can‚Äôt directly sample from it ‚Äî common in
            Bayesian inference where <span
            class="math inline">\(p(\theta|D) \propto
            p(D|\theta)p(\theta)\)</span> involves intractable
            normalizing constants. Metropolis-Hastings proposes new
            samples and accepts/rejects them to ensure the chain
            converges to <span class="math inline">\(p(x)\)</span>.
            After enough iterations (past burn-in), samples approximate
            draws from the target distribution.</p>
            <p><img src="figures/mcmc_sampling.png"
            alt="MCMC Sampling" /> <em>Figure: Metropolis-Hastings MCMC
            sampling from a mixture of Gaussians. Top: trace plots
            showing the chain exploring both modes. Bottom-left:
            histogram of samples matches the target distribution.
            Bottom-right: autocorrelation decays, indicating
            mixing.</em></p>
            <p><strong>MCMC in Modern ML</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>Application</th>
            <th>Use of MCMC</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Bayesian Neural Networks</strong></td>
            <td>Sample weight posteriors instead of point estimates</td>
            </tr>
            <tr>
            <td><strong>Latent Variable Models</strong></td>
            <td>Sample latent variables when integration is
            intractable</td>
            </tr>
            <tr>
            <td><strong>LLM Sampling</strong></td>
            <td>Not typically MCMC, but temperature sampling has similar
            flavor</td>
            </tr>
            <tr>
            <td><strong>Energy-Based Models</strong></td>
            <td>Langevin dynamics is a continuous-time MCMC variant</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="key-statistical-concepts">Key Statistical
            Concepts</h3>
            <p><strong>Expectation</strong>: <span
            class="math inline">\(\mathbb{E}[X] = \sum_x x \cdot P(X =
            x)\)</span> or <span class="math inline">\(\int x \cdot p(x)
            dx\)</span></p>
            <p><strong>Variance</strong>: <span
            class="math inline">\(\text{Var}(X) = \mathbb{E}[(X -
            \mathbb{E}[X])^2] = \mathbb{E}[X^2] -
            \mathbb{E}[X]^2\)</span></p>
            <p><strong>Covariance</strong>: <span
            class="math inline">\(\text{Cov}(X, Y) = \mathbb{E}[(X -
            \mathbb{E}[X])(Y - \mathbb{E}[Y])]\)</span></p>
            <p><strong>Independence</strong>: <span
            class="math inline">\(P(X, Y) = P(X)P(Y)\)</span> iff <span
            class="math inline">\(X\)</span> and <span
            class="math inline">\(Y\)</span> are independent</p>
            <p><strong>Conditional Independence</strong>: <span
            class="math inline">\(X \perp Y | Z\)</span> means <span
            class="math inline">\(P(X, Y|Z) = P(X|Z)P(Y|Z)\)</span></p>
            <hr />
            <h3 id="the-central-limit-theorem-clt">The Central Limit
            Theorem (CLT)</h3>
            <blockquote>
            <p><strong>üîë One of the most important theorems in
            statistics!</strong> The CLT explains why the normal
            distribution appears everywhere in nature and why averaging
            ‚Äúworks‚Äù in machine learning.</p>
            </blockquote>
            <p><strong>Statement</strong>: The average of many
            independent random variables tends toward a normal
            distribution, <strong>regardless of the original
            distribution</strong>.</p>
            <p><span class="math display">\[\frac{\bar{X}_n -
            \mu}{\sigma / \sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)
            \quad \text{as } n \to \infty\]</span></p>
            <hr />
            <h3 id="understanding-the-notation">Understanding the
            Notation</h3>
            <p>Let‚Äôs break down this formula piece by piece:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 28%" />
            <col style="width: 46%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            <th>Plain English</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(X_1, X_2, \ldots,
            X_n\)</span></td>
            <td>i.i.d. random variables</td>
            <td>Your individual data points (e.g., individual dice
            rolls)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\bar{X}_n =
            \frac{1}{n}\sum_{i=1}^n X_i\)</span></td>
            <td>Sample mean</td>
            <td>The average of your n samples</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\mu\)</span></td>
            <td>True population mean</td>
            <td><span class="math inline">\(\mathbb{E}[X_i]\)</span> ‚Äî
            what you‚Äôd get averaging infinitely many samples</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\sigma\)</span></td>
            <td>True population standard deviation</td>
            <td>How spread out individual samples are</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\sigma /
            \sqrt{n}\)</span></td>
            <td><strong>Standard error</strong></td>
            <td>How spread out <em>sample means</em> are ‚Äî key
            insight!</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\xrightarrow{d}\)</span></td>
            <td><strong>Converges in distribution</strong></td>
            <td>As n grows, the distribution <em>approaches</em> (it‚Äôs a
            limit statement)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\mathcal{N}(0,
            1)\)</span></td>
            <td>Standard normal distribution</td>
            <td>Bell curve with mean=0, variance=1</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The <span
            class="math inline">\(\xrightarrow{d}\)</span>
            notation</strong> is probability theory shorthand for
            ‚Äúconverges in distribution.‚Äù It means as <span
            class="math inline">\(n \to \infty\)</span>, the CDF of the
            left side approaches the CDF of <span
            class="math inline">\(\mathcal{N}(0,1)\)</span>. It‚Äôs NOT a
            variable ‚Äî it‚Äôs describing the type of convergence!</p>
            <hr />
            <h3 id="what-the-formula-actually-says-step-by-step">What
            the Formula Actually Says (Step by Step)</h3>
            <p><strong>Step 1: Start with sample mean <span
            class="math inline">\(\bar{X}_n\)</span></strong></p>
            <p>When you average <span class="math inline">\(n\)</span>
            samples, the result <span
            class="math inline">\(\bar{X}_n\)</span> has:</p>
            <ul>
            <li>Mean: <span class="math inline">\(\mathbb{E}[\bar{X}_n]
            = \mu\)</span> (same as individual samples)</li>
            <li>Variance: <span
            class="math inline">\(\text{Var}(\bar{X}_n) =
            \frac{\sigma^2}{n}\)</span> (shrinks as n grows!)</li>
            </ul>
            <p><strong>Step 2: Center by subtracting <span
            class="math inline">\(\mu\)</span></strong></p>
            <p><span class="math inline">\(\bar{X}_n - \mu\)</span>
            shifts the distribution so its mean is 0.</p>
            <p><strong>Step 3: Standardize by dividing by <span
            class="math inline">\(\sigma/\sqrt{n}\)</span></strong></p>
            <p><span class="math inline">\(\frac{\bar{X}_n -
            \mu}{\sigma/\sqrt{n}}\)</span> now has:</p>
            <ul>
            <li>Mean = 0</li>
            <li>Variance = 1</li>
            </ul>
            <p><strong>Step 4: CLT says this standardized quantity ‚Üí
            <span
            class="math inline">\(\mathcal{N}(0,1)\)</span></strong></p>
            <p>No matter what distribution the original <span
            class="math inline">\(X_i\)</span> came from!</p>
            <hr />
            <h3 id="why-does-averaging-produce-normality-intuition">Why
            Does Averaging Produce Normality? (Intuition)</h3>
            <p><strong>Informal Explanation</strong>: When you add many
            independent random variables:</p>
            <ul>
            <li>Extreme values in one direction tend to be ‚Äúcanceled
            out‚Äù by extreme values in the other</li>
            <li>The more variables you add, the more cancellation
            happens</li>
            <li>What‚Äôs left is a smooth, symmetric pile-up around the
            mean</li>
            <li>This pile-up shape is the bell curve!</li>
            </ul>
            <p><strong>Another way to think about it</strong>: The
            normal distribution is the ‚Äúmost random‚Äù distribution for a
            given mean and variance (maximum entropy). When you mix many
            random things together, you lose the specific structure of
            each one and approach this ‚Äúmaximally random‚Äù state.</p>
            <hr />
            <h3 id="worked-example-dice-rolling">Worked Example: Dice
            Rolling</h3>
            <p>Let‚Äôs see CLT in action with dice!</p>
            <p><strong>Setup</strong>: Roll a fair 6-sided die. Each
            roll has: - <span class="math inline">\(\mu =
            \frac{1+2+3+4+5+6}{6} = 3.5\)</span> - <span
            class="math inline">\(\sigma^2 = \frac{(1-3.5)^2 + (2-3.5)^2
            + \cdots + (6-3.5)^2}{6} = \frac{35}{12} \approx
            2.92\)</span> - <span class="math inline">\(\sigma \approx
            1.71\)</span></p>
            <p><strong>Single die roll (n=1)</strong>: Uniform
            distribution over {1,2,3,4,5,6} ‚Äî clearly NOT normal!</p>
            <p><strong>Average of 2 dice (n=2)</strong>: Triangular
            shape, peak at 3.5 ‚Äî getting closer to normal</p>
            <p><strong>Average of 30 dice (n=30)</strong>: Almost
            perfectly normal with: - Mean = 3.5 (unchanged) - Standard
            error = <span class="math inline">\(\frac{1.71}{\sqrt{30}}
            \approx 0.31\)</span> (much smaller spread!)</p>
            <div class="sourceCode" id="cb128"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate CLT with dice</span></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>n_experiments <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_dice <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">30</span>]:</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each experiment: roll n_dice dice and take the mean</span></span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>    means <span class="op">=</span> [np.mean(np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, n_dice)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_experiments)]</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;n=</span><span class="sc">{</span>n_dice<span class="sc">}</span><span class="ss">: mean=</span><span class="sc">{</span>np<span class="sc">.</span>mean(means)<span class="sc">:.2f}</span><span class="ss">, std=</span><span class="sc">{</span>np<span class="sc">.</span>std(means)<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Output:</span></span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a><span class="co"># n=1:  mean=3.50, std=1.71  (original distribution)</span></span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a><span class="co"># n=2:  mean=3.50, std=1.21  (1.71/‚àö2 = 1.21)</span></span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a><span class="co"># n=30: mean=3.50, std=0.31  (1.71/‚àö30 = 0.31)</span></span></code></pre></div>
            <p><img src="figures/clt_dice_example.png"
            alt="Central Limit Theorem - Dice Example" /> <em>Figure:
            CLT with dice rolls. Left: Single die roll is uniform (not
            normal). As we average more dice, the distribution of sample
            means approaches normal. The spread (standard error) shrinks
            as <span
            class="math inline">\(\sigma/\sqrt{n}\)</span>.</em></p>
            <hr />
            <h3 id="clt-with-different-starting-distributions">CLT with
            Different Starting Distributions</h3>
            <p>The remarkable thing about CLT: it works
            <strong>regardless</strong> of the original
            distribution!</p>
            <p><img src="figures/clt_visualization.png"
            alt="Central Limit Theorem Visualization" /> <em>Figure: CLT
            in action with three very different starting distributions
            (uniform, exponential, bimodal). By n=30, all converge to
            normal! The black curve is the theoretical CLT
            prediction.</em></p>
            <p><strong>Key Observations</strong>: - Row 1 (Uniform):
            Starts flat, becomes bell-shaped - Row 2 (Exponential):
            Starts heavily skewed right, becomes symmetric - Row 3
            (Bimodal): Starts with TWO peaks, converges to ONE bell
            curve!</p>
            <hr />
            <h3 id="the-standard-error-sigma-sqrtn">The Standard Error:
            <span class="math inline">\(\sigma / \sqrt{n}\)</span></h3>
            <p>This is the <strong>most practically important</strong>
            part of CLT!</p>
            <p><span class="math display">\[\text{Standard Error (SE)} =
            \frac{\sigma}{\sqrt{n}}\]</span></p>
            <p><strong>What it tells us</strong>: The standard deviation
            of sample means decreases as <span
            class="math inline">\(1/\sqrt{n}\)</span></p>
            <table>
            <thead>
            <tr>
            <th>Sample Size <span class="math inline">\(n\)</span></th>
            <th>Standard Error</th>
            <th>Relative to Original</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td><span class="math inline">\(\sigma\)</span></td>
            <td>100%</td>
            </tr>
            <tr>
            <td>4</td>
            <td><span class="math inline">\(\sigma/2\)</span></td>
            <td>50%</td>
            </tr>
            <tr>
            <td>16</td>
            <td><span class="math inline">\(\sigma/4\)</span></td>
            <td>25%</td>
            </tr>
            <tr>
            <td>100</td>
            <td><span class="math inline">\(\sigma/10\)</span></td>
            <td>10%</td>
            </tr>
            <tr>
            <td>10,000</td>
            <td><span class="math inline">\(\sigma/100\)</span></td>
            <td>1%</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Diminishing returns</strong>: To halve your
            uncertainty, you need 4√ó more samples!</p>
            <hr />
            <h3 id="why-clt-matters-for-machine-learning">Why CLT
            Matters for Machine Learning</h3>
            <p><strong>1. Mini-Batch Gradient Descent</strong></p>
            <p>The gradient estimate from a mini-batch is an average of
            individual sample gradients:</p>
            <p><span class="math display">\[\hat{g} =
            \frac{1}{B}\sum_{i=1}^{B} \nabla L(x_i)\]</span></p>
            <p>By CLT, this estimate is approximately normal, with
            standard error <span class="math inline">\(\propto
            1/\sqrt{B}\)</span>.</p>
            <ul>
            <li><strong>Larger batch</strong> ‚Üí more stable gradients
            (smaller variance)</li>
            <li><strong>But</strong>: 4√ó batch size only gives 2√ó
            reduction in variance</li>
            <li>This justifies using moderate batch sizes (e.g., 32-256)
            rather than huge ones</li>
            </ul>
            <p><strong>2. Confidence Intervals for Model
            Evaluation</strong></p>
            <p>When you measure accuracy on a test set of <span
            class="math inline">\(n\)</span> samples, the true accuracy
            is:</p>
            <p><span class="math display">\[\text{True accuracy} \in
            \text{Measured accuracy} \pm 1.96 \times
            \frac{\sigma}{\sqrt{n}}\]</span></p>
            <p>(95% confidence interval, using CLT)</p>
            <p><strong>3. Statistical Tests</strong></p>
            <p>The t-test, z-test, and many other tests assume sample
            means are normal ‚Äî CLT justifies this assumption even when
            individual observations aren‚Äôt normal.</p>
            <p><strong>4. Xavier/He Initialization</strong></p>
            <p>Weights are initialized as sums of small random values.
            By CLT, the distribution of weighted sums (activations)
            tends toward normal, which helps with stable training.</p>
            <hr />
            <h3 id="interview-questions">Interview Questions</h3>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is the Central Limit
            Theorem and why does it matter for ML?‚Äù</p>
            <p><strong>A</strong>: The CLT states that when you average
            many independent random variables, the result is
            approximately normally distributed ‚Äî regardless of the
            original distribution. The key formula is <span
            class="math inline">\(\frac{\bar{X}_n -
            \mu}{\sigma/\sqrt{n}} \to \mathcal{N}(0,1)\)</span>.</p>
            <p>For ML:</p>
            <ol type="1">
            <li><strong>Mini-batch gradients</strong> are averages, so
            they‚Äôre approximately normal ‚Äî this justifies SGD and helps
            us reason about optimization</li>
            <li><strong>Variance decreases as <span
            class="math inline">\(1/\sqrt{n}\)</span></strong>,
            explaining why larger batches give more stable (but not
            proportionally better) gradients</li>
            <li><strong>Statistical tests</strong> for model comparison
            rely on CLT to assume normality of sample means</li>
            <li><strong>Confidence intervals</strong> for model
            evaluation use CLT to quantify uncertainty</li>
            </ol>
            <p><strong>Interview Q</strong>: ‚ÄúWhat does the <span
            class="math inline">\(\xrightarrow{d}\)</span> notation mean
            in the CLT statement?‚Äù</p>
            <p><strong>A</strong>: The <span
            class="math inline">\(\xrightarrow{d}\)</span> means
            ‚Äúconverges in distribution‚Äù ‚Äî it‚Äôs saying that as <span
            class="math inline">\(n \to \infty\)</span>, the probability
            distribution of the quantity on the left approaches the
            normal distribution. It‚Äôs a statement about the limiting
            behavior of the CDF, not about individual values converging.
            This is distinct from almost sure convergence or convergence
            in probability.</p>
            <p><strong>Interview Q</strong>: ‚ÄúHow does batch size affect
            gradient variance, and what does CLT tell us?‚Äù</p>
            <p><strong>A</strong>: By CLT, the standard error of the
            batch gradient is <span
            class="math inline">\(\sigma/\sqrt{B}\)</span> where <span
            class="math inline">\(B\)</span> is batch size. Doubling the
            batch size reduces standard error by only <span
            class="math inline">\(\sqrt{2} \approx 1.41\)</span>, not by
            2. This means we get <strong>diminishing returns</strong>
            from larger batches: 4√ó batch size ‚Üí 2√ó reduction in
            variance. This is why practitioners often use moderate batch
            sizes and accept some gradient noise rather than always
            maximizing batch size.</p>
            <hr />
            <h3
            id="hypothesis-testing-and-statistical-inference">Hypothesis
            Testing and Statistical Inference</h3>
            <p>The CLT tells us that sample means are approximately
            normal. This is the foundation for <strong>hypothesis
            testing</strong> ‚Äî a framework for making decisions about
            populations based on sample data.</p>
            <h4 id="the-hypothesis-testing-framework">The Hypothesis
            Testing Framework</h4>
            <p><strong>The Setup</strong>:</p>
            <ol type="1">
            <li><strong>Null hypothesis (<span
            class="math inline">\(H_0\)</span>)</strong>: The ‚Äúdefault‚Äù
            assumption (typically: no effect, no difference)</li>
            <li><strong>Alternative hypothesis (<span
            class="math inline">\(H_1\)</span> or <span
            class="math inline">\(H_a\)</span>)</strong>: What we‚Äôre
            trying to prove (typically: there IS an effect)</li>
            <li><strong>Test statistic</strong>: A number computed from
            data that measures evidence against <span
            class="math inline">\(H_0\)</span></li>
            <li><strong>P-value</strong>: Probability of seeing data
            this extreme (or more) IF <span
            class="math inline">\(H_0\)</span> is true</li>
            <li><strong>Significance level (<span
            class="math inline">\(\alpha\)</span>)</strong>: Threshold
            for ‚Äúextreme enough‚Äù (commonly 0.05)</li>
            </ol>
            <p><strong>Decision Rule</strong>: Reject <span
            class="math inline">\(H_0\)</span> if p-value &lt; <span
            class="math inline">\(\alpha\)</span></p>
            <p><strong>Example</strong>: Testing if a new model is
            better than baseline</p>
            <ul>
            <li><span class="math inline">\(H_0\)</span>: New model
            accuracy = Baseline accuracy (no improvement)</li>
            <li><span class="math inline">\(H_1\)</span>: New model
            accuracy &gt; Baseline accuracy (improvement exists)</li>
            <li>Collect test data, compute accuracy difference</li>
            <li>If p-value &lt; 0.05 ‚Üí ‚Äústatistically significant
            improvement‚Äù</li>
            </ul>
            <hr />
            <h4 id="type-i-and-type-ii-errors">Type I and Type II
            Errors</h4>
            <p>When we make a decision, we can be wrong in two ways:</p>
            <p><strong>Hypothesis Testing Confusion Matrix</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 38%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th style="text-align: center;"><strong>Reality: <span
            class="math inline">\(H_0\)</span> True</strong></th>
            <th style="text-align: center;"><strong>Reality: <span
            class="math inline">\(H_0\)</span> False</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td></td>
            <td style="text-align: center;"><em>(No effect
            exists)</em></td>
            <td style="text-align: center;"><em>(Effect
            exists)</em></td>
            </tr>
            <tr>
            <td><strong>Decision: Reject <span
            class="math inline">\(H_0\)</span></strong></td>
            <td style="text-align: center;">‚ùå <strong>Type I Error
            (Œ±)</strong></td>
            <td style="text-align: center;">‚úì <strong>Correct
            Decision</strong></td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: center;">False Positive</td>
            <td style="text-align: center;">True Positive (Power =
            1-Œ≤)</td>
            </tr>
            <tr>
            <td><strong>Decision: Fail to Reject <span
            class="math inline">\(H_0\)</span></strong></td>
            <td style="text-align: center;">‚úì <strong>Correct
            Decision</strong></td>
            <td style="text-align: center;">‚ùå <strong>Type II Error
            (Œ≤)</strong></td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: center;">True Negative</td>
            <td style="text-align: center;">False Negative</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Memory Trick</strong>: - <strong>Type I</strong>
            = ‚ÄúFalse alarm‚Äù ‚Äî you rejected <span
            class="math inline">\(H_0\)</span> but shouldn‚Äôt have (Œ± =
            significance level) - <strong>Type II</strong> = ‚ÄúMissed
            detection‚Äù ‚Äî you failed to reject <span
            class="math inline">\(H_0\)</span> but should have (Œ≤ = miss
            rate)</p>
            <p><strong>Visual Intuition</strong>: The two error types
            correspond to different regions under the null and
            alternative distributions:</p>
            <p><img src="figures/type1_type2_errors.png"
            alt="Type I and Type II Errors" /> <em>Figure: Type I error
            (Œ±) is the area under H‚ÇÄ beyond the critical value ‚Äî
            rejecting H‚ÇÄ when it‚Äôs true. Type II error (Œ≤) is the area
            under H‚ÇÅ before the critical value ‚Äî failing to reject H‚ÇÄ
            when H‚ÇÅ is true. Power (1-Œ≤) is the remaining area under
            H‚ÇÅ.</em></p>
            <p><strong>Key Probabilities</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 23%" />
            <col style="width: 46%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Name</th>
            <th>Definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\alpha\)</span></td>
            <td><strong>Significance level</strong></td>
            <td><span class="math inline">\(P(\text{reject } H_0 \mid
            H_0 \text{ true})\)</span> = P(Type I error)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta\)</span></td>
            <td><strong>Type II error rate</strong></td>
            <td><span class="math inline">\(P(\text{fail to reject } H_0
            \mid H_0 \text{ false})\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(1 - \beta\)</span></td>
            <td><strong>Power</strong></td>
            <td><span class="math inline">\(P(\text{reject } H_0 \mid
            H_0 \text{ false})\)</span> = P(detecting real effect)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The Trade-off</strong>: You can‚Äôt minimize both
            errors simultaneously!</p>
            <ul>
            <li>Lower <span class="math inline">\(\alpha\)</span>
            (stricter threshold) ‚Üí fewer false positives BUT more false
            negatives</li>
            <li>Higher <span class="math inline">\(\alpha\)</span>
            (looser threshold) ‚Üí fewer false negatives BUT more false
            positives</li>
            </ul>
            <p><strong>Analogy to ML Classification</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Hypothesis Testing</th>
            <th>ML Classification</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Type I error (FP)</td>
            <td>False Positive Rate = FP/(FP+TN)</td>
            </tr>
            <tr>
            <td>Type II error (FN)</td>
            <td>False Negative Rate = FN/(FN+TP)</td>
            </tr>
            <tr>
            <td>Power (<span
            class="math inline">\(1-\beta\)</span>)</td>
            <td>Recall/Sensitivity = TP/(TP+FN)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(1-\alpha\)</span></td>
            <td>Specificity = TN/(TN+FP)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the difference
            between Type I and Type II errors?‚Äù</p>
            <p><strong>A</strong>: Type I error (false positive) is
            rejecting a true null hypothesis ‚Äî concluding there‚Äôs an
            effect when there isn‚Äôt one. Type II error (false negative)
            is failing to reject a false null ‚Äî missing a real effect.
            In ML terms, if we‚Äôre testing whether a new model is better:
            Type I = claiming improvement when there‚Äôs none (could waste
            resources deploying a useless model). Type II = missing a
            real improvement (could miss deploying a better model). The
            significance level <span
            class="math inline">\(\alpha\)</span> controls Type I; power
            (<span class="math inline">\(1-\beta\)</span>) controls Type
            II. There‚Äôs a trade-off ‚Äî reducing one typically increases
            the other.</p>
            <hr />
            <h4 id="mean-median-and-mode">Mean, Median, and Mode</h4>
            <p>Understanding measures of central tendency and their
            behavior in different distributions is fundamental for data
            analysis and ML preprocessing.</p>
            <p><strong>Definitions</strong>:</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 16%" />
            <col style="width: 22%" />
            <col style="width: 16%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>Measure</th>
            <th>Definition</th>
            <th>Formula</th>
            <th>Sensitivity to Outliers</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Mean</strong></td>
            <td>Sum divided by count (average)</td>
            <td><span class="math inline">\(\bar{x} =
            \frac{1}{n}\sum_{i=1}^{n} x_i\)</span></td>
            <td>High (outliers pull it)</td>
            </tr>
            <tr>
            <td><strong>Median</strong></td>
            <td>Middle value when sorted</td>
            <td>Middle element(s) of sorted data</td>
            <td>Low (robust)</td>
            </tr>
            <tr>
            <td><strong>Mode</strong></td>
            <td>Most frequent value</td>
            <td>Value with highest frequency</td>
            <td>None</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Relationship in Skewed
            Distributions</strong>:</p>
            <p><img src="figures/skewed_distributions.png"
            alt="Skewed Distributions" /> <em>Figure: The relationship
            between Mean, Median, and Mode depends on skewness. Left:
            Symmetric ‚Äî all three are equal. Middle: Right-skewed
            (income, house prices) ‚Äî the long tail pulls the mean to the
            right. Right: Left-skewed ‚Äî the tail pulls mean to the
            left.</em></p>
            <table>
            <colgroup>
            <col style="width: 36%" />
            <col style="width: 26%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>Distribution Type</th>
            <th>Relationship</th>
            <th>Real-World Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Symmetric</strong></td>
            <td>Mean = Median = Mode</td>
            <td>Heights, test scores (graded on curve)</td>
            </tr>
            <tr>
            <td><strong>Right-skewed</strong></td>
            <td>Mode &lt; Median &lt; Mean</td>
            <td>Income, house prices, company sizes, page views</td>
            </tr>
            <tr>
            <td><strong>Left-skewed</strong></td>
            <td>Mean &lt; Median &lt; Mode</td>
            <td>Age at death in developed countries, exam scores (easy
            test)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why does skewness affect the mean but not the
            median?</strong></p>
            <p>The mean is a <em>balance point</em> ‚Äî every value
            contributes proportionally. One billionaire among 100 people
            dramatically shifts the mean income:</p>
            <div class="sourceCode" id="cb129"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>incomes <span class="op">=</span> [<span class="dv">50000</span>] <span class="op">*</span> <span class="dv">99</span> <span class="op">+</span> [<span class="dv">10000000</span>]  <span class="co"># 99 people at $50k, 1 billionaire</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="bu">sum</span>(incomes) <span class="op">/</span> <span class="bu">len</span>(incomes)   <span class="co"># $149,500 (misleading!)</span></span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>median <span class="op">=</span> <span class="bu">sorted</span>(incomes)[<span class="dv">50</span>]          <span class="co"># $50,000 (representative)</span></span></code></pre></div>
            <p>The median only cares about <em>position</em>, not
            magnitude ‚Äî the billionaire is just ‚Äúthe largest value.‚Äù</p>
            <p><strong>When to use which</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 30%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>Use</th>
            <th>When</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Mean</strong></td>
            <td>Symmetric data, no outliers</td>
            <td>Sensor measurements, standardized tests</td>
            </tr>
            <tr>
            <td><strong>Median</strong></td>
            <td>Skewed data, outliers present</td>
            <td>Income, house prices, latency metrics</td>
            </tr>
            <tr>
            <td><strong>Mode</strong></td>
            <td>Categorical data, ‚Äúmost popular‚Äù</td>
            <td>Favorite color, product ratings, most common word</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúGiven a right-skewed
            income distribution, which measure of central tendency
            should you report and why?‚Äù</p>
            <p><strong>A</strong>: <strong>Median</strong>. Income
            distributions are right-skewed because there‚Äôs no lower
            bound on how poor someone can be (minimum ~$0), but no upper
            bound on wealth (billionaires). A few extremely high earners
            pull the mean up, making it unrepresentative of the
            ‚Äútypical‚Äù person. The median represents what the middle
            person earns. For example, in the US, mean household income
            (~$100k) is significantly higher than median income (~$75k)
            due to this skew. This is why government statistics
            typically report median income.</p>
            <p><strong>Python Example</strong>:</p>
            <div class="sourceCode" id="cb130"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated right-skewed income data (log-normal distribution)</span></span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true" tabindex="-1"></a>incomes <span class="op">=</span> np.random.lognormal(mean<span class="op">=</span><span class="fl">10.5</span>, sigma<span class="op">=</span><span class="fl">0.8</span>, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true" tabindex="-1"></a>mean_income <span class="op">=</span> np.mean(incomes)</span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true" tabindex="-1"></a>median_income <span class="op">=</span> np.median(incomes)</span>
<span id="cb130-9"><a href="#cb130-9" aria-hidden="true" tabindex="-1"></a>mode_income <span class="op">=</span> incomes[np.argmax(np.bincount(incomes.astype(<span class="bu">int</span>)))]  <span class="co"># Approximate</span></span>
<span id="cb130-10"><a href="#cb130-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb130-11"><a href="#cb130-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean: $</span><span class="sc">{</span>mean_income<span class="sc">:,.0f}</span><span class="ss">&quot;</span>)      <span class="co"># Mean: $48,123</span></span>
<span id="cb130-12"><a href="#cb130-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Median: $</span><span class="sc">{</span>median_income<span class="sc">:,.0f}</span><span class="ss">&quot;</span>)  <span class="co"># Median: $36,315</span></span>
<span id="cb130-13"><a href="#cb130-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean &gt; Median: Right-skewed!&quot;</span>)   <span class="co"># Confirms right skew</span></span></code></pre></div>
            <hr />
            <h4 id="percentiles-and-quantiles">Percentiles and
            Quantiles</h4>
            <p><strong>Quantile</strong>: The <span
            class="math inline">\(q\)</span>-th quantile (where <span
            class="math inline">\(0 \leq q \leq 1\)</span>) is the value
            below which a fraction <span
            class="math inline">\(q\)</span> of the data falls.</p>
            <p><strong>Percentile</strong>: The <span
            class="math inline">\(p\)</span>-th percentile is the value
            below which <span class="math inline">\(p\%\)</span> of the
            data falls. It‚Äôs just a quantile expressed as a
            percentage:</p>
            <p><span class="math display">\[\text{Percentile } p =
            \text{Quantile } \frac{p}{100}\]</span></p>
            <p><strong>Key Quartiles</strong> (divide data into 4
            parts):</p>
            <table>
            <thead>
            <tr>
            <th>Name</th>
            <th>Percentile</th>
            <th>Quantile</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Q1</strong> (First Quartile)</td>
            <td>25th</td>
            <td>0.25</td>
            <td>25% of data below</td>
            </tr>
            <tr>
            <td><strong>Q2</strong> (Median)</td>
            <td>50th</td>
            <td>0.50</td>
            <td>50% of data below</td>
            </tr>
            <tr>
            <td><strong>Q3</strong> (Third Quartile)</td>
            <td>75th</td>
            <td>0.75</td>
            <td>75% of data below</td>
            </tr>
            <tr>
            <td><strong>IQR</strong></td>
            <td>‚Äî</td>
            <td>Q3 - Q1</td>
            <td>Middle 50% range</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Common Percentiles Reference</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Percentile</th>
            <th>Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>P1, P99</strong></td>
            <td>Extreme outlier thresholds</td>
            </tr>
            <tr>
            <td><strong>P5, P95</strong></td>
            <td>Outlier thresholds, confidence bounds</td>
            </tr>
            <tr>
            <td><strong>P10, P90</strong></td>
            <td>Decile boundaries</td>
            </tr>
            <tr>
            <td><strong>P25, P50, P75</strong></td>
            <td>Quartiles (Q1, median, Q3)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Computing Percentiles in Python</strong>:</p>
            <div class="sourceCode" id="cb131"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.random.normal(<span class="dv">100</span>, <span class="dv">15</span>, <span class="dv">1000</span>)  <span class="co"># 1000 samples</span></span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Single percentile</span></span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a>median <span class="op">=</span> np.percentile(data, <span class="dv">50</span>)</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Median (P50): </span><span class="sc">{</span>median<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Multiple percentiles at once</span></span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a>percentiles <span class="op">=</span> np.percentile(data, [<span class="dv">25</span>, <span class="dv">50</span>, <span class="dv">75</span>])</span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Q1: </span><span class="sc">{</span>percentiles[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">, Median: </span><span class="sc">{</span>percentiles[<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">, Q3: </span><span class="sc">{</span>percentiles[<span class="dv">2</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a><span class="co"># IQR calculation</span></span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a>Q1, Q3 <span class="op">=</span> np.percentile(data, [<span class="dv">25</span>, <span class="dv">75</span>])</span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a>IQR <span class="op">=</span> Q3 <span class="op">-</span> Q1</span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;IQR: </span><span class="sc">{</span>IQR<span class="sc">:.2f}</span><span class="ss">&quot;</span>)</span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantile function (equivalent to percentile/100)</span></span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a>q_50 <span class="op">=</span> np.quantile(data, <span class="fl">0.5</span>)  <span class="co"># Same as np.percentile(data, 50)</span></span></code></pre></div>
            <p><img src="figures/percentiles_quantiles.png"
            alt="Percentiles and Quantiles" /> <em>Figure: Left:
            Histogram showing key percentile positions (P1, P5, Q1,
            Median, Q3, P95, P99). Right: CDF interpretation ‚Äî to find a
            percentile, draw horizontal line from y-axis to curve, then
            drop to x-axis.</em></p>
            <p><strong>ML Applications of Percentiles</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 41%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th>Application</th>
            <th>Percentiles Used</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Outlier detection</strong></td>
            <td>Below P1 or above P99 (or P5/P95 for stricter)</td>
            </tr>
            <tr>
            <td><strong>Winsorization</strong></td>
            <td>Cap values at P1/P99 to reduce outlier impact</td>
            </tr>
            <tr>
            <td><strong>Latency metrics</strong></td>
            <td>P50 (median), P95, P99 response times</td>
            </tr>
            <tr>
            <td><strong>Quantile regression</strong></td>
            <td>Predict specific quantiles instead of mean</td>
            </tr>
            <tr>
            <td><strong>Data quality</strong></td>
            <td>Check if distributions match expected percentiles</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Latency Percentiles in ML Systems</strong>:</p>
            <p>When monitoring ML model serving, percentiles are more
            informative than averages:</p>
            <pre><code>Average latency: 50ms  ‚Üê Looks good, but...

P50:  45ms   (median user experience)
P90:  80ms   (10% of users wait this long)
P95:  150ms  (5% of users wait this long)
P99:  500ms  (1% have terrible experience!)

The average hides the long tail!</code></pre>
            <p><strong>Why P99 matters</strong>: In large-scale systems
            serving millions of requests, even 1% experiencing P99
            latency means thousands of slow responses per minute.</p>
            <hr />
            <h4 id="confidence-intervals">Confidence Intervals</h4>
            <p>A <strong>confidence interval</strong> provides a range
            of plausible values for a parameter, not just a point
            estimate.</p>
            <p><strong>Formula for Mean</strong> (known variance or
            large n):</p>
            <p><span class="math display">\[\text{CI} = \bar{x} \pm z^*
            \cdot \frac{\sigma}{\sqrt{n}}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(\bar{x}\)</span> = sample
            mean</li>
            <li><span class="math inline">\(z^*\)</span> = critical
            value from standard normal (1.96 for 95% CI)</li>
            <li><span class="math inline">\(\sigma\)</span> = population
            standard deviation</li>
            <li><span class="math inline">\(n\)</span> = sample
            size</li>
            </ul>
            <p><strong>Common Critical Values</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Confidence Level</th>
            <th><span class="math inline">\(z^*\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>90%</td>
            <td>1.645</td>
            </tr>
            <tr>
            <td>95%</td>
            <td>1.96</td>
            </tr>
            <tr>
            <td>99%</td>
            <td>2.576</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: You test a model on 100 samples
            and get accuracy = 0.85.</p>
            <p>Assuming <span class="math inline">\(\sigma \approx
            0.35\)</span> (from <span
            class="math inline">\(\sqrt{p(1-p)}\)</span> for binary
            outcomes):</p>
            <p><span class="math display">\[\text{95% CI} = 0.85 \pm
            1.96 \cdot \frac{0.35}{\sqrt{100}} = 0.85 \pm 0.069 =
            [0.781, 0.919]\]</span></p>
            <p><strong>What ‚Äú95% Confident‚Äù Actually Means</strong>:</p>
            <blockquote>
            <p>‚ö†Ô∏è <strong>Common Misconception</strong>: ‚ÄúThere‚Äôs a 95%
            probability the true parameter is in this interval‚Äù</p>
            <p><strong>Correct Interpretation</strong>: If we repeated
            this experiment many times, 95% of the intervals we
            construct would contain the true parameter.</p>
            </blockquote>
            <p>The interval is fixed once computed. The true parameter
            either is or isn‚Äôt in it ‚Äî we just don‚Äôt know which. The
            ‚Äú95%‚Äù refers to the <em>procedure</em>, not this specific
            interval.</p>
            <p><strong>What Affects CI Width?</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 68%" />
            </colgroup>
            <thead>
            <tr>
            <th>Factor</th>
            <th>Effect on Width</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Larger n (more data)</td>
            <td><strong>Narrower</strong> (more precise) ‚Äî width <span
            class="math inline">\(\propto 1/\sqrt{n}\)</span></td>
            </tr>
            <tr>
            <td>Larger <span class="math inline">\(\sigma\)</span> (more
            variance)</td>
            <td><strong>Wider</strong> (less precise)</td>
            </tr>
            <tr>
            <td>Higher confidence level</td>
            <td><strong>Wider</strong> (more conservative)</td>
            </tr>
            </tbody>
            </table>
            <p><img src="figures/confidence_intervals_percentiles.png"
            alt="Confidence Intervals and Percentiles" /> <em>Figure:
            (Left) Repeated CI simulation ‚Äî about 95% of intervals
            contain the true mean (blue), while ~5% miss (red). This
            illustrates what ‚Äú95% confident‚Äù means. (Middle) Percentiles
            on a normal distribution, showing quartiles and extreme
            percentiles. (Right) CI width decreases with sample size but
            with diminishing returns ‚Äî need 4√ó data to halve the
            width.</em></p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat does a 95% confidence
            interval mean?‚Äù</p>
            <p><strong>A</strong>: A 95% CI means that if we repeated
            the experiment many times and constructed an interval each
            time using the same procedure, 95% of those intervals would
            contain the true parameter. It does NOT mean there‚Äôs a 95%
            probability the true value is in this specific interval ‚Äî
            the true value is fixed, and our interval either contains it
            or doesn‚Äôt. The confidence level refers to the reliability
            of the procedure, not a probability statement about this
            particular interval.</p>
            <hr />
            <h4 id="z-test-vs-t-test">Z-Test vs T-Test</h4>
            <p>Both test hypotheses about means, but differ in when to
            use them:</p>
            <table>
            <colgroup>
            <col style="width: 26%" />
            <col style="width: 36%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th><strong>Z-Test</strong></th>
            <th><strong>T-Test</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Variance</strong></td>
            <td>Known <span class="math inline">\(\sigma\)</span></td>
            <td>Unknown (estimated by <span
            class="math inline">\(s\)</span>)</td>
            </tr>
            <tr>
            <td><strong>Sample size</strong></td>
            <td>Large (<span class="math inline">\(n &gt; 30\)</span>)
            or known <span class="math inline">\(\sigma\)</span></td>
            <td>Any size (especially small)</td>
            </tr>
            <tr>
            <td><strong>Distribution</strong></td>
            <td>Standard Normal <span
            class="math inline">\(\mathcal{N}(0,1)\)</span></td>
            <td>Student‚Äôs t with df degrees of freedom</td>
            </tr>
            <tr>
            <td><strong>Formula</strong></td>
            <td><span class="math inline">\(z = \frac{\bar{x} -
            \mu_0}{\sigma/\sqrt{n}}\)</span></td>
            <td><span class="math inline">\(t = \frac{\bar{x} -
            \mu_0}{s/\sqrt{n}}\)</span></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why T-Test for Small Samples?</strong></p>
            <p>When we estimate variance from data using <span
            class="math inline">\(s = \sqrt{\frac{1}{n-1}\sum(x_i -
            \bar{x})^2}\)</span>, this estimate has uncertainty. The
            t-distribution accounts for this extra uncertainty ‚Äî it has
            heavier tails than the normal, especially for small <span
            class="math inline">\(n\)</span>.</p>
            <p><strong>Degrees of Freedom (df)</strong>:</p>
            <ul>
            <li>One-sample t-test: <span class="math inline">\(\text{df}
            = n - 1\)</span></li>
            <li>Two-sample t-test (equal var): <span
            class="math inline">\(\text{df} = n_1 + n_2 -
            2\)</span></li>
            <li>Paired t-test: <span class="math inline">\(\text{df} = n
            - 1\)</span> (number of pairs minus 1)</li>
            </ul>
            <p>As df ‚Üí ‚àû, the t-distribution ‚Üí standard normal.</p>
            <p><img src="figures/z_t_test_pvalue.png"
            alt="Z-Test vs T-Test and P-Values" /> <em>Figure: (Left)
            The t-distribution has heavier tails than the normal,
            especially for small degrees of freedom ‚Äî this accounts for
            extra uncertainty when estimating variance from data.
            (Middle) P-value is the probability of observing data this
            extreme or more under H‚ÇÄ. (Right) Two-tailed tests split Œ±
            between both tails; one-tailed tests put all Œ± in one
            direction.</em></p>
            <p><strong>Types of T-Tests</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 21%" />
            <col style="width: 35%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th>Type</th>
            <th>Use Case</th>
            <th>Hypotheses</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>One-sample</strong></td>
            <td>Compare sample mean to known value</td>
            <td><span class="math inline">\(H_0: \mu =
            \mu_0\)</span></td>
            </tr>
            <tr>
            <td><strong>Two-sample (independent)</strong></td>
            <td>Compare means of two groups</td>
            <td><span class="math inline">\(H_0: \mu_1 =
            \mu_2\)</span></td>
            </tr>
            <tr>
            <td><strong>Paired</strong></td>
            <td>Compare means of paired observations</td>
            <td><span class="math inline">\(H_0: \mu_{\text{diff}} =
            0\)</span></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: Comparing two models</p>
            <p>You have accuracy scores from 20 cross-validation folds
            for each model:</p>
            <ul>
            <li>Model A: <span class="math inline">\(\bar{x}_A =
            0.87\)</span>, <span class="math inline">\(s_A =
            0.03\)</span></li>
            <li>Model B: <span class="math inline">\(\bar{x}_B =
            0.84\)</span>, <span class="math inline">\(s_B =
            0.04\)</span></li>
            </ul>
            <p>Use a <strong>paired t-test</strong> (same folds, so
            paired):</p>
            <p><span class="math display">\[t = \frac{\bar{d}}{s_d /
            \sqrt{n}}\]</span></p>
            <p>where <span class="math inline">\(d_i = x_{A,i} -
            x_{B,i}\)</span> is the difference for each fold.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhen would you use a
            t-test instead of a z-test?‚Äù</p>
            <p><strong>A</strong>: Use a t-test when the population
            variance is unknown and must be estimated from the sample,
            which is almost always the case in practice. The
            t-distribution has heavier tails than the normal, accounting
            for the uncertainty in our variance estimate. This matters
            most for small samples (n &lt; 30); for large samples, t and
            z are nearly identical. In ML, when comparing model
            performance across cross-validation folds, we use a paired
            t-test because: (1) we don‚Äôt know the true variance of
            accuracy scores, and (2) the same folds create paired
            observations.</p>
            <hr />
            <h4 id="p-values-what-they-mean-and-dont-mean">P-Values:
            What They Mean (and Don‚Äôt Mean)</h4>
            <p><strong>Correct Definition</strong>:</p>
            <p><span class="math display">\[\text{p-value} =
            P(\text{data this extreme or more} \mid H_0 \text{ is
            true})\]</span></p>
            <p><strong>What the P-Value IS</strong>:</p>
            <ul>
            <li>The probability of observing results as extreme as ours
            IF there‚Äôs truly no effect</li>
            <li>A measure of evidence against <span
            class="math inline">\(H_0\)</span> (smaller = stronger
            evidence)</li>
            <li>A way to quantify ‚Äúhow surprised we‚Äôd be‚Äù under <span
            class="math inline">\(H_0\)</span></li>
            </ul>
            <p><strong>What the P-Value is NOT</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 68%" />
            <col style="width: 31%" />
            </colgroup>
            <thead>
            <tr>
            <th>‚ùå Common Misconception</th>
            <th>‚úì Reality</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>‚ÄúP(H‚ÇÄ is true)‚Äù</td>
            <td>We can‚Äôt compute this without a prior! (Bayesian
            territory)</td>
            </tr>
            <tr>
            <td>‚ÄúProbability the result is due to chance‚Äù</td>
            <td>Subtly wrong framing</td>
            </tr>
            <tr>
            <td>‚ÄúEffect size or practical importance‚Äù</td>
            <td>p = 0.001 doesn‚Äôt mean the effect is large</td>
            </tr>
            <tr>
            <td>‚ÄúProbability of replication‚Äù</td>
            <td>A different question entirely</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Effect Size vs Statistical
            Significance</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 60%" />
            </colgroup>
            <thead>
            <tr>
            <th>Term</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Statistical significance</strong></td>
            <td>p-value &lt; Œ± (evidence that an effect exists)</td>
            </tr>
            <tr>
            <td><strong>Practical significance</strong></td>
            <td>The effect is large enough to matter</td>
            </tr>
            </tbody>
            </table>
            <p>You can have:</p>
            <ul>
            <li><strong>Significant but not practical</strong>: Large n
            detects tiny, meaningless differences</li>
            <li><strong>Practical but not significant</strong>: Real,
            meaningful effect but small sample misses it</li>
            </ul>
            <p><strong>Example</strong>: Model A has 85.1% accuracy,
            Model B has 85.0%. With n = 100,000 test samples, this 0.1%
            difference might be statistically significant (p &lt; 0.05)
            but practically irrelevant.</p>
            <hr />
            <h4 id="multiple-testing-problem">Multiple Testing
            Problem</h4>
            <p><strong>The Problem</strong>: If you test 20 hypotheses
            at <span class="math inline">\(\alpha = 0.05\)</span>, you
            expect ~1 false positive even if all nulls are true!</p>
            <p><span class="math display">\[P(\text{at least one false
            positive}) = 1 - (1-\alpha)^m \approx 1 -
            e^{-m\alpha}\]</span></p>
            <p>For <span class="math inline">\(m = 20\)</span> tests at
            <span class="math inline">\(\alpha = 0.05\)</span>: <span
            class="math inline">\(P \approx 1 - 0.95^{20} \approx
            0.64\)</span> (64% chance of false positive!)</p>
            <p><strong>Solutions</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 26%" />
            <col style="width: 40%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Correction</th>
            <th>Use When</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Bonferroni</strong></td>
            <td>Use <span class="math inline">\(\alpha&#39; =
            \alpha/m\)</span> for each test</td>
            <td>Conservative, few tests</td>
            </tr>
            <tr>
            <td><strong>Holm-Bonferroni</strong></td>
            <td>Sequential correction</td>
            <td>Less conservative</td>
            </tr>
            <tr>
            <td><strong>False Discovery Rate (FDR)</strong></td>
            <td>Control expected proportion of false discoveries</td>
            <td>Many tests (genomics, etc.)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is the multiple
            testing problem?‚Äù</p>
            <p><strong>A</strong>: When you perform many hypothesis
            tests simultaneously, the probability of at least one false
            positive increases dramatically, even if all null hypotheses
            are true. With 20 tests at Œ± = 0.05, there‚Äôs about a 64%
            chance of at least one false positive. This matters in ML
            when: (1) comparing many hyperparameter configurations, (2)
            testing model performance across many metrics, (3) feature
            selection testing many features. Solutions include
            Bonferroni correction (divide Œ± by number of tests), or
            controlling the False Discovery Rate (FDR) which is less
            conservative and more commonly used in high-dimensional
            settings.</p>
            <hr />
            <h4
            id="statistical-testing-in-ml-practical-applications">Statistical
            Testing in ML: Practical Applications</h4>
            <p><strong>1. A/B Testing for Model Comparison</strong></p>
            <p>When deploying a new model vs.¬†baseline:</p>
            <div class="sourceCode" id="cb133"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conversion rates from A/B test</span></span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Control: 1000 users, 50 conversions</span></span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Treatment: 1000 users, 65 conversions</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a>n_control, conv_control <span class="op">=</span> <span class="dv">1000</span>, <span class="dv">50</span></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>n_treat, conv_treat <span class="op">=</span> <span class="dv">1000</span>, <span class="dv">65</span></span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Two-proportion z-test</span></span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>p_control <span class="op">=</span> conv_control <span class="op">/</span> n_control  <span class="co"># 0.05</span></span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>p_treat <span class="op">=</span> conv_treat <span class="op">/</span> n_treat        <span class="co"># 0.065</span></span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a>p_pooled <span class="op">=</span> (conv_control <span class="op">+</span> conv_treat) <span class="op">/</span> (n_control <span class="op">+</span> n_treat)</span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>se <span class="op">=</span> np.sqrt(p_pooled <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> p_pooled) <span class="op">*</span> (<span class="dv">1</span><span class="op">/</span>n_control <span class="op">+</span> <span class="dv">1</span><span class="op">/</span>n_treat))</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a>z_stat <span class="op">=</span> (p_treat <span class="op">-</span> p_control) <span class="op">/</span> se</span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-19"><a href="#cb133-19" aria-hidden="true" tabindex="-1"></a>p_value <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> stats.norm.cdf(z_stat)  <span class="co"># One-tailed test</span></span>
<span id="cb133-20"><a href="#cb133-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;z = </span><span class="sc">{</span>z_stat<span class="sc">:.3f}</span><span class="ss">, p = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb133-21"><a href="#cb133-21" aria-hidden="true" tabindex="-1"></a><span class="co"># z = 1.44, p = 0.075 ‚Üí Not significant at Œ± = 0.05</span></span></code></pre></div>
            <p><strong>2. Paired T-Test for
            Cross-Validation</strong></p>
            <div class="sourceCode" id="cb134"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy scores across 10 CV folds</span></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>model_a <span class="op">=</span> [<span class="fl">0.85</span>, <span class="fl">0.87</span>, <span class="fl">0.84</span>, <span class="fl">0.86</span>, <span class="fl">0.88</span>, <span class="fl">0.85</span>, <span class="fl">0.87</span>, <span class="fl">0.86</span>, <span class="fl">0.84</span>, <span class="fl">0.85</span>]</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>model_b <span class="op">=</span> [<span class="fl">0.82</span>, <span class="fl">0.84</span>, <span class="fl">0.83</span>, <span class="fl">0.84</span>, <span class="fl">0.85</span>, <span class="fl">0.83</span>, <span class="fl">0.84</span>, <span class="fl">0.83</span>, <span class="fl">0.82</span>, <span class="fl">0.83</span>]</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Paired t-test (same folds ‚Üí paired observations)</span></span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> stats.ttest_rel(model_a, model_b)</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;t = </span><span class="sc">{</span>t_stat<span class="sc">:.3f}</span><span class="ss">, p = </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a><span class="co"># t = 5.47, p = 0.0004 ‚Üí Significant!</span></span></code></pre></div>
            <p><strong>3. Bootstrap Confidence Intervals</strong></p>
            <p>For metrics without closed-form CIs:</p>
            <div class="sourceCode" id="cb135"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bootstrap_ci(data, metric_fn, n_bootstrap<span class="op">=</span><span class="dv">1000</span>, ci<span class="op">=</span><span class="fl">0.95</span>):</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Bootstrap confidence interval for any metric.&quot;&quot;&quot;</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a>    bootstrap_samples <span class="op">=</span> []</span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_bootstrap):</span>
<span id="cb135-9"><a href="#cb135-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resample with replacement</span></span>
<span id="cb135-10"><a href="#cb135-10" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.choice(n, size<span class="op">=</span>n, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb135-11"><a href="#cb135-11" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> data[indices]</span>
<span id="cb135-12"><a href="#cb135-12" aria-hidden="true" tabindex="-1"></a>        bootstrap_samples.append(metric_fn(sample))</span>
<span id="cb135-13"><a href="#cb135-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb135-14"><a href="#cb135-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Percentile method</span></span>
<span id="cb135-15"><a href="#cb135-15" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> ci</span>
<span id="cb135-16"><a href="#cb135-16" aria-hidden="true" tabindex="-1"></a>    lower <span class="op">=</span> np.percentile(bootstrap_samples, <span class="dv">100</span> <span class="op">*</span> alpha <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb135-17"><a href="#cb135-17" aria-hidden="true" tabindex="-1"></a>    upper <span class="op">=</span> np.percentile(bootstrap_samples, <span class="dv">100</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> alpha <span class="op">/</span> <span class="dv">2</span>))</span>
<span id="cb135-18"><a href="#cb135-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lower, upper</span>
<span id="cb135-19"><a href="#cb135-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-20"><a href="#cb135-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: CI for median accuracy</span></span>
<span id="cb135-21"><a href="#cb135-21" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> np.array([<span class="fl">0.85</span>, <span class="fl">0.87</span>, <span class="fl">0.84</span>, <span class="fl">0.86</span>, <span class="fl">0.88</span>, <span class="fl">0.85</span>, <span class="fl">0.87</span>])</span>
<span id="cb135-22"><a href="#cb135-22" aria-hidden="true" tabindex="-1"></a>ci_lower, ci_upper <span class="op">=</span> bootstrap_ci(accuracies, np.median)</span>
<span id="cb135-23"><a href="#cb135-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;95% CI for median: [</span><span class="sc">{</span>ci_lower<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>ci_upper<span class="sc">:.3f}</span><span class="ss">]&quot;</span>)</span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúHow would you determine if
            a new model is significantly better than the baseline?‚Äù</p>
            <p><strong>A</strong>: I‚Äôd use a paired t-test on
            cross-validation scores. First, train both models on the
            same CV folds to get paired accuracy scores. Then compute
            the t-statistic from the differences. This accounts for
            fold-to-fold variation and tests if the mean improvement is
            significantly different from zero. For large-scale A/B
            testing in production, I‚Äôd use a two-proportion z-test on
            conversion rates or relevant metrics, ensuring adequate
            sample size for power. I‚Äôd also consider effect size ‚Äî
            statistical significance doesn‚Äôt guarantee practical
            significance. Finally, for multiple comparisons (testing
            many models), I‚Äôd apply Bonferroni or FDR correction.</p>
            <hr />
            <h3 id="latent-variables">Latent Variables</h3>
            <p><strong>Definition</strong>: A <strong>latent
            variable</strong> is a variable that is not directly
            observed but is inferred from other observed variables.</p>
            <pre><code>Observed data X ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ Latent variable Z ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Unobserved cause
     ‚Üë                     ‚Üë
  (we see this)    (we infer this)</code></pre>
            <p><strong>Examples in ML</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Latent Variable</th>
            <th>What It Represents</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>VAE</strong></td>
            <td><span class="math inline">\(z\)</span></td>
            <td>Compressed representation</td>
            </tr>
            <tr>
            <td><strong>GMM</strong></td>
            <td>Cluster assignment</td>
            <td>Which Gaussian generated the point</td>
            </tr>
            <tr>
            <td><strong>LDA</strong></td>
            <td>Topic</td>
            <td>Topic mixture for documents</td>
            </tr>
            <tr>
            <td><strong>HMM</strong></td>
            <td>Hidden state</td>
            <td>Underlying state sequence</td>
            </tr>
            <tr>
            <td><strong>Word2Vec</strong></td>
            <td>Embedding</td>
            <td>Semantic meaning</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Variational Autoencoders (VAE)</strong>:</p>
            <pre><code>Input x ‚Üí [Encoder] ‚Üí Œº, œÉ ‚Üí Sample z ~ N(Œº, œÉ¬≤) ‚Üí [Decoder] ‚Üí Reconstructed x
                              ‚Üë
                        Latent variable!</code></pre>
            <p>The latent variable <span
            class="math inline">\(z\)</span> captures the ‚Äúessence‚Äù of
            the input in a compressed form.</p>
            <p><strong>Why Latent Variables Matter</strong>:</p>
            <ol type="1">
            <li><strong>Dimensionality reduction</strong>: High-dim data
            ‚Üí low-dim latent space</li>
            <li><strong>Generation</strong>: Sample <span
            class="math inline">\(z\)</span>, decode to generate new
            data</li>
            <li><strong>Disentanglement</strong>: Different <span
            class="math inline">\(z\)</span> dimensions might control
            different factors</li>
            <li><strong>Missing data</strong>: Treat missing values as
            latent variables to infer</li>
            </ol>
            <p><strong>Mathematical Framework</strong>:</p>
            <p>For observed <span class="math inline">\(x\)</span> and
            latent <span class="math inline">\(z\)</span>:</p>
            <p><span class="math display">\[p(x) = \int p(x|z) p(z)
            dz\]</span></p>
            <p>This integral is often intractable, leading to:</p>
            <ul>
            <li><strong>EM algorithm</strong>: Iterate between inferring
            <span class="math inline">\(z\)</span> and updating
            parameters</li>
            <li><strong>Variational inference</strong>: Approximate
            <span class="math inline">\(p(z|x)\)</span> with simpler
            <span class="math inline">\(q(z)\)</span></li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is a latent variable?
            Give an example.‚Äù</p>
            <p><strong>A</strong>: A latent variable is an unobserved
            variable that we infer from observed data. In a VAE, the
            latent variable <span class="math inline">\(z\)</span> is a
            low-dimensional representation learned by the encoder ‚Äî we
            never observe <span class="math inline">\(z\)</span>
            directly, but we infer it from the input image <span
            class="math inline">\(x\)</span>. In a Gaussian Mixture
            Model, the latent variable is the cluster assignment ‚Äî we
            don‚Äôt observe which Gaussian generated each point, but we
            infer it. Latent variables enable dimensionality reduction,
            generation, and modeling hidden structure in data.</p>
            <hr />
            <h2 id="calculus-for-machine-learning">3.3 Calculus for
            Machine Learning</h2>
            <h3 id="partial-derivatives-and-gradients">Partial
            Derivatives and Gradients</h3>
            <p>For <span class="math inline">\(f: \mathbb{R}^n \to
            \mathbb{R}\)</span>:</p>
            <p><span class="math display">\[\nabla f(\mathbf{x}) =
            \begin{bmatrix} \frac{\partial f}{\partial x_1} \\
            \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial
            f}{\partial x_n} \end{bmatrix}\]</span></p>
            <p>The gradient points in the direction of <strong>steepest
            increase</strong>.</p>
            <h3 id="chain-rule-for-neural-networks">Chain Rule for
            Neural Networks</h3>
            <p>For composed functions <span
            class="math inline">\(f(g(x))\)</span>:</p>
            <p><span class="math display">\[\frac{df}{dx} =
            \frac{df}{dg} \cdot \frac{dg}{dx}\]</span></p>
            <p><strong>Multivariate chain rule</strong>: <span
            class="math display">\[\frac{\partial L}{\partial x_i} =
            \sum_j \frac{\partial L}{\partial y_j} \frac{\partial
            y_j}{\partial x_i}\]</span></p>
            <p>This is the foundation of
            <strong>backpropagation</strong>.</p>
            <hr />
            <h3
            id="backpropagation-worked-example-2-layer-network">Backpropagation
            Worked Example: 2-Layer Network</h3>
            <p>Let‚Äôs trace gradients through a simple 2-layer network to
            see the chain rule in action.</p>
            <p><strong>Network</strong>:</p>
            <pre><code>Input x ‚Üí [Linear: W‚ÇÅ] ‚Üí z‚ÇÅ ‚Üí [ReLU] ‚Üí h ‚Üí [Linear: W‚ÇÇ] ‚Üí z‚ÇÇ ‚Üí [Sigmoid] ‚Üí ≈∑ ‚Üí [Loss] ‚Üí L</code></pre>
            <p><strong>Forward Pass</strong> (with concrete values):</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 45%" />
            <col style="width: 29%" />
            </colgroup>
            <thead>
            <tr>
            <th>Step</th>
            <th>Operation</th>
            <th>Value</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Input</td>
            <td><span class="math inline">\(x = 2\)</span></td>
            <td>2</td>
            </tr>
            <tr>
            <td>Linear 1</td>
            <td><span class="math inline">\(z_1 = w_1 \cdot x = 0.5
            \times 2\)</span></td>
            <td>1.0</td>
            </tr>
            <tr>
            <td>ReLU</td>
            <td><span class="math inline">\(h = \max(0,
            z_1)\)</span></td>
            <td>1.0</td>
            </tr>
            <tr>
            <td>Linear 2</td>
            <td><span class="math inline">\(z_2 = w_2 \cdot h = 2.0
            \times 1.0\)</span></td>
            <td>2.0</td>
            </tr>
            <tr>
            <td>Sigmoid</td>
            <td><span class="math inline">\(\hat{y} = \sigma(z_2) =
            \frac{1}{1+e^{-2}}\)</span></td>
            <td>0.88</td>
            </tr>
            <tr>
            <td>Loss (BCE)</td>
            <td><span class="math inline">\(L = -[y\log\hat{y} +
            (1-y)\log(1-\hat{y})]\)</span>, <span
            class="math inline">\(y=1\)</span></td>
            <td>0.13</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Backward Pass</strong> (computing gradients):</p>
            <p><strong>Step 1: <span
            class="math inline">\(\frac{\partial L}{\partial
            \hat{y}}\)</span></strong></p>
            <p>For BCE loss with <span
            class="math inline">\(y=1\)</span>: <span
            class="math display">\[L = -\log(\hat{y}) \implies
            \frac{\partial L}{\partial \hat{y}} = -\frac{1}{\hat{y}} =
            -\frac{1}{0.88} = -1.14\]</span></p>
            <p><strong>Step 2: <span
            class="math inline">\(\frac{\partial L}{\partial
            z_2}\)</span></strong> (through sigmoid)</p>
            <p>Using <span class="math inline">\(\sigma&#39;(z) =
            \sigma(z)(1-\sigma(z)) = 0.88 \times 0.12 =
            0.106\)</span>:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z_2} = \frac{\partial L}{\partial \hat{y}} \cdot
            \frac{\partial \hat{y}}{\partial z_2} = -1.14 \times 0.106 =
            -0.12\]</span></p>
            <p>(Or use the shortcut: for sigmoid + BCE, <span
            class="math inline">\(\frac{\partial L}{\partial z_2} =
            \hat{y} - y = 0.88 - 1 = -0.12\)</span>) ‚úì</p>
            <p><strong>Step 3: <span
            class="math inline">\(\frac{\partial L}{\partial
            w_2}\)</span></strong> (gradient for weight <span
            class="math inline">\(w_2\)</span>)</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            w_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial
            z_2}{\partial w_2} = -0.12 \times h = -0.12 \times 1.0 =
            -0.12\]</span></p>
            <p><strong>Step 4: <span
            class="math inline">\(\frac{\partial L}{\partial
            h}\)</span></strong> (backprop to hidden layer)</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            h} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial
            z_2}{\partial h} = -0.12 \times w_2 = -0.12 \times 2.0 =
            -0.24\]</span></p>
            <p><strong>Step 5: <span
            class="math inline">\(\frac{\partial L}{\partial
            z_1}\)</span></strong> (through ReLU)</p>
            <p>Since <span class="math inline">\(z_1 = 1.0 &gt;
            0\)</span>, ReLU derivative is 1: <span
            class="math display">\[\frac{\partial L}{\partial z_1} =
            \frac{\partial L}{\partial h} \cdot \frac{\partial
            h}{\partial z_1} = -0.24 \times 1 = -0.24\]</span></p>
            <p><strong>Step 6: <span
            class="math inline">\(\frac{\partial L}{\partial
            w_1}\)</span></strong> (gradient for weight <span
            class="math inline">\(w_1\)</span>)</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            w_1} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial
            z_1}{\partial w_1} = -0.24 \times x = -0.24 \times 2 =
            -0.48\]</span></p>
            <p><strong>Summary of Gradients</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Weight</th>
            <th>Gradient</th>
            <th>Interpretation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(w_2\)</span></td>
            <td>-0.12</td>
            <td>Increase <span class="math inline">\(w_2\)</span> to
            reduce loss</td>
            </tr>
            <tr>
            <td><span class="math inline">\(w_1\)</span></td>
            <td>-0.48</td>
            <td>Increase <span class="math inline">\(w_1\)</span> to
            reduce loss</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Update Rule</strong> (with <span
            class="math inline">\(\alpha = 0.1\)</span>): <span
            class="math display">\[w_1^{\text{new}} = w_1 - \alpha \cdot
            \frac{\partial L}{\partial w_1} = 0.5 - 0.1 \times (-0.48) =
            0.548\]</span> <span class="math display">\[w_2^{\text{new}}
            = w_2 - \alpha \cdot \frac{\partial L}{\partial w_2} = 2.0 -
            0.1 \times (-0.12) = 2.012\]</span></p>
            <p><strong>Interview Q</strong>: ‚ÄúWalk through
            backpropagation in a simple neural network.‚Äù</p>
            <p><strong>A</strong>: Backprop applies the chain rule
            backwards through the network. For a 2-layer net: (1)
            Compute forward pass to get predictions. (2) Compute loss
            derivative <span class="math inline">\(\frac{\partial
            L}{\partial \hat{y}}\)</span>. (3) Chain through output
            activation: <span class="math inline">\(\frac{\partial
            L}{\partial z_2} = \frac{\partial L}{\partial \hat{y}} \cdot
            \sigma&#39;(z_2)\)</span>. (4) Compute weight gradient:
            <span class="math inline">\(\frac{\partial L}{\partial w_2}
            = \frac{\partial L}{\partial z_2} \cdot h\)</span>. (5)
            Backprop to hidden layer: <span
            class="math inline">\(\frac{\partial L}{\partial h} =
            \frac{\partial L}{\partial z_2} \cdot w_2\)</span>. (6)
            Chain through hidden activation: <span
            class="math inline">\(\frac{\partial L}{\partial z_1} =
            \frac{\partial L}{\partial h} \cdot
            \text{ReLU}&#39;(z_1)\)</span>. (7) Compute first layer
            gradient: <span class="math inline">\(\frac{\partial
            L}{\partial w_1} = \frac{\partial L}{\partial z_1} \cdot
            x\)</span>. The key insight is that gradients flow backward,
            with each layer multiplying by its local derivative.</p>
            <h3 id="computational-graph-view">Computational Graph
            View</h3>
            <p>Another way to understand backprop is through
            computational graphs:</p>
            <pre><code>Forward:  x=2 ‚îÄ‚îÄ‚Üí [√ów‚ÇÅ] ‚îÄ‚îÄ‚Üí z‚ÇÅ=1 ‚îÄ‚îÄ‚Üí [ReLU] ‚îÄ‚îÄ‚Üí h=1 ‚îÄ‚îÄ‚Üí [√ów‚ÇÇ] ‚îÄ‚îÄ‚Üí z‚ÇÇ=2 ‚îÄ‚îÄ‚Üí [œÉ] ‚îÄ‚îÄ‚Üí ≈∑=0.88 ‚îÄ‚îÄ‚Üí [L] ‚îÄ‚îÄ‚Üí 0.13
                  w‚ÇÅ=0.5                                  w‚ÇÇ=2.0

Backward:      ‚Üê‚îÄ0.48‚îÄ‚îò     ‚Üê‚îÄ0.24‚îÄ‚îò      ‚Üê‚îÄ0.24‚îÄ‚îò     ‚Üê‚îÄ0.12‚îÄ‚îò      ‚Üê‚îÄ0.12‚îÄ‚îò       ‚Üê‚îÄ1.14‚îÄ‚îò
             ‚àÇL/‚àÇw‚ÇÅ        ‚àÇL/‚àÇz‚ÇÅ         ‚àÇL/‚àÇh        ‚àÇL/‚àÇw‚ÇÇ        ‚àÇL/‚àÇz‚ÇÇ         ‚àÇL/‚àÇ≈∑</code></pre>
            <p>Each node receives the gradient from downstream and: 1.
            Computes local derivative 2. Multiplies to get gradient for
            this node 3. Passes gradient upstream</p>
            <hr />
            <h3 id="jacobian-matrix">Jacobian Matrix</h3>
            <p>For <span class="math inline">\(\mathbf{f}: \mathbb{R}^n
            \to \mathbb{R}^m\)</span>:</p>
            <p><span class="math display">\[\mathbf{J} = \begin{bmatrix}
            \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp;
            \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots
            &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp;
            \cdots &amp; \frac{\partial f_m}{\partial x_n}
            \end{bmatrix}\]</span></p>
            <p><strong>Example</strong>: Softmax Jacobian</p>
            <p>For <span class="math inline">\(y_i =
            \frac{e^{x_i}}{\sum_j e^{x_j}}\)</span>:</p>
            <p><span class="math display">\[\frac{\partial y_i}{\partial
            x_j} = \begin{cases} y_i(1 - y_i) &amp; \text{if } i = j \\
            -y_i y_j &amp; \text{if } i \neq j \end{cases}\]</span></p>
            <h3 id="hessian-matrix">Hessian Matrix</h3>
            <p>Second-order partial derivatives:</p>
            <p><span class="math display">\[\mathbf{H} = \begin{bmatrix}
            \frac{\partial^2 f}{\partial x_1^2} &amp; \cdots &amp;
            \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \vdots
            &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial
            x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2
            f}{\partial x_n^2} \end{bmatrix}\]</span></p>
            <p><strong>Properties</strong>:</p>
            <ul>
            <li>Symmetric if <span class="math inline">\(f\)</span> is
            twice continuously differentiable</li>
            <li>Eigenvalues indicate curvature</li>
            <li>Used in second-order optimization (Newton‚Äôs method)</li>
            </ul>
            <h3
            id="hessian-eigenvalues-and-critical-point-stability-interview-deep-dive">Hessian
            Eigenvalues and Critical Point Stability (Interview Deep
            Dive)</h3>
            <p>At a <strong>critical point</strong> (where <span
            class="math inline">\(\nabla f = 0\)</span>), the Hessian‚Äôs
            eigenvalues tell us what <em>type</em> of critical point
            we‚Äôre at:</p>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 35%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>All Eigenvalues</th>
            <th>Critical Point Type</th>
            <th>Geometric Intuition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>All positive</strong></td>
            <td>Local minimum</td>
            <td>Bottom of a bowl ‚Äî curves up everywhere</td>
            </tr>
            <tr>
            <td><strong>All negative</strong></td>
            <td>Local maximum</td>
            <td>Top of a hill ‚Äî curves down everywhere</td>
            </tr>
            <tr>
            <td><strong>Mixed signs</strong></td>
            <td>Saddle point</td>
            <td>Horse saddle ‚Äî up in some directions, down in
            others</td>
            </tr>
            <tr>
            <td><strong>Some zero</strong></td>
            <td>Degenerate</td>
            <td>Flat in some directions ‚Äî inconclusive</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why This Matters for Deep Learning</strong>:</p>
            <ol type="1">
            <li><p><strong>Saddle points dominate in high
            dimensions</strong>: For a point to be a local minimum, ALL
            eigenvalues must be positive. With millions of parameters,
            this is statistically unlikely. Most critical points in
            neural network loss landscapes are <strong>saddle
            points</strong>, not local minima.</p></li>
            <li><p><strong>Gradient descent escapes saddles</strong>:
            SGD naturally finds directions with negative curvature
            (eigenvalues &lt; 0) and slides down. The problem is
            near-zero eigenvalues ‚Äî <strong>flat regions</strong> where
            gradients are tiny and training stalls.</p></li>
            <li><p><strong>Condition number affects
            optimization</strong>: The ratio of largest to smallest
            eigenvalue (<span class="math inline">\(\kappa =
            \lambda_{\max}/\lambda_{\min}\)</span>) is the
            <strong>condition number</strong>. Large <span
            class="math inline">\(\kappa\)</span> means the loss
            landscape is ‚Äúelongated‚Äù ‚Äî steep in some directions, flat in
            others ‚Äî causing slow convergence.</p></li>
            </ol>
            <p><strong>The Mathematical Story</strong>:</p>
            <p>At a critical point, Taylor expansion gives: <span
            class="math display">\[f(\mathbf{x} + \mathbf{h}) \approx
            f(\mathbf{x}) + \frac{1}{2}\mathbf{h}^T \mathbf{H}
            \mathbf{h}\]</span></p>
            <p>The quadratic form <span
            class="math inline">\(\mathbf{h}^T \mathbf{H}
            \mathbf{h}\)</span> determines local behavior: - If <span
            class="math inline">\(\mathbf{H}\)</span> is positive
            definite (all <span class="math inline">\(\lambda_i &gt;
            0\)</span>): <span class="math inline">\(\mathbf{h}^T
            \mathbf{H} \mathbf{h} &gt; 0\)</span> for all <span
            class="math inline">\(\mathbf{h}\)</span> ‚Üí local minimum -
            If <span class="math inline">\(\mathbf{H}\)</span> has mixed
            signs: some directions increase <span
            class="math inline">\(f\)</span>, others decrease ‚Üí
            saddle</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat‚Äôs the relationship
            between the Hessian‚Äôs eigenvalues and critical point
            stability?‚Äù</p>
            <p><strong>A</strong>: The Hessian matrix captures the
            curvature of the loss landscape. At a critical point where
            the gradient is zero, the eigenvalues determine the type of
            critical point: - <strong>All positive eigenvalues</strong>
            ‚Üí local minimum (positive definite, curves up in all
            directions) - <strong>All negative eigenvalues</strong> ‚Üí
            local maximum (negative definite)<br />
            - <strong>Mixed signs</strong> ‚Üí saddle point (indefinite,
            curves up in some directions, down in others)</p>
            <p>For deep learning, this is crucial because in high
            dimensions, saddle points vastly outnumber local minima. SGD
            escapes saddles by finding negative curvature directions.
            The real challenge is <strong>near-zero eigenvalues</strong>
            (flat regions) where progress stalls. This is partly why
            Adam works better than vanilla SGD ‚Äî it adapts learning
            rates based on gradient history, effectively approximating
            curvature information.</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúWhy are saddle points more
            common than local minima in high dimensions?‚Äù</p>
            <p><strong>A</strong>: For a critical point to be a local
            minimum, ALL eigenvalues of the Hessian must be positive. In
            a network with <span class="math inline">\(n\)</span>
            parameters, think of each eigenvalue as having roughly 50%
            chance of being positive (oversimplified). The probability
            of ALL <span class="math inline">\(n\)</span> being positive
            is approximately <span
            class="math inline">\((0.5)^n\)</span>, which becomes
            vanishingly small for large <span
            class="math inline">\(n\)</span>. With millions of
            parameters, the probability of finding a true local minimum
            is essentially zero. Instead, we encounter saddle points
            where some eigenvalues are positive and others negative.</p>
            <hr />
            <h3 id="important-derivatives-for-ml">Important Derivatives
            for ML</h3>
            <p><strong>Sigmoid</strong>: <span
            class="math display">\[\sigma(x) =
            \frac{1}{1+e^{-x}}\]</span></p>
            <p><span class="math display">\[\sigma&#39;(x) = \sigma(x)(1
            - \sigma(x))\]</span></p>
            <p><strong>Tanh</strong>: <span
            class="math display">\[\tanh&#39;(x) = 1 -
            \tanh^2(x)\]</span></p>
            <p><strong>ReLU</strong>: <span
            class="math display">\[\text{ReLU}&#39;(x) = \begin{cases} 1
            &amp; x &gt; 0 \\ 0 &amp; x \leq 0 \end{cases}\]</span></p>
            <p><strong>Softmax Cross-Entropy</strong> (combined): <span
            class="math display">\[L = -\sum_i y_i
            \log(\hat{y}_i)\]</span></p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z_i} = \hat{y}_i - y_i\]</span></p>
            <p>This elegant result makes training efficient!</p>
            <p><strong>Interview Q</strong>: ‚ÄúDerive the gradient of
            softmax cross-entropy loss.‚Äù</p>
            <p><strong>Answer</strong>: Let <span
            class="math inline">\(\hat{y}_i = \frac{e^{z_i}}{\sum_j
            e^{z_j}}\)</span> (softmax) and <span
            class="math inline">\(L = -\sum_k y_k \log
            \hat{y}_k\)</span> (cross-entropy).</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z_i} = -\sum_k y_k \frac{\partial \log \hat{y}_k}{\partial
            z_i} = -\sum_k y_k \frac{1}{\hat{y}_k} \frac{\partial
            \hat{y}_k}{\partial z_i}\]</span></p>
            <p>Using the softmax Jacobian: <span
            class="math display">\[\frac{\partial L}{\partial z_i} =
            -y_i(1 - \hat{y}_i) + \sum_{k \neq i} y_k \hat{y}_i = -y_i +
            \hat{y}_i \sum_k y_k = \hat{y}_i - y_i\]</span></p>
            <p>(since <span class="math inline">\(\sum_k y_k =
            1\)</span> for one-hot labels)</p>
            <hr />
            <h2 id="information-theory">3.4 Information Theory</h2>
            <h3 id="why-information-theory-matters-for-ml">Why
            Information Theory Matters for ML</h3>
            <p>Information theory provides the <strong>mathematical
            foundation</strong> for:</p>
            <ul>
            <li>Cross-entropy loss (why we use it!)</li>
            <li>KL divergence in VAEs, RLHF, DPO</li>
            <li>Mutual information in representation learning</li>
            <li>Understanding model compression</li>
            </ul>
            <h3 id="entropy-measuring-uncertainty">Entropy: Measuring
            Uncertainty</h3>
            <p><strong>Definition</strong>: Average ‚Äúsurprise‚Äù or
            uncertainty in a random variable.</p>
            <p><span class="math display">\[H(X) = -\sum_{x} p(x) \log
            p(x) = \mathbb{E}[-\log p(X)]\]</span></p>
            <p><strong>Intuition</strong>:</p>
            <ul>
            <li>Low entropy = predictable (certain)</li>
            <li>High entropy = unpredictable (uncertain)</li>
            </ul>
            <p><strong>Properties</strong>:</p>
            <ul>
            <li><span class="math inline">\(H(X) \geq 0\)</span> (always
            non-negative)</li>
            <li><span class="math inline">\(H(X) = 0\)</span> iff <span
            class="math inline">\(X\)</span> is deterministic</li>
            <li>Maximum when uniform: <span class="math inline">\(H(X) =
            \log |X|\)</span></li>
            </ul>
            <h3 id="example-coin-flips">Example: Coin Flips</h3>
            <table>
            <thead>
            <tr>
            <th>Coin</th>
            <th><span
            class="math inline">\(p(\text{heads})\)</span></th>
            <th>Entropy <span class="math inline">\(H\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Fair</td>
            <td>0.5</td>
            <td>1 bit (maximum!)</td>
            </tr>
            <tr>
            <td>Biased</td>
            <td>0.9</td>
            <td>0.47 bits</td>
            </tr>
            <tr>
            <td>Two-headed</td>
            <td>1.0</td>
            <td>0 bits (certain)</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb140"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(p):</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Entropy of Bernoulli distribution&quot;&quot;&quot;</span></span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> p <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>p <span class="op">*</span> np.log2(p) <span class="op">-</span> (<span class="dv">1</span><span class="op">-</span>p) <span class="op">*</span> np.log2(<span class="dv">1</span><span class="op">-</span>p)</span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fair coin has maximum entropy</span></span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Fair coin: </span><span class="sc">{</span>entropy(<span class="fl">0.5</span>)<span class="sc">:.2f}</span><span class="ss"> bits&quot;</span>)      <span class="co"># 1.00</span></span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Biased (0.9): </span><span class="sc">{</span>entropy(<span class="fl">0.9</span>)<span class="sc">:.2f}</span><span class="ss"> bits&quot;</span>)   <span class="co"># 0.47</span></span></code></pre></div>
            <hr />
            <h3
            id="cross-entropy-comparing-distributions">Cross-Entropy:
            Comparing Distributions</h3>
            <p><strong>Definition</strong>: Expected ‚Äúsurprise‚Äù when
            using distribution <span class="math inline">\(Q\)</span> to
            encode samples from <span
            class="math inline">\(P\)</span>.</p>
            <p><span class="math display">\[H(P, Q) = -\sum_{x} p(x)
            \log q(x) = \mathbb{E}_{x \sim P}[-\log Q(x)]\]</span></p>
            <p><strong>Key insight</strong>: Cross-entropy is minimized
            when <span class="math inline">\(Q = P\)</span>.</p>
            <h3 id="why-cross-entropy-is-our-loss-function">Why
            Cross-Entropy is Our Loss Function</h3>
            <p>For classification with true distribution <span
            class="math inline">\(P\)</span> (one-hot) and predicted
            <span class="math inline">\(Q\)</span> (softmax output):</p>
            <p><span class="math display">\[H(P, Q) = -\sum_{k} y_k \log
            \hat{y}_k = -\log \hat{y}_c\]</span></p>
            <p>This is exactly our <strong>categorical cross-entropy
            loss</strong>!</p>
            <p><strong>Why it works</strong>:</p>
            <ul>
            <li>Minimizing <span class="math inline">\(H(P, Q)\)</span>
            pushes <span class="math inline">\(Q\)</span> toward <span
            class="math inline">\(P\)</span></li>
            <li>Equivalent to maximum likelihood estimation</li>
            <li>Penalizes confident wrong predictions heavily</li>
            </ul>
            <hr />
            <h3 id="kl-divergence-distance-between-distributions">KL
            Divergence: Distance Between Distributions</h3>
            <p><strong>Definition</strong>: ‚ÄúExtra bits‚Äù needed when
            using <span class="math inline">\(Q\)</span> instead of
            <span class="math inline">\(P\)</span>.</p>
            <p><span class="math display">\[D_{KL}(P \| Q) = \sum_{x}
            p(x) \log \frac{p(x)}{q(x)} = \mathbb{E}_{x \sim
            P}\left[\log \frac{P(x)}{Q(x)}\right]\]</span></p>
            <p><strong>The Fundamental Relationship</strong>: <span
            class="math display">\[\boxed{H(P, Q) = H(P) + D_{KL}(P \|
            Q)}\]</span></p>
            <p>Cross-entropy = Entropy + KL Divergence</p>
            <p><strong>Properties</strong>:</p>
            <ul>
            <li><span class="math inline">\(D_{KL}(P \| Q) \geq
            0\)</span> (Gibbs‚Äô inequality)</li>
            <li><span class="math inline">\(D_{KL}(P \| Q) = 0\)</span>
            iff <span class="math inline">\(P = Q\)</span></li>
            <li><strong>NOT symmetric</strong>: <span
            class="math inline">\(D_{KL}(P \| Q) \neq D_{KL}(Q \|
            P)\)</span></li>
            </ul>
            <h3 id="forward-vs-reverse-kl">Forward vs Reverse KL</h3>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th>Forward KL: <span class="math inline">\(D_{KL}(P \|
            Q)\)</span></th>
            <th>Reverse KL: <span class="math inline">\(D_{KL}(Q \|
            P)\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Optimizes</td>
            <td><span class="math inline">\(Q\)</span> to cover all of
            <span class="math inline">\(P\)</span></td>
            <td><span class="math inline">\(Q\)</span> to match mode of
            <span class="math inline">\(P\)</span></td>
            </tr>
            <tr>
            <td>Behavior</td>
            <td><strong>Mean-seeking</strong></td>
            <td><strong>Mode-seeking</strong></td>
            </tr>
            <tr>
            <td>Zero-forcing</td>
            <td>When <span class="math inline">\(p(x) &gt; 0\)</span>,
            need <span class="math inline">\(q(x) &gt; 0\)</span></td>
            <td>When <span class="math inline">\(q(x) &gt; 0\)</span>,
            need <span class="math inline">\(p(x) &gt; 0\)</span></td>
            </tr>
            <tr>
            <td>Used in</td>
            <td>VAE decoder</td>
            <td>Variational inference</td>
            </tr>
            </tbody>
            </table>
            <p><img src="figures/kl_divergence.png"
            alt="Forward vs Reverse KL Divergence" /> <em>Figure:
            Forward KL (D(P||Q)) forces Q to cover all modes of P.
            Reverse KL (D(Q||P)) allows Q to pick a single
            mode.</em></p>
            <h3 id="kl-in-rlhf-and-dpo">KL in RLHF and DPO</h3>
            <p><strong>RLHF objective</strong>: <span
            class="math display">\[\max_\pi \mathbb{E}[R(x, y)] - \beta
            \cdot D_{KL}(\pi_\theta \| \pi_{\text{ref}})\]</span></p>
            <p><strong>Understanding the notation</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\pi\)</span></td>
            <td>A policy ‚Äî a probability distribution over outputs <span
            class="math inline">\(y\)</span> given input <span
            class="math inline">\(x\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\pi_\theta\)</span></td>
            <td>The policy we‚Äôre training (parameterized by neural
            network weights <span
            class="math inline">\(\theta\)</span>)</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\pi_{\text{ref}}\)</span></td>
            <td>The reference policy ‚Äî typically the SFT (supervised
            fine-tuned) model we start from</td>
            </tr>
            <tr>
            <td><span class="math inline">\(R(x, y)\)</span></td>
            <td>Reward function ‚Äî how good is response <span
            class="math inline">\(y\)</span> for prompt <span
            class="math inline">\(x\)</span> (from reward model)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta\)</span></td>
            <td>KL penalty coefficient ‚Äî controls the trade-off between
            maximizing reward and staying close to reference</td>
            </tr>
            <tr>
            <td><span class="math inline">\(D_{KL}(\pi_\theta \|
            \pi_{\text{ref}})\)</span></td>
            <td>How much our trained policy has diverged from the
            reference</td>
            </tr>
            </tbody>
            </table>
            <p><strong>What this objective says in plain
            English</strong>:</p>
            <p>‚ÄúFind a policy <span
            class="math inline">\(\pi_\theta\)</span> that generates
            high-reward responses, BUT don‚Äôt drift too far from the
            original SFT model.‚Äù</p>
            <p><strong>Why KL penalty?</strong></p>
            <ul>
            <li><strong>Prevents reward hacking</strong>: Without the KL
            term, the model could find ‚Äúcheats‚Äù ‚Äî outputs that fool the
            reward model but are nonsensical. The KL term says ‚Äústay
            close to sensible outputs.‚Äù</li>
            <li><strong>Maintains fluency</strong>: The SFT model
            already produces fluent, coherent text. Drifting too far
            might hurt language quality.</li>
            <li><strong>Regularization</strong>: Prevents overfitting to
            the reward model‚Äôs quirks.</li>
            <li><span class="math inline">\(\beta\)</span> controls the
            trade-off: High <span class="math inline">\(\beta\)</span> ‚Üí
            stay very close to reference. Low <span
            class="math inline">\(\beta\)</span> ‚Üí optimize reward more
            aggressively.</li>
            </ul>
            <p><strong>DPO implicit reward</strong>: <span
            class="math display">\[R(x, y) = \beta \log
            \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} +
            \text{const}\]</span></p>
            <p><strong>Understanding DPO notation</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\pi^*\)</span></td>
            <td>The optimal policy ‚Äî what we‚Äôre trying to learn</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\pi_{\text{ref}}\)</span></td>
            <td>Reference policy (same as in RLHF ‚Äî the SFT starting
            point)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta\)</span></td>
            <td>Temperature parameter controlling how peaked the optimal
            policy is</td>
            </tr>
            </tbody>
            </table>
            <p><strong>DPO insight</strong>: DPO shows that the optimal
            RLHF policy has a closed-form relationship with the reward.
            Instead of training a separate reward model and doing RL,
            DPO directly optimizes the policy using preference data. The
            log ratio <span class="math inline">\(\log
            \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)}\)</span> measures
            how much more likely the optimal policy makes a response
            compared to the reference ‚Äî this IS the implicit reward (up
            to scaling).</p>
            <hr />
            <h3 id="mutual-information">Mutual Information</h3>
            <p><strong>Definition</strong>: Information shared between
            two variables.</p>
            <p><span class="math display">\[I(X; Y) = H(X) - H(X|Y) =
            H(Y) - H(Y|X)\]</span></p>
            <p><span class="math display">\[I(X; Y) = D_{KL}(P(X, Y) \|
            P(X)P(Y))\]</span></p>
            <p><strong>Understanding the formula</strong>:</p>
            <ul>
            <li><span class="math inline">\(H(X)\)</span> = total
            uncertainty about <span class="math inline">\(X\)</span>
            before knowing anything</li>
            <li><span class="math inline">\(H(X|Y)\)</span> = remaining
            uncertainty about <span class="math inline">\(X\)</span>
            <em>after</em> knowing <span
            class="math inline">\(Y\)</span></li>
            <li><span class="math inline">\(I(X; Y) = H(X) -
            H(X|Y)\)</span> = how much uncertainty <em>reduces</em> when
            you learn <span class="math inline">\(Y\)</span></li>
            </ul>
            <p><strong>Intuition (Plain English)</strong>:</p>
            <p>Mutual information measures <strong>how much knowing one
            variable tells you about another</strong>.</p>
            <ul>
            <li>If <span class="math inline">\(X\)</span> and <span
            class="math inline">\(Y\)</span> are independent: knowing
            <span class="math inline">\(Y\)</span> tells you nothing
            about <span class="math inline">\(X\)</span>, so <span
            class="math inline">\(I(X;Y) = 0\)</span></li>
            <li>If <span class="math inline">\(X\)</span> completely
            determines <span class="math inline">\(Y\)</span>: knowing
            <span class="math inline">\(Y\)</span> tells you everything
            about <span class="math inline">\(X\)</span>, so <span
            class="math inline">\(I(X;Y) = H(X)\)</span> (maximum)</li>
            </ul>
            <p><strong>Example</strong>: Weather and Umbrella</p>
            <p>Let <span class="math inline">\(X\)</span> = ‚ÄúIs it
            raining?‚Äù and <span class="math inline">\(Y\)</span> = ‚ÄúIs
            someone carrying an umbrella?‚Äù</p>
            <ul>
            <li>Without seeing umbrellas: <span
            class="math inline">\(H(X)\)</span> = some uncertainty about
            rain</li>
            <li>After seeing many umbrellas: <span
            class="math inline">\(H(X|Y)\)</span> = much less
            uncertainty (probably raining!)</li>
            <li><span class="math inline">\(I(X;Y)\)</span> = the
            reduction in uncertainty = high (they‚Äôre correlated)</li>
            </ul>
            <p>If instead <span class="math inline">\(Y\)</span> =
            ‚ÄúWhat‚Äôs the stock price?‚Äù, then <span
            class="math inline">\(I(X;Y) \approx 0\)</span> ‚Äî stock
            prices tell you nothing about rain.</p>
            <p><strong>Alternative formula intuition</strong>:</p>
            <p><span class="math display">\[I(X; Y) = D_{KL}(P(X, Y) \|
            P(X)P(Y))\]</span></p>
            <p>This measures how different the joint distribution <span
            class="math inline">\(P(X,Y)\)</span> is from what it would
            be if <span class="math inline">\(X\)</span> and <span
            class="math inline">\(Y\)</span> were independent <span
            class="math inline">\(P(X)P(Y)\)</span>. High divergence =
            strong dependence = high mutual information.</p>
            <p><strong>Properties</strong>:</p>
            <ul>
            <li><span class="math inline">\(I(X; Y) \geq 0\)</span>
            (always non-negative)</li>
            <li><span class="math inline">\(I(X; Y) = 0\)</span> iff
            <span class="math inline">\(X \perp Y\)</span>
            (independent)</li>
            <li><span class="math inline">\(I(X; Y) = I(Y; X)\)</span>
            (symmetric ‚Äî unlike KL divergence!)</li>
            <li><span class="math inline">\(I(X; X) = H(X)\)</span>
            (information with yourself = your entropy)</li>
            </ul>
            <p><strong>ML Applications</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 44%" />
            <col style="width: 55%" />
            </colgroup>
            <thead>
            <tr>
            <th>Application</th>
            <th>How MI is Used</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>InfoNCE loss</strong> (contrastive
            learning)</td>
            <td>Maximize MI between different views of same data
            (positive pairs)</td>
            </tr>
            <tr>
            <td><strong>Information bottleneck</strong></td>
            <td>Learn representation <span
            class="math inline">\(Z\)</span> that has high <span
            class="math inline">\(I(Z; Y)\)</span> (predictive) but low
            <span class="math inline">\(I(Z; X)\)</span>
            (compressed)</td>
            </tr>
            <tr>
            <td><strong>Feature selection</strong></td>
            <td>Select features with high <span
            class="math inline">\(I(\text{feature};
            \text{target})\)</span></td>
            </tr>
            <tr>
            <td><strong>GANs</strong></td>
            <td>Some variants maximize MI between generated images and
            latent codes</td>
            </tr>
            <tr>
            <td><strong>Representation learning</strong></td>
            <td>Maximize MI between representations of related
            samples</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is mutual information
            and when would you use it?‚Äù</p>
            <p><strong>A</strong>: Mutual information <span
            class="math inline">\(I(X;Y)\)</span> measures how much
            knowing <span class="math inline">\(Y\)</span> reduces
            uncertainty about <span class="math inline">\(X\)</span>. It
            equals <span class="math inline">\(H(X) - H(X|Y)\)</span>:
            your initial uncertainty minus remaining uncertainty after
            observing <span class="math inline">\(Y\)</span>. It‚Äôs
            symmetric and zero iff variables are independent. In ML,
            it‚Äôs used in contrastive learning (InfoNCE maximizes MI
            between augmented views), the information bottleneck (trade
            off compression vs prediction), and feature selection (pick
            features most informative about the target).</p>
            <h3
            id="perplexity-the-language-models-confusion-metric">Perplexity:
            The Language Model‚Äôs Confusion Metric</h3>
            <blockquote>
            <p><strong>üîë Perplexity is arguably the most important
            metric for evaluating language models.</strong> It measures
            how ‚Äúsurprised‚Äù or ‚Äúconfused‚Äù a model is when predicting
            text.</p>
            </blockquote>
            <p><strong>Definition</strong>: Perplexity is the
            exponential of the average negative log-likelihood
            (cross-entropy):</p>
            <p><span class="math display">\[\text{PPL} =
            \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P(x_i |
            x_{&lt;i})\right) =
            \exp(\mathcal{L}_{\text{NLL}})\]</span></p>
            <p>Equivalently, for cross-entropy <span
            class="math inline">\(H\)</span>:</p>
            <p><span class="math display">\[\text{PPL} = 2^{H} \quad
            \text{(if using log base 2)}\]</span> <span
            class="math display">\[\text{PPL} = e^{H} \quad \text{(if
            using natural log)}\]</span></p>
            <hr />
            <h3 id="the-nll-perplexity-connection">The NLL ‚ÜîÔ∏é Perplexity
            Connection</h3>
            <table>
            <colgroup>
            <col style="width: 28%" />
            <col style="width: 25%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>Quantity</th>
            <th>Formula</th>
            <th>Interpretation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Likelihood</strong></td>
            <td><span class="math inline">\(\prod_i P(x_i \mid
            x_{&lt;i})\)</span></td>
            <td>Probability of seeing the data</td>
            </tr>
            <tr>
            <td><strong>Log-likelihood</strong></td>
            <td><span class="math inline">\(\sum_i \log P(x_i \mid
            x_{&lt;i})\)</span></td>
            <td>Sum of log-probabilities</td>
            </tr>
            <tr>
            <td><strong>Negative log-likelihood (NLL)</strong></td>
            <td><span class="math inline">\(-\frac{1}{N}\sum_i \log
            P(x_i \mid x_{&lt;i})\)</span></td>
            <td>Average cross-entropy loss</td>
            </tr>
            <tr>
            <td><strong>Perplexity</strong></td>
            <td><span
            class="math inline">\(\exp(\text{NLL})\)</span></td>
            <td>Effective vocabulary size</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The key relationship</strong>: <span
            class="math display">\[\boxed{\text{PPL} =
            \exp(\text{Average NLL per token}) =
            \exp(\text{Cross-entropy})}\]</span></p>
            <hr />
            <h3
            id="intuition-perplexity-as-effective-vocabulary-size">Intuition:
            Perplexity as ‚ÄúEffective Vocabulary Size‚Äù</h3>
            <p><strong>The best intuition</strong>: Perplexity measures
            the <strong>effective number of choices</strong> the model
            is considering at each step.</p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>Perplexity</th>
            <th>Model Behavior</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>PPL = 1</strong></td>
            <td>Perfect prediction ‚Äî model knows exactly what comes
            next</td>
            </tr>
            <tr>
            <td><strong>PPL = 10</strong></td>
            <td>Model is as confused as choosing uniformly among 10
            words</td>
            </tr>
            <tr>
            <td><strong>PPL = 100</strong></td>
            <td>Model is choosing among ~100 equally likely words</td>
            </tr>
            <tr>
            <td><strong>PPL = 50,000</strong></td>
            <td>Random guessing over entire vocabulary</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: If a model has perplexity 25 on
            a test set, it means the model is, on average, as uncertain
            as if it were choosing uniformly among 25 equally likely
            words at each position.</p>
            <p><strong>Why exponential?</strong> The NLL is in log-space
            (bits or nats). Exponentiating converts back to a
            probability-like quantity. If the model assigns probability
            <span class="math inline">\(p\)</span> to each token on
            average, then: - NLL = <span class="math inline">\(-\log
            p\)</span> - PPL = <span class="math inline">\(\exp(-\log p)
            = 1/p\)</span></p>
            <p>So PPL = 10 means the model assigns ~10% probability to
            the correct token on average.</p>
            <hr />
            <h3 id="worked-example-computing-perplexity">Worked Example:
            Computing Perplexity</h3>
            <p><strong>Sentence</strong>: ‚ÄúThe cat sat‚Äù</p>
            <p><strong>Model predictions</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Position</th>
            <th>True Token</th>
            <th>P(token)</th>
            <th>-log P(token)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>‚ÄúThe‚Äù</td>
            <td>0.10</td>
            <td>2.30</td>
            </tr>
            <tr>
            <td>2</td>
            <td>‚Äúcat‚Äù</td>
            <td>0.05</td>
            <td>3.00</td>
            </tr>
            <tr>
            <td>3</td>
            <td>‚Äúsat‚Äù</td>
            <td>0.20</td>
            <td>1.61</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Average NLL</strong>: <span
            class="math inline">\(\frac{2.30 + 3.00 + 1.61}{3} =
            2.30\)</span></p>
            <p><strong>Perplexity</strong>: <span
            class="math inline">\(\exp(2.30) \approx 10\)</span></p>
            <p><strong>Interpretation</strong>: The model is as confused
            as if choosing among ~10 equally likely options per
            token.</p>
            <div class="sourceCode" id="cb141"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_perplexity(model, tokenizer, text):</span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Compute perplexity of a text sequence.&quot;&quot;&quot;</span></span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.encode(text, return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>)</span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb141-9"><a href="#cb141-9" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(tokens, labels<span class="op">=</span>tokens)</span>
<span id="cb141-10"><a href="#cb141-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs.loss  <span class="co"># Cross-entropy loss (NLL per token)</span></span>
<span id="cb141-11"><a href="#cb141-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb141-12"><a href="#cb141-12" aria-hidden="true" tabindex="-1"></a>    perplexity <span class="op">=</span> torch.exp(loss)</span>
<span id="cb141-13"><a href="#cb141-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> perplexity.item()</span>
<span id="cb141-14"><a href="#cb141-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-15"><a href="#cb141-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Example output: perplexity = 15.3</span></span>
<span id="cb141-16"><a href="#cb141-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpretation: Model is ~15-way confused on average</span></span></code></pre></div>
            <hr />
            <h3 id="why-perplexity-over-raw-accuracy">Why Perplexity
            Over Raw Accuracy?</h3>
            <table>
            <thead>
            <tr>
            <th>Metric</th>
            <th>Problem</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Accuracy</strong></td>
            <td>Binary (right/wrong) ‚Äî ignores confidence</td>
            </tr>
            <tr>
            <td><strong>Raw NLL</strong></td>
            <td>Unbounded, hard to interpret</td>
            </tr>
            <tr>
            <td><strong>Perplexity</strong></td>
            <td>Intuitive scale (effective vocabulary size)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: Two models predicting
            ‚Äúcat‚Äù:</p>
            <ul>
            <li>Model A: P(‚Äúcat‚Äù) = 0.51, P(‚Äúdog‚Äù) = 0.49 ‚Üí correct but
            barely</li>
            <li>Model B: P(‚Äúcat‚Äù) = 0.95, P(‚Äúdog‚Äù) = 0.05 ‚Üí correct and
            confident</li>
            </ul>
            <p>Both have 100% accuracy, but Model B has much lower
            perplexity (better).</p>
            <hr />
            <h3 id="typical-perplexity-values">Typical Perplexity
            Values</h3>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Dataset</th>
            <th>Perplexity</th>
            <th>Notes</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Random (50k vocab)</td>
            <td>Any</td>
            <td>~50,000</td>
            <td>Uniform distribution over vocab</td>
            </tr>
            <tr>
            <td>N-gram model</td>
            <td>PTB</td>
            <td>~150-200</td>
            <td>Traditional baseline</td>
            </tr>
            <tr>
            <td>LSTM</td>
            <td>PTB</td>
            <td>~60-80</td>
            <td>Recurrent baseline</td>
            </tr>
            <tr>
            <td>GPT-2 (small)</td>
            <td>WikiText-103</td>
            <td>~29</td>
            <td>117M parameters</td>
            </tr>
            <tr>
            <td>GPT-2 (large)</td>
            <td>WikiText-103</td>
            <td>~18</td>
            <td>1.5B parameters</td>
            </tr>
            <tr>
            <td>GPT-3</td>
            <td>Penn Treebank</td>
            <td>~20</td>
            <td>175B parameters</td>
            </tr>
            <tr>
            <td>State-of-the-art</td>
            <td>WikiText-103</td>
            <td>~10-15</td>
            <td>Modern transformers</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Rule of thumb</strong>: Lower is better. A 10%
            reduction in perplexity is meaningful.</p>
            <hr />
            <h3 id="perplexity-limitations">Perplexity Limitations</h3>
            <table>
            <colgroup>
            <col style="width: 48%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>Limitation</th>
            <th>Explanation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Doesn‚Äôt measure generation quality</strong></td>
            <td>Low PPL ‚â† coherent, interesting text</td>
            </tr>
            <tr>
            <td><strong>Doesn‚Äôt measure factuality</strong></td>
            <td>Model can be confident and wrong</td>
            </tr>
            <tr>
            <td><strong>Tokenization-dependent</strong></td>
            <td>Different tokenizers ‚Üí different PPL (not
            comparable)</td>
            </tr>
            <tr>
            <td><strong>Domain-specific</strong></td>
            <td>PPL on code ‚â† PPL on prose (can‚Äôt compare directly)</td>
            </tr>
            <tr>
            <td><strong>Not directly interpretable for
            generation</strong></td>
            <td>A model with PPL=20 doesn‚Äôt generate text that‚Äôs ‚Äú20-way
            confused‚Äù</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Important caveat</strong>: Perplexity measures
            how well a model <em>predicts</em> text, not how well it
            <em>generates</em> text. A model can have low perplexity but
            still generate repetitive, boring, or nonsensical
            outputs.</p>
            <hr />
            <h3
            id="per-token-vs-per-character-vs-per-word-perplexity">Per-Token
            vs Per-Character vs Per-Word Perplexity</h3>
            <p>Different tokenization schemes affect perplexity:</p>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 18%" />
            <col style="width: 33%" />
            <col style="width: 20%" />
            </colgroup>
            <thead>
            <tr>
            <th>Granularity</th>
            <th>Formula</th>
            <th>Typical Values</th>
            <th>Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Per-token (BPE)</strong></td>
            <td><span class="math inline">\(\exp(\text{loss per BPE
            token})\)</span></td>
            <td>10-50</td>
            <td>Most common for LLMs</td>
            </tr>
            <tr>
            <td><strong>Per-character</strong></td>
            <td><span class="math inline">\(\exp(\text{loss per
            character})\)</span></td>
            <td>1.5-3.0</td>
            <td>Character-level models</td>
            </tr>
            <tr>
            <td><strong>Per-word</strong></td>
            <td><span class="math inline">\(\exp(\text{loss per
            word})\)</span></td>
            <td>50-200</td>
            <td>Traditional NLP</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Conversion</strong>: You can roughly convert
            between them, but it‚Äôs complicated by tokenization details.
            <strong>Never compare perplexities across different
            tokenizers!</strong></p>
            <hr />
            <h3
            id="interview-q-what-is-perplexity-and-how-does-it-relate-to-cross-entropy">Interview
            Q: ‚ÄúWhat is perplexity and how does it relate to
            cross-entropy?‚Äù</h3>
            <p><strong>A</strong>: Perplexity is the exponential of
            cross-entropy: <span class="math inline">\(\text{PPL} =
            \exp(H) = \exp(-\frac{1}{N}\sum_i \log
            P(x_i|x_{&lt;i}))\)</span>. It measures how ‚Äúconfused‚Äù a
            model is when predicting text, interpretable as the
            effective vocabulary size the model is choosing from. A
            perplexity of 25 means the model is as uncertain as if
            uniformly choosing among 25 options per token.</p>
            <p>Lower perplexity = better model. It‚Äôs the standard metric
            for language model evaluation because: (1) it‚Äôs on an
            interpretable scale (unlike raw NLL), (2) it accounts for
            confidence (unlike accuracy), and (3) it directly measures
            the training objective (cross-entropy loss). However, it
            doesn‚Äôt capture generation quality, factuality, or other
            aspects important for deployed models.</p>
            <p><strong>Interview Q</strong>: ‚ÄúIf a model has perplexity
            20, what does that mean?‚Äù</p>
            <p><strong>A</strong>: A perplexity of 20 means the model
            is, on average, as uncertain as if it were uniformly
            choosing among 20 equally likely tokens at each position.
            Equivalently, the model assigns about 5% probability (1/20)
            to the correct token on average. This is computed as the
            exponential of the average negative log-likelihood. Lower is
            better ‚Äî state-of-the-art language models achieve
            perplexities of 10-20 on common benchmarks like
            WikiText-103.</p>
            <hr />
            <h3 id="summary-table">Summary Table</h3>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 32%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Concept</th>
            <th>Formula</th>
            <th>Measures</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Entropy</strong></td>
            <td><span class="math inline">\(H(X) = -\sum p \log
            p\)</span></td>
            <td>Uncertainty in <span
            class="math inline">\(X\)</span></td>
            </tr>
            <tr>
            <td><strong>Cross-Entropy</strong></td>
            <td><span class="math inline">\(H(P,Q) = -\sum p \log
            q\)</span></td>
            <td>Avg bits using <span class="math inline">\(Q\)</span>
            for <span class="math inline">\(P\)</span></td>
            </tr>
            <tr>
            <td><strong>KL Divergence</strong></td>
            <td><span class="math inline">\(D_{KL}(P\|Q) = \sum p \log
            \frac{p}{q}\)</span></td>
            <td>‚ÄúDistance‚Äù <span class="math inline">\(P\)</span> to
            <span class="math inline">\(Q\)</span></td>
            </tr>
            <tr>
            <td><strong>Mutual Info</strong></td>
            <td><span class="math inline">\(I(X;Y) = H(X) -
            H(X|Y)\)</span></td>
            <td>Shared information</td>
            </tr>
            <tr>
            <td><strong>Perplexity</strong></td>
            <td><span class="math inline">\(\text{PPL} =
            \exp(H(P,Q))\)</span></td>
            <td>Effective vocabulary size</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-why-use-cross-entropy-loss-instead-of-kl-divergence">Interview
            Q: ‚ÄúWhy use cross-entropy loss instead of KL
            divergence?‚Äù</h3>
            <p><strong>A</strong>: For classification, they‚Äôre
            equivalent! Since the true distribution <span
            class="math inline">\(P\)</span> is fixed (one-hot labels),
            <span class="math inline">\(H(P)\)</span> is constant.
            Therefore:</p>
            <p><span class="math display">\[\arg\min_\theta H(P,
            Q_\theta) = \arg\min_\theta [H(P) + D_{KL}(P \| Q_\theta)] =
            \arg\min_\theta D_{KL}(P \| Q_\theta)\]</span></p>
            <p>We use cross-entropy because:</p>
            <ol type="1">
            <li>It‚Äôs simpler to compute (don‚Äôt need <span
            class="math inline">\(H(P)\)</span>)</li>
            <li>For one-hot labels, <span class="math inline">\(H(P) =
            0\)</span> anyway</li>
            <li>Cross-entropy loss = negative log-likelihood (connects
            to MLE)</li>
            </ol>
            <h3
            id="interview-q-whats-the-relationship-between-cross-entropy-and-mle">Interview
            Q: ‚ÄúWhat‚Äôs the relationship between cross-entropy and
            MLE?‚Äù</h3>
            <p><strong>A</strong>: They‚Äôre the same! Minimizing
            cross-entropy loss is equivalent to maximizing
            log-likelihood:</p>
            <p><span class="math display">\[\min_\theta H(P, Q_\theta) =
            \min_\theta \left[-\sum_i \log Q_\theta(y_i | x_i)\right] =
            \max_\theta \sum_i \log Q_\theta(y_i | x_i)\]</span></p>
            <p>This is why cross-entropy is the ‚Äúnatural‚Äù loss for
            classification ‚Äî it has deep probabilistic foundations.</p>
            <hr />
            <h2 id="additional-mathematical-foundations">3.5 Additional
            Mathematical Foundations</h2>
            <h3 id="trace-and-frobenius-norm">Trace and Frobenius
            Norm</h3>
            <p><strong>Trace</strong>: Sum of diagonal elements of a
            square matrix.</p>
            <p><span class="math display">\[\text{tr}(\mathbf{A}) =
            \sum_{i=1}^{n} A_{ii}\]</span></p>
            <p><strong>Properties</strong>: - <span
            class="math inline">\(\text{tr}(\mathbf{A} + \mathbf{B}) =
            \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B})\)</span> -
            <span class="math inline">\(\text{tr}(\mathbf{AB}) =
            \text{tr}(\mathbf{BA})\)</span> (cyclic property ‚Äî even if
            <span class="math inline">\(\mathbf{AB} \neq
            \mathbf{BA}\)</span>!) - <span
            class="math inline">\(\text{tr}(\mathbf{A}) = \sum_i
            \lambda_i\)</span> (sum of eigenvalues)</p>
            <p><strong>Frobenius Norm</strong>: ‚ÄúEuclidean norm‚Äù for
            matrices.</p>
            <p><span class="math display">\[\|\mathbf{A}\|_F =
            \sqrt{\sum_{i,j} A_{ij}^2} =
            \sqrt{\text{tr}(\mathbf{A}^T\mathbf{A})} = \sqrt{\sum_i
            \sigma_i^2}\]</span></p>
            <p><strong>Why It Matters for ML</strong>: - <strong>L2
            regularization</strong>: <span class="math inline">\(\lambda
            \|\mathbf{W}\|_F^2 = \lambda \sum_{ij} W_{ij}^2\)</span>
            (weight decay!) - <strong>Low-rank approximation
            error</strong>: <span class="math inline">\(\|\mathbf{A} -
            \mathbf{A}_k\|_F^2 = \sum_{i &gt; k} \sigma_i^2\)</span> -
            <strong>Gradient norms</strong>: Measured in Frobenius norm
            for matrices</p>
            <hr />
            <h3 id="matrix-calculus-identities">Matrix Calculus
            Identities</h3>
            <p>Essential identities for deriving gradients in ML:</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 38%" />
            <col style="width: 22%" />
            </colgroup>
            <thead>
            <tr>
            <th>Expression</th>
            <th>Derivative</th>
            <th>Notes</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\mathbf{a}^T
            \mathbf{x}\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{x}} = \mathbf{a}\)</span></td>
            <td>Linear term</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\mathbf{x}^T \mathbf{A}
            \mathbf{x}\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{x}} = (\mathbf{A} +
            \mathbf{A}^T)\mathbf{x}\)</span></td>
            <td>Quadratic form</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\mathbf{x}^T \mathbf{A}
            \mathbf{x}\)</span> (symmetric <span
            class="math inline">\(\mathbf{A}\)</span>)</td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{x}} = 2\mathbf{A}\mathbf{x}\)</span></td>
            <td>Symmetric case</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\|\mathbf{Ax} -
            \mathbf{b}\|^2\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{x}} = 2\mathbf{A}^T(\mathbf{Ax} -
            \mathbf{b})\)</span></td>
            <td>Least squares!</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\text{tr}(\mathbf{A}\mathbf{X})\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{X}} = \mathbf{A}^T\)</span></td>
            <td>Trace derivative</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\text{tr}(\mathbf{X}^T\mathbf{A}\mathbf{X})\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{X}} = (\mathbf{A} +
            \mathbf{A}^T)\mathbf{X}\)</span></td>
            <td>Matrix quadratic</td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\log\det(\mathbf{X})\)</span></td>
            <td><span class="math inline">\(\frac{\partial}{\partial
            \mathbf{X}} = \mathbf{X}^{-T}\)</span></td>
            <td>Log-determinant</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Worked Example: Deriving Ridge Regression
            Gradient</strong></p>
            <p>For Ridge Regression: <span class="math inline">\(L =
            \|\mathbf{Xw} - \mathbf{y}\|^2 +
            \lambda\|\mathbf{w}\|^2\)</span></p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            \mathbf{w}} = 2\mathbf{X}^T(\mathbf{Xw} - \mathbf{y}) +
            2\lambda\mathbf{w}\]</span></p>
            <p>Setting to zero: <span
            class="math display">\[\mathbf{X}^T\mathbf{Xw} +
            \lambda\mathbf{w} = \mathbf{X}^T\mathbf{y}\]</span> <span
            class="math display">\[(\mathbf{X}^T\mathbf{X} +
            \lambda\mathbf{I})\mathbf{w} =
            \mathbf{X}^T\mathbf{y}\]</span> <span
            class="math display">\[\mathbf{w}^* =
            (\mathbf{X}^T\mathbf{X} +
            \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
            <p><strong>Interview Q</strong>: ‚ÄúDerive the closed-form
            solution for Ridge Regression.‚Äù</p>
            <p><strong>A</strong>: Start with <span
            class="math inline">\(L = \|\mathbf{Xw} - \mathbf{y}\|^2 +
            \lambda\|\mathbf{w}\|^2\)</span>. Take derivative: <span
            class="math inline">\(\frac{\partial L}{\partial \mathbf{w}}
            = 2\mathbf{X}^T(\mathbf{Xw} - \mathbf{y}) +
            2\lambda\mathbf{w}\)</span>. Set to zero and rearrange:
            <span class="math inline">\((\mathbf{X}^T\mathbf{X} +
            \lambda\mathbf{I})\mathbf{w} =
            \mathbf{X}^T\mathbf{y}\)</span>. The solution is <span
            class="math inline">\(\mathbf{w}^* = (\mathbf{X}^T\mathbf{X}
            + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.
            The <span class="math inline">\(\lambda\mathbf{I}\)</span>
            term ensures invertibility even if <span
            class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is
            singular.</p>
            <hr />
            <h3
            id="constrained-optimization-lagrange-multipliers">Constrained
            Optimization: Lagrange Multipliers</h3>
            <p><strong>Problem</strong>: Optimize <span
            class="math inline">\(f(\mathbf{x})\)</span> subject to
            constraint <span class="math inline">\(g(\mathbf{x}) =
            0\)</span>.</p>
            <p><strong>Method</strong>: Introduce Lagrange multiplier
            <span class="math inline">\(\lambda\)</span> and optimize
            the <strong>Lagrangian</strong>:</p>
            <p><span class="math display">\[\mathcal{L}(\mathbf{x},
            \lambda) = f(\mathbf{x}) - \lambda \cdot
            g(\mathbf{x})\]</span></p>
            <p><strong>Conditions for optimum</strong>: <span
            class="math display">\[\nabla_\mathbf{x} \mathcal{L} = 0
            \quad \text{and} \quad g(\mathbf{x}) = 0\]</span></p>
            <p><strong>Intuition</strong>: At the optimum, the gradient
            of <span class="math inline">\(f\)</span> is parallel to the
            gradient of <span class="math inline">\(g\)</span>. The
            multiplier <span class="math inline">\(\lambda\)</span>
            gives the ‚Äúexchange rate‚Äù ‚Äî how much the optimal value
            changes per unit relaxation of the constraint.</p>
            <h3 id="worked-example-maximum-entropy-distribution">Worked
            Example: Maximum Entropy Distribution</h3>
            <p><strong>Problem</strong>: Find the probability
            distribution <span class="math inline">\(p(x)\)</span> over
            <span class="math inline">\(\{1, 2, \ldots, n\}\)</span>
            that maximizes entropy, subject to <span
            class="math inline">\(\sum_i p_i = 1\)</span>.</p>
            <p><strong>Setup</strong>: - Maximize: <span
            class="math inline">\(H(p) = -\sum_i p_i \log p_i\)</span> -
            Subject to: <span class="math inline">\(g(p) = \sum_i p_i -
            1 = 0\)</span></p>
            <p><strong>Lagrangian</strong>: <span
            class="math display">\[\mathcal{L} = -\sum_i p_i \log p_i -
            \lambda\left(\sum_i p_i - 1\right)\]</span></p>
            <p><strong>Take derivative with respect to <span
            class="math inline">\(p_j\)</span></strong>: <span
            class="math display">\[\frac{\partial \mathcal{L}}{\partial
            p_j} = -\log p_j - 1 - \lambda = 0\]</span></p>
            <p><span class="math display">\[p_j =
            e^{-1-\lambda}\]</span></p>
            <p>Since this is the same for all <span
            class="math inline">\(j\)</span>, all <span
            class="math inline">\(p_j\)</span> are equal! Using the
            constraint: <span class="math display">\[n \cdot p_j = 1
            \implies p_j = \frac{1}{n}\]</span></p>
            <p><strong>Result</strong>: The <strong>uniform
            distribution</strong> maximizes entropy ‚Äî confirming our
            intuition that ‚Äúmost uncertain‚Äù = ‚Äúmost spread out.‚Äù</p>
            <h3 id="kkt-conditions-inequality-constraints">KKT
            Conditions (Inequality Constraints)</h3>
            <p>For problems with inequality constraints: minimize <span
            class="math inline">\(f(\mathbf{x})\)</span> subject to
            <span class="math inline">\(g_i(\mathbf{x}) \leq
            0\)</span>.</p>
            <p><strong>Karush-Kuhn-Tucker (KKT) conditions</strong>:</p>
            <ol type="1">
            <li><strong>Stationarity</strong>: <span
            class="math inline">\(\nabla f(\mathbf{x}^*) + \sum_i \mu_i
            \nabla g_i(\mathbf{x}^*) = 0\)</span></li>
            <li><strong>Primal feasibility</strong>: <span
            class="math inline">\(g_i(\mathbf{x}^*) \leq 0\)</span> for
            all <span class="math inline">\(i\)</span></li>
            <li><strong>Dual feasibility</strong>: <span
            class="math inline">\(\mu_i \geq 0\)</span> for all <span
            class="math inline">\(i\)</span></li>
            <li><strong>Complementary slackness</strong>: <span
            class="math inline">\(\mu_i \cdot g_i(\mathbf{x}^*) =
            0\)</span> for all <span
            class="math inline">\(i\)</span></li>
            </ol>
            <p><strong>Complementary slackness</strong> is the key
            insight: either the constraint is <strong>active</strong>
            (<span class="math inline">\(g_i = 0\)</span>) or the
            multiplier is <strong>zero</strong> (<span
            class="math inline">\(\mu_i = 0\)</span>). You can‚Äôt have
            both non-zero.</p>
            <p><strong>ML Application: SVM Dual</strong></p>
            <p>The SVM optimization uses KKT conditions. Support vectors
            are exactly the points where the constraint is active (they
            lie on the margin). The dual problem:</p>
            <p><span class="math display">\[\max_\alpha \sum_i \alpha_i
            - \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j
            \mathbf{x}_i^T\mathbf{x}_j\]</span></p>
            <p>subject to <span class="math inline">\(\alpha_i \geq
            0\)</span> and <span class="math inline">\(\sum_i \alpha_i
            y_i = 0\)</span>.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat are Lagrange
            multipliers and when do you use them?‚Äù</p>
            <p><strong>A</strong>: Lagrange multipliers convert a
            constrained optimization problem into an unconstrained one.
            For minimizing <span
            class="math inline">\(f(\mathbf{x})\)</span> subject to
            <span class="math inline">\(g(\mathbf{x}) = 0\)</span>, we
            form the Lagrangian <span class="math inline">\(\mathcal{L}
            = f - \lambda g\)</span> and optimize over both <span
            class="math inline">\(\mathbf{x}\)</span> and <span
            class="math inline">\(\lambda\)</span>. At the optimum,
            <span class="math inline">\(\nabla f\)</span> is parallel to
            <span class="math inline">\(\nabla g\)</span>. In ML, they
            appear in: (1) deriving the SVM dual, (2) showing that
            maximum entropy gives uniform distribution, (3) constrained
            optimization in neural architecture search. KKT conditions
            extend this to inequality constraints.</p>
            <hr />
            <h3 id="matrix-decompositions-summary">Matrix Decompositions
            Summary</h3>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 12%" />
            <col style="width: 29%" />
            <col style="width: 27%" />
            </colgroup>
            <thead>
            <tr>
            <th>Decomposition</th>
            <th>Form</th>
            <th>Requirements</th>
            <th>ML Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Eigendecomposition</strong></td>
            <td><span class="math inline">\(\mathbf{A} =
            \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}\)</span></td>
            <td>Square matrix</td>
            <td>PCA (covariance matrix)</td>
            </tr>
            <tr>
            <td><strong>SVD</strong></td>
            <td><span class="math inline">\(\mathbf{A} =
            \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T\)</span></td>
            <td>Any matrix</td>
            <td>Low-rank approx, LoRA</td>
            </tr>
            <tr>
            <td><strong>Cholesky</strong></td>
            <td><span class="math inline">\(\mathbf{A} =
            \mathbf{L}\mathbf{L}^T\)</span></td>
            <td>PSD matrix</td>
            <td>Sampling from Gaussians</td>
            </tr>
            <tr>
            <td><strong>QR</strong></td>
            <td><span class="math inline">\(\mathbf{A} =
            \mathbf{Q}\mathbf{R}\)</span></td>
            <td>Any matrix</td>
            <td>Numerical stability</td>
            </tr>
            <tr>
            <td><strong>LU</strong></td>
            <td><span class="math inline">\(\mathbf{A} =
            \mathbf{L}\mathbf{U}\)</span></td>
            <td>Square, invertible</td>
            <td>Solving linear systems</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Eigendecomposition vs SVD</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Eigendecomposition</th>
            <th>SVD</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Applies to</td>
            <td>Square matrices only</td>
            <td>Any matrix</td>
            </tr>
            <tr>
            <td>Vectors</td>
            <td>May not be orthogonal</td>
            <td>Always orthogonal</td>
            </tr>
            <tr>
            <td>Values</td>
            <td>Can be negative/complex</td>
            <td>Always non-negative real</td>
            </tr>
            <tr>
            <td>Relation</td>
            <td>For symmetric <span
            class="math inline">\(\mathbf{A}\)</span>: singular values
            =</td>
            <td>eigenvalues</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhen would you use
            Cholesky decomposition?‚Äù</p>
            <p><strong>A</strong>: Cholesky decomposes a positive
            semi-definite matrix as <span
            class="math inline">\(\mathbf{A} =
            \mathbf{L}\mathbf{L}^T\)</span> where <span
            class="math inline">\(\mathbf{L}\)</span> is lower
            triangular. Use it when: (1) Sampling from multivariate
            Gaussians: <span class="math inline">\(\mathbf{x} =
            \boldsymbol{\mu} + \mathbf{L}\mathbf{z}\)</span> where <span
            class="math inline">\(\mathbf{z} \sim \mathcal{N}(0,
            I)\)</span>. (2) Solving linear systems <span
            class="math inline">\(\mathbf{Ax} = \mathbf{b}\)</span> when
            <span class="math inline">\(\mathbf{A}\)</span> is PSD ‚Äî
            it‚Äôs twice as fast as LU. (3) Computing log-determinants:
            <span class="math inline">\(\log\det(\mathbf{A}) = 2\sum_i
            \log L_{ii}\)</span>. It‚Äôs more numerically stable than
            eigendecomposition for covariance matrices.</p>
            <hr />
            <h3 id="numerical-stability-log-sum-exp-trick">Numerical
            Stability: Log-Sum-Exp Trick</h3>
            <p><strong>The Problem</strong>: Computing <span
            class="math inline">\(\log\left(\sum_i
            e^{x_i}\right)\)</span> for softmax/cross-entropy causes
            numerical overflow/underflow.</p>
            <div class="sourceCode" id="cb142"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a><span class="co"># This OVERFLOWS for large values</span></span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1000</span>, <span class="dv">1001</span>, <span class="dv">1002</span>])</span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a>np.log(np.<span class="bu">sum</span>(np.exp(x)))  <span class="co"># inf! (overflow)</span></span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a><span class="co"># This UNDERFLOWS for very negative values</span></span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1000</span>, <span class="op">-</span><span class="dv">1001</span>, <span class="op">-</span><span class="dv">1002</span>])</span>
<span id="cb142-9"><a href="#cb142-9" aria-hidden="true" tabindex="-1"></a>np.log(np.<span class="bu">sum</span>(np.exp(x)))  <span class="co"># -inf! (underflow)</span></span></code></pre></div>
            <p><strong>The Solution</strong>: Log-Sum-Exp trick</p>
            <p><span class="math display">\[\log\sum_i e^{x_i} =
            \max_j(x_j) + \log\sum_i e^{x_i - \max_j(x_j)}\]</span></p>
            <div class="sourceCode" id="cb143"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsumexp_stable(x):</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Numerically stable log-sum-exp&quot;&quot;&quot;</span></span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> np.<span class="bu">max</span>(x)  <span class="co"># Shift by max</span></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c <span class="op">+</span> np.log(np.<span class="bu">sum</span>(np.exp(x <span class="op">-</span> c)))</span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Now it works!</span></span>
<span id="cb143-7"><a href="#cb143-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1000</span>, <span class="dv">1001</span>, <span class="dv">1002</span>])</span>
<span id="cb143-8"><a href="#cb143-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(logsumexp_stable(x))  <span class="co"># 1002.41 (correct!)</span></span></code></pre></div>
            <p><strong>Why It Works</strong>: Subtracting the max
            ensures <span class="math inline">\(x_i - \max(x) \leq
            0\)</span>, so <span class="math inline">\(e^{x_i - \max(x)}
            \leq 1\)</span> ‚Äî no overflow. And at least one term equals
            <span class="math inline">\(e^0 = 1\)</span>, so the sum
            <span class="math inline">\(\geq 1\)</span> ‚Äî no underflow
            in the log.</p>
            <p><strong>Stable Softmax</strong>:</p>
            <div class="sourceCode" id="cb144"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax_stable(x):</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Numerically stable softmax&quot;&quot;&quot;</span></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>    x_shifted <span class="op">=</span> x <span class="op">-</span> np.<span class="bu">max</span>(x)  <span class="co"># Shift for stability</span></span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x_shifted)</span>
<span id="cb144-5"><a href="#cb144-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> np.<span class="bu">sum</span>(exp_x)</span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúHow do you compute softmax
            numerically stably?‚Äù</p>
            <p><strong>A</strong>: Subtract the maximum value before
            exponentiating: <span
            class="math inline">\(\text{softmax}(x_i) = \frac{e^{x_i -
            \max(x)}}{\sum_j e^{x_j - \max(x)}}\)</span>. This is
            mathematically equivalent (the max cancels) but prevents
            overflow. Without this, <span
            class="math inline">\(e^{1000}\)</span> overflows to
            infinity. With the shift, the largest exponent is <span
            class="math inline">\(e^0 = 1\)</span>. Similarly for
            log-sum-exp: <span class="math inline">\(\log\sum e^{x_i} =
            \max(x) + \log\sum e^{x_i - \max(x)}\)</span>.</p>
            <hr />
            <h3 id="positive-semi-definite-psd-matrices">Positive
            Semi-Definite (PSD) Matrices</h3>
            <p><strong>Definition</strong>: A symmetric matrix <span
            class="math inline">\(\mathbf{A}\)</span> is
            <strong>positive semi-definite</strong> if:</p>
            <p><span class="math display">\[\mathbf{x}^T \mathbf{A}
            \mathbf{x} \geq 0 \quad \text{for all }
            \mathbf{x}\]</span></p>
            <p><strong>Equivalent conditions</strong> (all mean the same
            thing):</p>
            <ol type="1">
            <li>All eigenvalues <span class="math inline">\(\lambda_i
            \geq 0\)</span></li>
            <li><span class="math inline">\(\mathbf{x}^T \mathbf{A}
            \mathbf{x} \geq 0\)</span> for all <span
            class="math inline">\(\mathbf{x}\)</span></li>
            <li><span class="math inline">\(\mathbf{A} =
            \mathbf{B}^T\mathbf{B}\)</span> for some matrix <span
            class="math inline">\(\mathbf{B}\)</span></li>
            <li>All principal minors are non-negative</li>
            <li>Cholesky decomposition exists</li>
            </ol>
            <p><strong>Why PSD Matters in ML</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>Context</th>
            <th>Why PSD</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Covariance matrices</strong></td>
            <td>Always PSD by construction: <span
            class="math inline">\(\mathbf{\Sigma} =
            \mathbb{E}[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^T]\)</span></td>
            </tr>
            <tr>
            <td><strong>Kernel matrices</strong></td>
            <td>Gram matrix <span class="math inline">\(K_{ij} = k(x_i,
            x_j)\)</span> must be PSD for valid kernels</td>
            </tr>
            <tr>
            <td><strong>Hessian at minimum</strong></td>
            <td>Must be PSD (positive = local min)</td>
            </tr>
            <tr>
            <td><strong>Optimization</strong></td>
            <td>Convex functions have PSD Hessian</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhy must covariance
            matrices be positive semi-definite?‚Äù</p>
            <p><strong>A</strong>: By definition, <span
            class="math inline">\(\mathbf{\Sigma} =
            \mathbb{E}[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^T]\)</span>.
            For any vector <span
            class="math inline">\(\mathbf{v}\)</span>: <span
            class="math inline">\(\mathbf{v}^T\mathbf{\Sigma}\mathbf{v}
            = \mathbb{E}[(\mathbf{v}^T(\mathbf{x}-\boldsymbol{\mu}))^2]
            = \mathbb{E}[z^2] \geq 0\)</span> where <span
            class="math inline">\(z =
            \mathbf{v}^T(\mathbf{x}-\boldsymbol{\mu})\)</span> is a
            scalar. Since this is an expected squared value, it‚Äôs always
            non-negative. This also ensures the Gaussian PDF is
            well-defined (we need <span
            class="math inline">\(\det(\mathbf{\Sigma}) &gt; 0\)</span>
            for strictly PD).</p>
            <hr />
            <h2 id="common-interview-gotchas">3.6 Common Interview
            Gotchas</h2>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 60%" />
            </colgroup>
            <thead>
            <tr>
            <th>Question</th>
            <th>Gotcha Answer</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>‚ÄúIs KL divergence a metric?‚Äù</td>
            <td><strong>No!</strong> Not symmetric (<span
            class="math inline">\(D_{KL}(P\|Q) \neq
            D_{KL}(Q\|P)\)</span>) and doesn‚Äôt satisfy triangle
            inequality</td>
            </tr>
            <tr>
            <td>‚ÄúCan entropy be negative?‚Äù</td>
            <td><strong>Yes</strong> ‚Äî for continuous distributions,
            differential entropy can be negative (e.g., uniform on <span
            class="math inline">\([0, 0.5]\)</span>)</td>
            </tr>
            <tr>
            <td>‚ÄúWhat‚Äôs the difference between MLE and MAP?‚Äù</td>
            <td>MAP = MLE + prior. MAP with Gaussian prior = MLE + L2
            regularization</td>
            </tr>
            <tr>
            <td>‚ÄúIs the Hessian always symmetric?‚Äù</td>
            <td><strong>Yes</strong> ‚Äî if the function is twice
            continuously differentiable (Schwarz‚Äôs theorem)</td>
            </tr>
            <tr>
            <td>‚ÄúCan eigenvalues be complex?‚Äù</td>
            <td><strong>Yes</strong> ‚Äî for non-symmetric matrices. But
            covariance/kernel matrices (symmetric) have real
            eigenvalues</td>
            </tr>
            <tr>
            <td>‚ÄúIs PCA the same as SVD?‚Äù</td>
            <td><strong>Related but different</strong>: PCA on centered
            data = SVD of data matrix. PCA finds eigenvectors of <span
            class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>; SVD
            works directly on <span
            class="math inline">\(\mathbf{X}\)</span></td>
            </tr>
            <tr>
            <td>‚ÄúWhy log-likelihood instead of likelihood?‚Äù</td>
            <td>Numerical stability (products ‚Üí sums), easier
            optimization (log is monotonic), connects to information
            theory</td>
            </tr>
            <tr>
            <td>‚ÄúIs cross-entropy symmetric?‚Äù</td>
            <td><strong>No!</strong> <span class="math inline">\(H(P,Q)
            \neq H(Q,P)\)</span> in general</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="quick-reference-card">3.7 Quick Reference Card</h2>
            <h3 id="linear-algebra-essentials">Linear Algebra
            Essentials</h3>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 29%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Operation</th>
            <th>Formula</th>
            <th>Dimension</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Matrix multiply</td>
            <td><span class="math inline">\((\mathbf{AB})_{ij} = \sum_k
            A_{ik}B_{kj}\)</span></td>
            <td><span class="math inline">\((m \times k)(k \times n) =
            (m \times n)\)</span></td>
            </tr>
            <tr>
            <td>Dot product</td>
            <td><span class="math inline">\(\mathbf{x} \cdot \mathbf{y}
            = \sum_i x_i y_i\)</span></td>
            <td>Scalar</td>
            </tr>
            <tr>
            <td>Outer product</td>
            <td><span
            class="math inline">\(\mathbf{x}\mathbf{y}^T\)</span></td>
            <td><span class="math inline">\((n \times 1)(1 \times m) =
            (n \times m)\)</span></td>
            </tr>
            <tr>
            <td>Trace</td>
            <td><span class="math inline">\(\text{tr}(\mathbf{A}) =
            \sum_i A_{ii}\)</span></td>
            <td>Scalar</td>
            </tr>
            <tr>
            <td>Frobenius norm</td>
            <td><span class="math inline">\(\|\mathbf{A}\|_F =
            \sqrt{\sum_{ij} A_{ij}^2}\)</span></td>
            <td>Scalar</td>
            </tr>
            <tr>
            <td>Determinant</td>
            <td><span class="math inline">\(\det(\mathbf{A}) = \prod_i
            \lambda_i\)</span></td>
            <td>Scalar</td>
            </tr>
            </tbody>
            </table>
            <h3 id="key-derivatives">Key Derivatives</h3>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Function</th>
            <th>Derivative</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline">\(\mathbf{a}^T\mathbf{x}\)</span></td>
            <td><span class="math inline">\(\mathbf{a}\)</span></td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\mathbf{x}^T\mathbf{Ax}\)</span>
            (symmetric)</td>
            <td><span class="math inline">\(2\mathbf{Ax}\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\|\mathbf{Ax} -
            \mathbf{b}\|^2\)</span></td>
            <td><span class="math inline">\(2\mathbf{A}^T(\mathbf{Ax} -
            \mathbf{b})\)</span></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\sigma(x) =
            \frac{1}{1+e^{-x}}\)</span></td>
            <td><span
            class="math inline">\(\sigma(x)(1-\sigma(x))\)</span></td>
            </tr>
            <tr>
            <td><span
            class="math inline">\(\log(\sigma(x))\)</span></td>
            <td><span class="math inline">\(1 - \sigma(x)\)</span></td>
            </tr>
            <tr>
            <td>Softmax + CE loss</td>
            <td><span class="math inline">\(\hat{y} - y\)</span></td>
            </tr>
            </tbody>
            </table>
            <h3 id="probability-essentials">Probability Essentials</h3>
            <table>
            <thead>
            <tr>
            <th>Concept</th>
            <th>Formula</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Bayes‚Äô theorem</td>
            <td><span class="math inline">\(P(A|B) =
            \frac{P(B|A)P(A)}{P(B)}\)</span></td>
            </tr>
            <tr>
            <td>Entropy</td>
            <td><span class="math inline">\(H(X) = -\sum_x p(x)\log
            p(x)\)</span></td>
            </tr>
            <tr>
            <td>Cross-entropy</td>
            <td><span class="math inline">\(H(P,Q) = -\sum_x p(x)\log
            q(x)\)</span></td>
            </tr>
            <tr>
            <td>KL divergence</td>
            <td><span class="math inline">\(D_{KL}(P\|Q) = \sum_x
            p(x)\log\frac{p(x)}{q(x)}\)</span></td>
            </tr>
            <tr>
            <td>Mutual information</td>
            <td><span class="math inline">\(I(X;Y) = H(X) -
            H(X|Y)\)</span></td>
            </tr>
            </tbody>
            </table>
            <h3 id="critical-relationships">Critical Relationships</h3>
            <p><span class="math display">\[\boxed{H(P, Q) = H(P) +
            D_{KL}(P \| Q)}\]</span></p>
            <p><span class="math display">\[\boxed{\text{Cross-Entropy
            Loss} = \text{Negative Log-Likelihood}}\]</span></p>
            <p><span class="math display">\[\boxed{\text{MAP with
            Gaussian prior} = \text{MLE} + \text{L2
            regularization}}\]</span></p>
            <p><span class="math display">\[\boxed{\det(\mathbf{A}) =
            \prod_i \lambda_i, \quad \text{tr}(\mathbf{A}) = \sum_i
            \lambda_i}\]</span></p>
            <hr />
            <h1 id="part-4-ml-fundamentals">Part 4: ML Fundamentals</h1>
            <h2 id="bias-variance-tradeoff">4.1 Bias-Variance
            Tradeoff</h2>
            <h3 id="what-this-means-for-beginners-1">What This Means
            (For Beginners)</h3>
            <p>Imagine you‚Äôre learning to throw darts at a target:</p>
            <ul>
            <li><strong>High bias</strong> = You always miss in the same
            direction. You‚Äôre consistently wrong. Maybe you aim too far
            left every time. Your ‚Äúmental model‚Äù of where to throw is
            fundamentally flawed.</li>
            <li><strong>High variance</strong> = Your throws are
            scattered all over the place. Sometimes you hit, sometimes
            you miss wildly in different directions. You‚Äôre
            inconsistent.</li>
            </ul>
            <p>The <strong>ideal</strong> is low bias (accurate on
            average) AND low variance (consistent). The challenge is
            that reducing one often increases the other:</p>
            <ul>
            <li><strong>Simple model</strong> (like a straight line):
            Low variance (stable predictions), but high bias (might miss
            the true pattern)</li>
            <li><strong>Complex model</strong> (wiggly curve): Low bias
            (can capture any pattern), but high variance (too sensitive
            to noise)</li>
            </ul>
            <p><strong>The key insight</strong>: You need to find the
            ‚Äúsweet spot‚Äù ‚Äî a model complex enough to capture the real
            pattern, but simple enough not to memorize noise.</p>
            <hr />
            <h3 id="the-decomposition">The Decomposition</h3>
            <p>For a model‚Äôs prediction <span
            class="math inline">\(\hat{f}(x)\)</span> on a test point,
            the expected squared error can be decomposed:</p>
            <p><span class="math display">\[\mathbb{E}[(y -
            \hat{f}(x))^2] = \underbrace{(\mathbb{E}[\hat{f}(x)] -
            f(x))^2}_{\text{Bias}^2} +
            \underbrace{\mathbb{E}[(\hat{f}(x) -
            \mathbb{E}[\hat{f}(x)])^2]}_{\text{Variance}} +
            \underbrace{\sigma^2}_{\text{Irreducible
            Noise}}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(y = f(x) + \epsilon\)</span>
            is the true data-generating process (true function <span
            class="math inline">\(f(x)\)</span> plus noise <span
            class="math inline">\(\epsilon\)</span>)</li>
            <li><span class="math inline">\(\hat{f}(x)\)</span> is our
            model‚Äôs prediction (trained on a random training set)</li>
            <li><span class="math inline">\(f(x)\)</span> is the true
            underlying function we‚Äôre trying to learn</li>
            <li><span class="math inline">\(\epsilon \sim \mathcal{N}(0,
            \sigma^2)\)</span> is irreducible noise, independent of our
            model</li>
            <li>The expectation <span
            class="math inline">\(\mathbb{E}[\cdot]\)</span> is over
            different training sets</li>
            </ul>
            <p><strong>Interpreting each term:</strong></p>
            <ul>
            <li><strong>Bias¬≤</strong> = <span
            class="math inline">\((\mathbb{E}[\hat{f}(x)] -
            f(x))^2\)</span> ‚Äî How far off is our model <em>on
            average</em>? Error from wrong assumptions
            (underfitting)</li>
            <li><strong>Variance</strong> = <span
            class="math inline">\(\mathbb{E}[(\hat{f}(x) -
            \mathbb{E}[\hat{f}(x)])^2]\)</span> ‚Äî How much does our
            model change with different training sets? Error from
            sensitivity to training data (overfitting)</li>
            <li><strong>Irreducible error</strong> = <span
            class="math inline">\(\sigma^2\)</span> ‚Äî Noise inherent in
            the data that no model can eliminate</li>
            </ul>
            <hr />
            <h3 id="intuition">Intuition</h3>
            <p><img src="figures/bias_variance.png"
            alt="Bias-Variance Tradeoff" /> <em>Figure: Visualization of
            the bias-variance tradeoff showing underfitting (high bias),
            optimal fit, and overfitting (high variance).</em></p>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>High Bias (Underfitting)</th>
            <th>High Variance (Overfitting)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Model</strong></td>
            <td>Too simple (e.g., line)</td>
            <td>Too complex (e.g., wiggly)</td>
            </tr>
            <tr>
            <td><strong>Pattern</strong></td>
            <td>Misses the true pattern</td>
            <td>Captures noise</td>
            </tr>
            <tr>
            <td><strong>Training Error</strong></td>
            <td>High</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>Test Error</strong></td>
            <td>High</td>
            <td>High</td>
            </tr>
            <tr>
            <td><strong>Problem</strong></td>
            <td>Wrong assumptions</td>
            <td>Too sensitive to training data</td>
            </tr>
            </tbody>
            </table>
            <h4 id="why-high-bias-means-underfitting">Why High Bias
            Means Underfitting</h4>
            <p><strong>High bias</strong> = the model is <strong>too
            simple</strong> to capture the true underlying pattern.</p>
            <ul>
            <li><strong>What happens</strong>: The model makes overly
            simplistic assumptions (e.g., ‚Äúthe relationship is linear‚Äù
            when it‚Äôs actually curved). It cannot even fit the training
            data well.</li>
            <li><strong>Training error</strong>: <strong>High</strong> ‚Äî
            the model fails to capture the pattern even in the data it‚Äôs
            trained on</li>
            <li><strong>Test error</strong>: <strong>High</strong> ‚Äî
            since it didn‚Äôt learn the pattern, it also fails on new
            data</li>
            <li><strong>Visual</strong>: Imagine fitting a straight line
            to clearly curved data ‚Äî the line misses most points</li>
            </ul>
            <p><strong>Key insight</strong>: With high bias, if you
            trained the same simple model on different training sets,
            you‚Äôd get roughly the same (wrong) answer each time. The
            model is consistently wrong because its assumptions are
            fundamentally flawed.</p>
            <h4 id="why-high-variance-means-overfitting">Why High
            Variance Means Overfitting</h4>
            <p><strong>High variance</strong> = the model is <strong>too
            complex</strong> and memorizes training data, including
            noise.</p>
            <ul>
            <li><strong>What happens</strong>: The model fits the
            training data too perfectly ‚Äî it captures not just the true
            pattern but also random noise specific to that particular
            training set</li>
            <li><strong>Training error</strong>: <strong>Low</strong> ‚Äî
            the model fits training points extremely well (sometimes
            perfectly)</li>
            <li><strong>Test error</strong>: <strong>High</strong> ‚Äî the
            noise patterns won‚Äôt repeat in new data, so those ‚Äúlearned‚Äù
            wiggles are wrong</li>
            <li><strong>Visual</strong>: A wiggly curve that passes
            through every training point, but would miss new test
            points</li>
            </ul>
            <p><strong>Key insight</strong>: With high variance, if you
            trained the same complex model on different training sets,
            you‚Äôd get wildly different models each time. The predictions
            are highly sensitive to which specific training examples you
            happened to use ‚Äî that‚Äôs the ‚Äúvariance.‚Äù</p>
            <p><strong>Why it fails to generalize</strong>: The wiggles
            that chase training noise are <strong>specific to that
            training set</strong>. New test data will have different
            noise, so those same wiggles become errors.</p>
            <hr />
            <h3 id="complexity-tradeoff">Complexity Tradeoff</h3>
            <p><img src="figures/complexity_tradeoff.png"
            alt="Bias-Variance Tradeoff" /> <em>Figure: The
            bias-variance tradeoff. Training error decreases with
            complexity, while test error is U-shaped. The sweet spot
            minimizes test error.</em></p>
            <hr />
            <h3 id="mathematical-example">Mathematical Example</h3>
            <p>Consider fitting polynomials to noisy data:</p>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Bias</th>
            <th>Variance</th>
            <th>Total Error</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Degree 1 (line)</td>
            <td>High</td>
            <td>Low</td>
            <td>High (underfitting)</td>
            </tr>
            <tr>
            <td>Degree 3</td>
            <td>Medium</td>
            <td>Medium</td>
            <td><strong>Low</strong> (optimal)</td>
            </tr>
            <tr>
            <td>Degree 15</td>
            <td>Low</td>
            <td>High</td>
            <td>High (overfitting)</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="controlling-the-tradeoff">Controlling the
            Tradeoff</h3>
            <table>
            <thead>
            <tr>
            <th>To Reduce Bias</th>
            <th>To Reduce Variance</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>More complex model</td>
            <td>Simpler model</td>
            </tr>
            <tr>
            <td>More features</td>
            <td>Fewer features</td>
            </tr>
            <tr>
            <td>Less regularization</td>
            <td>More regularization</td>
            </tr>
            <tr>
            <td>Boosting</td>
            <td>Bagging/Ensembles</td>
            </tr>
            <tr>
            <td></td>
            <td>More training data</td>
            </tr>
            <tr>
            <td></td>
            <td>Dropout</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="interview-follow-ups">Interview Follow-ups</h3>
            <p><strong>Q</strong>: ‚ÄúYou have high bias. What do you
            do?‚Äù</p>
            <p><strong>A</strong>:</p>
            <ul>
            <li>Use a more complex model (more layers, more
            neurons)</li>
            <li>Add more features or feature engineering</li>
            <li>Reduce regularization</li>
            <li>Train longer</li>
            <li>Use boosting ensemble methods</li>
            </ul>
            <p><strong>Q</strong>: ‚ÄúYou have high variance. What do you
            do?‚Äù</p>
            <p><strong>A</strong>:</p>
            <ul>
            <li>Get more training data</li>
            <li>Reduce model complexity</li>
            <li>Add regularization (L1, L2, dropout)</li>
            <li>Use early stopping</li>
            <li>Use bagging/ensembles</li>
            <li>Feature selection (remove irrelevant features)</li>
            </ul>
            <p><strong>Q</strong>: ‚ÄúHow do you know if you have high
            bias vs high variance?‚Äù</p>
            <p><strong>A</strong>:</p>
            <ul>
            <li><strong>High bias</strong>: Training error is high,
            similar to test error</li>
            <li><strong>High variance</strong>: Training error is low,
            test error is much higher</li>
            <li>Look at learning curves: plot training/validation error
            vs.¬†training set size</li>
            </ul>
            <hr />
            <h2 id="overfitting-and-underfitting">4.2 Overfitting and
            Underfitting</h2>
            <h3 id="what-this-means-for-beginners-2">What This Means
            (For Beginners)</h3>
            <p>Think of learning like studying for an exam:</p>
            <p><strong>Overfitting = Memorizing without
            understanding</strong></p>
            <ul>
            <li>You memorize every practice question and answer
            word-for-word</li>
            <li>On the practice test, you get 100%</li>
            <li>But on the real exam with slightly different questions,
            you fail</li>
            <li>You learned the specific examples, not the underlying
            concepts</li>
            </ul>
            <p><strong>Underfitting = Not studying enough</strong></p>
            <ul>
            <li>You barely looked at the material</li>
            <li>You do poorly on both practice tests AND the real
            exam</li>
            <li>Your ‚Äúmodel‚Äù of the subject is too simple to be
            useful</li>
            </ul>
            <p><strong>Good fit = Understanding the
            concepts</strong></p>
            <ul>
            <li>You understand the underlying patterns</li>
            <li>You can apply knowledge to new, unseen questions</li>
            <li>Good performance on both practice AND real exams</li>
            </ul>
            <pre><code>The goal of ML is NOT to memorize training data perfectly.
The goal is to learn patterns that work on NEW, unseen data.</code></pre>
            <p><strong>Real-world example</strong>:</p>
            <p>Imagine a model that predicts house prices:</p>
            <ul>
            <li><strong>Overfit model</strong>: ‚ÄúHouse at 123 Main St
            sold for $500k, so any house at 123 Main St = $500k‚Äù</li>
            <li><strong>Underfit model</strong>: ‚ÄúAll houses cost
            $300k‚Äù</li>
            <li><strong>Good model</strong>: ‚ÄúPrice ‚âà $200/sqft √ó size +
            $50k√óbedrooms + neighborhood_factor‚Äù</li>
            </ul>
            <hr />
            <h3 id="what-is-overfitting">What is Overfitting?</h3>
            <p><strong>Overfitting</strong> occurs when a model learns
            the training data too well, including noise and random
            fluctuations, causing it to perform poorly on new, unseen
            data.</p>
            <p><strong>Signs of Overfitting</strong>:</p>
            <ul>
            <li>Low training loss, high validation/test loss</li>
            <li>Gap between training and validation accuracy</li>
            <li>Model performance degrades as training continues</li>
            </ul>
            <h3 id="what-is-underfitting">What is Underfitting?</h3>
            <p><strong>Underfitting</strong> occurs when a model is too
            simple to capture the underlying patterns in the data.</p>
            <p><strong>Signs of Underfitting</strong>:</p>
            <ul>
            <li>High training loss</li>
            <li>Training and validation loss are both high and
            similar</li>
            <li>Model doesn‚Äôt improve much with more training</li>
            </ul>
            <hr />
            <h3 id="detecting-overfitting">Detecting Overfitting</h3>
            <p><strong>Learning Curves</strong>:</p>
            <p><img src="figures/learning_curves.png"
            alt="Learning Curves" /> <em>Figure: Learning curves showing
            training vs validation loss. The gap between curves
            indicates overfitting.</em></p>
            <hr />
            <h3 id="prevention-techniques">Prevention Techniques</h3>
            <table>
            <colgroup>
            <col style="width: 28%" />
            <col style="width: 36%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th>Technique</th>
            <th>How It Works</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>L1/L2 Regularization</strong></td>
            <td>Penalize large weights</td>
            <td>Always (baseline)</td>
            </tr>
            <tr>
            <td><strong>Dropout</strong></td>
            <td>Randomly zero neurons</td>
            <td>Deep networks</td>
            </tr>
            <tr>
            <td><strong>Early Stopping</strong></td>
            <td>Stop when val loss increases</td>
            <td>Simple, effective</td>
            </tr>
            <tr>
            <td><strong>Data Augmentation</strong></td>
            <td>Create more training data</td>
            <td>Images, text</td>
            </tr>
            <tr>
            <td><strong>Batch Normalization</strong></td>
            <td>Normalize activations</td>
            <td>Deep networks</td>
            </tr>
            <tr>
            <td><strong>Weight Decay</strong></td>
            <td>Same as L2 in SGD</td>
            <td>AdamW</td>
            </tr>
            <tr>
            <td><strong>Reduce Model Size</strong></td>
            <td>Fewer parameters</td>
            <td>Last resort</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="early-stopping">Early Stopping</h3>
            <div class="sourceCode" id="cb146"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>best_val_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>)</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>patience <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a>patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb146-5"><a href="#cb146-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(max_epochs):</span>
<span id="cb146-6"><a href="#cb146-6" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> train_one_epoch()</span>
<span id="cb146-7"><a href="#cb146-7" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> validate()</span>
<span id="cb146-8"><a href="#cb146-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb146-9"><a href="#cb146-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> val_loss <span class="op">&lt;</span> best_val_loss:</span>
<span id="cb146-10"><a href="#cb146-10" aria-hidden="true" tabindex="-1"></a>        best_val_loss <span class="op">=</span> val_loss</span>
<span id="cb146-11"><a href="#cb146-11" aria-hidden="true" tabindex="-1"></a>        save_checkpoint()</span>
<span id="cb146-12"><a href="#cb146-12" aria-hidden="true" tabindex="-1"></a>        patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb146-13"><a href="#cb146-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb146-14"><a href="#cb146-14" aria-hidden="true" tabindex="-1"></a>        patience_counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb146-15"><a href="#cb146-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb146-16"><a href="#cb146-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> patience_counter <span class="op">&gt;=</span> patience:</span>
<span id="cb146-17"><a href="#cb146-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;Early stopping!&quot;</span>)</span>
<span id="cb146-18"><a href="#cb146-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div>
            <hr />
            <h3 id="the-curse-of-dimensionality">The Curse of
            Dimensionality</h3>
            <h4 id="what-this-means-for-beginners-3">What This Means
            (For Beginners)</h4>
            <p>Imagine you‚Äôre searching for gold nuggets in a field:</p>
            <ul>
            <li><strong>1D (a line)</strong>: With 10 probes, you cover
            the line pretty well</li>
            <li><strong>2D (a square field)</strong>: With 10 probes,
            there are huge gaps ‚Äî you need 100 probes (10√ó10 grid)</li>
            <li><strong>3D (a cube)</strong>: You‚Äôd need 1,000 probes
            (10√ó10√ó10)</li>
            </ul>
            <p><strong>The problem</strong>: As dimensions grow, the
            space explodes exponentially, but your data stays the same
            size!</p>
            <pre><code>Dimensions:    1      2       3       ...     100
Cells needed:  10     100     1,000           10^100 (more than atoms in universe!)
Your data:     1000   1000    1000            1000</code></pre>
            <p><strong>Result</strong>: With high dimensions but fixed
            data, your samples become incredibly sparse ‚Äî like 100
            people trying to cover the entire Earth. There‚Äôs not enough
            data to learn patterns reliably.</p>
            <hr />
            <h4 id="the-hughes-phenomenon-why-accuracy-can-decrease">The
            Hughes Phenomenon (Why Accuracy Can DECREASE)</h4>
            <p>This is the counterintuitive result that often surprises
            people: <strong>adding more features can HURT
            performance</strong>.</p>
            <p><img src="figures/curse_of_dimensionality.png"
            alt="Curse of Dimensionality" /> <em>Figure: Left: The
            Hughes phenomenon ‚Äî accuracy peaks then declines as
            dimensions increase. More data delays the curse. Right:
            Distance concentration ‚Äî all points become equidistant in
            high dimensions.</em></p>
            <p><strong>What happens</strong>:</p>
            <ol type="1">
            <li><strong>Phase 1 (beneficial)</strong>: Adding relevant
            features ‚Üí accuracy improves</li>
            <li><strong>Peak</strong>: Optimal number of features for
            your sample size</li>
            <li><strong>Phase 2 (curse)</strong>: More features ‚Üí
            accuracy DECREASES due to overfitting</li>
            </ol>
            <p><strong>Why does accuracy decrease?</strong></p>
            <ul>
            <li>Same N samples now spread across D-dimensional
            space</li>
            <li>Each parameter estimated from fewer ‚Äúeffective‚Äù
            samples</li>
            <li>Model starts fitting noise (overfitting)</li>
            <li>Training error stays low, test error increases</li>
            </ul>
            <p><strong>Rule of thumb</strong>: For reliable estimation,
            you need roughly <span class="math inline">\(n \geq
            5d\)</span> to <span class="math inline">\(10d\)</span>
            samples (linear models), and exponentially more for
            non-parametric methods like k-NN.</p>
            <hr />
            <h4 id="mathematical-foundation">Mathematical
            Foundation</h4>
            <p><strong>Data Sparsity</strong>:</p>
            <pre><code>1D: 10 points cover [0,1] well
2D: 10 points ‚Üí need 10¬≤ = 100 to cover unit square
3D: 10 points ‚Üí need 10¬≥ = 1000 to cover unit cube
...
100D: need 10^100 points (impossible!)</code></pre>
            <p><strong>Distance Concentration</strong>: In high
            dimensions, the ratio of max to min distance approaches
            1:</p>
            <p><span class="math display">\[\lim_{d \to \infty}
            \frac{\text{dist}_{max} -
            \text{dist}_{min}}{\text{dist}_{min}} \to 0\]</span></p>
            <p>This means ALL points become roughly equidistant ‚Äî
            ‚Äúnearest neighbor‚Äù becomes meaningless!</p>
            <p><strong>Volume Concentration</strong>: In a
            high-dimensional hypersphere, almost all volume is
            concentrated in a thin shell near the surface (not uniformly
            distributed).</p>
            <hr />
            <h4 id="methods-affected-by-the-curse">Methods Affected by
            the Curse</h4>
            <p>The curse affects <strong>all</strong> machine learning
            methods, not just k-NN:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 75%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Why It Fails in High-D</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>k-NN</strong></td>
            <td>Distances become meaningless; ‚Äúnearest‚Äù is random</td>
            </tr>
            <tr>
            <td><strong>Kernel methods (RBF SVM)</strong></td>
            <td>Kernel similarity degenerates; all points equally
            similar</td>
            </tr>
            <tr>
            <td><strong>Linear regression</strong></td>
            <td>D parameters with N samples ‚Üí overfitting when D &gt;
            N</td>
            </tr>
            <tr>
            <td><strong>Gaussian models</strong></td>
            <td>Covariance matrix needs O(D¬≤) samples to estimate</td>
            </tr>
            <tr>
            <td><strong>Clustering</strong></td>
            <td>Clusters become indistinguishable</td>
            </tr>
            <tr>
            <td><strong>Decision trees</strong></td>
            <td>Splits become less meaningful; more ways to overfit</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h4 id="key-insights-1">Key Insights</h4>
            <ol type="1">
            <li><strong>Volume concentrates in corners</strong>: In high
            dimensions, most volume of a hypercube is in the
            corners</li>
            <li><strong>Distance becomes meaningless</strong>: All
            points are roughly equidistant in high dimensions</li>
            <li><strong>Data is sparse</strong>: Need exponentially more
            data to maintain density</li>
            <li><strong>Overfitting increases</strong>: More dimensions
            = more ways to fit noise with limited data</li>
            </ol>
            <hr />
            <h4 id="solutions-3">Solutions</h4>
            <table>
            <colgroup>
            <col style="width: 41%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th>Solution</th>
            <th>How It Helps</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Dimensionality reduction</strong> (PCA,
            autoencoders)</td>
            <td>Project to lower-D space where data is dense</td>
            </tr>
            <tr>
            <td><strong>Feature selection</strong></td>
            <td>Remove irrelevant features, reduce D</td>
            </tr>
            <tr>
            <td><strong>Regularization</strong> (L1, L2)</td>
            <td>Constrain model complexity, prevent overfitting</td>
            </tr>
            <tr>
            <td><strong>More data</strong></td>
            <td>Pushes the ‚Äúpeak‚Äù to higher D (delays the curse)</td>
            </tr>
            <tr>
            <td><strong>Domain knowledge</strong></td>
            <td>Use only meaningful features</td>
            </tr>
            <tr>
            <td><strong>Manifold hypothesis</strong></td>
            <td>Exploit the fact that real data lies on low-D
            manifolds</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h4
            id="interview-q-why-does-k-nn-fail-in-high-dimensions">Interview
            Q: ‚ÄúWhy does k-NN fail in high dimensions?‚Äù</h4>
            <p><strong>A</strong>: In high dimensions, the curse of
            dimensionality causes all points to become approximately
            equidistant. The ratio of the distance to the nearest
            neighbor vs.¬†the farthest neighbor approaches 1. This makes
            the ‚Äúnearest‚Äù neighbor essentially random, destroying k-NN‚Äôs
            ability to find meaningful neighbors.</p>
            <h4 id="interview-q-what-is-the-hughes-phenomenon">Interview
            Q: ‚ÄúWhat is the Hughes phenomenon?‚Äù</h4>
            <p><strong>A</strong>: The Hughes phenomenon states that
            with a fixed training set size, classifier accuracy
            initially improves as you add features, but then peaks and
            actually DECREASES as dimensionality continues to grow. This
            happens because more features means more parameters to
            estimate from the same data ‚Äî eventually, the model overfits
            to noise. The peak occurs earlier with smaller training
            sets. This is why feature selection and dimensionality
            reduction are important: more features isn‚Äôt always
            better.</p>
            <h4
            id="interview-q-how-does-the-curse-of-dimensionality-relate-to-overfitting">Interview
            Q: ‚ÄúHow does the curse of dimensionality relate to
            overfitting?‚Äù</h4>
            <p><strong>A</strong>: The curse of dimensionality directly
            causes overfitting. With fixed N samples in D dimensions, as
            D grows: (1) data becomes sparse ‚Äî fewer samples per region
            of space, (2) more parameters to estimate from the same
            data, (3) the model can find spurious patterns in noise that
            don‚Äôt generalize. Essentially, high-D space gives the model
            ‚Äúroom‚Äù to memorize rather than learn. This is why
            regularization and dimensionality reduction help ‚Äî they
            combat the curse by constraining the effective
            complexity.</p>
            <hr />
            <h3 id="the-manifold-hypothesis-why-deep-learning-works">The
            Manifold Hypothesis: Why Deep Learning Works</h3>
            <p><strong>What is a Manifold?</strong></p>
            <p>A <strong>manifold</strong> is a lower-dimensional
            surface embedded in a higher-dimensional space. Think of it
            like this:</p>
            <pre><code>3D Space with a 2D Manifold:

The surface of Earth is a 2D manifold (you can describe any point with 2 numbers: 
latitude and longitude) embedded in 3D space (x, y, z).

A sheet of paper (2D) crumpled into a ball still occupies 3D space,
but the paper itself is intrinsically 2D.</code></pre>
            <p><strong>The Manifold Hypothesis</strong></p>
            <p>Real-world high-dimensional data (images, text, audio)
            doesn‚Äôt fill the entire high-dimensional space uniformly.
            Instead, it lies on or near a <strong>low-dimensional
            manifold</strong>.</p>
            <pre><code>Example: Images of faces

A 256√ó256 grayscale image has 65,536 dimensions.
But not every combination of pixels is a valid face!

Random pixels:          Valid face:
‚ñà‚ñà‚ñà‚ñí‚ñë‚ñì‚ñí‚ñà‚ñë‚ñì‚ñë‚ñà‚ñí          [A recognizable
‚ñë‚ñì‚ñà‚ñí‚ñì‚ñë‚ñà‚ñí‚ñà‚ñë‚ñì‚ñí           human face]
(noise - not on         (lies on the
the manifold)           &quot;face manifold&quot;)</code></pre>
            <p><strong>Why This Matters</strong>:</p>
            <ol type="1">
            <li><p><strong>The curse is less severe</strong>: We don‚Äôt
            need to fill all of <span
            class="math inline">\(\mathbb{R}^{65536}\)</span> ‚Äî just the
            low-dimensional face manifold (maybe ~100-1000
            dimensions)</p></li>
            <li><p><strong>Deep learning learns the manifold</strong>:
            Neural networks learn to map data to and from this
            manifold</p>
            <ul>
            <li><strong>Encoder</strong>: High-dim ‚Üí Low-dim (find the
            manifold coordinates)</li>
            <li><strong>Decoder</strong>: Low-dim ‚Üí High-dim
            (reconstruct from manifold)</li>
            </ul></li>
            <li><p><strong>Interpolation makes sense</strong>: Moving
            along the manifold produces valid data</p>
            <pre><code>Face A ‚Üí [interpolate on manifold] ‚Üí Face B
(smooth transition through valid faces)

vs.

Face A ‚Üí [interpolate in pixel space] ‚Üí Face B
(goes through ghostly invalid images)</code></pre></li>
            </ol>
            <p><strong>Evidence for the Manifold
            Hypothesis</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 43%" />
            <col style="width: 56%" />
            </colgroup>
            <thead>
            <tr>
            <th>Evidence</th>
            <th>Explanation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Autoencoders work</strong></td>
            <td>Can compress images to &lt;1% of original dimensions and
            reconstruct</td>
            </tr>
            <tr>
            <td><strong>GANs generate realistic data</strong></td>
            <td>Learning to sample from the manifold produces valid
            images</td>
            </tr>
            <tr>
            <td><strong>Interpolation works</strong></td>
            <td>Latent space interpolation produces valid intermediate
            samples</td>
            </tr>
            <tr>
            <td><strong>Adversarial examples exist</strong></td>
            <td>Small perturbations leave the manifold ‚Üí
            misclassification</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is the manifold
            hypothesis and why does it help with the curse of
            dimensionality?‚Äù</p>
            <p><strong>A</strong>: The manifold hypothesis states that
            real-world high-dimensional data (like images) actually lies
            on or near a much lower-dimensional manifold embedded in
            that space. A 256√ó256 image has 65,536 dimensions, but valid
            faces might only span a ~100-1000 dimensional manifold. This
            helps overcome the curse of dimensionality because we don‚Äôt
            need exponentially more data to cover the full space ‚Äî we
            only need enough to cover the manifold. Deep learning
            exploits this by learning representations that capture the
            intrinsic structure of this manifold.</p>
            <p><strong>Interview Q</strong>: ‚ÄúHow do autoencoders relate
            to manifolds?‚Äù</p>
            <p><strong>A</strong>: Autoencoders learn to map data to a
            low-dimensional latent space (encoder) and back (decoder).
            The bottleneck forces the network to discover the manifold
            structure ‚Äî the latent space approximates coordinates on the
            manifold. If the data truly lies on a low-dimensional
            manifold, compression is possible without losing essential
            information. The reconstruction error measures how well the
            learned manifold approximates the true data manifold.</p>
            <hr />
            <h3 id="the-no-free-lunch-theorem">The No Free Lunch
            Theorem</h3>
            <p><strong>Statement</strong>: Averaged over all possible
            problems, no learning algorithm is better than any other ‚Äî
            including random guessing.</p>
            <p><strong>What It Means</strong>:</p>
            <ul>
            <li>There‚Äôs no ‚Äúuniversal best‚Äù algorithm</li>
            <li>Every algorithm has problems it excels at and problems
            it fails on</li>
            <li>Good performance on one problem class comes at the cost
            of worse performance on others</li>
            </ul>
            <p><strong>Implications for ML</strong>:</p>
            <ol type="1">
            <li><strong>Domain knowledge matters</strong>: Choose
            algorithms suited to your problem structure</li>
            <li><strong>No silver bullet</strong>: Deep learning isn‚Äôt
            always the answer</li>
            <li><strong>Benchmarks can mislead</strong>: Algorithm A
            beating B on ImageNet doesn‚Äôt mean A is universally
            better</li>
            </ol>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is the No Free Lunch
            theorem and what does it imply?‚Äù</p>
            <p><strong>A</strong>: The No Free Lunch theorem states that
            averaged over all possible problems, no learning algorithm
            performs better than any other. This means there‚Äôs no
            universally best algorithm ‚Äî every method excels on some
            problems and fails on others. The implication is that we
            should choose algorithms based on problem structure and
            domain knowledge, not just benchmark performance. Deep
            learning works well for problems with hierarchical structure
            (images, language), but simpler methods may be better for
            tabular data or small datasets.</p>
            <hr />
            <h3 id="interview-follow-ups-1">Interview Follow-ups</h3>
            <p><strong>Q</strong>: ‚ÄúYour model is overfitting. Walk me
            through your debugging process.‚Äù</p>
            <p><strong>A</strong>:</p>
            <ol type="1">
            <li><strong>Verify</strong>: Check train vs validation loss
            curves</li>
            <li><strong>Data</strong>: Is there enough data? Can we
            augment?</li>
            <li><strong>Regularization</strong>: Add/increase L2,
            dropout</li>
            <li><strong>Architecture</strong>: Is model too
            complex?</li>
            <li><strong>Early stopping</strong>: Implement if not
            present</li>
            <li><strong>Cross-validation</strong>: Ensure consistent
            results</li>
            </ol>
            <p><strong>Q</strong>: ‚ÄúWhat‚Äôs the difference between
            dropout and regularization?‚Äù</p>
            <p><strong>A</strong>:</p>
            <ul>
            <li><strong>L2 regularization</strong>: Adds penalty term to
            loss, shrinks all weights toward zero smoothly</li>
            <li><strong>Dropout</strong>: Randomly sets neurons to zero
            during training, forces redundancy and prevents
            co-adaptation</li>
            <li><strong>Both</strong> reduce overfitting but through
            different mechanisms</li>
            <li>L2 is deterministic; dropout introduces
            stochasticity</li>
            </ul>
            <p><strong>Q</strong>: ‚ÄúShould you apply dropout during
            inference?‚Äù</p>
            <p><strong>A</strong>: No! During inference:</p>
            <ul>
            <li>Set model to eval mode: <code>model.eval()</code></li>
            <li>Dropout is disabled</li>
            <li>Alternatively, multiply weights by (1-p) to account for
            expected value</li>
            <li>PyTorch handles this automatically with
            <code>model.eval()</code></li>
            </ul>
            <hr />
            <h2 id="model-selection-and-evaluation">4.3 Model Selection
            and Evaluation</h2>
            <h3 id="trainvalidationtest-split">Train/Validation/Test
            Split</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Full Dataset                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Training Set (60%)    ‚îÇ  Validation (20%) ‚îÇ  Test Set (20%)‚îÇ
‚îÇ                         ‚îÇ                   ‚îÇ                ‚îÇ
‚îÇ   - Train model         ‚îÇ  - Tune hyper-    ‚îÇ  - Final       ‚îÇ
‚îÇ   - Update weights      ‚îÇ    parameters     ‚îÇ    evaluation  ‚îÇ
‚îÇ                         ‚îÇ  - Model          ‚îÇ  - Report      ‚îÇ
‚îÇ                         ‚îÇ    selection      ‚îÇ    results     ‚îÇ
‚îÇ                         ‚îÇ  - Early stopping ‚îÇ  - NEVER touch ‚îÇ
‚îÇ                         ‚îÇ                   ‚îÇ    during dev  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="k-fold-cross-validation">K-Fold
            Cross-Validation</h3>
            <p><strong>Why K-Fold?</strong> A single train/test split
            might be ‚Äúlucky‚Äù or ‚Äúunlucky‚Äù ‚Äî your test set could happen
            to contain easy or hard examples, giving a misleading
            performance estimate. K-Fold averages over K different
            splits, giving a more reliable estimate of how your model
            will perform on unseen data.</p>
            <p>When data is limited:</p>
            <pre><code>Fold 1: [Val] [Train] [Train] [Train] [Train]
Fold 2: [Train] [Val] [Train] [Train] [Train]
Fold 3: [Train] [Train] [Val] [Train] [Train]
Fold 4: [Train] [Train] [Train] [Val] [Train]
Fold 5: [Train] [Train] [Train] [Train] [Val]

Final score = average of all fold scores</code></pre>
            <p><strong>Typical values</strong>: K=5 or K=10. Every
            example is used for validation exactly once.</p>
            <hr />
            <h3 id="evaluation-metrics">Evaluation Metrics</h3>
            <h4 id="the-confusion-matrix-foundation-of-all-metrics">The
            Confusion Matrix: Foundation of All Metrics</h4>
            <p>Before understanding metrics, you need to understand the
            <strong>confusion matrix</strong> ‚Äî a 2√ó2 table showing all
            possible outcomes:</p>
            <pre><code>                   Predicted
              Positive    Negative
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   Positive ‚îÇ    TP     ‚îÇ    FN     ‚îÇ ‚Üê Recall = TP/(TP+FN)
            ‚îÇ (Hit! ‚úì)  ‚îÇ (Missed!) ‚îÇ  &quot;Of actual positives, how many caught?&quot;
Actual      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    
            ‚îÇ    FP     ‚îÇ    TN     ‚îÇ
   Negative ‚îÇ (False    ‚îÇ(Correctly ‚îÇ
            ‚îÇ  alarm!)  ‚îÇ ignored ‚úì)‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚Üë
         Precision = TP/(TP+FP)
         &quot;Of predicted positives, how many correct?&quot;</code></pre>
            <p><strong>Memory trick:</strong> - <strong>Recall</strong>
            ‚Üí <strong>Row</strong>-based: ‚ÄúOf all actual positives (top
            row), how many did I catch?‚Äù - <strong>Precision</strong> ‚Üí
            <strong>Column</strong>-based: ‚ÄúOf all I predicted positive
            (left column), how many were correct?‚Äù</p>
            <p><strong>Key Formulas</strong> (reference these with the
            matrix above):</p>
            <p><span class="math display">\[\text{Precision} =
            \frac{TP}{TP + FP} \quad \text{(left column)}\]</span></p>
            <p><span class="math display">\[\text{Recall} = \frac{TP}{TP
            + FN} \quad \text{(top row)}\]</span></p>
            <p><span class="math display">\[\text{F1} = \frac{2 \cdot
            \text{Precision} \cdot \text{Recall}}{\text{Precision} +
            \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP +
            FN}\]</span></p>
            <p><strong>Intuitive names</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 11%" />
            <col style="width: 20%" />
            <col style="width: 27%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>Term</th>
            <th>Full Name</th>
            <th>Plain English</th>
            <th>Example (Spam Filter)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>TP</strong></td>
            <td>True Positive</td>
            <td>Correctly caught</td>
            <td>Spam correctly sent to spam folder</td>
            </tr>
            <tr>
            <td><strong>TN</strong></td>
            <td>True Negative</td>
            <td>Correctly ignored</td>
            <td>Normal email stays in inbox</td>
            </tr>
            <tr>
            <td><strong>FP</strong></td>
            <td>False Positive</td>
            <td>False alarm (Type I)</td>
            <td>Normal email wrongly marked as spam üò†</td>
            </tr>
            <tr>
            <td><strong>FN</strong></td>
            <td>False Negative</td>
            <td>Missed it (Type II)</td>
            <td>Spam slips into inbox üò†</td>
            </tr>
            </tbody>
            </table>
            <h4 id="concrete-example-spam-detection">Concrete Example:
            Spam Detection</h4>
            <p>Imagine your spam filter processed 100 emails:</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 27%" />
            <col style="width: 25%" />
            <col style="width: 16%" />
            </colgroup>
            <thead>
            <tr>
            <th>Email Content</th>
            <th>Actually Is</th>
            <th>Model Says</th>
            <th>Result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>‚ÄúNigerian prince needs help‚Ä¶‚Äù</td>
            <td>Spam</td>
            <td>Spam</td>
            <td><strong>TP</strong> ‚úì</td>
            </tr>
            <tr>
            <td>‚ÄúMeeting at 3pm tomorrow‚Äù</td>
            <td>Not Spam</td>
            <td>Not Spam</td>
            <td><strong>TN</strong> ‚úì</td>
            </tr>
            <tr>
            <td>‚ÄúSale! 50% off today only!‚Äù</td>
            <td>Not Spam</td>
            <td>Spam</td>
            <td><strong>FP</strong> ‚ùå (oops, missed a sale)</td>
            </tr>
            <tr>
            <td>‚ÄúFree iPhone! Click here!‚Äù</td>
            <td>Spam</td>
            <td>Not Spam</td>
            <td><strong>FN</strong> ‚ùå (spam in inbox)</td>
            </tr>
            </tbody>
            </table>
            <p>Summary: TP=40, TN=45, FP=5, FN=10</p>
            <h4
            id="classification-metrics-with-intuition">Classification
            Metrics with Intuition</h4>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 14%" />
            <col style="width: 15%" />
            <col style="width: 38%" />
            <col style="width: 31%" />
            </colgroup>
            <thead>
            <tr>
            <th>Metric</th>
            <th>Formula</th>
            <th><strong>Intuitive Question</strong></th>
            <th>When to Optimize</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Accuracy</strong></td>
            <td><span class="math inline">\(\frac{TP +
            TN}{Total}\)</span></td>
            <td>‚ÄúOf all emails, how many did I classify correctly?‚Äù</td>
            <td>Balanced classes only</td>
            </tr>
            <tr>
            <td><strong>Precision</strong></td>
            <td><span class="math inline">\(\frac{TP}{TP +
            FP}\)</span></td>
            <td>‚ÄúOf emails I flagged as spam, how many were actually
            spam?‚Äù</td>
            <td>When FP is costly</td>
            </tr>
            <tr>
            <td><strong>Recall</strong></td>
            <td><span class="math inline">\(\frac{TP}{TP +
            FN}\)</span></td>
            <td>‚ÄúOf all actual spam, how much did I catch?‚Äù</td>
            <td>When FN is costly</td>
            </tr>
            <tr>
            <td><strong>F1 Score</strong></td>
            <td><span class="math inline">\(\frac{2 \cdot P \cdot R}{P +
            R}\)</span></td>
            <td>‚ÄúBalanced performance on both Precision and Recall‚Äù</td>
            <td>Imbalanced classes</td>
            </tr>
            <tr>
            <td><strong>AUC-ROC</strong></td>
            <td>Area under ROC</td>
            <td>‚ÄúHow well can I rank spam vs not-spam?‚Äù</td>
            <td>When threshold varies</td>
            </tr>
            </tbody>
            </table>
            <h4 id="deep-dive-roc-curves-and-auc">Deep Dive: ROC Curves
            and AUC</h4>
            <p>The <strong>ROC (Receiver Operating Characteristic)
            curve</strong> plots the tradeoff between catching positives
            (TPR) and false alarms (FPR) at every possible
            threshold.</p>
            <p><strong>The Axes</strong>:</p>
            <ul>
            <li><strong>Y-axis (TPR)</strong>: True Positive Rate =
            Recall = TP / (TP + FN) ‚Äî ‚ÄúOf all actual positives, what
            fraction did I catch?‚Äù</li>
            <li><strong>X-axis (FPR)</strong>: False Positive Rate = FP
            / (FP + TN) ‚Äî ‚ÄúOf all actual negatives, what fraction did I
            wrongly flag?‚Äù</li>
            </ul>
            <p><strong>Interpreting the Curve</strong>:</p>
            <ul>
            <li><strong>Top-left corner (0, 1)</strong>: Perfect
            classifier ‚Äî 100% TPR, 0% FPR</li>
            <li><strong>Diagonal line</strong>: Random guessing ‚Äî you
            catch positives at the same rate you create false
            alarms</li>
            <li><strong>Below diagonal</strong>: Worse than random (flip
            your predictions!)</li>
            <li><strong>Area under curve (AUC)</strong>: Single number
            summarizing overall performance</li>
            </ul>
            <p><strong>AUC: The Probabilistic
            Interpretation</strong></p>
            <p><span class="math display">\[\text{AUC} =
            P(\text{score}(\text{positive}) &gt;
            \text{score}(\text{negative}))\]</span></p>
            <p><strong>In plain English</strong>: If you randomly pick
            one positive example and one negative example, AUC is the
            probability that your model assigns a higher score to the
            positive one.</p>
            <table>
            <thead>
            <tr>
            <th>AUC Value</th>
            <th>Interpretation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1.0</td>
            <td>Perfect: Model always ranks positives above
            negatives</td>
            </tr>
            <tr>
            <td>0.9</td>
            <td>Excellent: 90% chance of correct ranking</td>
            </tr>
            <tr>
            <td>0.7-0.8</td>
            <td>Good: Useful discrimination</td>
            </tr>
            <tr>
            <td>0.5</td>
            <td>Random: No better than coin flip</td>
            </tr>
            <tr>
            <td>&lt; 0.5</td>
            <td>Worse than random: Flip your predictions</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why AUC is Useful</strong>:</p>
            <ol type="1">
            <li><strong>Threshold-independent</strong>: Evaluates all
            possible thresholds at once</li>
            <li><strong>Handles imbalance</strong>: Unlike accuracy,
            doesn‚Äôt reward predicting the majority class</li>
            <li><strong>Ranking metric</strong>: Good for problems where
            you care about ordering (e.g., ‚Äúshow me the most likely
            fraud cases first‚Äù)</li>
            </ol>
            <p><img src="figures/roc_curve.png" alt="ROC Curve" />
            <em>Figure: ROC curves comparing different classifiers. The
            area under the curve (AUC) measures overall discrimination
            ability. The diagonal represents random guessing.</em></p>
            <hr />
            <h4 id="when-to-prioritize-which-metric">When to Prioritize
            Which Metric</h4>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 33%" />
            <col style="width: 13%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Prioritize</th>
            <th>Why</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>FP is costly</strong></td>
            <td><strong>Precision</strong></td>
            <td>False alarms are expensive/annoying</td>
            <td>Spam filter: Don‚Äôt want legit email in spam</td>
            </tr>
            <tr>
            <td><strong>FN is costly</strong></td>
            <td><strong>Recall</strong></td>
            <td>Missing cases is dangerous</td>
            <td>Cancer screening: Don‚Äôt want to miss cancer</td>
            </tr>
            <tr>
            <td><strong>Both matter</strong></td>
            <td><strong>F1 Score</strong></td>
            <td>Balance between P and R</td>
            <td>General classification</td>
            </tr>
            <tr>
            <td><strong>Classes balanced</strong></td>
            <td><strong>Accuracy</strong></td>
            <td>Simple and interpretable</td>
            <td>Coin flip prediction (50/50)</td>
            </tr>
            <tr>
            <td><strong>Classes imbalanced</strong></td>
            <td><strong>F1 or AUC</strong></td>
            <td>Accuracy is misleading</td>
            <td>Fraud detection (99.9% non-fraud)</td>
            </tr>
            </tbody>
            </table>
            <h4 id="the-accuracy-trap-why-accuracy-can-lie">The Accuracy
            Trap (Why Accuracy Can Lie)</h4>
            <p><strong>Scenario</strong>: 1000 patients, 10 have cancer,
            990 don‚Äôt.</p>
            <p><strong>Dumb model</strong>: ‚ÄúNobody has cancer‚Äù (always
            predict negative)</p>
            <ul>
            <li>Accuracy = 990/1000 = <strong>99%</strong> üéâ</li>
            <li>Recall = 0/10 = <strong>0%</strong> üíÄ (missed ALL
            cancer patients!)</li>
            </ul>
            <p><strong>Lesson</strong>: With imbalanced classes, high
            accuracy is meaningless. Use F1 or AUC instead.</p>
            <hr />
            <h4 id="handling-imbalanced-data">Handling Imbalanced
            Data</h4>
            <p><strong>Interview Q: ‚ÄúHow do you handle imbalanced
            datasets?‚Äù</strong></p>
            <p><strong>A</strong>: There are several complementary
            approaches:</p>
            <p><strong>Quick Reference: Imbalanced Data
            Solutions</strong></p>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 29%" />
            <col style="width: 21%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Technique</th>
            <th>How It Works</th>
            <th>Best For</th>
            <th>Complexity</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Class Weights</strong></td>
            <td>Penalize minority errors more in loss</td>
            <td>Any imbalance</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>Oversampling (SMOTE)</strong></td>
            <td>Create synthetic minority examples</td>
            <td>Moderate imbalance (1:10-1:100)</td>
            <td>Medium</td>
            </tr>
            <tr>
            <td><strong>Undersampling</strong></td>
            <td>Remove majority examples</td>
            <td>When majority is very large</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>Threshold Tuning</strong></td>
            <td>Adjust decision threshold from 0.5</td>
            <td>After training, any model</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>AUC-PR Metric</strong></td>
            <td>Evaluate with precision-recall curve</td>
            <td>Severe imbalance (1:100+)</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>Anomaly Detection</strong></td>
            <td>Treat minority as outliers</td>
            <td>Extreme imbalance (1:1000+)</td>
            <td>High</td>
            </tr>
            <tr>
            <td><strong>Ensemble Methods</strong></td>
            <td>Balanced RF, EasyEnsemble</td>
            <td>Moderate to severe</td>
            <td>Medium</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <p><strong>1. Resampling Techniques</strong></p>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 37%" />
            <col style="width: 16%" />
            <col style="width: 16%" />
            </colgroup>
            <thead>
            <tr>
            <th>Technique</th>
            <th>How It Works</th>
            <th>Pros</th>
            <th>Cons</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Oversampling minority</strong></td>
            <td>Duplicate minority examples</td>
            <td>More data to learn from</td>
            <td>Risk of overfitting</td>
            </tr>
            <tr>
            <td><strong>SMOTE</strong></td>
            <td>Synthesize new minority examples via interpolation</td>
            <td>Creates novel examples</td>
            <td>Can create unrealistic examples</td>
            </tr>
            <tr>
            <td><strong>Undersampling majority</strong></td>
            <td>Remove majority examples</td>
            <td>Faster training</td>
            <td>Lose information</td>
            </tr>
            <tr>
            <td><strong>Combination</strong></td>
            <td>Oversample minority + undersample majority</td>
            <td>Balanced approach</td>
            <td>Requires tuning</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb155"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.under_sampling <span class="im">import</span> RandomUnderSampler</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Combination approach</span></span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;over&#39;</span>, SMOTE(sampling_strategy<span class="op">=</span><span class="fl">0.5</span>)),  <span class="co"># Minority to 50% of majority</span></span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>    (<span class="st">&#39;under&#39;</span>, RandomUnderSampler(sampling_strategy<span class="op">=</span><span class="fl">0.8</span>))  <span class="co"># Majority to 80% of minority</span></span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a>X_resampled, y_resampled <span class="op">=</span> pipeline.fit_resample(X, y)</span></code></pre></div>
            <p><strong>2. Class Weights</strong></p>
            <p>Adjust the loss function to penalize minority class
            errors more heavily:</p>
            <p><span class="math display">\[\mathcal{L} = -\sum_i
            w_{y_i} \cdot \log(\hat{y}_i)\]</span></p>
            <p>where <span class="math inline">\(w_{y_i}\)</span> is the
            weight for class <span class="math inline">\(y_i\)</span>,
            typically computed as <span class="math inline">\(w_c =
            \frac{N}{K \cdot n_c}\)</span> (inverse frequency
            weighting), with <span class="math inline">\(N\)</span> =
            total samples, <span class="math inline">\(K\)</span> =
            number of classes, <span class="math inline">\(n_c\)</span>
            = samples in class <span
            class="math inline">\(c\)</span>.</p>
            <div class="sourceCode" id="cb156"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scikit-learn</span></span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(class_weight<span class="op">=</span><span class="st">&#39;balanced&#39;</span>)</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>class_counts <span class="op">=</span> torch.bincount(y_train)</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> class_counts.<span class="bu">float</span>()</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(weight<span class="op">=</span>weights)</span></code></pre></div>
            <p><strong>3. Threshold Tuning</strong></p>
            <p>Default threshold of 0.5 isn‚Äôt optimal for imbalanced
            data. Find threshold that:</p>
            <ul>
            <li>Maximizes F1 score, or</li>
            <li>Achieves desired precision/recall tradeoff</li>
            </ul>
            <div class="sourceCode" id="cb157"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_curve</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a>precisions, recalls, thresholds <span class="op">=</span> precision_recall_curve(y_true, y_scores)</span>
<span id="cb157-4"><a href="#cb157-4" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (precisions <span class="op">*</span> recalls) <span class="op">/</span> (precisions <span class="op">+</span> recalls <span class="op">+</span> <span class="fl">1e-8</span>)</span>
<span id="cb157-5"><a href="#cb157-5" aria-hidden="true" tabindex="-1"></a>optimal_threshold <span class="op">=</span> thresholds[np.argmax(f1_scores)]</span></code></pre></div>
            <p><strong>4. Use Appropriate Metrics</strong></p>
            <ul>
            <li><strong>AUC-PR (Precision-Recall)</strong> &gt; AUC-ROC
            for highly imbalanced data</li>
            <li><strong>F1-score</strong> or <strong>F2-score</strong>
            (if recall is more important)</li>
            <li><strong>Matthews Correlation Coefficient (MCC)</strong>
            ‚Äî balanced even with imbalance</li>
            </ul>
            <p><strong>5. Algorithmic Approaches</strong></p>
            <ul>
            <li><strong>Cost-sensitive learning</strong>: Different
            misclassification costs for different classes</li>
            <li><strong>Ensemble methods</strong>: Balanced Random
            Forest, EasyEnsemble</li>
            <li><strong>Anomaly detection</strong>: Treat minority class
            as anomalies if extreme imbalance</li>
            </ul>
            <p><strong>Summary table of when to use what:</strong></p>
            <table>
            <thead>
            <tr>
            <th>Imbalance Ratio</th>
            <th>Recommended Approach</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1:10</td>
            <td>Class weights + threshold tuning</td>
            </tr>
            <tr>
            <td>1:100</td>
            <td>SMOTE + class weights + AUC-PR</td>
            </tr>
            <tr>
            <td>1:1000+</td>
            <td>Anomaly detection or cost-sensitive ensembles</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h4 id="precision-recall-tradeoff">Precision-Recall
            Tradeoff</h4>
            <p>You can‚Äôt maximize both ‚Äî there‚Äôs always a tradeoff:</p>
            <table>
            <thead>
            <tr>
            <th>Threshold</th>
            <th>Behavior</th>
            <th>Precision</th>
            <th>Recall</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>High (strict)</td>
            <td>Only flag obvious spam</td>
            <td>High ‚Üë</td>
            <td>Low ‚Üì</td>
            </tr>
            <tr>
            <td>Low (lenient)</td>
            <td>Flag anything suspicious</td>
            <td>Low ‚Üì</td>
            <td>High ‚Üë</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: Lower your spam threshold ‚Üí
            catch more spam (‚Üë Recall) but also more false alarms (‚Üì
            Precision)</p>
            <hr />
            <h4
            id="the-precision-recall-duality-a-cannot-tolerate-framework">The
            Precision-Recall Duality: A ‚ÄúCannot Tolerate‚Äù Framework</h4>
            <p>There‚Äôs a fundamental duality between precision and
            recall that can be understood through a ‚Äúcannot tolerate‚Äù
            framing:</p>
            <table>
            <colgroup>
            <col style="width: 13%" />
            <col style="width: 15%" />
            <col style="width: 37%" />
            <col style="width: 32%" />
            </colgroup>
            <thead>
            <tr>
            <th>Metric</th>
            <th>Formula</th>
            <th><strong>‚ÄúCannot Tolerate‚Äù</strong></th>
            <th>Action to Improve</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Precision</strong></td>
            <td>TP / (TP + <strong>FP</strong>)</td>
            <td>False Positives (false alarms)</td>
            <td>Decrease FP ‚Üí be more strict</td>
            </tr>
            <tr>
            <td><strong>Recall</strong></td>
            <td>TP / (TP + <strong>FN</strong>)</td>
            <td>False Negatives (missed cases)</td>
            <td>Decrease FN ‚Üí be more lenient</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why the duality exists</strong>: Adjusting the
            classification threshold affects both metrics in opposite
            directions:</p>
            <pre><code>Raise threshold (be stricter):
  ‚Üí Fewer false alarms (‚Üì FP) ‚Üí ‚Üë Precision
  ‚Üí But also miss more real cases (‚Üë FN) ‚Üí ‚Üì Recall

Lower threshold (be more lenient):
  ‚Üí Catch more real cases (‚Üì FN) ‚Üí ‚Üë Recall  
  ‚Üí But also more false alarms (‚Üë FP) ‚Üí ‚Üì Precision</code></pre>
            <p><strong>Using the ‚ÄúCannot Tolerate‚Äù
            Framework</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 31%" />
            <col style="width: 18%" />
            <col style="width: 20%" />
            </colgroup>
            <thead>
            <tr>
            <th>When You‚Ä¶</th>
            <th>Optimize For</th>
            <th>Reduce</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Cannot tolerate false positives</strong></td>
            <td><strong>Precision</strong></td>
            <td>FP</td>
            <td>Spam filter: Don‚Äôt send legit email to spam</td>
            </tr>
            <tr>
            <td><strong>Cannot tolerate false negatives</strong></td>
            <td><strong>Recall</strong></td>
            <td>FN</td>
            <td>Cancer screening: Don‚Äôt miss any cancer</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The F1 score</strong> exists precisely because
            you usually can‚Äôt have both ‚Äî it‚Äôs the harmonic mean that
            provides a balanced measure when both false positives and
            false negatives matter.</p>
            <h4 id="regression-metrics">Regression Metrics</h4>
            <table>
            <colgroup>
            <col style="width: 19%" />
            <col style="width: 21%" />
            <col style="width: 28%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>Metric</th>
            <th>Formula</th>
            <th>Properties</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MSE</strong></td>
            <td><span class="math inline">\(\frac{1}{n}\sum(y -
            \hat{y})^2\)</span></td>
            <td>Penalizes large errors more</td>
            <td>When outliers matter</td>
            </tr>
            <tr>
            <td><strong>MAE</strong></td>
            <td><span class="math inline">\(\frac{1}{n}\sum|y -
            \hat{y}|\)</span></td>
            <td>All errors weighted equally</td>
            <td>Robust to outliers</td>
            </tr>
            <tr>
            <td><strong>R¬≤</strong></td>
            <td><span class="math inline">\(1 -
            \frac{SS_{res}}{SS_{tot}}\)</span></td>
            <td>% of variance explained</td>
            <td>Interpretability</td>
            </tr>
            </tbody>
            </table>
            <h4
            id="interview-q-when-would-you-use-precision-vs-recall">Interview
            Q: ‚ÄúWhen would you use Precision vs Recall?‚Äù</h4>
            <p><strong>A</strong>: It depends on the cost of errors:</p>
            <ul>
            <li><p><strong>Prioritize Precision</strong> when false
            positives are costly:</p>
            <ul>
            <li>Spam filter: A false positive means a legitimate email
            goes to spam ‚Äî user misses important info</li>
            <li>Recommendation system: Showing irrelevant items annoys
            users</li>
            </ul></li>
            <li><p><strong>Prioritize Recall</strong> when false
            negatives are costly:</p>
            <ul>
            <li>Cancer screening: A false negative means missing cancer
            ‚Äî could be fatal</li>
            <li>Fraud detection: A false negative means fraud goes
            undetected ‚Äî financial loss</li>
            <li>Search engines: Missing a relevant result frustrates
            users</li>
            </ul></li>
            <li><p><strong>Use F1</strong> when you need balance,
            especially with imbalanced classes</p></li>
            </ul>
            <h4
            id="interview-q-describe-a-scenario-where-you-would-prioritize-recall-over-precision">Interview
            Q: ‚ÄúDescribe a scenario where you would prioritize Recall
            over Precision‚Äù</h4>
            <p><strong>A</strong>: The classic example is <strong>cancer
            screening</strong> (or any medical diagnostic test). Let me
            walk through why:</p>
            <p><strong>The Setup</strong>: We‚Äôre building a system to
            screen mammograms for breast cancer.</p>
            <table>
            <colgroup>
            <col style="width: 19%" />
            <col style="width: 32%" />
            <col style="width: 47%" />
            </colgroup>
            <thead>
            <tr>
            <th>Outcome</th>
            <th>What It Means</th>
            <th>Real-World Consequence</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>True Positive (TP)</strong></td>
            <td>Cancer detected correctly</td>
            <td>Patient gets treatment ‚úì</td>
            </tr>
            <tr>
            <td><strong>True Negative (TN)</strong></td>
            <td>Healthy patient cleared</td>
            <td>Patient goes home happy ‚úì</td>
            </tr>
            <tr>
            <td><strong>False Positive (FP)</strong></td>
            <td>Healthy patient flagged</td>
            <td>Patient gets a biopsy (uncomfortable, anxious,
            ~$1000)</td>
            </tr>
            <tr>
            <td><strong>False Negative (FN)</strong></td>
            <td>Cancer missed!</td>
            <td>Patient goes home thinking they‚Äôre healthy. Cancer grows
            undetected. Potentially fatal.</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The Cost Asymmetry</strong>:</p>
            <ul>
            <li><strong>Cost of FP</strong>: Anxiety + additional
            testing + ~$1,000 ‚Üí <strong>Inconvenient but
            manageable</strong></li>
            <li><strong>Cost of FN</strong>: Delayed treatment ‚Üí cancer
            progresses ‚Üí <strong>potentially fatal</strong></li>
            </ul>
            <p>The costs are wildly asymmetric: <strong>missing cancer
            can kill someone; a false alarm just means more
            tests</strong>.</p>
            <p><strong>The Decision</strong>: We should optimize for
            <strong>high Recall</strong> (catch all true cancers) even
            at the expense of lower Precision (more false alarms).</p>
            <p><span class="math display">\[\text{Recall} = \frac{TP}{TP
            + FN} \rightarrow \text{Maximize to minimize missed
            cancers}\]</span></p>
            <p><strong>Concrete Numbers</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>FP (false alarms)</th>
            <th>FN (missed cancer)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Model A</strong></td>
            <td>90%</td>
            <td>70%</td>
            <td>10 false alarms</td>
            <td><strong>30 missed cancers</strong></td>
            </tr>
            <tr>
            <td><strong>Model B</strong></td>
            <td>60%</td>
            <td>95%</td>
            <td>40 false alarms</td>
            <td><strong>5 missed cancers</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Model B is better!</strong> We accept 30 more
            false alarms (extra biopsies) to catch 25 more cancers.</p>
            <p><strong>Other Recall-Critical Scenarios</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 17%" />
            <col style="width: 41%" />
            <col style="width: 41%" />
            </colgroup>
            <thead>
            <tr>
            <th>Domain</th>
            <th>Why Recall Matters</th>
            <th>Acceptable FP Cost</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Airport security</strong></td>
            <td>Missing a threat could kill hundreds</td>
            <td>Extra manual screenings</td>
            </tr>
            <tr>
            <td><strong>Fraud detection</strong></td>
            <td>Missing fraud costs millions</td>
            <td>Extra transaction reviews</td>
            </tr>
            <tr>
            <td><strong>Manufacturing safety</strong></td>
            <td>Defective airplane part ‚Üí crash</td>
            <td>Extra QA inspections</td>
            </tr>
            <tr>
            <td><strong>Child safety filters</strong></td>
            <td>Missing harmful content unacceptable</td>
            <td>Some false blocks</td>
            </tr>
            <tr>
            <td><strong>Disease outbreak detection</strong></td>
            <td>Missing an outbreak ‚Üí pandemic</td>
            <td>False alerts investigated</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Key Insight</strong>: When the cost of a False
            Negative is catastrophic and irreversible (death, massive
            financial loss, safety hazard), while the cost of a False
            Positive is merely inconvenient (extra tests, manual review,
            minor delay), <strong>always prioritize Recall</strong>.</p>
            <p><strong>Follow-up</strong>: ‚ÄúHow would you implement this
            in practice?‚Äù</p>
            <p><strong>A</strong>: Lower the classification threshold.
            Instead of predicting ‚Äúcancer‚Äù when probability &gt; 0.5,
            predict ‚Äúcancer‚Äù when probability &gt; 0.1 (or even lower).
            This catches more true positives but also more false
            positives ‚Äî exactly the tradeoff we want.</p>
            <div class="sourceCode" id="cb159"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard threshold (balanced)</span></span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (probabilities <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a><span class="co"># High-recall threshold for medical screening</span></span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (probabilities <span class="op">&gt;</span> <span class="fl">0.1</span>).astype(<span class="bu">int</span>)  <span class="co"># Flag anything remotely suspicious</span></span></code></pre></div>
            <h4
            id="interview-q-why-is-accuracy-a-bad-metric-for-imbalanced-datasets">Interview
            Q: ‚ÄúWhy is accuracy a bad metric for imbalanced
            datasets?‚Äù</h4>
            <p><strong>A</strong>: Because a naive classifier can
            achieve high accuracy by simply predicting the majority
            class. In fraud detection (0.1% fraud), predicting ‚Äúnot
            fraud‚Äù for everything gives 99.9% accuracy but 0% recall ‚Äî
            you catch zero fraud. Metrics like F1, Precision, Recall, or
            AUC-ROC measure performance on the minority class
            specifically, giving you meaningful signal about model
            quality.</p>
            <hr />
            <h4
            id="another-example-bug-detection-static-analysis">Another
            Example: Bug Detection / Static Analysis</h4>
            <p>The same confusion matrix framework applies to static
            analysis tools that detect bugs in code:</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th><strong>Predicted: Bug</strong> (Tool flags as
            buggy)</th>
            <th><strong>Predicted: No Bug</strong> (Tool says OK)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Actual: Bug</strong> (Code has a real bug)</td>
            <td><strong>True Positive (TP)</strong> ‚Äî Correctly detected
            bug</td>
            <td><strong>False Negative (FN)</strong> ‚Äî Missed bug!
            üò±</td>
            </tr>
            <tr>
            <td><strong>Actual: No Bug</strong> (Code is correct)</td>
            <td><strong>False Positive (FP)</strong> ‚Äî False alarm!
            üò§</td>
            <td><strong>True Negative (TN)</strong> ‚Äî Correctly
            passed</td>
            </tr>
            </tbody>
            </table>
            <p><strong>What is a False Positive (FP) in Static
            Analysis?</strong></p>
            <p>FP = Tool flags code as buggy, but the code is actually
            correct.</p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 35%" />
            <col style="width: 16%" />
            </colgroup>
            <thead>
            <tr>
            <th>False Positive Scenario</th>
            <th>What the tool says</th>
            <th>Reality</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Null pointer warning</strong></td>
            <td>‚ÄúVariable <code>x</code> might be null here‚Äù</td>
            <td>Developer knows <code>x</code> is validated
            upstream</td>
            </tr>
            <tr>
            <td><strong>Resource leak warning</strong></td>
            <td>‚ÄúFile handle never closed‚Äù</td>
            <td>Actually closed in a <code>finally</code> block the tool
            missed</td>
            </tr>
            <tr>
            <td><strong>Thread safety warning</strong></td>
            <td>‚ÄúPotential race condition‚Äù</td>
            <td>Code is single-threaded or properly synchronized</td>
            </tr>
            <tr>
            <td><strong>Dead code warning</strong></td>
            <td>‚ÄúThis branch is unreachable‚Äù</td>
            <td>Branch is reachable via reflection or dynamic
            dispatch</td>
            </tr>
            <tr>
            <td><strong>SQL injection warning</strong></td>
            <td>‚ÄúUser input in query‚Äù</td>
            <td>Input is already sanitized/validated</td>
            </tr>
            <tr>
            <td><strong>Integer overflow warning</strong></td>
            <td>‚ÄúPotential overflow‚Äù</td>
            <td>Values are bounded by business logic</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why FP/FN Rates Matter in Static
            Analysis</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>High FP Rate Problems</th>
            <th>High FN Rate Problems</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Alert fatigue</strong> ‚Äî developers ignore all
            warnings</td>
            <td><strong>Real bugs slip through</strong></td>
            </tr>
            <tr>
            <td><strong>Wasted time</strong> ‚Äî investigating
            non-issues</td>
            <td><strong>False confidence</strong> ‚Äî ‚Äútool said it‚Äôs
            fine‚Äù</td>
            </tr>
            <tr>
            <td><strong>Tool abandonment</strong> ‚Äî teams stop using
            it</td>
            <td><strong>Security vulnerabilities</strong> ‚Äî missed
            exploits</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The trade-off</strong>: Most static analysis
            tools let you tune sensitivity:</p>
            <ul>
            <li><strong>High sensitivity</strong> ‚Üí Catches more bugs
            BUT more false alarms</li>
            <li><strong>Low sensitivity</strong> ‚Üí Fewer false alarms
            BUT misses real bugs</li>
            </ul>
            <p><strong>Key Metrics for Static Analysis
            Tools</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 34%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th>Metric</th>
            <th>Formula</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Precision</strong></td>
            <td>TP / (TP + FP)</td>
            <td>‚ÄúOf all flagged issues, how many are real bugs?‚Äù</td>
            </tr>
            <tr>
            <td><strong>Recall</strong></td>
            <td>TP / (TP + FN)</td>
            <td>‚ÄúOf all real bugs, how many did we catch?‚Äù</td>
            </tr>
            <tr>
            <td><strong>False Positive Rate</strong></td>
            <td>FP / (FP + TN)</td>
            <td>‚ÄúOf all correct code, how much did we wrongly
            flag?‚Äù</td>
            </tr>
            </tbody>
            </table>
            <p><strong>In practice</strong>: Tools like SonarQube,
            Coverity, or CodeQL often have precision around 30-70% ‚Äî
            meaning 30-70% of warnings are real issues. This is why
            ‚Äútriaging‚Äù warnings is a standard practice.</p>
            <hr />
            <h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3>
            <p><strong>Grid Search</strong>: Try all combinations</p>
            <div class="sourceCode" id="cb160"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a>param_grid <span class="op">=</span> {</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;learning_rate&#39;</span>: [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>],</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;hidden_size&#39;</span>: [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>],</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;dropout&#39;</span>: [<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]</span>
<span id="cb160-5"><a href="#cb160-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb160-6"><a href="#cb160-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 √ó 3 √ó 3 = 27 experiments</span></span></code></pre></div>
            <p><strong>Random Search</strong>: Sample randomly (often
            better!)</p>
            <ul>
            <li>More efficient for high-dimensional spaces</li>
            <li>Can find good values in unexplored regions</li>
            </ul>
            <p><strong>Bayesian Optimization</strong>: Model the
            objective function</p>
            <ul>
            <li>Use Gaussian processes to predict performance</li>
            <li>Sample where expected improvement is highest</li>
            </ul>
            <hr />
            <h2 id="generative-vs-discriminative-models">4.4 Generative
            vs Discriminative Models</h2>
            <h3 id="the-key-distinction">The Key Distinction</h3>
            <table>
            <colgroup>
            <col style="width: 24%" />
            <col style="width: 32%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th>Model Type</th>
            <th>What It Learns</th>
            <th>Question It Answers</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Discriminative</strong></td>
            <td><span class="math inline">\(P(y\|x)\)</span></td>
            <td>‚ÄúGiven input <span class="math inline">\(x\)</span>,
            what‚Äôs the label <span
            class="math inline">\(y\)</span>?‚Äù</td>
            </tr>
            <tr>
            <td><strong>Generative</strong></td>
            <td><span class="math inline">\(P(x, y)\)</span> or <span
            class="math inline">\(P(x)\)</span></td>
            <td>‚ÄúHow was this data generated?‚Äù</td>
            </tr>
            </tbody>
            </table>
            <h3 id="discriminative-models">Discriminative Models</h3>
            <p><strong>Focus</strong>: Learn the decision boundary
            directly.</p>
            <p><span class="math display">\[P(y|x) = \text{classifier
            output}\]</span></p>
            <p><strong>Examples</strong>:</p>
            <ul>
            <li>Logistic Regression</li>
            <li>SVMs</li>
            <li>Neural Networks (classification)</li>
            <li>Random Forests</li>
            </ul>
            <p><strong>Advantages</strong>:</p>
            <ul>
            <li>Often simpler, more accurate for classification</li>
            <li>Don‚Äôt need to model full data distribution</li>
            <li>Directly optimize what we care about</li>
            </ul>
            <hr />
            <h3 id="generative-models">Generative Models</h3>
            <p><strong>Focus</strong>: Model how data is generated, can
            sample new data.</p>
            <p><span class="math display">\[P(x, y) =
            P(x|y)P(y)\]</span></p>
            <p>Or for unsupervised: <span class="math display">\[P(x) =
            \int P(x|z)P(z)dz\]</span></p>
            <p><strong>Examples</strong>:</p>
            <ul>
            <li>Naive Bayes</li>
            <li>Gaussian Mixture Models (GMM)</li>
            <li>Hidden Markov Models (HMM)</li>
            <li>Variational Autoencoders (VAE)</li>
            <li>Generative Adversarial Networks (GAN)</li>
            <li>Diffusion Models</li>
            <li>Large Language Models (GPT, LLaMA)</li>
            </ul>
            <h3 id="how-generative-models-enable-classification">How
            Generative Models Enable Classification</h3>
            <p>Using Bayes‚Äô rule, generative models can classify:</p>
            <p><span class="math display">\[P(y|x) =
            \frac{P(x|y)P(y)}{P(x)} \propto P(x|y)P(y)\]</span></p>
            <p><strong>Naive Bayes example</strong>:</p>
            <ol type="1">
            <li>Learn <span
            class="math inline">\(P(x|y=\text{spam})\)</span> and <span
            class="math inline">\(P(x|y=\text{ham})\)</span></li>
            <li>Learn prior <span
            class="math inline">\(P(y=\text{spam})\)</span></li>
            <li>Classify: <span class="math inline">\(\arg\max_y
            P(x|y)P(y)\)</span></li>
            </ol>
            <h3 id="modern-generative-models">Modern Generative
            Models</h3>
            <p><strong>VAE (Variational Autoencoder)</strong>:</p>
            <pre><code>x ‚Üí [Encoder] ‚Üí Œº, œÉ ‚Üí z ~ N(Œº,œÉ¬≤) ‚Üí [Decoder] ‚Üí xÃÇ</code></pre>
            <ul>
            <li>Learns latent representation <span
            class="math inline">\(z\)</span></li>
            <li>Can generate new samples by sampling <span
            class="math inline">\(z\)</span> and decoding</li>
            </ul>
            <p><strong>GAN (Generative Adversarial
            Network)</strong>:</p>
            <pre><code>z ~ N(0,1) ‚Üí [Generator] ‚Üí fake_x
                              ‚Üì
real_x ‚Üí [Discriminator] ‚Üí real/fake?</code></pre>
            <ul>
            <li>Generator tries to fool discriminator</li>
            <li>Discriminator tries to distinguish real from fake</li>
            </ul>
            <p><strong>Diffusion Models</strong> (DALL-E, Stable
            Diffusion):</p>
            <pre><code>x‚ÇÄ ‚Üí add noise ‚Üí ... ‚Üí x‚Çú (pure noise)
x‚Çú ‚Üí denoise ‚Üí ... ‚Üí x‚ÇÄ (clean image)</code></pre>
            <ul>
            <li>Learn to reverse noise process</li>
            <li>State-of-the-art image generation</li>
            </ul>
            <p><strong>LLMs as Generative Models</strong>: <span
            class="math display">\[P(x_1, x_2, \ldots, x_T) =
            \prod_{t=1}^{T} P(x_t | x_{&lt;t})\]</span></p>
            <ul>
            <li>Model distribution over sequences</li>
            <li>Generate by sampling token by token</li>
            </ul>
            <hr />
            <h3 id="when-to-use-which-3">When to Use Which?</h3>
            <table>
            <thead>
            <tr>
            <th>Use Case</th>
            <th>Recommended</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Classification only</td>
            <td>Discriminative</td>
            </tr>
            <tr>
            <td>Need to generate samples</td>
            <td>Generative</td>
            </tr>
            <tr>
            <td>Small dataset</td>
            <td>Generative (can use priors)</td>
            </tr>
            <tr>
            <td>Missing data</td>
            <td>Generative (can marginalize)</td>
            </tr>
            <tr>
            <td>Anomaly detection</td>
            <td>Generative (low <span
            class="math inline">\(P(x)\)</span> = anomaly)</td>
            </tr>
            <tr>
            <td>Semi-supervised</td>
            <td>Generative</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-the-difference-between-generative-and-discriminative-models">Interview
            Q: ‚ÄúWhat‚Äôs the difference between generative and
            discriminative models?‚Äù</h3>
            <p><strong>A</strong>: Discriminative models learn <span
            class="math inline">\(P(y|x)\)</span> ‚Äî the decision
            boundary to classify inputs. Generative models learn <span
            class="math inline">\(P(x,y)\)</span> or <span
            class="math inline">\(P(x)\)</span> ‚Äî how the data was
            generated, enabling both classification (via Bayes‚Äô rule)
            and sampling new data. Examples: Logistic regression is
            discriminative (directly models <span
            class="math inline">\(P(y|x)\)</span>); Naive Bayes is
            generative (models <span
            class="math inline">\(P(x|y)P(y)\)</span>). Modern
            generative models like VAEs, GANs, and LLMs can generate
            realistic images and text by learning the data
            distribution.</p>
            <h3
            id="interview-q-why-are-llms-considered-generative-models">Interview
            Q: ‚ÄúWhy are LLMs considered generative models?‚Äù</h3>
            <p><strong>A</strong>: LLMs model the joint distribution of
            text as <span class="math inline">\(P(x_1, \ldots, x_T) =
            \prod_t P(x_t|x_{&lt;t})\)</span>. They learn how text is
            generated ‚Äî given a context, what‚Äôs the likely next token.
            This allows them to generate new text by sampling
            autoregressively. Unlike a discriminative classifier that
            would just predict a label, LLMs can produce novel sequences
            that follow the learned distribution of natural
            language.</p>
            <hr />
            <h2 id="curriculum-learning">4.5 Curriculum Learning</h2>
            <h3 id="what-is-curriculum-learning">What is Curriculum
            Learning?</h3>
            <p><strong>Curriculum learning</strong> is a training
            strategy where the model learns <strong>simple concepts
            first, then progressively harder ones</strong> ‚Äî mimicking
            how humans and animals learn.</p>
            <p><strong>Core idea</strong>: Instead of randomly sampling
            training examples, organize them from easy to difficult.</p>
            <pre><code>Traditional Training:          Curriculum Learning:

Random order:                  Structured order:
  [Hard] [Easy] [Medium]         [Easy] ‚Üí [Medium] ‚Üí [Hard]
  [Easy] [Hard] [Medium]         [Easy] ‚Üí [Medium] ‚Üí [Hard]
  [Medium] [Easy] [Hard]         [Easy] ‚Üí [Medium] ‚Üí [Hard]</code></pre>
            <h3 id="motivation-how-humans-learn">Motivation: How Humans
            Learn</h3>
            <p>The motivation comes from the observation that
            <strong>humans and animals learn better when trained with a
            curriculum-like strategy</strong>:</p>
            <ul>
            <li>Children learn to walk before they run</li>
            <li>Math: arithmetic ‚Üí algebra ‚Üí calculus</li>
            <li>Language: simple words ‚Üí sentences ‚Üí complex
            grammar</li>
            <li>Music: scales ‚Üí simple songs ‚Üí concertos</li>
            </ul>
            <p>Research in cognitive science suggests this isn‚Äôt just
            convenient ‚Äî it‚Äôs <strong>more efficient</strong> for
            learning.</p>
            <hr />
            <h3 id="how-it-works-1">How It Works</h3>
            <ol type="1">
            <li><strong>Define a difficulty measure</strong> for each
            training example</li>
            <li><strong>Sort or weight examples</strong> by
            difficulty</li>
            <li><strong>Start training on easy examples</strong></li>
            <li><strong>Gradually introduce harder
            examples</strong></li>
            </ol>
            <div class="sourceCode" id="cb165"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> curriculum_training(model, dataset, epochs, difficulty_fn):</span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort examples by difficulty</span></span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>    difficulties <span class="op">=</span> [difficulty_fn(example) <span class="cf">for</span> example <span class="kw">in</span> dataset]</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a>    sorted_indices <span class="op">=</span> np.argsort(difficulties)</span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute curriculum pace (how much data to use)</span></span>
<span id="cb165-8"><a href="#cb165-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Start with easy 20%, end with all 100%</span></span>
<span id="cb165-9"><a href="#cb165-9" aria-hidden="true" tabindex="-1"></a>        fraction <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, <span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.8</span> <span class="op">*</span> (epoch <span class="op">/</span> epochs))</span>
<span id="cb165-10"><a href="#cb165-10" aria-hidden="true" tabindex="-1"></a>        n_examples <span class="op">=</span> <span class="bu">int</span>(fraction <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="cb165-11"><a href="#cb165-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb165-12"><a href="#cb165-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train on easiest n_examples</span></span>
<span id="cb165-13"><a href="#cb165-13" aria-hidden="true" tabindex="-1"></a>        curriculum_indices <span class="op">=</span> sorted_indices[:n_examples]</span>
<span id="cb165-14"><a href="#cb165-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb165-15"><a href="#cb165-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx <span class="kw">in</span> curriculum_indices:</span>
<span id="cb165-16"><a href="#cb165-16" aria-hidden="true" tabindex="-1"></a>            train_step(model, dataset[idx])</span></code></pre></div>
            <h3 id="defining-difficulty">Defining Difficulty</h3>
            <p>Different metrics for ‚Äúeasy‚Äù vs ‚Äúhard‚Äù:</p>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 30%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>Domain</th>
            <th>Easy</th>
            <th>Hard</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Sequence tasks</strong></td>
            <td>Short sequences</td>
            <td>Long sequences</td>
            </tr>
            <tr>
            <td><strong>Language modeling</strong></td>
            <td>Common words, simple grammar</td>
            <td>Rare words, complex syntax</td>
            </tr>
            <tr>
            <td><strong>Image classification</strong></td>
            <td>Clear, centered objects</td>
            <td>Occluded, cluttered scenes</td>
            </tr>
            <tr>
            <td><strong>Math problems</strong></td>
            <td>Single-digit arithmetic</td>
            <td>Multi-step word problems</td>
            </tr>
            <tr>
            <td><strong>Translation</strong></td>
            <td>Short sentences, common words</td>
            <td>Long sentences, rare words</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Common difficulty measures</strong>:</p>
            <ul>
            <li><strong>Loss-based</strong>: Examples with lower initial
            loss are ‚Äúeasier‚Äù</li>
            <li><strong>Length-based</strong>: Shorter examples are
            easier</li>
            <li><strong>Confidence-based</strong>: Examples the model is
            confident on are easier</li>
            <li><strong>Human-defined</strong>: Domain experts rank
            difficulty</li>
            </ul>
            <hr />
            <h3 id="self-paced-learning">Self-Paced Learning</h3>
            <p>A variant where the <strong>model decides</strong> what‚Äôs
            easy:</p>
            <p><span class="math display">\[\min_{w, v} \sum_i v_i
            L(x_i, y_i; w) - \lambda \sum_i v_i\]</span></p>
            <p>where <span class="math inline">\(v_i \in [0,1]\)</span>
            is a weight for each example. The model ‚Äúchooses‚Äù to focus
            on examples it can learn from (moderate difficulty).</p>
            <hr />
            <h3 id="why-does-it-work-1">Why Does It Work?</h3>
            <ol type="1">
            <li><strong>Avoids early confusion</strong>: Random hard
            examples early on can push weights in wrong directions</li>
            <li><strong>Better gradient signal</strong>: Easy examples
            provide cleaner gradients before tackling noise</li>
            <li><strong>Implicit regularization</strong>: Learning
            simple patterns first acts as inductive bias</li>
            <li><strong>Faster convergence</strong>: The model builds on
            solid foundations rather than fighting contradictory
            signals</li>
            </ol>
            <hr />
            <h3 id="curriculum-learning-in-sequence-models">Curriculum
            Learning in Sequence Models</h3>
            <p>Particularly important for sequence-to-sequence
            tasks:</p>
            <pre><code>Stage 1: Learn on sequences of length 1-10
Stage 2: Learn on sequences of length 10-50
Stage 3: Learn on sequences of length 50-200
Stage 4: Learn on full-length sequences</code></pre>
            <p><strong>Connection to teacher forcing</strong> (Section
            6.1): Curriculum learning can determine when to transition
            from teacher forcing to self-generated inputs.</p>
            <h3 id="examples-in-practice">Examples in Practice</h3>
            <table>
            <thead>
            <tr>
            <th>Application</th>
            <th>Curriculum Strategy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Machine Translation</strong></td>
            <td>Short sentences ‚Üí long sentences</td>
            </tr>
            <tr>
            <td><strong>Sorting networks</strong></td>
            <td>Sort 5 numbers ‚Üí sort 100 numbers</td>
            </tr>
            <tr>
            <td><strong>Math reasoning</strong></td>
            <td>Single-step ‚Üí multi-step problems</td>
            </tr>
            <tr>
            <td><strong>Code generation</strong></td>
            <td>Simple functions ‚Üí complex programs</td>
            </tr>
            <tr>
            <td><strong>RL (games)</strong></td>
            <td>Easy levels ‚Üí hard levels</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="anti-curriculum-hard-first">Anti-Curriculum (Hard
            First)</h3>
            <p>Sometimes training on <strong>hard examples
            first</strong> works better:</p>
            <ul>
            <li><strong>Bootstrapping</strong>: Hard examples might
            contain more information</li>
            <li><strong>Active learning</strong>: Focus compute on
            uncertain regions</li>
            <li><strong>Adversarial training</strong>: Hardest examples
            reveal model weaknesses</li>
            </ul>
            <p>The best strategy is often task-dependent!</p>
            <hr />
            <h3
            id="interview-q-what-is-curriculum-learning-and-why-does-it-help">Interview
            Q: ‚ÄúWhat is curriculum learning and why does it help?‚Äù</h3>
            <p><strong>A</strong>: Curriculum learning trains models on
            easy examples first, then progressively harder ones ‚Äî
            similar to how humans learn (arithmetic before calculus).
            The motivation comes from cognitive science showing this is
            more efficient for learning. It works by: (1) defining a
            difficulty measure (e.g., sequence length, loss), (2)
            sorting examples by difficulty, (3) gradually exposing the
            model to harder examples during training. It helps because
            early training on easy examples provides cleaner gradient
            signals and builds foundational representations before
            tackling complex cases. It‚Äôs particularly useful for
            sequence tasks where starting with short sequences prevents
            the model from getting confused by long-range dependencies
            it can‚Äôt yet handle.</p>
            <h3
            id="interview-q-how-would-you-implement-curriculum-learning-for-a-language-model">Interview
            Q: ‚ÄúHow would you implement curriculum learning for a
            language model?‚Äù</h3>
            <p><strong>A</strong>: For a language model, I would:</p>
            <ol type="1">
            <li><strong>Define difficulty</strong>: Use sequence length
            as the primary measure (short = easy), optionally combined
            with perplexity from a simpler model</li>
            <li><strong>Bucket the data</strong>: Group sequences into
            length bins (e.g., 0-64, 64-256, 256-1024, 1024+
            tokens)</li>
            <li><strong>Training schedule</strong>: Start with shortest
            bucket for N steps, then include next bucket, repeat until
            all data is used</li>
            <li><strong>Alternative</strong>: Use a continuous schedule
            where the maximum sequence length increases linearly or
            exponentially with training step</li>
            </ol>
            <p>This helps the model learn basic patterns and local
            dependencies before tackling long-range dependencies.</p>
            <hr />
            <h2
            id="transfer-learning-domain-adaptation-and-few-shot-learning">4.6
            Transfer Learning, Domain Adaptation, and Few-Shot
            Learning</h2>
            <h3 id="transfer-learning">Transfer Learning</h3>
            <p><strong>Transfer learning</strong> uses knowledge learned
            from one task to improve performance on a different but
            related task.</p>
            <pre><code>Source Task                    Target Task
(ImageNet: 1M images)          (Medical imaging: 1K images)
       ‚Üì                              ‚Üì
[Pretrained CNN]  ‚îÄ‚îÄtransfer‚îÄ‚îÄ‚Üí  [Fine-tuned model]
       ‚Üì                              ‚Üì
Generic features              Task-specific features
(edges, textures)             (tumor patterns)</code></pre>
            <p><strong>Why it works</strong>: Early layers learn general
            features (edges, shapes) that transfer well. Later layers
            learn task-specific features that need fine-tuning.</p>
            <p><strong>Common approaches</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 36%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>Approach</th>
            <th>Description</th>
            <th>When to Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Feature extraction</strong></td>
            <td>Freeze pretrained layers, train new classifier</td>
            <td>Small target dataset</td>
            </tr>
            <tr>
            <td><strong>Fine-tuning</strong></td>
            <td>Unfreeze some/all layers, train with small LR</td>
            <td>Medium target dataset</td>
            </tr>
            <tr>
            <td><strong>Full retraining</strong></td>
            <td>Use pretrained weights as initialization</td>
            <td>Large target dataset</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Modern examples</strong>: BERT/GPT pretrained on
            web text, fine-tuned for specific NLP tasks;
            ImageNet-pretrained CNNs for medical imaging.</p>
            <h3 id="domain-adaptation">Domain Adaptation</h3>
            <p><strong>Domain adaptation</strong> is a special case of
            transfer learning where the task is the same but the data
            distribution differs.</p>
            <pre><code>Source Domain                  Target Domain
(Synthetic images)             (Real photos)
       ‚Üì                              ‚Üì
   P_source(x)        ‚â†          P_target(x)
       ‚Üì                              ‚Üì
Same task: classify objects in both domains</code></pre>
            <p><strong>The problem</strong>: Model trained on source
            domain performs poorly on target domain due to
            <strong>domain shift</strong> (distribution mismatch).</p>
            <p><strong>Approaches</strong>:</p>
            <ul>
            <li><strong>Discrepancy-based</strong>: Minimize distance
            between source and target feature distributions (e.g.,
            Maximum Mean Discrepancy)</li>
            <li><strong>Adversarial</strong>: Train domain
            discriminator, learn domain-invariant features</li>
            <li><strong>Self-training</strong>: Use confident
            predictions on target domain as pseudo-labels</li>
            </ul>
            <p><strong>Example</strong>: Training on simulation (cheap
            labels) ‚Üí deploying on real robots (no labels).</p>
            <h3 id="one-shot-learning">One-Shot Learning</h3>
            <p><strong>One-shot learning</strong> learns to recognize
            new classes from just <strong>one example</strong> per
            class.</p>
            <pre><code>Support Set:                   Query:
[1 example of &quot;cat&quot;]           [New image] ‚Üí &quot;Is this a cat?&quot;
[1 example of &quot;dog&quot;]           
[1 example of &quot;bird&quot;]          </code></pre>
            <p><strong>Key insight</strong>: Don‚Äôt learn to classify
            directly. Learn a <strong>similarity function</strong> that
            compares images.</p>
            <p><strong>Approaches</strong>:</p>
            <ul>
            <li><strong>Siamese Networks</strong>: Twin networks learn
            embedding where similar items are close</li>
            <li><strong>Matching Networks</strong>: Attention over
            support set embeddings</li>
            <li><strong>Prototypical Networks</strong>: Compare query to
            class prototypes (mean embeddings)</li>
            </ul>
            <p><strong>Meta-learning connection</strong>: Learn to learn
            from few examples by training on many few-shot tasks.</p>
            <h3 id="zero-shot-learning">Zero-Shot Learning</h3>
            <p><strong>Zero-shot learning</strong> recognizes classes
            <strong>never seen during training</strong> by leveraging
            auxiliary information (attributes, descriptions,
            embeddings).</p>
            <pre><code>Training classes: cat, dog, car, truck (with images)
Test class: zebra (NO images seen!)
                    ‚Üì
Auxiliary info: &quot;zebra = horse-like + black-and-white stripes&quot;
                    ‚Üì
Model: Maps images and descriptions to shared embedding space</code></pre>
            <p><strong>How it works</strong>:</p>
            <ol type="1">
            <li>Learn joint embedding of images and semantic
            descriptions</li>
            <li>At test time, embed the new class description</li>
            <li>Classify by finding nearest class in embedding
            space</li>
            </ol>
            <p><strong>Relationship to Transfer Learning</strong>:</p>
            <ul>
            <li>Zero-shot IS a form of transfer learning</li>
            <li>Knowledge transfers from seen classes to unseen
            classes</li>
            <li>The ‚Äúbridge‚Äù is semantic similarity (attributes, word
            embeddings, descriptions)</li>
            </ul>
            <p><strong>Modern zero-shot with LLMs</strong>:</p>
            <pre><code>GPT/CLIP: Trained on image-text pairs from the web
         Never explicitly trained on &quot;zebra classification&quot;
         But can classify zebras because it learned the concept
         from text descriptions during pretraining</code></pre>
            <p><strong>CLIP‚Äôs zero-shot approach</strong>:</p>
            <ul>
            <li>Train: Align images with text descriptions</li>
            <li>Test: Embed class name ‚Äúzebra‚Äù, find images closest to
            that embedding</li>
            <li>No task-specific training needed!</li>
            </ul>
            <h3 id="comparison-table-1">Comparison Table</h3>
            <table>
            <colgroup>
            <col style="width: 18%" />
            <col style="width: 27%" />
            <col style="width: 27%" />
            <col style="width: 27%" />
            </colgroup>
            <thead>
            <tr>
            <th>Paradigm</th>
            <th>Training Data</th>
            <th>Test Scenario</th>
            <th>Key Mechanism</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Transfer Learning</strong></td>
            <td>Source task data</td>
            <td>Different but related task</td>
            <td>Feature reuse</td>
            </tr>
            <tr>
            <td><strong>Domain Adaptation</strong></td>
            <td>Source domain + (unlabeled) target</td>
            <td>Same task, different distribution</td>
            <td>Distribution alignment</td>
            </tr>
            <tr>
            <td><strong>One-Shot Learning</strong></td>
            <td>Many tasks with few examples</td>
            <td>New classes, 1 example each</td>
            <td>Similarity learning</td>
            </tr>
            <tr>
            <td><strong>Zero-Shot Learning</strong></td>
            <td>Classes + semantic info</td>
            <td>Completely new classes</td>
            <td>Semantic transfer</td>
            </tr>
            </tbody>
            </table>
            <h3 id="in-context-learning-gpt-3-style-few-shot">In-Context
            Learning (GPT-3 Style Few-Shot)</h3>
            <p><strong>A modern form of few-shot learning</strong> that
            emerged with large language models like GPT-3: instead of
            updating model weights, you simply provide examples in the
            prompt!</p>
            <pre><code>Traditional few-shot:          In-context learning:
Train on k examples            No training!
  ‚Üì                            Just provide k examples in prompt
Update weights                    ‚Üì
  ‚Üì                            Model &quot;understands&quot; the task
Run inference                  from the context</code></pre>
            <p><strong>How it works</strong>:</p>
            <pre><code>Prompt:
&quot;Classify the sentiment of these reviews:

Review: &#39;This movie was fantastic!&#39; ‚Üí Positive
Review: &#39;Terrible waste of time&#39; ‚Üí Negative
Review: &#39;Best purchase I ever made&#39; ‚Üí Positive

Review: &#39;I want a refund&#39; ‚Üí &quot;

Model output: &quot;Negative&quot;</code></pre>
            <p><strong>Why it works</strong>: Large language models
            pretrained on massive text develop general-purpose
            reasoning. The few examples in the prompt help the model
            understand the <strong>task format</strong> and
            <strong>desired output style</strong> ‚Äî no gradient updates
            needed.</p>
            <p><strong>Key differences from traditional
            few-shot</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Traditional Few-Shot</th>
            <th>In-Context Learning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Weight updates</strong></td>
            <td>Yes (meta-learning)</td>
            <td>No (frozen model)</td>
            </tr>
            <tr>
            <td><strong>Where examples go</strong></td>
            <td>Training set</td>
            <td>Prompt itself</td>
            </tr>
            <tr>
            <td><strong>Model size needed</strong></td>
            <td>Any</td>
            <td>Very large (billions of params)</td>
            </tr>
            <tr>
            <td><strong>Task switching</strong></td>
            <td>Requires retraining</td>
            <td>Just change the prompt</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Limitations</strong>:</p>
            <ul>
            <li>Context length limits how many examples you can
            provide</li>
            <li>Sensitive to prompt formatting and example ordering</li>
            <li>Works best with very large models (&gt;1B
            parameters)</li>
            <li>May not match fine-tuning performance on specialized
            tasks</li>
            </ul>
            <h3
            id="interview-q-whats-the-relationship-between-zero-shot-learning-and-transfer-learning">Interview
            Q: ‚ÄúWhat‚Äôs the relationship between zero-shot learning and
            transfer learning?‚Äù</h3>
            <p><strong>A</strong>: Zero-shot learning is a form of
            transfer learning where knowledge transfers from seen to
            unseen classes via semantic similarity. In traditional
            transfer learning, we transfer learned features to a new
            task with some labeled examples. In zero-shot, we transfer
            to classes with NO examples by using auxiliary information
            (attributes, text descriptions) as the bridge. Modern
            approaches like CLIP learn joint image-text embeddings,
            enabling zero-shot classification by comparing images to
            text descriptions of classes never seen during training.</p>
            <h3
            id="interview-q-how-does-gpt-3s-in-context-learning-differ-from-traditional-few-shot-learning">Interview
            Q: ‚ÄúHow does GPT-3‚Äôs in-context learning differ from
            traditional few-shot learning?‚Äù</h3>
            <p><strong>A</strong>: Traditional few-shot learning
            (meta-learning) trains a model to adapt quickly by updating
            weights based on a small support set. GPT-3‚Äôs in-context
            learning requires <strong>no weight updates</strong> ‚Äî the
            model is frozen, and examples are simply placed in the
            prompt. The model‚Äôs pretrained knowledge allows it to
            understand the task from context. This is more flexible
            (switch tasks by changing prompts) but requires very large
            models and is limited by context length. It‚Äôs not true
            ‚Äúlearning‚Äù in the sense of parameter updates, but rather
            leveraging the model‚Äôs existing knowledge to pattern-match
            to the task format.</p>
            <hr />
            <h2 id="soft-labels-vs-hard-labels">4.7 Soft Labels vs Hard
            Labels</h2>
            <h3 id="what-this-means-for-beginners-4">What This Means
            (For Beginners)</h3>
            <p>Think about how confident you are when answering
            questions:</p>
            <p><strong>Hard labels = Absolute certainty</strong> - ‚ÄúIs
            this a picture of a cat?‚Äù ‚Üí ‚ÄúYES, 100% cat, 0% anything
            else‚Äù - Like answering a test question with complete
            confidence, even when you‚Äôre not sure</p>
            <p><strong>Soft labels = Honest uncertainty</strong> - ‚ÄúIs
            this a picture of a cat?‚Äù ‚Üí ‚ÄúPretty sure it‚Äôs a cat (70%),
            but it could be a dog (20%), maybe something else (10%)‚Äù -
            Like saying ‚ÄúI think it‚Äôs A, but B is also possible‚Äù</p>
            <p><strong>Why does this matter?</strong></p>
            <p>Imagine a fluffy animal that looks like both a cat and a
            dog:</p>
            <pre><code>Hard label:  &quot;It&#39;s a cat&quot;  (100% cat, period)
Soft label:  &quot;Probably a cat (60%), maybe a dog (35%), other (5%)&quot;</code></pre>
            <p>The soft label is more <strong>honest</strong> about
            uncertainty. When we train a model with soft labels, we‚Äôre
            saying ‚Äúdon‚Äôt be overconfident‚Äù ‚Äî which helps the model make
            better predictions on tricky cases.</p>
            <p><strong>Real-world example</strong>: A doctor diagnosing
            a patient. Instead of ‚ÄúYou definitely have condition X‚Äù
            (hard), a more nuanced ‚Äú80% chance it‚Äôs condition X, 15%
            chance it‚Äôs condition Y‚Äù (soft) is often more useful and
            honest.</p>
            <h3 id="what-are-hard-labels">What Are Hard Labels?</h3>
            <p><strong>Hard labels</strong> are one-hot encoded vectors
            where exactly one class has probability 1 and all others
            have probability 0.</p>
            <pre><code>Class:     Cat    Dog    Bird   Fish
Hard:     [1.0,  0.0,   0.0,   0.0]   ‚Üê &quot;This is definitely a cat&quot;</code></pre>
            <p><strong>Used with</strong>: Standard cross-entropy
            loss.</p>
            <h3 id="what-are-soft-labels">What Are Soft Labels?</h3>
            <p><strong>Soft labels</strong> are probability
            distributions where multiple classes can have non-zero
            probabilities.</p>
            <pre><code>Class:     Cat    Dog    Bird   Fish
Soft:     [0.7,  0.2,   0.05,  0.05]  ‚Üê &quot;Probably a cat, maybe a dog&quot;</code></pre>
            <p><strong>Where do soft labels come from?</strong></p>
            <ol type="1">
            <li><strong>Human annotators disagreeing</strong> (multiple
            people label, average their votes)</li>
            <li><strong>Teacher model outputs</strong> (knowledge
            distillation)</li>
            <li><strong>Label smoothing</strong> (artificial
            softening)</li>
            <li><strong>Noisy labels</strong> (uncertainty in ground
            truth)</li>
            </ol>
            <h3 id="label-smoothing">Label Smoothing</h3>
            <p>A simple technique to convert hard labels to soft
            labels:</p>
            <p><span class="math display">\[y_{\text{smooth}} = (1 -
            \epsilon) \cdot y_{\text{hard}} +
            \frac{\epsilon}{K}\]</span></p>
            <p>where <span class="math inline">\(\epsilon\)</span> is
            the smoothing factor (typically 0.1) and <span
            class="math inline">\(K\)</span> is the number of
            classes.</p>
            <p><strong>Example</strong> (<span
            class="math inline">\(\epsilon = 0.1\)</span>, <span
            class="math inline">\(K = 4\)</span>):</p>
            <pre><code>Hard:   [1.0,  0.0,   0.0,   0.0]
Smooth: [0.925, 0.025, 0.025, 0.025]</code></pre>
            <div class="sourceCode" id="cb178"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> label_smoothing(labels, num_classes, epsilon<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb178-2"><a href="#cb178-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Convert hard labels to smoothed soft labels&quot;&quot;&quot;</span></span>
<span id="cb178-3"><a href="#cb178-3" aria-hidden="true" tabindex="-1"></a>    smooth <span class="op">=</span> torch.full((<span class="bu">len</span>(labels), num_classes), epsilon <span class="op">/</span> num_classes)</span>
<span id="cb178-4"><a href="#cb178-4" aria-hidden="true" tabindex="-1"></a>    smooth.scatter_(<span class="dv">1</span>, labels.unsqueeze(<span class="dv">1</span>), <span class="dv">1</span> <span class="op">-</span> epsilon <span class="op">+</span> epsilon <span class="op">/</span> num_classes)</span>
<span id="cb178-5"><a href="#cb178-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> smooth</span></code></pre></div>
            <h3 id="why-use-soft-labels">Why Use Soft Labels?</h3>
            <table>
            <colgroup>
            <col style="width: 40%" />
            <col style="width: 59%" />
            </colgroup>
            <thead>
            <tr>
            <th>Benefit</th>
            <th>Explanation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Regularization</strong></td>
            <td>Prevents model from becoming overconfident</td>
            </tr>
            <tr>
            <td><strong>Better calibration</strong></td>
            <td>Output probabilities are more meaningful</td>
            </tr>
            <tr>
            <td><strong>Captures ambiguity</strong></td>
            <td>Some examples genuinely belong to multiple classes</td>
            </tr>
            <tr>
            <td><strong>Reduces overfitting</strong></td>
            <td>Softer targets are harder to memorize</td>
            </tr>
            <tr>
            <td><strong>Knowledge transfer</strong></td>
            <td>Teacher models provide richer supervision</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-overconfidence-problem">The Overconfidence
            Problem</h3>
            <p><strong>Hard labels encourage extreme
            predictions</strong>:</p>
            <pre><code>Prediction: [0.999, 0.0005, 0.0003, 0.0002]
Target:     [1.0,   0.0,    0.0,    0.0]

Cross-entropy: -log(0.999) ‚âà 0.001  ‚Üê Very small loss</code></pre>
            <p>The model is rewarded for being 99.9% confident, even
            when that confidence isn‚Äôt warranted. This leads to:</p>
            <ul>
            <li>Poor calibration (confidence ‚â† accuracy)</li>
            <li>Vulnerability to adversarial examples</li>
            <li>Poor generalization</li>
            </ul>
            <p><strong>Soft labels fix this</strong>:</p>
            <pre><code>Prediction: [0.999, 0.0005, 0.0003, 0.0002]
Target:     [0.925, 0.025,  0.025,  0.025]

Cross-entropy: -0.925*log(0.999) - 0.025*log(0.0005) - ... ‚âà 0.2</code></pre>
            <p>Now the model is penalized for being too confident about
            non-target classes!</p>
            <h3 id="loss-function-for-soft-labels">Loss Function for
            Soft Labels</h3>
            <p>The same cross-entropy formula works, but now sums over
            all classes:</p>
            <p><span class="math display">\[\mathcal{L} =
            -\sum_{k=1}^{K} y_k^{\text{soft}}
            \log(\hat{y}_k)\]</span></p>
            <p>For hard labels, this reduces to <span
            class="math inline">\(-\log(\hat{y}_c)\)</span> (only true
            class matters).</p>
            <h3
            id="interview-q-whats-the-difference-between-hard-and-soft-labels">Interview
            Q: ‚ÄúWhat‚Äôs the difference between hard and soft
            labels?‚Äù</h3>
            <p><strong>A</strong>: Hard labels are one-hot encoded (100%
            probability on one class, 0% on others). Soft labels are
            probability distributions where multiple classes can have
            non-zero probability. Soft labels come from label smoothing,
            teacher model predictions (distillation), or annotator
            disagreement. Benefits include: (1) regularization ‚Äî
            prevents overconfidence, (2) better calibration ‚Äî output
            probabilities are more meaningful, (3) capturing genuine
            ambiguity ‚Äî some images really are ambiguous. Label
            smoothing is a common technique: replace hard [1,0,0,0] with
            smooth [0.925, 0.025, 0.025, 0.025].</p>
            <hr />
            <h2 id="knowledge-distillation">4.8 Knowledge
            Distillation</h2>
            <h3 id="what-this-means-for-beginners-5">What This Means
            (For Beginners)</h3>
            <p>Think about how an expert teaches a student:</p>
            <p><strong>The expert (teacher model):</strong> - Spent
            years learning (trained on massive data) - Has deep
            understanding (large, complex model) - Can explain nuances
            (‚ÄúThis looks like a cat, but has some dog-like features‚Äù) -
            Is slow and expensive to consult (takes lots of compute)</p>
            <p><strong>The student (student model):</strong> - Wants to
            learn quickly (smaller, faster model) - Can‚Äôt spend years
            studying (limited training budget) - Needs the essentials,
            not everything (efficient deployment)</p>
            <p><strong>Traditional learning (without
            distillation):</strong> - Student only sees textbook
            answers: ‚ÄúThis is a cat‚Äù (hard labels) - Misses the expert‚Äôs
            nuanced understanding</p>
            <p><strong>Learning with distillation:</strong> - Student
            learns from expert‚Äôs reasoning: ‚ÄúThis is 70% cat, 25% dog,
            5% other‚Äù - Gets the expert‚Äôs intuition about similarities
            between classes - Learns much faster and almost as well!</p>
            <p><strong>Real-world analogy</strong>: A senior doctor
            teaching a resident. Instead of just saying ‚Äúthis patient
            has condition X,‚Äù the senior doctor explains: ‚ÄúIt‚Äôs probably
            X (80%), but watch out for Y (15%) ‚Äî here‚Äôs why they look
            similar, and here are the key differences.‚Äù The resident
            learns faster and develops better intuition.</p>
            <p><strong>Why this matters for ML</strong>: We can train
            huge models (GPT-4 scale) on massive compute, then ‚Äúdistill‚Äù
            their knowledge into small models that run on phones ‚Äî
            getting most of the quality at a fraction of the cost.</p>
            <h3 id="what-is-knowledge-distillation">What is Knowledge
            Distillation?</h3>
            <p><strong>Knowledge distillation</strong> transfers
            knowledge from a large, accurate ‚Äúteacher‚Äù model to a
            smaller, faster ‚Äústudent‚Äù model.</p>
            <pre><code>                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Teacher Model  ‚îÇ
                    ‚îÇ   (Large, slow) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    Soft predictions (dark knowledge)
                             ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Student Model  ‚îÇ
                    ‚îÇ  (Small, fast)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Key insight</strong>: The teacher‚Äôs soft
            predictions contain more information than hard labels.</p>
            <h3 id="why-soft-predictions-dark-knowledge">Why Soft
            Predictions (‚ÄúDark Knowledge‚Äù)?</h3>
            <p>Consider a teacher predicting on a cat image:</p>
            <pre><code>Hard label:        [1.0, 0.0,  0.0,  0.0]   (Cat, Dog, Bird, Fish)
Teacher output:    [0.7, 0.25, 0.04, 0.01]</code></pre>
            <p><strong>What the soft predictions tell us</strong>:</p>
            <ul>
            <li>This cat looks somewhat like a dog (0.25)</li>
            <li>It doesn‚Äôt look like a bird or fish (0.04, 0.01)</li>
            <li>There‚Äôs inherent similarity between cats and dogs</li>
            </ul>
            <p>This <strong>relational information</strong> is lost with
            hard labels but preserved in soft targets!</p>
            <h3 id="temperature-in-distillation">Temperature in
            Distillation</h3>
            <p>To extract more information, we ‚Äúsoften‚Äù the teacher‚Äôs
            outputs using temperature:</p>
            <p><span class="math display">\[q_i = \frac{\exp(z_i /
            T)}{\sum_j \exp(z_j / T)}\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Temperature <span class="math inline">\(T\)</span></th>
            <th>Effect</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(T = 1\)</span></td>
            <td>Normal softmax</td>
            </tr>
            <tr>
            <td><span class="math inline">\(T &gt; 1\)</span></td>
            <td>Softer distribution (more info in non-top classes)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(T \to \infty\)</span></td>
            <td>Uniform distribution</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Typical</strong>: <span class="math inline">\(T =
            2\)</span> to <span class="math inline">\(T =
            20\)</span></p>
            <h3 id="distillation-loss">Distillation Loss</h3>
            <p>The student is trained on a combination of:</p>
            <ol type="1">
            <li><strong>Hard loss</strong>: Cross-entropy with true
            labels</li>
            <li><strong>Soft loss</strong>: Cross-entropy with teacher‚Äôs
            soft predictions</li>
            </ol>
            <p><span class="math display">\[\mathcal{L} = \alpha \cdot
            \mathcal{L}_{\text{hard}} + (1-\alpha) \cdot T^2 \cdot
            \mathcal{L}_{\text{soft}}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(\mathcal{L}_{\text{hard}} =
            \text{CE}(y_{\text{true}}, \text{student}(x;
            T=1))\)</span></li>
            <li><span class="math inline">\(\mathcal{L}_{\text{soft}} =
            \text{CE}(\text{teacher}(x; T), \text{student}(x;
            T))\)</span></li>
            <li><span class="math inline">\(T^2\)</span> scaling because
            gradients scale as <span
            class="math inline">\(1/T^2\)</span> with temperature</li>
            </ul>
            <h3 id="knowledge-distillation-implementation">Knowledge
            Distillation Implementation</h3>
            <div class="sourceCode" id="cb183"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distillation_loss(student_logits, teacher_logits, labels, </span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>                      temperature<span class="op">=</span><span class="fl">4.0</span>, alpha<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Combined distillation loss.</span></span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb183-7"><a href="#cb183-7" aria-hidden="true" tabindex="-1"></a><span class="co">        student_logits: Raw outputs from student</span></span>
<span id="cb183-8"><a href="#cb183-8" aria-hidden="true" tabindex="-1"></a><span class="co">        teacher_logits: Raw outputs from teacher</span></span>
<span id="cb183-9"><a href="#cb183-9" aria-hidden="true" tabindex="-1"></a><span class="co">        labels: Ground truth labels (hard)</span></span>
<span id="cb183-10"><a href="#cb183-10" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature: Softening temperature</span></span>
<span id="cb183-11"><a href="#cb183-11" aria-hidden="true" tabindex="-1"></a><span class="co">        alpha: Weight for hard loss vs soft loss</span></span>
<span id="cb183-12"><a href="#cb183-12" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb183-13"><a href="#cb183-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Hard loss (with true labels, T=1)</span></span>
<span id="cb183-14"><a href="#cb183-14" aria-hidden="true" tabindex="-1"></a>    hard_loss <span class="op">=</span> F.cross_entropy(student_logits, labels)</span>
<span id="cb183-15"><a href="#cb183-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb183-16"><a href="#cb183-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Soft loss (with teacher predictions, T=temperature)</span></span>
<span id="cb183-17"><a href="#cb183-17" aria-hidden="true" tabindex="-1"></a>    soft_student <span class="op">=</span> F.log_softmax(student_logits <span class="op">/</span> temperature, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb183-18"><a href="#cb183-18" aria-hidden="true" tabindex="-1"></a>    soft_teacher <span class="op">=</span> F.softmax(teacher_logits <span class="op">/</span> temperature, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb183-19"><a href="#cb183-19" aria-hidden="true" tabindex="-1"></a>    soft_loss <span class="op">=</span> F.kl_div(soft_student, soft_teacher, reduction<span class="op">=</span><span class="st">&#39;batchmean&#39;</span>)</span>
<span id="cb183-20"><a href="#cb183-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb183-21"><a href="#cb183-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combined loss (T^2 scaling for soft loss)</span></span>
<span id="cb183-22"><a href="#cb183-22" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> alpha <span class="op">*</span> hard_loss <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> alpha) <span class="op">*</span> (temperature <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> soft_loss</span>
<span id="cb183-23"><a href="#cb183-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total_loss</span></code></pre></div>
            <h3 id="types-of-distillation">Types of Distillation</h3>
            <table>
            <colgroup>
            <col style="width: 17%" />
            <col style="width: 55%" />
            <col style="width: 26%" />
            </colgroup>
            <thead>
            <tr>
            <th>Type</th>
            <th>What‚Äôs Transferred</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Response-based</strong></td>
            <td>Final layer outputs (logits/softmax)</td>
            <td>Original KD</td>
            </tr>
            <tr>
            <td><strong>Feature-based</strong></td>
            <td>Intermediate layer activations</td>
            <td>FitNets</td>
            </tr>
            <tr>
            <td><strong>Relation-based</strong></td>
            <td>Relationships between examples</td>
            <td>RKD</td>
            </tr>
            <tr>
            <td><strong>Self-distillation</strong></td>
            <td>Teacher = Student (different version)</td>
            <td>Born-Again Networks</td>
            </tr>
            </tbody>
            </table>
            <h3 id="examples-in-practice-1">Examples in Practice</h3>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 23%" />
            <col style="width: 23%" />
            <col style="width: 20%" />
            </colgroup>
            <thead>
            <tr>
            <th>Application</th>
            <th>Teacher</th>
            <th>Student</th>
            <th>Result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>DistilBERT</strong></td>
            <td>BERT-base (110M)</td>
            <td>DistilBERT (66M)</td>
            <td>97% perf, 60% params</td>
            </tr>
            <tr>
            <td><strong>TinyBERT</strong></td>
            <td>BERT-base</td>
            <td>TinyBERT (14.5M)</td>
            <td>96% perf, 13% params</td>
            </tr>
            <tr>
            <td><strong>MobileNets</strong></td>
            <td>Large CNNs</td>
            <td>MobileNet</td>
            <td>Real-time on phones</td>
            </tr>
            <tr>
            <td><strong>GPT-2 ‚Üí DistilGPT-2</strong></td>
            <td>GPT-2</td>
            <td>Smaller GPT-2</td>
            <td>Similar quality</td>
            </tr>
            </tbody>
            </table>
            <h3 id="why-does-distillation-work">Why Does Distillation
            Work?</h3>
            <ol type="1">
            <li><strong>Richer supervision</strong>: Soft targets
            provide more bits of information than one-hot labels</li>
            <li><strong>Implicit data augmentation</strong>: Teacher‚Äôs
            smoothed predictions act like averaging over data
            augmentations</li>
            <li><strong>Regularization</strong>: Softer targets prevent
            overfitting</li>
            <li><strong>Transfer of generalization</strong>: Teacher
            learned good representations that transfer</li>
            </ol>
            <h3
            id="interview-q-how-does-knowledge-distillation-work-and-why-is-it-effective">Interview
            Q: ‚ÄúHow does knowledge distillation work and why is it
            effective?‚Äù</h3>
            <p><strong>A</strong>: Knowledge distillation trains a small
            ‚Äústudent‚Äù model to mimic a large ‚Äúteacher‚Äù model. The key is
            using the teacher‚Äôs soft predictions (probabilities) rather
            than hard labels. Soft predictions contain ‚Äúdark knowledge‚Äù
            ‚Äî relationships between classes (e.g., cat is more similar
            to dog than to airplane).</p>
            <p>The distillation loss combines: (1) hard cross-entropy
            with true labels, and (2) KL divergence between teacher and
            student soft outputs, with a temperature parameter (T=2-20)
            to soften distributions. The <span
            class="math inline">\(T^2\)</span> scaling compensates for
            reduced gradient magnitude at higher temperatures.</p>
            <p>It‚Äôs effective because soft targets provide richer
            supervision than one-hot labels ‚Äî they encode the teacher‚Äôs
            learned structure about class relationships and
            uncertainties. Examples like DistilBERT achieve 97% of
            BERT‚Äôs performance with 60% fewer parameters.</p>
            <hr />
            <h2 id="contrastive-learning">4.9 Contrastive Learning</h2>
            <h3 id="what-this-means-for-beginners-6">What This Means
            (For Beginners)</h3>
            <p>Think about how you organize your photo album:</p>
            <p><strong>You naturally group similar things
            together:</strong> - All photos of your cat go in one
            section - All photos of your dog go in another section<br />
            - Vacation photos in another section</p>
            <p><strong>Even without labels, you know what‚Äôs
            ‚Äúsimilar‚Äù:</strong> - Two photos of your cat (taken at
            different angles) ‚Üí <strong>same group</strong> - A photo of
            your cat vs.¬†a photo of your dog ‚Üí <strong>different
            groups</strong></p>
            <p><strong>Contrastive learning teaches a computer to do
            exactly this:</strong></p>
            <pre><code>Question: &quot;Are these two images of the same thing?&quot;

Same cat, different photos:     Cat vs Dog:
[Cat sleeping] [Cat playing]    [Cat] [Dog]
        ‚Üì                           ‚Üì
   &quot;Pull close!&quot;              &quot;Push apart!&quot;</code></pre>
            <p><strong>The magic trick ‚Äî no labels needed!</strong></p>
            <p>Instead of someone telling the computer ‚Äúthis is a cat,‚Äù
            we just say: - ‚ÄúThese two are augmentations of the
            <strong>same</strong> image‚Äù ‚Üí pull them close - ‚ÄúThese two
            are <strong>different</strong> images‚Äù ‚Üí push them apart</p>
            <p>The computer learns to recognize cats and dogs just by
            learning what makes images similar or different!</p>
            <p><strong>Real-world analogy</strong>: A child learning to
            recognize animals at a zoo. Nobody gives them a test with
            labels. They just notice ‚Äúthat striped animal looks like the
            other striped one‚Äù and ‚Äúthe big gray one is different from
            the small brown one.‚Äù They learn categories naturally
            through comparison.</p>
            <p><strong>Why this matters</strong>: Labeled data is
            expensive. Contrastive learning lets us use billions of
            unlabeled images/text to learn powerful representations.</p>
            <h3 id="what-is-contrastive-learning">What is Contrastive
            Learning?</h3>
            <p><strong>Contrastive learning</strong> learns
            representations by <strong>pulling similar pairs
            close</strong> and <strong>pushing dissimilar pairs
            apart</strong> in embedding space.</p>
            <pre><code>Embedding Space:

     ‚óè  anchor (cat image)
    /|\
   / | \
  ‚Üô  ‚Üì  ‚Üò
 ‚óè   ‚óè   ‚óè
 cat  dog  car
 (pull) (push) (push)
 close  apart  apart</code></pre>
            <p><strong>Key insight</strong>: You don‚Äôt need labels ‚Äî
            just a notion of ‚Äúsimilar‚Äù and ‚Äúdissimilar‚Äù pairs.</p>
            <h3 id="where-do-pairs-come-from">Where Do Pairs Come
            From?</h3>
            <p><strong>Self-supervised</strong> (no labels needed):</p>
            <ul>
            <li><strong>Same image, different augmentations</strong> ‚Üí
            positive pair</li>
            <li><strong>Different images</strong> ‚Üí negative pairs</li>
            </ul>
            <pre><code>Original image ‚Üí [RandomCrop] ‚Üí View 1  ‚îê
                                        ‚îú‚îÄ‚Üí Positive pair (should be close)
Original image ‚Üí [ColorJitter] ‚Üí View 2 ‚îò

Different image ‚Üí [Augment] ‚Üí View 3  ‚îÄ‚îÄ‚Üí Negative pair (should be far)</code></pre>
            <h3 id="the-infonce-loss-nt-xent">The InfoNCE Loss
            (NT-Xent)</h3>
            <p>For an anchor <span class="math inline">\(x\)</span>,
            positive <span class="math inline">\(x^+\)</span>, and
            negatives <span class="math inline">\(\{x^-_1, \ldots,
            x^-_N\}\)</span>:</p>
            <p><span class="math display">\[\mathcal{L} = -\log
            \frac{\exp(\text{sim}(x, x^+) / \tau)}{\exp(\text{sim}(x,
            x^+) / \tau) + \sum_{i=1}^{N} \exp(\text{sim}(x, x^-_i) /
            \tau)}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(\text{sim}(a, b) = \frac{a
            \cdot b}{\|a\| \|b\|}\)</span> (cosine similarity)</li>
            <li><span class="math inline">\(\tau\)</span> is a
            temperature parameter (typically 0.07-0.5)</li>
            </ul>
            <p><strong>Intuition</strong>: This is just softmax
            cross-entropy where:</p>
            <ul>
            <li>The positive pair should have the highest similarity
            (class 1)</li>
            <li>All negatives should have low similarity (class 0)</li>
            </ul>
            <h3 id="simclr-a-simple-framework">SimCLR: A Simple
            Framework</h3>
            <pre><code>Image ‚Üí [Augment 1] ‚Üí [Encoder] ‚Üí [Projection] ‚Üí z_i  ‚îÄ‚îê
                                                        ‚îú‚îÄ‚Üí Contrastive Loss
Image ‚Üí [Augment 2] ‚Üí [Encoder] ‚Üí [Projection] ‚Üí z_j  ‚îÄ‚îò</code></pre>
            <p><strong>Components</strong>:</p>
            <ol type="1">
            <li><strong>Data augmentation</strong>: Random crop, color
            jitter, blur, flip</li>
            <li><strong>Encoder</strong>: ResNet (extract features)</li>
            <li><strong>Projection head</strong>: MLP (maps features to
            contrastive space)</li>
            <li><strong>Contrastive loss</strong>: InfoNCE across
            batch</li>
            </ol>
            <h3 id="simclr-implementation">SimCLR Implementation</h3>
            <div class="sourceCode" id="cb188"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simclr_loss(z_i, z_j, temperature<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb188-2"><a href="#cb188-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb188-3"><a href="#cb188-3" aria-hidden="true" tabindex="-1"></a><span class="co">    SimCLR contrastive loss.</span></span>
<span id="cb188-4"><a href="#cb188-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb188-5"><a href="#cb188-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb188-6"><a href="#cb188-6" aria-hidden="true" tabindex="-1"></a><span class="co">        z_i: Embeddings of first augmented view [batch_size, embed_dim]</span></span>
<span id="cb188-7"><a href="#cb188-7" aria-hidden="true" tabindex="-1"></a><span class="co">        z_j: Embeddings of second augmented view [batch_size, embed_dim]</span></span>
<span id="cb188-8"><a href="#cb188-8" aria-hidden="true" tabindex="-1"></a><span class="co">        temperature: Temperature parameter</span></span>
<span id="cb188-9"><a href="#cb188-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb188-10"><a href="#cb188-10" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> z_i.shape[<span class="dv">0</span>]</span>
<span id="cb188-11"><a href="#cb188-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-12"><a href="#cb188-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize embeddings</span></span>
<span id="cb188-13"><a href="#cb188-13" aria-hidden="true" tabindex="-1"></a>    z_i <span class="op">=</span> F.normalize(z_i, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb188-14"><a href="#cb188-14" aria-hidden="true" tabindex="-1"></a>    z_j <span class="op">=</span> F.normalize(z_j, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb188-15"><a href="#cb188-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-16"><a href="#cb188-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate all embeddings</span></span>
<span id="cb188-17"><a href="#cb188-17" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.cat([z_i, z_j], dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># [2*batch_size, embed_dim]</span></span>
<span id="cb188-18"><a href="#cb188-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-19"><a href="#cb188-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute similarity matrix</span></span>
<span id="cb188-20"><a href="#cb188-20" aria-hidden="true" tabindex="-1"></a>    sim_matrix <span class="op">=</span> torch.mm(z, z.T) <span class="op">/</span> temperature  <span class="co"># [2*batch, 2*batch]</span></span>
<span id="cb188-21"><a href="#cb188-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-22"><a href="#cb188-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create labels: positive pairs are (i, batch+i) and (batch+i, i)</span></span>
<span id="cb188-23"><a href="#cb188-23" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.cat([</span>
<span id="cb188-24"><a href="#cb188-24" aria-hidden="true" tabindex="-1"></a>        torch.arange(batch_size, <span class="dv">2</span><span class="op">*</span>batch_size),</span>
<span id="cb188-25"><a href="#cb188-25" aria-hidden="true" tabindex="-1"></a>        torch.arange(batch_size)</span>
<span id="cb188-26"><a href="#cb188-26" aria-hidden="true" tabindex="-1"></a>    ]).to(z.device)</span>
<span id="cb188-27"><a href="#cb188-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-28"><a href="#cb188-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mask out self-similarity (diagonal)</span></span>
<span id="cb188-29"><a href="#cb188-29" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.eye(<span class="dv">2</span> <span class="op">*</span> batch_size, dtype<span class="op">=</span><span class="bu">bool</span>).to(z.device)</span>
<span id="cb188-30"><a href="#cb188-30" aria-hidden="true" tabindex="-1"></a>    sim_matrix.masked_fill_(mask, <span class="op">-</span><span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>))</span>
<span id="cb188-31"><a href="#cb188-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb188-32"><a href="#cb188-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-entropy loss</span></span>
<span id="cb188-33"><a href="#cb188-33" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(sim_matrix, labels)</span>
<span id="cb188-34"><a href="#cb188-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss</span></code></pre></div>
            <h3 id="why-does-contrastive-learning-work">Why Does
            Contrastive Learning Work?</h3>
            <ol type="1">
            <li><strong>Invariance</strong>: By augmenting the same
            image, we teach the model that certain transformations don‚Äôt
            change identity</li>
            <li><strong>Discrimination</strong>: By pushing different
            images apart, we learn distinctive features</li>
            <li><strong>No labels needed</strong>: Self-supervision from
            data structure itself</li>
            </ol>
            <h3 id="clip-contrastive-language-image-pretraining">CLIP:
            Contrastive Language-Image Pretraining</h3>
            <p><strong>Extend contrastive learning to images and
            text</strong>:</p>
            <pre><code>Image ‚Üí [Image Encoder] ‚Üí Image embedding ‚îÄ‚îê
                                            ‚îú‚îÄ‚Üí Contrastive Loss
Text  ‚Üí [Text Encoder]  ‚Üí Text embedding  ‚îÄ‚îò

Positive pairs: (image, its caption)
Negative pairs: (image, other captions) and (caption, other images)</code></pre>
            <p><strong>Loss</strong>: Symmetric InfoNCE across
            image-text pairs</p>
            <p><span class="math display">\[\mathcal{L} =
            \frac{1}{2}(\mathcal{L}_{\text{image‚Üítext}} +
            \mathcal{L}_{\text{text‚Üíimage}})\]</span></p>
            <h3 id="comparison-contrastive-vs-generative">Comparison:
            Contrastive vs Generative</h3>
            <table>
            <colgroup>
            <col style="width: 14%" />
            <col style="width: 38%" />
            <col style="width: 47%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Contrastive (SimCLR)</th>
            <th>Generative (Autoencoder)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Objective</strong></td>
            <td>Pull/push pairs</td>
            <td>Reconstruct input</td>
            </tr>
            <tr>
            <td><strong>Output</strong></td>
            <td>Embedding</td>
            <td>Reconstruction</td>
            </tr>
            <tr>
            <td><strong>Negatives</strong></td>
            <td>Required</td>
            <td>Not needed</td>
            </tr>
            <tr>
            <td><strong>What‚Äôs learned</strong></td>
            <td>Discriminative features</td>
            <td>Everything (may be wasteful)</td>
            </tr>
            <tr>
            <td><strong>Downstream</strong></td>
            <td>Classification, retrieval</td>
            <td>Generation, denoising</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-what-is-contrastive-learning-and-why-is-it-effective">Interview
            Q: ‚ÄúWhat is contrastive learning and why is it
            effective?‚Äù</h3>
            <p><strong>A</strong>: Contrastive learning learns
            representations by pulling similar pairs close and pushing
            dissimilar pairs apart in embedding space. In
            self-supervised settings (SimCLR, CLIP), positive pairs come
            from augmentations of the same image, and negatives from
            different images.</p>
            <p>The key loss is InfoNCE: <span
            class="math inline">\(-\log \frac{\exp(\text{sim}(x,
            x^+)/\tau)}{\sum \exp(\text{sim}(x, x_i)/\tau)}\)</span> ‚Äî
            essentially softmax cross-entropy where the positive should
            have highest similarity.</p>
            <p>It‚Äôs effective because: (1) it learns
            augmentation-invariant features without labels, (2) it‚Äôs
            scalable ‚Äî more negatives = better representations, (3) it
            focuses on discriminative features unlike reconstruction
            which captures everything. CLIP extends this to image-text
            pairs, enabling powerful zero-shot classification by
            comparing images to text descriptions.</p>
            <hr />
            <h2 id="decision-trees-ensemble-methods">4.10 Decision Trees
            &amp; Ensemble Methods</h2>
            <h3 id="what-this-means-for-beginners-7">What This Means
            (For Beginners)</h3>
            <p><strong>Decision Tree = A Game of 20
            Questions</strong></p>
            <p>Imagine you‚Äôre playing a guessing game: - ‚ÄúIs it bigger
            than a breadbox?‚Äù ‚Üí Yes/No - ‚ÄúIs it alive?‚Äù ‚Üí Yes/No - ‚ÄúCan
            you eat it?‚Äù ‚Üí Yes/No</p>
            <p>A decision tree works exactly like this! It asks a series
            of yes/no questions about features until it reaches an
            answer.</p>
            <pre><code>                    Is income &gt; $50K?
                    /              \
                  Yes               No
                  /                   \
           Age &gt; 35?              Has degree?
           /      \                /       \
         Yes      No             Yes        No
          |        |              |          |
      Approve   Review        Review      Reject</code></pre>
            <p><strong>Why trees are intuitive</strong>: You can explain
            the decision! ‚ÄúWe rejected because income &lt; $50K AND no
            degree.‚Äù Try explaining a neural network‚Äôs decision‚Ä¶</p>
            <h3 id="decision-trees">4.10.1 Decision Trees</h3>
            <h4 id="how-a-decision-tree-works">How a Decision Tree
            Works</h4>
            <p>A decision tree is a hierarchical structure where: -
            <strong>Internal nodes</strong>: Ask questions about
            features (splits) - <strong>Branches</strong>: Represent
            answers (usually binary: yes/no) - <strong>Leaf
            nodes</strong>: Make predictions (class label or value)</p>
            <pre><code>                    Feature X‚ÇÅ ‚â§ 5?
                    /            \
                  Yes             No
                  /                 \
          Feature X‚ÇÇ ‚â§ 3?      Feature X‚ÇÉ ‚â§ 7?
          /          \          /          \
       Class A    Class B   Class B    Class A</code></pre>
            <h4 id="how-splits-are-chosen-information-gain">How Splits
            Are Chosen: Information Gain</h4>
            <p>The key question: <strong>Which feature should we split
            on?</strong></p>
            <p>We want splits that create <strong>pure</strong> groups
            (all same class). We measure ‚Äúimpurity‚Äù using:</p>
            <p><strong>Entropy</strong> (from information theory):</p>
            <p><span class="math display">\[H(S) = -\sum_{c \in
            \text{classes}} p_c \log_2(p_c)\]</span></p>
            <ul>
            <li><span class="math inline">\(H = 0\)</span>: Pure (all
            same class) ‚Äî perfect!</li>
            <li><span class="math inline">\(H = 1\)</span>: Maximum
            impurity (50/50 split for binary)</li>
            </ul>
            <p><strong>Information Gain</strong> = Entropy before split
            - Weighted entropy after split:</p>
            <p><span class="math display">\[\text{IG}(S, A) = H(S) -
            \sum_{v \in \text{values}(A)} \frac{|S_v|}{|S|}
            H(S_v)\]</span></p>
            <p><strong>Choose the feature with highest information
            gain!</strong></p>
            <h4 id="worked-example-building-a-tree">Worked Example:
            Building a Tree</h4>
            <p>Dataset: Should we play tennis?</p>
            <table>
            <thead>
            <tr>
            <th>Outlook</th>
            <th>Temp</th>
            <th>Humidity</th>
            <th>Windy</th>
            <th>Play?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Sunny</td>
            <td>Hot</td>
            <td>High</td>
            <td>No</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Sunny</td>
            <td>Hot</td>
            <td>High</td>
            <td>Yes</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Overcast</td>
            <td>Hot</td>
            <td>High</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Rain</td>
            <td>Mild</td>
            <td>High</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Rain</td>
            <td>Cool</td>
            <td>Normal</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Rain</td>
            <td>Cool</td>
            <td>Normal</td>
            <td>Yes</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Overcast</td>
            <td>Cool</td>
            <td>Normal</td>
            <td>Yes</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Sunny</td>
            <td>Mild</td>
            <td>High</td>
            <td>No</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Sunny</td>
            <td>Cool</td>
            <td>Normal</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            <tr>
            <td>Rain</td>
            <td>Mild</td>
            <td>Normal</td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Step 1: Calculate base entropy</strong></p>
            <p>9 examples: 5 Yes, 4 No ‚Üí <span class="math inline">\(H =
            -\frac{5}{9}\log_2\frac{5}{9} - \frac{4}{9}\log_2\frac{4}{9}
            \approx 0.99\)</span></p>
            <p><strong>Step 2: Calculate information gain for each
            feature</strong></p>
            <p>For ‚ÄúOutlook‚Äù: - Sunny (3): 1 Yes, 2 No ‚Üí <span
            class="math inline">\(H = 0.92\)</span> - Overcast (2): 2
            Yes, 0 No ‚Üí <span class="math inline">\(H = 0\)</span>
            (pure!) - Rain (4): 3 Yes, 1 No ‚Üí <span
            class="math inline">\(H = 0.81\)</span></p>
            <p><span class="math display">\[\text{IG(Outlook)} = 0.99 -
            \frac{3}{9}(0.92) - \frac{2}{9}(0) - \frac{4}{9}(0.81) =
            0.25\]</span></p>
            <p>(Similarly calculate for other features and pick highest
            IG)</p>
            <p><strong>Step 3: Recursively split</strong> until stopping
            criteria met.</p>
            <h4 id="gini-impurity-alternative-to-entropy">Gini Impurity
            (Alternative to Entropy)</h4>
            <p><span class="math display">\[\text{Gini}(S) = 1 -
            \sum_{c} p_c^2\]</span></p>
            <ul>
            <li>Gini = 0: Pure (perfect)</li>
            <li>Gini = 0.5: Maximum impurity (50/50 for binary)</li>
            </ul>
            <p><strong>Gini vs Entropy</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Gini</th>
            <th>Entropy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Computation</td>
            <td>Faster (no log)</td>
            <td>Slower</td>
            </tr>
            <tr>
            <td>Behavior</td>
            <td>Tends to isolate frequent class</td>
            <td>More balanced</td>
            </tr>
            <tr>
            <td>Default</td>
            <td>sklearn default</td>
            <td>ID3 algorithm</td>
            </tr>
            </tbody>
            </table>
            <p>In practice, they give similar results.</p>
            <h4 id="for-regression-variance-reduction">For Regression:
            Variance Reduction</h4>
            <p>Instead of entropy/Gini, minimize variance in each
            split:</p>
            <p><span class="math display">\[\text{Reduction} =
            \text{Var}(S) - \sum_v \frac{|S_v|}{|S|}
            \text{Var}(S_v)\]</span></p>
            <p>The prediction at each leaf is the <strong>mean</strong>
            of training values in that region.</p>
            <h4 id="overfitting-and-pruning">Overfitting and
            Pruning</h4>
            <p><strong>Problem</strong>: Trees can grow until every leaf
            has one sample ‚Äî perfect training accuracy, terrible
            generalization!</p>
            <p><strong>Solutions</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Technique</th>
            <th>How It Works</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Max depth</strong></td>
            <td>Limit tree depth (e.g., max_depth=5)</td>
            </tr>
            <tr>
            <td><strong>Min samples split</strong></td>
            <td>Need at least N samples to split</td>
            </tr>
            <tr>
            <td><strong>Min samples leaf</strong></td>
            <td>Each leaf must have at least N samples</td>
            </tr>
            <tr>
            <td><strong>Post-pruning</strong></td>
            <td>Grow full tree, then prune based on validation</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb192"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Without regularization (will overfit)</span></span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a>tree_overfit <span class="op">=</span> DecisionTreeClassifier()</span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a><span class="co"># With regularization</span></span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>tree_regular <span class="op">=</span> DecisionTreeClassifier(</span>
<span id="cb192-8"><a href="#cb192-8" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb192-9"><a href="#cb192-9" aria-hidden="true" tabindex="-1"></a>    min_samples_split<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb192-10"><a href="#cb192-10" aria-hidden="true" tabindex="-1"></a>    min_samples_leaf<span class="op">=</span><span class="dv">5</span></span>
<span id="cb192-11"><a href="#cb192-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
            <h4
            id="interview-q-how-do-decision-trees-choose-splits">Interview
            Q: ‚ÄúHow do decision trees choose splits?‚Äù</h4>
            <p><strong>A</strong>: Decision trees choose splits by
            maximizing information gain (or Gini gain). For each
            feature, we calculate how much splitting on it reduces
            impurity. Information gain = parent entropy - weighted
            average of child entropies. We pick the feature with highest
            gain, then recurse. For regression trees, we minimize
            variance instead of entropy. The greedy approach (best split
            at each step) doesn‚Äôt guarantee globally optimal tree, but
            works well in practice.</p>
            <h4
            id="interview-q-whats-the-difference-between-gini-and-entropy">Interview
            Q: ‚ÄúWhat‚Äôs the difference between Gini and Entropy?‚Äù</h4>
            <p><strong>A</strong>: Both measure impurity ‚Äî how mixed the
            classes are in a node. Entropy uses logarithms: <span
            class="math inline">\(H = -\sum p_i \log p_i\)</span>,
            ranging from 0 (pure) to 1 (for binary). Gini uses squared
            probabilities: <span class="math inline">\(G = 1 - \sum
            p_i^2\)</span>, ranging from 0 to 0.5 (for binary). Gini is
            computationally faster (no log), and tends to isolate the
            most frequent class in its own branch. In practice, they
            give very similar results ‚Äî sklearn uses Gini by
            default.</p>
            <h4
            id="interview-q-why-are-decision-trees-prone-to-overfitting">Interview
            Q: ‚ÄúWhy are decision trees prone to overfitting?‚Äù</h4>
            <p><strong>A</strong>: A fully-grown decision tree can
            create a leaf for every training example, achieving 0
            training error by memorizing the data. This happens because:
            (1) trees have high variance ‚Äî small changes in data can
            completely change the tree structure, (2) they can learn
            arbitrary decision boundaries without penalty, (3) no
            built-in regularization. Solutions include limiting depth,
            requiring minimum samples per leaf, pruning, or using
            ensembles (Random Forest) that average many trees to reduce
            variance.</p>
            <hr />
            <h3 id="ensemble-methods-bagging-vs-boosting">4.10.2
            Ensemble Methods: Bagging vs Boosting</h3>
            <h4 id="the-big-picture">The Big Picture</h4>
            <p><strong>Ensemble methods</strong> combine multiple weak
            learners into a strong learner.</p>
            <pre><code>                 ‚îå‚îÄ‚îÄ‚îÄ Model 1 ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ               ‚îÇ
Input ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îú‚îÄ‚îÄ‚îÄ Model 2 ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí Combine ‚îÄ‚îÄ‚Üí Final Prediction
                 ‚îÇ               ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ Model 3 ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Why ensembles work</strong>: Different models
            make different errors. By combining them, errors cancel
            out!</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 33%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Bagging</th>
            <th>Boosting</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Strategy</strong></td>
            <td>Parallel, average predictions</td>
            <td>Sequential, fix mistakes</td>
            </tr>
            <tr>
            <td><strong>Reduces</strong></td>
            <td>Variance</td>
            <td>Bias (then variance)</td>
            </tr>
            <tr>
            <td><strong>Trees</strong></td>
            <td>Independent</td>
            <td>Dependent on previous</td>
            </tr>
            <tr>
            <td><strong>Training</strong></td>
            <td>Can parallelize</td>
            <td>Must be sequential</td>
            </tr>
            <tr>
            <td><strong>Example</strong></td>
            <td>Random Forest</td>
            <td>XGBoost, AdaBoost</td>
            </tr>
            </tbody>
            </table>
            <h4 id="bagging-bootstrap-aggregating">Bagging (Bootstrap
            Aggregating)</h4>
            <p><strong>Idea</strong>: Train many models on different
            random subsets of data, then average.</p>
            <pre><code>Original Data: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
                              ‚Üì
              Bootstrap Sample 1: [1, 1, 3, 4, 4, 6, 7, 9, 9, 10]  ‚Üí Model 1
              Bootstrap Sample 2: [2, 2, 3, 5, 6, 7, 8, 9, 10, 10] ‚Üí Model 2
              Bootstrap Sample 3: [1, 3, 4, 5, 5, 6, 8, 8, 9, 10]  ‚Üí Model 3
                              ‚Üì
                    Average predictions</code></pre>
            <p><strong>Why it reduces variance</strong>:</p>
            <p>If each model has variance <span
            class="math inline">\(\sigma^2\)</span>, the average of
            <span class="math inline">\(n\)</span>
            <strong>independent</strong> models has variance:</p>
            <p><span class="math display">\[\text{Var}(\bar{X}) =
            \frac{\sigma^2}{n}\]</span></p>
            <p>More models ‚Üí lower variance!</p>
            <p><strong>But there‚Äôs a catch</strong>: Bootstrap samples
            are correlated (drawn from same data), so variance reduction
            isn‚Äôt as dramatic as <span
            class="math inline">\(\sigma^2/n\)</span>. Random Forest
            addresses this with additional randomization.</p>
            <h4 id="boosting">Boosting</h4>
            <p><strong>Idea</strong>: Train models sequentially, each
            focusing on the mistakes of previous ones.</p>
            <pre><code>Step 1: Train Model 1 on original data
        ‚Üí Some examples misclassified

Step 2: Train Model 2, upweight misclassified examples
        ‚Üí Focus on hard cases

Step 3: Train Model 3, upweight remaining errors
        ...

Final: Weighted combination of all models</code></pre>
            <p><strong>Key intuition</strong>: Each model ‚Äúcorrects‚Äù the
            errors of the previous ensemble.</p>
            <h4 id="adaboost-adaptive-boosting">AdaBoost (Adaptive
            Boosting)</h4>
            <p><strong>Algorithm</strong>:</p>
            <ol type="1">
            <li>Initialize equal weights: <span
            class="math inline">\(w_i = 1/n\)</span></li>
            <li>For each round <span class="math inline">\(t\)</span>:
            <ul>
            <li>Train weak learner <span
            class="math inline">\(h_t\)</span> on weighted data</li>
            <li>Compute error: <span class="math inline">\(\epsilon_t =
            \sum_{\text{wrong}} w_i\)</span></li>
            <li>Compute model weight: <span
            class="math inline">\(\alpha_t =
            \frac{1}{2}\ln\frac{1-\epsilon_t}{\epsilon_t}\)</span></li>
            <li>Update weights: <span class="math inline">\(w_i
            \leftarrow w_i \cdot \exp(\pm\alpha_t)\)</span> (+ if wrong,
            - if right)</li>
            </ul></li>
            <li>Final: <span class="math inline">\(H(x) =
            \text{sign}(\sum_t \alpha_t h_t(x))\)</span></li>
            </ol>
            <p><strong>Key insight</strong>: Models with lower error get
            higher weight <span
            class="math inline">\(\alpha_t\)</span>.</p>
            <h4 id="gradient-boosting">Gradient Boosting</h4>
            <p><strong>Core idea</strong>: Instead of reweighting
            samples, fit the <strong>residuals</strong> (errors).</p>
            <pre><code>Model 1: Predict y
         Residual = y - pred_1

Model 2: Predict residual_1
         Residual = y - (pred_1 + pred_2)

Model 3: Predict residual_2
         ...

Final: pred_1 + pred_2 + pred_3 + ...</code></pre>
            <p><strong>Mathematical view</strong>: We‚Äôre doing gradient
            descent in <strong>function space</strong>!</p>
            <p><span class="math display">\[F_m(x) = F_{m-1}(x) + \eta
            \cdot h_m(x)\]</span></p>
            <p>where <span class="math inline">\(h_m\)</span> is trained
            to predict the negative gradient of the loss.</p>
            <p>For squared error loss: negative gradient = residual =
            <span class="math inline">\(y - F_{m-1}(x)\)</span></p>
            <p><strong>Interview Q</strong>: ‚ÄúHow does gradient boosting
            differ from AdaBoost?‚Äù</p>
            <p><strong>A</strong>: AdaBoost reweights training samples ‚Äî
            misclassified examples get higher weight in subsequent
            rounds, and the final prediction is a weighted vote.
            Gradient boosting fits residuals ‚Äî each new model predicts
            the error of the current ensemble, and predictions are
            summed (not voted). Gradient boosting is more general: it
            works with any differentiable loss function by fitting the
            negative gradient, while AdaBoost is specific to exponential
            loss. Both reduce bias by iteratively correcting errors, but
            gradient boosting‚Äôs residual fitting is more flexible.</p>
            <hr />
            <h3 id="random-forest">4.10.3 Random Forest</h3>
            <h4 id="what-is-random-forest">What is Random Forest?</h4>
            <p>Random Forest = <strong>Bagging</strong> +
            <strong>Feature Randomness</strong></p>
            <pre><code>                     Bootstrap     Random Feature     Decision
                      Sample        Subset at         Tree
                                   Each Split
Original Data ‚îÄ‚îÄ‚Üí Sample 1 ‚îÄ‚îÄ‚Üí ‚àö Features 1,3,7 ‚îÄ‚îÄ‚Üí Tree 1
              ‚îÄ‚îÄ‚Üí Sample 2 ‚îÄ‚îÄ‚Üí ‚àö Features 2,4,5 ‚îÄ‚îÄ‚Üí Tree 2
              ‚îÄ‚îÄ‚Üí Sample 3 ‚îÄ‚îÄ‚Üí ‚àö Features 1,5,8 ‚îÄ‚îÄ‚Üí Tree 3
                              ...
                                    ‚Üì
                         Average / Majority Vote</code></pre>
            <p><strong>Key innovation</strong>: At each split, only
            consider a <strong>random subset</strong> of features.</p>
            <ul>
            <li>Classification: typically <span
            class="math inline">\(\sqrt{d}\)</span> features</li>
            <li>Regression: typically <span
            class="math inline">\(d/3\)</span> features</li>
            </ul>
            <h4 id="why-feature-randomness-helps">Why Feature Randomness
            Helps</h4>
            <p>Without feature randomness, all trees would split on the
            best feature first ‚Üí <strong>correlated trees</strong>!</p>
            <p>Variance of average of <span
            class="math inline">\(n\)</span> correlated variables: <span
            class="math display">\[\text{Var}(\bar{X}) =
            \frac{\sigma^2}{n} + \frac{n-1}{n}\rho\sigma^2\]</span></p>
            <p>where <span class="math inline">\(\rho\)</span> is
            correlation between trees.</p>
            <p>By randomizing features, we <strong>decorrelate</strong>
            trees ‚Üí more variance reduction!</p>
            <h4 id="out-of-bag-oob-error">Out-of-Bag (OOB) Error</h4>
            <p><strong>Free validation!</strong> Each tree is trained on
            ~63% of data (bootstrap). The other ~37% (out-of-bag
            samples) can be used for validation.</p>
            <div class="sourceCode" id="cb198"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb198-1"><a href="#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb198-2"><a href="#cb198-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb198-3"><a href="#cb198-3" aria-hidden="true" tabindex="-1"></a>rf <span class="op">=</span> RandomForestClassifier(</span>
<span id="cb198-4"><a href="#cb198-4" aria-hidden="true" tabindex="-1"></a>    n_estimators<span class="op">=</span><span class="dv">100</span>,      <span class="co"># Number of trees</span></span>
<span id="cb198-5"><a href="#cb198-5" aria-hidden="true" tabindex="-1"></a>    max_features<span class="op">=</span><span class="st">&#39;sqrt&#39;</span>,   <span class="co"># Features per split</span></span>
<span id="cb198-6"><a href="#cb198-6" aria-hidden="true" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="va">None</span>,        <span class="co"># Grow full trees</span></span>
<span id="cb198-7"><a href="#cb198-7" aria-hidden="true" tabindex="-1"></a>    oob_score<span class="op">=</span><span class="va">True</span>         <span class="co"># Compute OOB error</span></span>
<span id="cb198-8"><a href="#cb198-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb198-9"><a href="#cb198-9" aria-hidden="true" tabindex="-1"></a>rf.fit(X_train, y_train)</span>
<span id="cb198-10"><a href="#cb198-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;OOB Score: </span><span class="sc">{</span>rf<span class="sc">.</span>oob_score_<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># Free validation!</span></span></code></pre></div>
            <h4 id="random-forest-hyperparameters">Random Forest
            Hyperparameters</h4>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 22%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>Parameter</th>
            <th>Effect</th>
            <th>Typical Values</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>n_estimators</code></td>
            <td>More trees ‚Üí better (diminishing returns)</td>
            <td>100-1000</td>
            </tr>
            <tr>
            <td><code>max_features</code></td>
            <td>Lower ‚Üí less correlated trees, higher bias</td>
            <td>sqrt(d) for classification</td>
            </tr>
            <tr>
            <td><code>max_depth</code></td>
            <td>None (full trees) usually best</td>
            <td>None or 10-30</td>
            </tr>
            <tr>
            <td><code>min_samples_leaf</code></td>
            <td>Regularization</td>
            <td>1-5</td>
            </tr>
            </tbody>
            </table>
            <h4
            id="interview-q-why-does-random-forest-use-random-feature-subsets">Interview
            Q: ‚ÄúWhy does Random Forest use random feature subsets?‚Äù</h4>
            <p><strong>A</strong>: To decorrelate the trees. Without
            feature randomness, if one feature is much stronger than
            others, every tree would split on it first, making all trees
            nearly identical ‚Äî you‚Äôd get N copies of the same tree. By
            considering only a random subset of features at each split,
            different trees learn different aspects of the data. This
            decorrelation is crucial: the variance of an average
            decreases more when the components are uncorrelated. Random
            Forest‚Äôs effectiveness comes from combining many diverse
            trees, not many copies of the same tree.</p>
            <h4
            id="interview-q-what-is-oob-error-and-why-is-it-useful">Interview
            Q: ‚ÄúWhat is OOB error and why is it useful?‚Äù</h4>
            <p><strong>A</strong>: Out-of-Bag error is a validation
            estimate that comes ‚Äúfor free‚Äù with bagging. Each tree is
            trained on a bootstrap sample (~63% of data). For each
            training example, we can get predictions from the ~37% of
            trees that didn‚Äôt see it. Aggregating these gives an OOB
            prediction for every example. OOB error is the error rate on
            these predictions. It‚Äôs useful because: (1) no need for a
            separate validation set ‚Äî all data can train, (2) unbiased
            estimate ‚Äî each prediction is from trees that never saw that
            example, (3) cheap to compute. For most cases, OOB error
            closely matches cross-validation error.</p>
            <hr />
            <h3 id="gradient-boosting-xgboostlightgbm">4.10.4 Gradient
            Boosting (XGBoost/LightGBM)</h3>
            <h4 id="xgboost-extreme-gradient-boosting">XGBoost: eXtreme
            Gradient Boosting</h4>
            <p>XGBoost is gradient boosting with several innovations
            that made it dominate Kaggle competitions:</p>
            <table>
            <colgroup>
            <col style="width: 46%" />
            <col style="width: 53%" />
            </colgroup>
            <thead>
            <tr>
            <th>Innovation</th>
            <th>What It Does</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Regularization</strong></td>
            <td>Adds L1/L2 penalty to leaf weights</td>
            </tr>
            <tr>
            <td><strong>Second-order gradients</strong></td>
            <td>Uses Newton‚Äôs method (Hessian) for faster
            convergence</td>
            </tr>
            <tr>
            <td><strong>Approximate splits</strong></td>
            <td>Weighted quantile sketch for efficiency</td>
            </tr>
            <tr>
            <td><strong>Sparsity-aware</strong></td>
            <td>Native handling of missing values</td>
            </tr>
            <tr>
            <td><strong>Column subsampling</strong></td>
            <td>Like Random Forest, adds randomness</td>
            </tr>
            <tr>
            <td><strong>Cache-aware</strong></td>
            <td>Optimized memory access patterns</td>
            </tr>
            </tbody>
            </table>
            <h4 id="xgboost-objective">XGBoost Objective</h4>
            <p><span class="math display">\[\mathcal{L} = \sum_{i}
            l(y_i, \hat{y}_i) + \sum_{k} \Omega(f_k)\]</span></p>
            <p>where: - <span class="math inline">\(l\)</span> is any
            differentiable loss (MSE, logistic, etc.) - <span
            class="math inline">\(\Omega(f) = \gamma T +
            \frac{1}{2}\lambda\|w\|^2\)</span> penalizes complexity -
            <span class="math inline">\(T\)</span> = number of leaves -
            <span class="math inline">\(w\)</span> = leaf weights -
            <span class="math inline">\(\gamma\)</span> = cost per leaf
            (pruning parameter) - <span
            class="math inline">\(\lambda\)</span> = L2 regularization
            on weights</p>
            <h4 id="how-xgboost-handles-missing-values">How XGBoost
            Handles Missing Values</h4>
            <p>During tree construction, XGBoost learns the
            <strong>optimal default direction</strong> for missing
            values:</p>
            <pre><code>              Feature X‚ÇÅ ‚â§ 5?
              /            \
            Yes             No
           (and Missing)    </code></pre>
            <p>For each split, XGBoost tries both directions for missing
            values and picks whichever gives lower loss. This means: -
            No need for imputation - Missing values are used as
            information!</p>
            <h4 id="lightgbm-faster-gradient-boosting">LightGBM: Faster
            Gradient Boosting</h4>
            <p>Key differences from XGBoost:</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 33%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>XGBoost</th>
            <th>LightGBM</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Tree growth</strong></td>
            <td>Level-wise (breadth-first)</td>
            <td>Leaf-wise (best-first)</td>
            </tr>
            <tr>
            <td><strong>Split finding</strong></td>
            <td>Pre-sorted or histogram</td>
            <td>Histogram-based</td>
            </tr>
            <tr>
            <td><strong>Speed</strong></td>
            <td>Baseline</td>
            <td>10-20x faster</td>
            </tr>
            <tr>
            <td><strong>Memory</strong></td>
            <td>Higher</td>
            <td>Lower</td>
            </tr>
            <tr>
            <td><strong>Risk</strong></td>
            <td>More balanced trees</td>
            <td>May overfit if not careful</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Leaf-wise growth</strong>: Instead of growing all
            nodes at depth <span class="math inline">\(d\)</span> before
            depth <span class="math inline">\(d+1\)</span>, LightGBM
            always splits the leaf with highest gain. This can lead to
            deeper, more unbalanced trees but often better accuracy.</p>
            <pre><code>Level-wise (XGBoost):          Leaf-wise (LightGBM):
      [Split]                        [Split]
     /       \                      /       \
  [Split]   [Split]              [Leaf]   [Split]
  /   \     /    \                        /      \
 ...  ... ...   ...                   [Split]  [Leaf]
                                      /     \
                                   [...]   [...]</code></pre>
            <h4 id="key-hyperparameters">Key Hyperparameters</h4>
            <div class="sourceCode" id="cb201"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb201-2"><a href="#cb201-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb201-3"><a href="#cb201-3" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> {</span>
<span id="cb201-4"><a href="#cb201-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;learning_rate&#39;</span>: <span class="fl">0.1</span>,      <span class="co"># Step size shrinkage (lower = more robust)</span></span>
<span id="cb201-5"><a href="#cb201-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;n_estimators&#39;</span>: <span class="dv">100</span>,       <span class="co"># Number of boosting rounds</span></span>
<span id="cb201-6"><a href="#cb201-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;max_depth&#39;</span>: <span class="dv">6</span>,            <span class="co"># Maximum tree depth</span></span>
<span id="cb201-7"><a href="#cb201-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;min_child_weight&#39;</span>: <span class="dv">1</span>,     <span class="co"># Minimum sum of weights in child</span></span>
<span id="cb201-8"><a href="#cb201-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;subsample&#39;</span>: <span class="fl">0.8</span>,          <span class="co"># Row subsampling ratio</span></span>
<span id="cb201-9"><a href="#cb201-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;colsample_bytree&#39;</span>: <span class="fl">0.8</span>,   <span class="co"># Column subsampling ratio</span></span>
<span id="cb201-10"><a href="#cb201-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;reg_alpha&#39;</span>: <span class="dv">0</span>,            <span class="co"># L1 regularization</span></span>
<span id="cb201-11"><a href="#cb201-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;reg_lambda&#39;</span>: <span class="dv">1</span>,           <span class="co"># L2 regularization</span></span>
<span id="cb201-12"><a href="#cb201-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
            <p><strong>Most important</strong>:
            <code>learning_rate</code> and <code>n_estimators</code>
            have a tradeoff ‚Äî lower learning rate needs more estimators
            but often gives better results.</p>
            <h4
            id="interview-q-how-does-xgboost-handle-missing-values">Interview
            Q: ‚ÄúHow does XGBoost handle missing values?‚Äù</h4>
            <p><strong>A</strong>: XGBoost learns the optimal default
            direction for missing values during training. At each split,
            it tries sending missing values left AND right, measuring
            the gain each way, and picks the better direction. This
            default direction is stored with the tree. At prediction
            time, if a value is missing, it goes the learned default
            direction. This is powerful because: (1) no imputation
            needed, (2) the model can learn that ‚Äúmissing‚Äù is actually
            informative (e.g., missing income might indicate
            unemployment), (3) it works automatically without feature
            engineering.</p>
            <h4
            id="interview-q-whats-the-difference-between-xgboost-and-lightgbm">Interview
            Q: ‚ÄúWhat‚Äôs the difference between XGBoost and
            LightGBM?‚Äù</h4>
            <p><strong>A</strong>: Both are gradient boosting
            implementations, but LightGBM is optimized for speed. Key
            differences: (1) <strong>Tree growth</strong>: XGBoost grows
            level-wise (all nodes at depth d before d+1), LightGBM grows
            leaf-wise (always split best leaf). Leaf-wise often produces
            more accurate but potentially overfitting trees. (2)
            <strong>Split finding</strong>: LightGBM uses
            histogram-based binning which is faster and uses less
            memory. (3) <strong>Categorical features</strong>: LightGBM
            handles them natively without one-hot encoding. LightGBM is
            typically 10-20x faster but XGBoost is more battle-tested.
            For most tabular problems, both work well.</p>
            <hr />
            <h3 id="trees-vs-neural-networks-when-to-use-what">4.10.5
            Trees vs Neural Networks: When to Use What</h3>
            <h4 id="decision-guide">Decision Guide</h4>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 46%" />
            <col style="width: 17%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Best Choice</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Tabular data, &lt; 10K samples</strong></td>
            <td>XGBoost/Random Forest</td>
            <td>Trees dominate small tabular</td>
            </tr>
            <tr>
            <td><strong>Tabular data, &gt; 100K samples</strong></td>
            <td>Still trees, but NNs competitive</td>
            <td>Trees are hard to beat</td>
            </tr>
            <tr>
            <td><strong>Images</strong></td>
            <td>Neural Networks (CNN)</td>
            <td>Spatial structure needs convolutions</td>
            </tr>
            <tr>
            <td><strong>Text</strong></td>
            <td>Neural Networks (Transformer)</td>
            <td>Sequential structure, embeddings</td>
            </tr>
            <tr>
            <td><strong>Audio</strong></td>
            <td>Neural Networks</td>
            <td>Spectral/temporal patterns</td>
            </tr>
            <tr>
            <td><strong>Interpretability critical</strong></td>
            <td>Decision Tree / Random Forest</td>
            <td>Feature importance, explainability</td>
            </tr>
            <tr>
            <td><strong>Deployment on edge</strong></td>
            <td>XGBoost</td>
            <td>Smaller models, no GPU needed</td>
            </tr>
            <tr>
            <td><strong>Mixed modalities</strong></td>
            <td>Neural Networks</td>
            <td>Can combine image + text + tabular</td>
            </tr>
            </tbody>
            </table>
            <h4 id="why-trees-dominate-tabular-data">Why Trees Dominate
            Tabular Data</h4>
            <p>Several hypotheses:</p>
            <ol type="1">
            <li><strong>Axis-aligned splits</strong>: Tabular features
            are often independent; trees naturally learn axis-aligned
            boundaries that match feature importance</li>
            <li><strong>Handling heterogeneous data</strong>: Trees
            naturally handle mixed types (categorical + continuous)
            without normalization</li>
            <li><strong>Missing values</strong>: XGBoost handles missing
            values natively and often beneficially</li>
            <li><strong>Feature interactions</strong>: Trees find
            interactions (splits after splits) more easily than NNs on
            small data</li>
            <li><strong>Robustness</strong>: Less sensitive to
            hyperparameters, outliers, feature scaling</li>
            </ol>
            <h4 id="when-neural-networks-win-on-tabular">When Neural
            Networks Win on Tabular</h4>
            <ul>
            <li>Very large datasets (millions of rows)</li>
            <li>When representation learning helps (embeddings for
            high-cardinality categoricals)</li>
            <li>Multi-task learning (share representations across
            tasks)</li>
            <li>End-to-end learning with other modalities (tabular +
            images)</li>
            </ul>
            <h4 id="comparison-table-2">Comparison Table</h4>
            <table>
            <colgroup>
            <col style="width: 19%" />
            <col style="width: 40%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Trees (XGBoost)</th>
            <th>Neural Networks</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Data requirements</strong></td>
            <td>Works with thousands</td>
            <td>Needs millions for best</td>
            </tr>
            <tr>
            <td><strong>Feature engineering</strong></td>
            <td>Less needed</td>
            <td>More needed (or learn it)</td>
            </tr>
            <tr>
            <td><strong>Training time</strong></td>
            <td>Minutes</td>
            <td>Hours/Days</td>
            </tr>
            <tr>
            <td><strong>Inference</strong></td>
            <td>Fast, CPU</td>
            <td>Can be slow, GPU</td>
            </tr>
            <tr>
            <td><strong>Interpretability</strong></td>
            <td>Feature importance</td>
            <td>Black box</td>
            </tr>
            <tr>
            <td><strong>Handling missing</strong></td>
            <td>Native</td>
            <td>Need imputation</td>
            </tr>
            <tr>
            <td><strong>Scaling/Normalization</strong></td>
            <td>Not needed</td>
            <td>Critical</td>
            </tr>
            <tr>
            <td><strong>Structured data</strong></td>
            <td>Images, audio</td>
            <td>Tabular, time series</td>
            </tr>
            </tbody>
            </table>
            <h4
            id="interview-q-when-would-you-use-random-forest-vs-a-neural-network">Interview
            Q: ‚ÄúWhen would you use Random Forest vs a Neural
            Network?‚Äù</h4>
            <p><strong>A</strong>: I‚Äôd use Random Forest (or XGBoost)
            for: (1) tabular/structured data, especially with &lt; 100K
            samples, (2) when interpretability matters (feature
            importance), (3) limited compute budget, (4) heterogeneous
            features (mixed categorical/continuous). I‚Äôd use Neural
            Networks for: (1) images, text, audio (structured
            spatial/temporal data), (2) very large datasets where
            representation learning helps, (3) multi-modal problems
            (combine text + images), (4) when I need end-to-end
            differentiability. In my experience, for most Kaggle-style
            tabular problems, gradient boosting (XGBoost/LightGBM) is
            the default winning choice.</p>
            <h4
            id="interview-q-why-do-tree-based-methods-often-beat-nns-on-tabular-data">Interview
            Q: ‚ÄúWhy do tree-based methods often beat NNs on tabular
            data?‚Äù</h4>
            <p><strong>A</strong>: Several factors: (1)
            <strong>Axis-aligned splits</strong> match how tabular
            features typically matter ‚Äî each feature contributes
            somewhat independently, which trees capture naturally. (2)
            <strong>Robustness</strong> ‚Äî trees don‚Äôt need careful
            feature scaling, learning rate tuning, or batch
            normalization. (3) <strong>Native handling of missing values
            and categoricals</strong> ‚Äî XGBoost learns optimal paths for
            missing data, while NNs need preprocessing. (4)
            <strong>Efficiency on small data</strong> ‚Äî trees can find
            complex interactions with thousands of examples; NNs need
            orders of magnitude more. (5) <strong>Ensemble variance
            reduction</strong> ‚Äî Random Forest and boosting reduce
            variance effectively. Recent work (TabNet, FT-Transformer)
            has made NNs more competitive on tabular, but trees remain
            the default choice.</p>
            <hr />
            <h1 id="part-5-optimization">Part 5: Optimization</h1>
            <p>Optimization is the engine that powers machine learning.
            Given a model architecture and a loss function, optimization
            algorithms search through the vast space of possible
            parameter values to find those that minimize the loss. This
            search is what transforms a randomly initialized neural
            network into a useful model.</p>
            <p>The fundamental challenge is that we cannot simply
            ‚Äúsolve‚Äù for the optimal parameters ‚Äî the loss landscape is
            complex, high-dimensional, and non-convex. Instead, we use
            <strong>iterative</strong> methods that start from some
            initial point and repeatedly take steps that (hopefully)
            decrease the loss.</p>
            <h2 id="the-core-update-rule-1">5.1 The Core Update
            Rule</h2>
            <p>At the heart of all gradient-based optimization is a
            simple idea: if we know which direction increases the loss
            (the gradient), we should step in the
            <strong>opposite</strong> direction to decrease it. This
            leads to the fundamental update rule:</p>
            <p><span class="math display">\[w_{t+1} = w_t - \alpha
            \nabla \ell(w_t)\]</span></p>
            <p>The negative sign is crucial ‚Äî the gradient <span
            class="math inline">\(\nabla \ell(w_t)\)</span> points
            toward steepest <em>increase</em>, so we move opposite to
            it. The learning rate <span
            class="math inline">\(\alpha\)</span> controls how big a
            step we take.</p>
            <table>
            <thead>
            <tr>
            <th>Expression</th>
            <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(w_t\)</span></td>
            <td>Current parameters</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\ell(w_t)\)</span></td>
            <td>Loss at current parameters</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\nabla
            \ell(w_t)\)</span></td>
            <td>Gradient (direction of steepest increase)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(-\alpha \nabla
            \ell(w_t)\)</span></td>
            <td>Step toward lower loss</td>
            </tr>
            <tr>
            <td><span class="math inline">\(w_{t+1}\)</span></td>
            <td>Updated parameters</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="stochastic-gradient-descent-sgd-1">5.2 Stochastic
            Gradient Descent (SGD)</h2>
            <h3 id="full-batch-vs-mini-batch">Full-Batch vs
            Mini-Batch</h3>
            <p>The ‚Äútrue‚Äù gradient is the average over the entire
            dataset:</p>
            <p><span class="math display">\[\nabla \ell(w) = \frac{1}{N}
            \sum_{i=1}^{N} \nabla \ell_i(w)\]</span></p>
            <p>For modern datasets with millions or billions of
            examples, computing this exactly is prohibitively expensive
            ‚Äî we‚Äôd need to process the entire dataset just to take one
            step! Instead, we approximate the gradient using a small
            random subset called a <strong>mini-batch</strong> <span
            class="math inline">\(B\)</span>:</p>
            <p><span class="math display">\[g_t = \frac{1}{|B|} \sum_{i
            \in B} \nabla \ell_i(w_t)\]</span></p>
            <p><strong>Key property</strong>: <span
            class="math inline">\(\mathbb{E}[g_t] = \nabla
            \ell(w_t)\)</span> ‚Äî unbiased estimator!</p>
            <p>This is the key insight of SGD: the mini-batch gradient
            is ‚Äúcorrect on average.‚Äù While any single estimate may be
            noisy, we‚Äôre not systematically wrong in any direction. Over
            many steps, the noise averages out and we make progress
            toward the optimum.</p>
            <h3 id="variance-batch-size-tradeoff">Variance-Batch Size
            Tradeoff</h3>
            <p>The choice of batch size creates a fundamental tradeoff
            between the quality of each gradient estimate and
            computational efficiency:</p>
            <table>
            <thead>
            <tr>
            <th>Batch Size</th>
            <th>Variance</th>
            <th>Updates/Epoch</th>
            <th>Compute/Update</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Small (32)</td>
            <td>High</td>
            <td>Many</td>
            <td>Fast</td>
            </tr>
            <tr>
            <td>Large (4096)</td>
            <td>Low</td>
            <td>Few</td>
            <td>Slow</td>
            </tr>
            </tbody>
            </table>
            <p>Small batches give noisy estimates but let us update more
            frequently per epoch. Large batches give cleaner estimates
            but fewer updates. In practice, there‚Äôs often a ‚Äúsweet spot‚Äù
            (typically 32-512) that balances these factors. Very large
            batches can also hurt generalization, a phenomenon known as
            the ‚Äúgeneralization gap.‚Äù</p>
            <h3 id="why-noise-can-help">Why Noise Can Help</h3>
            <p>SGD noise provides <strong>implicit
            regularization</strong>:</p>
            <ul>
            <li>Escapes sharp minima</li>
            <li>Converges to flatter minima that generalize better</li>
            </ul>
            <hr />
            <h2 id="momentum-methods-1">5.3 Momentum Methods</h2>
            <h3 id="the-problem-oscillations">The Problem:
            Oscillations</h3>
            <p>Imagine a loss landscape shaped like a long, narrow
            valley ‚Äî steep walls on either side, but a gentle slope
            along the valley floor toward the minimum. This is called an
            <strong>ill-conditioned</strong> landscape, and it‚Äôs common
            in neural networks.</p>
            <p>Plain SGD struggles here: the steep gradient across the
            valley causes large oscillations side-to-side, while the
            gentle gradient along the valley makes progress slow. The
            algorithm zig-zags back and forth, wasting most of its
            effort fighting the walls rather than descending.</p>
            <h3 id="classical-momentum-1">Classical Momentum</h3>
            <p>The solution is borrowed from physics: give the optimizer
            <strong>momentum</strong>, like a heavy ball rolling
            downhill. The ball accumulates velocity as it rolls ‚Äî
            oscillations cancel out (left pushes cancel right pushes),
            while consistent motion builds up (downhill pushes
            accumulate).</p>
            <p>Mathematically, we maintain a <strong>velocity</strong>
            <span class="math inline">\(v_t\)</span> that accumulates
            gradient information:</p>
            <p><span class="math display">\[v_{t+1} = \beta v_t + \nabla
            \ell(w_t)\]</span></p>
            <p><span class="math display">\[w_{t+1} = w_t - \alpha
            v_{t+1}\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Meaning</th>
            <th>Typical Value</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(v_t\)</span></td>
            <td>Velocity</td>
            <td>-</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta\)</span></td>
            <td>Momentum coefficient</td>
            <td><strong>0.9</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Effective learning rate</strong> in consistent
            direction: <span class="math inline">\(\frac{\alpha}{1 -
            \beta}\)</span></p>
            <p>For <span class="math inline">\(\beta = 0.9\)</span>:
            effective LR is <strong>10√ó larger</strong>!</p>
            <h3 id="nesterov-accelerated-gradient-nag-1">Nesterov
            Accelerated Gradient (NAG)</h3>
            <p>Standard momentum has a flaw: it computes the gradient at
            the current position, then applies momentum. But we
            <em>know</em> we‚Äôre about to move in the momentum direction
            ‚Äî why not compute the gradient where we‚Äôll actually end
            up?</p>
            <p>Nesterov momentum ‚Äúlooks ahead‚Äù by computing the gradient
            at the anticipated next position:</p>
            <p><span class="math display">\[v_{t+1} = \beta v_t + \nabla
            \ell(w_t - \alpha \beta v_t)\]</span></p>
            <p><span class="math display">\[w_{t+1} = w_t - \alpha
            v_{t+1}\]</span></p>
            <p>This provides a ‚Äúcorrection‚Äù ‚Äî if momentum is carrying us
            too far, the gradient at the look-ahead position will point
            back, damping the overshoot. Nesterov converges faster than
            standard momentum, especially near the optimum.</p>
            <div class="sourceCode" id="cb202"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch</span></span>
<span id="cb202-2"><a href="#cb202-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(params, lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, nesterov<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
            <hr />
            <h2 id="adaptive-learning-rate-methods-1">5.4 Adaptive
            Learning Rate Methods</h2>
            <p>A single global learning rate treats all parameters
            equally ‚Äî but not all parameters are equal. Some parameters
            (like biases or those in later layers) may need larger
            updates, while others (like those in earlier layers or with
            sparse gradients) may need smaller updates. <strong>Adaptive
            methods</strong> maintain per-parameter learning rates that
            automatically adjust based on gradient history.</p>
            <h3 id="adam-2015">Adam (2015)</h3>
            <p>Adam (‚ÄúAdaptive Moment Estimation‚Äù) is the workhorse of
            deep learning optimization. It combines two ideas:
            <strong>momentum</strong> (using exponentially weighted
            average of past gradients) and <strong>RMSprop</strong>
            (scaling learning rate by gradient magnitude):</p>
            <p><span class="math display">\[m_{t+1} = \beta_1 m_t +
            (1-\beta_1) g_t \quad \text{(momentum)}\]</span></p>
            <p><span class="math display">\[v_{t+1} = \beta_2 v_t +
            (1-\beta_2) g_t^2 \quad \text{(RMSprop)}\]</span></p>
            <p><span class="math display">\[\hat{m}_{t+1} = m_{t+1} / (1
            - \beta_1^{t+1}) \quad \text{(bias correction)}\]</span></p>
            <p><span class="math display">\[\hat{v}_{t+1} = v_{t+1} / (1
            - \beta_2^{t+1}) \quad \text{(bias correction)}\]</span></p>
            <p><span class="math display">\[w_{t+1} = w_t - \alpha
            \frac{\hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} +
            \epsilon}\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Parameter</th>
            <th>Default</th>
            <th>Purpose</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\beta_1\)</span></td>
            <td>0.9</td>
            <td>Momentum decay</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\beta_2\)</span></td>
            <td>0.999</td>
            <td>Squared gradient decay</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\epsilon\)</span></td>
            <td><span class="math inline">\(10^{-8}\)</span></td>
            <td>Numerical stability</td>
            </tr>
            </tbody>
            </table>
            <h3 id="adamw-the-interview-question">AdamW: The Interview
            Question!</h3>
            <p><strong>Problem with Adam + L2</strong>: <span
            class="math display">\[g_{\text{reg}} = g + \lambda
            w\]</span></p>
            <p>Adam divides by <span
            class="math inline">\(\sqrt{v}\)</span>, which
            <strong>scales down regularization</strong>!</p>
            <p><strong>AdamW Solution</strong>: Decouple weight decay:
            <span class="math display">\[w_{t+1} = w_t - \alpha
            \frac{\hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} + \epsilon} -
            \alpha \lambda w_t\]</span></p>
            <p><strong>AdamW is the default for Transformers and
            LLMs.</strong></p>
            <h3
            id="interview-q-whats-the-difference-between-adam-and-adamw">Interview
            Q: ‚ÄúWhat‚Äôs the difference between Adam and AdamW?‚Äù</h3>
            <p><strong>A</strong>: In Adam with L2 regularization, the
            weight decay term goes through the adaptive learning rate
            scaling, which weakens regularization for parameters with
            large gradient variance. AdamW decouples weight decay from
            the gradient-based update, applying it directly to weights.
            This gives proper regularization regardless of gradient
            history and is essential for training Transformers.</p>
            <hr />
            <h2 id="learning-rate-schedules-1">5.5 Learning Rate
            Schedules</h2>
            <p>The learning rate is perhaps the most important
            hyperparameter. Too large and training diverges or
            oscillates wildly; too small and training takes forever or
            gets stuck. Rather than using a fixed learning rate, modern
            practice uses <strong>schedules</strong> that vary the
            learning rate during training.</p>
            <h3 id="warmup-critical-for-transformers">Warmup: Critical
            for Transformers</h3>
            <p>Adam and other adaptive optimizers estimate gradient
            statistics (first and second moments) from recent gradients.
            At the start of training, these estimates are initialized to
            zero and based on very few samples ‚Äî they‚Äôre essentially
            meaningless. Taking large steps based on unreliable
            statistics leads to <strong>unstable early
            training</strong>.</p>
            <p><strong>Linear warmup</strong> solves this by starting
            with a tiny learning rate and gradually increasing it,
            giving the moment estimates time to become reliable:</p>
            <p><strong>Linear Warmup</strong>: <span
            class="math display">\[\alpha_t = \alpha_{\max} \cdot
            \frac{t}{T_{\text{warmup}}}\]</span></p>
            <h3 id="cosine-annealing-1">Cosine Annealing</h3>
            <p>After warmup, we typically want to <strong>decay</strong>
            the learning rate ‚Äî take smaller steps as we get closer to
            the optimum, allowing fine-grained refinement without
            overshooting. Cosine annealing provides a smooth, gradual
            decay:</p>
            <p><span class="math display">\[\alpha_t = \alpha_{\min} +
            \frac{1}{2}(\alpha_{\max} - \alpha_{\min})\left(1 +
            \cos\left(\frac{\pi t}{T}\right)\right)\]</span></p>
            <p>Unlike step decay (which has discontinuous jumps), cosine
            decay is smooth, which empirically helps optimization and
            leads to better final performance on Transformers.</p>
            <p><img src="figures/lr_schedules.png"
            alt="Learning Rate Schedules" /> <em>Figure: Comparison of
            different learning rate schedules: Step Decay, Cosine
            Annealing, Linear Warmup + Decay, and Exponential
            Decay.</em></p>
            <h3 id="comparison-1">Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Schedule</th>
            <th>Best For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Step decay</strong></td>
            <td>CNNs, classic vision</td>
            </tr>
            <tr>
            <td><strong>Cosine</strong></td>
            <td>Transformers, LLMs</td>
            </tr>
            <tr>
            <td><strong>Linear decay</strong></td>
            <td>Fine-tuning</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="regularization-l1-and-l2-1">5.6 Regularization: L1
            and L2</h2>
            <p>A model that fits the training data perfectly may perform
            terribly on new data ‚Äî it has <strong>overfit</strong> by
            memorizing noise rather than learning generalizable
            patterns. Regularization combats this by adding a penalty
            that discourages overly complex models.</p>
            <p>The key insight is that large weights often indicate
            overfitting: the model is contorting itself to fit
            idiosyncratic training examples. By penalizing large
            weights, we encourage simpler models that are more likely to
            generalize.</p>
            <h3 id="l2-regularization-ridge-weight-decay-1">L2
            Regularization (Ridge / Weight Decay)</h3>
            <p><span class="math display">\[\text{Loss} =
            L_{\text{data}} + \frac{\lambda}{2} \sum_j
            w_j^2\]</span></p>
            <p><span class="math display">\[\frac{\partial
            \text{Loss}}{\partial w_j} = \frac{\partial L}{\partial w_j}
            + \lambda w_j\]</span></p>
            <p><strong>Effect</strong>: Shrinks weights toward zero
            proportionally to their magnitude. The gradient <span
            class="math inline">\(\lambda w_j\)</span> is larger for
            large weights, so they get penalized more. However, L2 never
            drives weights to exactly zero ‚Äî it keeps all features, just
            with smaller coefficients.</p>
            <h3 id="l1-regularization-lasso-2">L1 Regularization
            (Lasso)</h3>
            <p><span class="math display">\[\text{Loss} =
            L_{\text{data}} + \lambda \sum_j |w_j|\]</span></p>
            <p><strong>Effect</strong>: Pushes weights to exactly zero ‚Üí
            <strong>sparsity</strong> and <strong>feature
            selection</strong></p>
            <p>The geometry explains why: L2‚Äôs penalty is a sphere (all
            directions penalized equally), while L1‚Äôs penalty is a
            diamond. Solutions tend to occur at corners of the
            constraint region, and the diamond‚Äôs corners lie on the axes
            ‚Äî corresponding to some weights being exactly zero. This
            makes L1 particularly useful when you suspect many features
            are irrelevant.</p>
            <h3 id="comparison-2">Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Property</th>
            <th>L1</th>
            <th>L2</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Sparsity</td>
            <td>Yes (exact zeros)</td>
            <td>No</td>
            </tr>
            <tr>
            <td>Feature selection</td>
            <td>Automatic</td>
            <td>No</td>
            </tr>
            <tr>
            <td>When to use</td>
            <td>Many features, few relevant</td>
            <td>All features useful</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="gradient-instability">5.7 Gradient Instability</h2>
            <h3 id="vanishing-gradients-2">Vanishing Gradients</h3>
            <p><strong>Causes</strong>:</p>
            <ul>
            <li>Sigmoid/tanh saturation</li>
            <li>Deep networks with small weights</li>
            </ul>
            <p><strong>Solutions</strong>:</p>
            <ul>
            <li>ReLU activation</li>
            <li>Proper initialization (He, Xavier)</li>
            <li>Residual connections</li>
            <li>Batch/Layer normalization</li>
            </ul>
            <h3
            id="vanishing-gradients-the-mathematical-story-interview-deep-dive">Vanishing
            Gradients: The Mathematical Story (Interview Deep Dive)</h3>
            <p>Understanding <strong>why</strong> gradients vanish
            requires understanding the chain rule through multiple
            layers. This is a common interview question framed as: ‚ÄúWhy
            do vanishing gradients happen? Explain mathematically.‚Äù</p>
            <p><strong>The Setup: Gradient Through a Deep
            Network</strong></p>
            <p>Consider a simple deep network with <span
            class="math inline">\(L\)</span> layers: <span
            class="math display">\[h_l = \sigma(W_l
            h_{l-1})\]</span></p>
            <p>where <span class="math inline">\(\sigma\)</span> is an
            activation function (e.g., sigmoid, tanh) and <span
            class="math inline">\(h_0 = x\)</span> (input).</p>
            <p>To compute <span class="math inline">\(\frac{\partial
            L}{\partial W_1}\)</span> (gradient for the first layer), we
            apply the chain rule:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            h_1} = \frac{\partial L}{\partial h_L} \cdot \frac{\partial
            h_L}{\partial h_{L-1}} \cdot \frac{\partial
            h_{L-1}}{\partial h_{L-2}} \cdots \frac{\partial
            h_2}{\partial h_1}\]</span></p>
            <p>Each term <span class="math inline">\(\frac{\partial
            h_l}{\partial h_{l-1}}\)</span> is a <strong>Jacobian
            matrix</strong>.</p>
            <p><strong>The Jacobian of One Layer</strong></p>
            <p>For <span class="math inline">\(h_l = \sigma(W_l
            h_{l-1})\)</span>:</p>
            <p><span class="math display">\[\frac{\partial h_l}{\partial
            h_{l-1}} = \text{diag}(\sigma&#39;(z_l)) \cdot
            W_l\]</span></p>
            <p>where <span class="math inline">\(z_l = W_l
            h_{l-1}\)</span> (pre-activation) and <span
            class="math inline">\(\sigma&#39;\)</span> is the derivative
            of the activation.</p>
            <p><strong>The Product of Jacobians</strong></p>
            <p>The gradient through <span
            class="math inline">\(L\)</span> layers becomes: <span
            class="math display">\[\frac{\partial L}{\partial h_1} =
            \frac{\partial L}{\partial h_L} \cdot \prod_{l=2}^{L}
            \left[\text{diag}(\sigma&#39;(z_l)) \cdot
            W_l\right]\]</span></p>
            <p>This is a <strong>product of <span
            class="math inline">\(L-1\)</span> matrices</strong>. The
            problem:</p>
            <table>
            <colgroup>
            <col style="width: 22%" />
            <col style="width: 33%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>If</th>
            <th>Then</th>
            <th>Result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\|\text{diag}(\sigma&#39;)
            \cdot W\| &lt; 1\)</span></td>
            <td>Product shrinks exponentially</td>
            <td><strong>Vanishing gradients</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\|\text{diag}(\sigma&#39;)
            \cdot W\| &gt; 1\)</span></td>
            <td>Product grows exponentially</td>
            <td><strong>Exploding gradients</strong></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why Sigmoid/Tanh Cause Vanishing</strong></p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 30%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th>Activation</th>
            <th>Derivative</th>
            <th>Maximum Value</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Sigmoid: <span class="math inline">\(\sigma(x) =
            \frac{1}{1+e^{-x}}\)</span></td>
            <td><span
            class="math inline">\(\sigma(x)(1-\sigma(x))\)</span></td>
            <td><strong>0.25</strong> (at <span
            class="math inline">\(x=0\)</span>)</td>
            </tr>
            <tr>
            <td>Tanh: <span class="math inline">\(\tanh(x)\)</span></td>
            <td><span class="math inline">\(1 - \tanh^2(x)\)</span></td>
            <td><strong>1.0</strong> (at <span
            class="math inline">\(x=0\)</span>)</td>
            </tr>
            </tbody>
            </table>
            <p>For sigmoid: Even at the <em>best</em> case, each layer
            multiplies the gradient by at most 0.25. Through 10 layers:
            <span class="math inline">\(0.25^{10} \approx
            10^{-6}\)</span>. The gradient effectively disappears!</p>
            <p>For tanh: Better (max derivative = 1), but saturates for
            large <span class="math inline">\(|x|\)</span>, where <span
            class="math inline">\(\tanh&#39;(x) \to 0\)</span>.</p>
            <p><strong>Concrete Example</strong></p>
            <p>With sigmoid activation and <span class="math inline">\(L
            = 10\)</span> layers: - Best case (all activations at 0):
            gradient scaled by <span class="math inline">\(0.25^{10}
            \approx 10^{-6}\)</span> - Typical case: gradient scaled by
            <span class="math inline">\(\approx 10^{-10}\)</span> or
            smaller</p>
            <p>Early layers receive essentially zero gradient signal ‚Äî
            they don‚Äôt learn!</p>
            <p><strong>Why ReLU Helps</strong></p>
            <p><span class="math display">\[\text{ReLU}&#39;(x) =
            \begin{cases} 1 &amp; x &gt; 0 \\ 0 &amp; x \leq 0
            \end{cases}\]</span></p>
            <p>For positive activations, the gradient passes through
            unchanged (multiplied by 1). No exponential decay! The
            problem shifts to ‚Äúdead neurons‚Äù where <span
            class="math inline">\(x \leq 0\)</span> always, but this is
            less catastrophic than vanishing gradients everywhere.</p>
            <p><strong>Why Residual Connections Help</strong></p>
            <p>With residual connections: <span
            class="math inline">\(h_{l+1} = h_l + F(h_l,
            W_l)\)</span></p>
            <p>The gradient becomes: <span
            class="math display">\[\frac{\partial h_{l+1}}{\partial h_l}
            = I + \frac{\partial F}{\partial h_l}\]</span></p>
            <p>Even if <span class="math inline">\(\frac{\partial
            F}{\partial h_l} \approx 0\)</span>, the gradient still
            flows through the identity <span
            class="math inline">\(I\)</span>. The gradient has a
            ‚Äúhighway‚Äù that bypasses the vanishing problem.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhy do vanishing gradients
            happen? Explain mathematically.‚Äù</p>
            <p><strong>A</strong>: Vanishing gradients occur because
            backpropagation computes gradients through the chain rule,
            which involves multiplying Jacobians from each layer. For a
            layer <span class="math inline">\(h_l = \sigma(W_l
            h_{l-1})\)</span>, the Jacobian is <span
            class="math inline">\(\frac{\partial h_l}{\partial h_{l-1}}
            = \text{diag}(\sigma&#39;(z_l)) \cdot W_l\)</span>.</p>
            <p>The gradient for early layers is a <strong>product of L-1
            such Jacobians</strong>. If the spectral norm of each
            Jacobian is less than 1 ‚Äî which happens when using sigmoid
            (max derivative 0.25) or tanh in saturation ‚Äî the product
            decays exponentially. For a 10-layer network with sigmoid,
            the gradient to layer 1 is scaled by roughly <span
            class="math inline">\(0.25^{10} \approx 10^{-6}\)</span>,
            meaning early layers receive essentially zero learning
            signal.</p>
            <p><strong>Solutions</strong>: ReLU (derivative = 1 for
            positive inputs, no exponential decay), residual connections
            (identity path bypasses the vanishing Jacobians), proper
            initialization (Xavier/He to keep Jacobian norms ‚âà 1), and
            LayerNorm (prevents activations from saturating).</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúWhy do residual
            connections help with vanishing gradients?‚Äù</p>
            <p><strong>A</strong>: With <span
            class="math inline">\(h_{l+1} = h_l + F(h_l)\)</span>, the
            Jacobian becomes <span class="math inline">\(\frac{\partial
            h_{l+1}}{\partial h_l} = I + \frac{\partial F}{\partial
            h_l}\)</span>. Even if <span
            class="math inline">\(\frac{\partial F}{\partial h_l}
            \approx 0\)</span> (the ‚Äúlearning part‚Äù vanishes), the
            gradient still flows through the identity matrix <span
            class="math inline">\(I\)</span>. This creates a ‚Äúgradient
            highway‚Äù ‚Äî the gradient can skip layers entirely rather than
            being forced through every transformation. This is why
            ResNets can train 100+ layer networks where plain networks
            fail after 20-30 layers.</p>
            <h3 id="exploding-gradients-2">Exploding Gradients</h3>
            <p><strong>Causes</strong>:</p>
            <ul>
            <li>Large weights</li>
            <li>RNNs multiplying same matrix</li>
            </ul>
            <p><strong>Solutions</strong>:</p>
            <ul>
            <li>Gradient clipping</li>
            <li>Proper initialization</li>
            <li>LSTM/GRU gates</li>
            <li>Lower learning rate</li>
            </ul>
            <h3
            id="exploding-gradients-the-mathematical-story-interview-deep-dive">Exploding
            Gradients: The Mathematical Story (Interview Deep Dive)</h3>
            <p>Exploding gradients are the <strong>mirror
            problem</strong> of vanishing gradients ‚Äî same math,
            opposite direction.</p>
            <p><strong>Same Jacobian Product, Different
            Regime</strong></p>
            <p>Recall the gradient through <span
            class="math inline">\(L\)</span> layers: <span
            class="math display">\[\frac{\partial L}{\partial h_1} =
            \frac{\partial L}{\partial h_L} \cdot \prod_{l=2}^{L}
            \left[\text{diag}(\sigma&#39;(z_l)) \cdot
            W_l\right]\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Spectral Norm</th>
            <th>Effect</th>
            <th>Problem</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\|J_l\| &lt; 1\)</span></td>
            <td>Shrinks</td>
            <td><strong>Vanishing</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">\(\|J_l\| = 1\)</span></td>
            <td>Stable</td>
            <td>‚úì Ideal</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\|J_l\| &gt; 1\)</span></td>
            <td>Grows</td>
            <td><strong>Exploding</strong></td>
            </tr>
            </tbody>
            </table>
            <p>If <span class="math inline">\(\|J_l\| = 1.5\)</span> for
            each layer, after 10 layers: <span
            class="math inline">\(1.5^{10} \approx 57\)</span>. After 50
            layers: <span class="math inline">\(1.5^{50} \approx 6
            \times 10^{8}\)</span>!</p>
            <p><strong>Why RNNs Are Particularly Vulnerable</strong></p>
            <p>In feedforward networks, each layer has different weights
            <span class="math inline">\(W_1, W_2, \ldots,
            W_L\)</span>.</p>
            <p>In RNNs, the <strong>same weight matrix <span
            class="math inline">\(W_{hh}\)</span></strong> is multiplied
            at every time step:</p>
            <p><span class="math display">\[\frac{\partial h_T}{\partial
            h_1} = \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}
            = \prod_{t=2}^{T} \text{diag}(\sigma&#39;(z_t)) \cdot
            W_{hh}\]</span></p>
            <p>If the largest eigenvalue of <span
            class="math inline">\(W_{hh}\)</span> is <span
            class="math inline">\(\lambda_{\max} &gt; 1\)</span>, then:
            <span class="math display">\[\|(\text{diag}(\sigma&#39;)
            \cdot W_{hh})^T\| \approx \lambda_{\max}^T\]</span></p>
            <p>For a sequence of length 100 with <span
            class="math inline">\(\lambda_{\max} = 1.1\)</span>: <span
            class="math inline">\(1.1^{100} \approx
            13{,}780\)</span>!</p>
            <p><strong>How to Detect Exploding Gradients</strong></p>
            <table>
            <thead>
            <tr>
            <th>Symptom</th>
            <th>What You‚Äôll See</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Loss becomes NaN</strong></td>
            <td><code>loss: nan</code> after a few iterations</td>
            </tr>
            <tr>
            <td><strong>Loss jumps wildly</strong></td>
            <td>2.5 ‚Üí 847 ‚Üí 0.001 ‚Üí inf</td>
            </tr>
            <tr>
            <td><strong>Weights become NaN</strong></td>
            <td>Model parameters contain <code>nan</code> or
            <code>inf</code></td>
            </tr>
            <tr>
            <td><strong>Gradient norm spikes</strong></td>
            <td>Monitoring shows sudden 1000√ó increase</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb203"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monitoring gradient norms during training</span></span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a>total_norm <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb203-3"><a href="#cb203-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb203-4"><a href="#cb203-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb203-5"><a href="#cb203-5" aria-hidden="true" tabindex="-1"></a>        param_norm <span class="op">=</span> p.grad.data.norm(<span class="dv">2</span>)</span>
<span id="cb203-6"><a href="#cb203-6" aria-hidden="true" tabindex="-1"></a>        total_norm <span class="op">+=</span> param_norm.item() <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb203-7"><a href="#cb203-7" aria-hidden="true" tabindex="-1"></a>total_norm <span class="op">=</span> total_norm <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb203-8"><a href="#cb203-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Gradient norm: </span><span class="sc">{</span>total_norm<span class="sc">}</span><span class="ss">&quot;</span>)  <span class="co"># Should be stable, not exploding</span></span></code></pre></div>
            <p><strong>Why Gradient Clipping Works</strong></p>
            <p>Gradient clipping bounds the gradient magnitude without
            changing direction:</p>
            <p><span class="math display">\[g_{\text{clipped}} =
            \begin{cases}
            g &amp; \text{if } \|g\| \leq \tau \\
            \tau \cdot \frac{g}{\|g\|} &amp; \text{if } \|g\| &gt; \tau
            \end{cases}\]</span></p>
            <p>This is <strong>clipping by norm</strong> ‚Äî preserves
            gradient direction, just limits step size.</p>
            <p><strong>Clipping by Norm vs Clipping by
            Value</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 36%" />
            <col style="width: 32%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Formula</th>
            <th>Effect</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>By norm</strong> (preferred)</td>
            <td>Scale whole gradient if <span
            class="math inline">\(\|g\| &gt; \tau\)</span></td>
            <td>Preserves direction</td>
            </tr>
            <tr>
            <td><strong>By value</strong></td>
            <td>Clip each <span class="math inline">\(g_i\)</span> to
            <span class="math inline">\([-\tau, \tau]\)</span></td>
            <td>Distorts direction</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb204"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clipping by norm (recommended)</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Clipping by value (less common)</span></span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_value_(model.parameters(), clip_value<span class="op">=</span><span class="fl">1.0</span>)</span></code></pre></div>
            <p><strong>Choosing the Clipping Threshold <span
            class="math inline">\(\tau\)</span></strong>:</p>
            <ul>
            <li><strong>Too small</strong>: Slows learning (gradients
            always clipped)</li>
            <li><strong>Too large</strong>: Doesn‚Äôt prevent
            explosions</li>
            <li><strong>Typical values</strong>: 1.0 to 5.0 (depends on
            model)</li>
            <li><strong>Strategy</strong>: Monitor gradient norms, set
            <span class="math inline">\(\tau\)</span> just above typical
            values</li>
            </ul>
            <p><strong>Why Proper Initialization Prevents
            Explosions</strong></p>
            <p><strong>Xavier/Glorot</strong> (for tanh/sigmoid): <span
            class="math display">\[W \sim \mathcal{N}\left(0,
            \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)\]</span></p>
            <p><strong>He/Kaiming</strong> (for ReLU): <span
            class="math display">\[W \sim \mathcal{N}\left(0,
            \frac{2}{n_{\text{in}}}\right)\]</span></p>
            <p>These keep <span class="math inline">\(\text{Var}(h_l)
            \approx \text{Var}(h_{l-1})\)</span>, preventing both
            vanishing and exploding activations.</p>
            <p><strong>Interview Q</strong>: ‚ÄúWhat causes exploding
            gradients and how do you fix them?‚Äù</p>
            <p><strong>A</strong>: Exploding gradients occur when the
            product of Jacobians through the network has spectral norm
            &gt; 1, causing gradients to grow exponentially with depth.
            This is especially problematic in RNNs where the same weight
            matrix is multiplied at every time step ‚Äî if its largest
            eigenvalue exceeds 1, gradients explode as <span
            class="math inline">\(\lambda^T\)</span> for sequence length
            <span class="math inline">\(T\)</span>.</p>
            <p><strong>Detection</strong>: Loss becomes NaN, wild loss
            oscillations, or gradient norm spikes.</p>
            <p><strong>Solutions</strong>: 1. <strong>Gradient clipping
            by norm</strong> ‚Äî the primary defense; clip to <span
            class="math inline">\(\|g\| \leq \tau\)</span> (typically
            1.0-5.0) while preserving direction 2. <strong>Proper
            initialization</strong> ‚Äî Xavier/He initialization keeps
            layer-wise variance stable 3. <strong>LSTM/GRU
            gates</strong> ‚Äî learnable gates can ‚Äúclose‚Äù to block
            exploding gradients (forget gate ‚Üí 0) 4. <strong>Lower
            learning rate</strong> ‚Äî limits the damage from any single
            large gradient</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúWhy clip by norm rather
            than by value?‚Äù</p>
            <p><strong>A</strong>: Clipping by norm preserves the
            <strong>direction</strong> of the gradient ‚Äî all components
            are scaled equally. Clipping by value treats each component
            independently, which can drastically change the gradient
            direction. For example, if the gradient is
            <code>[100, 1]</code> and we clip by value to 10, we get
            <code>[10, 1]</code> ‚Äî the direction changed from
            mostly-first-dimension to roughly equal. Clipping by norm
            gives <code>[10, 0.1]</code> ‚Äî same direction, just smaller
            magnitude. Preserving direction is important because the
            gradient direction is informative about how to decrease the
            loss.</p>
            <h3 id="gradient-clipping-2">Gradient Clipping</h3>
            <p><span class="math display">\[g \leftarrow \min\left(1,
            \frac{\tau}{\|g\|}\right) \cdot g\]</span></p>
            <div class="sourceCode" id="cb205"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span></code></pre></div>
            <h3 id="residual-connections">Residual Connections</h3>
            <p><span class="math display">\[h_{l+1} = h_l + F(h_l,
            W_l)\]</span></p>
            <p>Even if <span class="math inline">\(\frac{\partial
            F}{\partial h_l} \approx 0\)</span>, gradient flows through
            the identity path!</p>
            <hr />
            <h2 id="batch-normalization-1">5.8 Batch Normalization</h2>
            <h3 id="the-problem-5">The Problem</h3>
            <p>During training, the distribution of inputs to each layer
            changes as parameters update. This <strong>internal
            covariate shift</strong> makes training unstable and
            requires careful initialization and low learning rates.</p>
            <h3 id="the-solution-normalize-each-layer-1">The Solution:
            Normalize Each Layer</h3>
            <p>For a mini-batch of activations <span
            class="math inline">\(\{x_1, \ldots, x_m\}\)</span>:</p>
            <p><strong>Step 1: Compute batch statistics</strong> <span
            class="math display">\[\mu_B = \frac{1}{m}\sum_{i=1}^m x_i,
            \quad \sigma_B^2 = \frac{1}{m}\sum_{i=1}^m (x_i -
            \mu_B)^2\]</span></p>
            <p><strong>Step 2: Normalize</strong> <span
            class="math display">\[\hat{x}_i = \frac{x_i -
            \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]</span></p>
            <p><strong>Step 3: Scale and shift (learnable)</strong>
            <span class="math display">\[y_i = \gamma \hat{x}_i +
            \beta\]</span></p>
            <p>The learnable parameters <span
            class="math inline">\(\gamma\)</span> and <span
            class="math inline">\(\beta\)</span> allow the network to
            <strong>undo</strong> the normalization if needed!</p>
            <h3 id="where-to-apply">Where to Apply?</h3>
            <p><strong>Before activation</strong> (original paper):</p>
            <pre><code>Linear ‚Üí BatchNorm ‚Üí ReLU</code></pre>
            <p><strong>After activation</strong> (sometimes used):</p>
            <pre><code>Linear ‚Üí ReLU ‚Üí BatchNorm</code></pre>
            <h3 id="training-vs-inference-2">Training vs Inference</h3>
            <table>
            <thead>
            <tr>
            <th>Phase</th>
            <th><span class="math inline">\(\mu\)</span>, <span
            class="math inline">\(\sigma^2\)</span></th>
            <th>Behavior</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Training</strong></td>
            <td>Batch statistics</td>
            <td>Different each batch</td>
            </tr>
            <tr>
            <td><strong>Inference</strong></td>
            <td>Running average</td>
            <td>Fixed (deterministic)</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb208"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch tracks running statistics automatically</span></span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>bn <span class="op">=</span> nn.BatchNorm1d(num_features)</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Training: uses batch stats, updates running stats</span></span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> bn(x)</span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Inference: uses stored running stats</span></span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb208-10"><a href="#cb208-10" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> bn(x)</span></code></pre></div>
            <h3 id="benefits-1">Benefits</h3>
            <p>Batch normalization provides several interrelated
            benefits:</p>
            <ol type="1">
            <li><p><strong>Allows higher learning rates</strong>:
            Because activations are normalized, gradient magnitudes are
            more consistent across layers. Without BatchNorm, large
            activations in one layer can cause exploding gradients;
            BatchNorm prevents this, enabling more aggressive learning
            rates.</p></li>
            <li><p><strong>Reduces sensitivity to
            initialization</strong>: Poor initialization can cause
            activations to be too large or too small. BatchNorm rescales
            them to a standard range, making the network robust to
            initialization choices.</p></li>
            <li><p><strong>Regularization effect</strong>: Each
            mini-batch has different examples, so the batch statistics
            are slightly different each time. This injects noise into
            the activations, similar to dropout, providing implicit
            regularization.</p></li>
            <li><p><strong>Speeds up convergence</strong>: By
            normalizing activations, BatchNorm creates a smoother loss
            landscape with fewer sharp cliffs and valleys. Smoother
            landscapes are easier to optimize.</p></li>
            </ol>
            <h3 id="batch-norm-vs-layer-norm">Batch Norm vs Layer
            Norm</h3>
            <table>
            <thead>
            <tr>
            <th></th>
            <th>Batch Norm</th>
            <th>Layer Norm</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Normalizes over</strong></td>
            <td>Batch dimension</td>
            <td>Feature dimension</td>
            </tr>
            <tr>
            <td><strong>Best for</strong></td>
            <td>CNNs, large batches</td>
            <td>RNNs, Transformers</td>
            </tr>
            <tr>
            <td><strong>Batch dependency</strong></td>
            <td>Yes (problematic for small batches)</td>
            <td>No</td>
            </tr>
            <tr>
            <td><strong>Sequence data</strong></td>
            <td>Awkward</td>
            <td>Natural</td>
            </tr>
            </tbody>
            </table>
            <p><img src="figures/batch_layer_norm.png"
            alt="Batch Norm vs Layer Norm" /> <em>Figure: Batch
            Normalization vs Layer Normalization. BatchNorm (left)
            computes mean and variance across the batch dimension for
            each feature (highlighted column). LayerNorm (right)
            computes statistics across the feature dimension for each
            sample independently (highlighted row). This makes LayerNorm
            batch-independent and suitable for variable-length
            sequences.</em></p>
            <p><strong>Why Transformers use LayerNorm</strong>: Batch
            statistics don‚Äôt make sense when batch size = 1 during
            generation, or when sequences have different lengths.</p>
            <h3
            id="interview-q-whats-the-difference-between-batchnorm-and-layernorm">Interview
            Q: ‚ÄúWhat‚Äôs the difference between BatchNorm and
            LayerNorm?‚Äù</h3>
            <p><strong>A</strong>: BatchNorm normalizes across the batch
            dimension for each feature, computing mean and variance over
            all examples in a mini-batch. LayerNorm normalizes across
            the feature dimension for each example independently.
            BatchNorm works well for CNNs but requires consistent batch
            sizes and doesn‚Äôt work for sequence generation where batch
            size is 1. LayerNorm is batch-independent, making it
            suitable for Transformers and variable-length sequences.</p>
            <h3
            id="interview-q-what-does-batch-normalization-do-and-why-does-it-help-training">Interview
            Q: ‚ÄúWhat does batch normalization do and why does it help
            training?‚Äù</h3>
            <p><strong>A</strong>: Batch normalization normalizes the
            output of a previous layer by subtracting the batch mean and
            dividing by the batch standard deviation, then applies
            learnable scale (<span
            class="math inline">\(\gamma\)</span>) and shift (<span
            class="math inline">\(\beta\)</span>) parameters.</p>
            <p><strong>Why it helps</strong>:</p>
            <ol type="1">
            <li><strong>Reduces internal covariate shift</strong>:
            Ensures the mean and variance of inputs to each layer stay
            consistent during training, even as earlier layer weights
            change.</li>
            <li><strong>Constrains gradient updates</strong>: The
            gradient will never propose an operation that acts simply to
            increase the standard deviation or mean of a layer‚Äôs
            activations ‚Äî any such change would be immediately
            normalized away. This forces gradients to find more
            meaningful updates.</li>
            <li><strong>Enables higher learning rates</strong>: Since
            activations are normalized, gradient magnitudes are more
            consistent across layers.</li>
            <li><strong>Implicit regularization</strong>: The noise from
            batch statistics acts like dropout, providing a
            regularization effect.</li>
            </ol>
            <hr />
            <h2 id="newtons-method-second-order-optimization">5.9
            Newton‚Äôs Method (Second-Order Optimization)</h2>
            <h3 id="the-idea-1">The Idea</h3>
            <p>Instead of just following the gradient, use
            <strong>curvature information</strong> to take smarter
            steps.</p>
            <p><strong>Gradient descent</strong>: Linear approximation
            <span class="math display">\[w_{t+1} = w_t - \alpha \nabla
            \ell(w_t)\]</span></p>
            <p><strong>Newton‚Äôs method</strong>: Quadratic approximation
            <span class="math display">\[w_{t+1} = w_t - H^{-1} \nabla
            \ell(w_t)\]</span></p>
            <p>where <span class="math inline">\(H\)</span> is the
            Hessian matrix (second derivatives).</p>
            <h3 id="why-its-faster-in-theory">Why It‚Äôs Faster (In
            Theory)</h3>
            <p>Newton‚Äôs method accounts for curvature:</p>
            <ul>
            <li>In flat directions: takes larger steps</li>
            <li>In steep directions: takes smaller steps</li>
            <li>Finds the minimum in <strong>one step</strong> for
            quadratic functions!</li>
            </ul>
            <h3 id="why-we-dont-use-it-for-deep-learning">Why We Don‚Äôt
            Use It for Deep Learning</h3>
            <table>
            <thead>
            <tr>
            <th>Problem</th>
            <th>Impact</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Computing Hessian</strong></td>
            <td><span class="math inline">\(O(n^2)\)</span> memory for
            <span class="math inline">\(n\)</span> parameters</td>
            </tr>
            <tr>
            <td><strong>Inverting Hessian</strong></td>
            <td><span class="math inline">\(O(n^3)\)</span> time</td>
            </tr>
            <tr>
            <td><strong>Non-convexity</strong></td>
            <td>Can diverge at saddle points</td>
            </tr>
            <tr>
            <td><strong>Stochasticity</strong></td>
            <td>Noisy gradients ‚Üí noisy Hessian</td>
            </tr>
            </tbody>
            </table>
            <p>For a 7B parameter model: Hessian would need <span
            class="math inline">\((7 \times 10^9)^2 = 5 \times
            10^{19}\)</span> floats!</p>
            <h3 id="approximations-used-in-practice">Approximations Used
            in Practice</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Approximation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Adam</strong></td>
            <td>Diagonal approximation (<span
            class="math inline">\(1/\sqrt{v}\)</span> ‚âà curvature)</td>
            </tr>
            <tr>
            <td><strong>L-BFGS</strong></td>
            <td>Low-rank Hessian approximation</td>
            </tr>
            <tr>
            <td><strong>K-FAC</strong></td>
            <td>Kronecker-factored curvature</td>
            </tr>
            <tr>
            <td><strong>Shampoo</strong></td>
            <td>Block-diagonal approximation</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-why-dont-we-use-newtons-method-for-neural-networks">Interview
            Q: ‚ÄúWhy don‚Äôt we use Newton‚Äôs method for neural
            networks?‚Äù</h3>
            <p><strong>A</strong>: The Hessian matrix for neural
            networks has billions of parameters, making it impossible to
            compute (<span class="math inline">\(O(n^2)\)</span> memory)
            or invert (<span class="math inline">\(O(n^3)\)</span>
            time). Additionally, neural network loss landscapes are
            non-convex with many saddle points where Newton‚Äôs method can
            diverge. Instead, we use approximations: Adam‚Äôs second
            moment provides a diagonal estimate of curvature, while
            methods like K-FAC and Shampoo use block-diagonal or
            Kronecker approximations for better conditioning.</p>
            <hr />
            <h1 id="part-6-sequence-models">Part 6: Sequence Models</h1>
            <h2 id="recurrent-neural-networks-rnns">6.1 Recurrent Neural
            Networks (RNNs)</h2>
            <h3 id="the-problem-variable-length-sequences">The Problem:
            Variable-Length Sequences</h3>
            <p>Feedforward networks require fixed-size inputs ‚Äî if you
            train a network to classify 100-dimensional vectors, every
            input must be exactly 100 dimensions. But real-world
            sequential data doesn‚Äôt work that way. A tweet might be 10
            words, a news article 500 words, and a novel millions of
            words. Audio clips vary in duration, and time series like
            stock prices or sensor readings stream indefinitely.</p>
            <p>Naive solutions fail: - <strong>Truncation</strong>:
            Cutting sequences to a fixed length loses information (what
            if the key insight is in word 101?) -
            <strong>Padding</strong>: Adding zeros to reach a fixed
            length wastes computation and can confuse the model -
            <strong>Bag of words</strong>: Throwing away order loses
            meaning (‚Äúdog bites man‚Äù ‚â† ‚Äúman bites dog‚Äù)</p>
            <p>The fundamental insight of RNNs is that we need an
            architecture that can process sequences <strong>one element
            at a time</strong>, maintaining a running summary of what it
            has seen so far. This summary ‚Äî called the <strong>hidden
            state</strong> ‚Äî acts as the network‚Äôs memory.</p>
            <h3 id="rnn-architecture">RNN Architecture</h3>
            <pre><code>x‚ÇÅ ‚îÄ‚îÄ‚Üí [RNN] ‚îÄ‚îÄ‚Üí h‚ÇÅ ‚îÄ‚îÄ‚Üí [RNN] ‚îÄ‚îÄ‚Üí h‚ÇÇ ‚îÄ‚îÄ‚Üí [RNN] ‚îÄ‚îÄ‚Üí h‚ÇÉ
         ‚Üë               ‚Üë               ‚Üë
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    Same weights!</code></pre>
            <h3 id="forward-pass">Forward Pass</h3>
            <p>At each time step <span class="math inline">\(t\)</span>,
            the RNN takes two inputs: the current element <span
            class="math inline">\(x_t\)</span> and the previous hidden
            state <span class="math inline">\(h_{t-1}\)</span>. It
            combines them to produce a new hidden state <span
            class="math inline">\(h_t\)</span>, which serves both as the
            output and as the memory passed to the next step:</p>
            <p><span class="math display">\[h_t = \tanh(W_{xh} x_t +
            W_{hh} h_{t-1} + b_h)\]</span></p>
            <p><span class="math display">\[y_t = W_{hy} h_t +
            b_y\]</span></p>
            <p><strong>Parameter sharing is the key insight</strong>:
            The same weight matrices <span
            class="math inline">\(W_{xh}\)</span>, <span
            class="math inline">\(W_{hh}\)</span> are applied at every
            time step. This is what makes RNNs handle variable-length
            sequences ‚Äî whether your input has 5 tokens or 500, the same
            parameters process each step. This is analogous to how a CNN
            applies the same filter across all spatial positions.
            Parameter sharing also provides a form of regularization and
            reduces the total number of parameters dramatically (imagine
            needing separate weights for position 1, position 2, ‚Ä¶
            position 1000!).</p>
            <p>The hidden state <span class="math inline">\(h_t\)</span>
            acts as a <strong>compressed summary</strong> of everything
            the network has seen up to time <span
            class="math inline">\(t\)</span>. Theoretically, it encodes
            ‚ÄúI‚Äôve seen x‚ÇÅ, then x‚ÇÇ, ‚Ä¶, up to x‚Çú‚Äù in a fixed-size vector.
            In practice, this compression is lossy, especially for long
            sequences ‚Äî one of the key limitations that LSTMs and
            attention mechanisms address.</p>
            <h3 id="backpropagation-through-time-bptt">Backpropagation
            Through Time (BPTT)</h3>
            <p>Unroll the network and apply standard backprop:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            W_{hh}} = \sum_{t=1}^{T} \frac{\partial L_t}{\partial
            W_{hh}}\]</span></p>
            <p>Each term involves: <span
            class="math display">\[\frac{\partial h_t}{\partial h_{t-1}}
            = W_{hh}^T \cdot
            \text{diag}(\tanh&#39;(z_{t-1}))\]</span></p>
            <h3 id="the-vanishing-gradient-problem-1">The Vanishing
            Gradient Problem</h3>
            <p><span class="math display">\[\frac{\partial h_T}{\partial
            h_1} = \prod_{t=2}^{T} \frac{\partial h_t}{\partial
            h_{t-1}}\]</span></p>
            <p>If <span class="math inline">\(\|W_{hh}\| &lt; 1\)</span>
            or <span class="math inline">\(\tanh&#39;\)</span> is small:
            gradients vanish exponentially!</p>
            <p>For sequence of length 100: <span
            class="math inline">\((0.9)^{100} \approx
            0.00003\)</span></p>
            <p><strong>Interview Q</strong>: ‚ÄúWhy do RNNs struggle with
            long sequences?‚Äù</p>
            <p><strong>A</strong>: Due to vanishing gradients. The
            gradient flows backward through time via repeated
            multiplication of the weight matrix. If eigenvalues &lt; 1,
            gradients vanish exponentially. If &gt; 1, they explode.
            This makes it hard to learn long-range dependencies because
            early time steps get negligible gradient signal.</p>
            <h3 id="teacher-forcing">Teacher Forcing</h3>
            <p><strong>The Problem</strong>: During training, if the
            model makes an error at time <span
            class="math inline">\(t\)</span>, that error propagates to
            all future steps. Early in training, errors compound
            catastrophically.</p>
            <p><strong>The Solution</strong>: <strong>Teacher
            forcing</strong> feeds the <strong>ground-truth
            token</strong> at time <span
            class="math inline">\(t\)</span> as input for time <span
            class="math inline">\(t+1\)</span>, instead of the model‚Äôs
            own prediction.</p>
            <pre><code>Without teacher forcing:      With teacher forcing:

x‚ÇÅ ‚Üí [RNN] ‚Üí ≈∑‚ÇÅ              x‚ÇÅ ‚Üí [RNN] ‚Üí ≈∑‚ÇÅ
      ‚Üì                            ‚Üì
≈∑‚ÇÅ ‚Üí [RNN] ‚Üí ≈∑‚ÇÇ (error!)     y‚ÇÅ ‚Üí [RNN] ‚Üí ≈∑‚ÇÇ (correct input!)
      ‚Üì                            ‚Üì
≈∑‚ÇÇ ‚Üí [RNN] ‚Üí ≈∑‚ÇÉ (worse!)     y‚ÇÇ ‚Üí [RNN] ‚Üí ≈∑‚ÇÉ (correct input!)</code></pre>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li>Faster convergence (no error accumulation)</li>
            <li>More stable training</li>
            <li>Easier to parallelize (all inputs known)</li>
            </ul>
            <p><strong>The Exposure Bias Problem</strong>:</p>
            <p>At inference time, we don‚Äôt have ground-truth ‚Äî we must
            use our own predictions. The model has never seen its own
            mistakes during training!</p>
            <p><strong>Solutions</strong>:</p>
            <ol type="1">
            <li><strong>Scheduled sampling</strong>: Gradually decrease
            teacher forcing probability during training</li>
            <li><strong>Curriculum learning</strong>: Start with teacher
            forcing, transition to self-generated</li>
            <li><strong>Beam search</strong>: Explore multiple
            hypotheses at inference</li>
            </ol>
            <div class="sourceCode" id="cb211"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scheduled sampling</span></span>
<span id="cb211-2"><a href="#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_teacher_forcing_prob(epoch, decay<span class="op">=</span><span class="fl">0.05</span>):</span>
<span id="cb211-3"><a href="#cb211-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(<span class="fl">0.0</span>, <span class="fl">1.0</span> <span class="op">-</span> epoch <span class="op">*</span> decay)</span>
<span id="cb211-4"><a href="#cb211-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-5"><a href="#cb211-5" aria-hidden="true" tabindex="-1"></a><span class="co"># During training</span></span>
<span id="cb211-6"><a href="#cb211-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb211-7"><a href="#cb211-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> teacher_forcing_prob:</span>
<span id="cb211-8"><a href="#cb211-8" aria-hidden="true" tabindex="-1"></a>        input_t <span class="op">=</span> ground_truth[t]  <span class="co"># Teacher forcing</span></span>
<span id="cb211-9"><a href="#cb211-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb211-10"><a href="#cb211-10" aria-hidden="true" tabindex="-1"></a>        input_t <span class="op">=</span> model_output[t<span class="op">-</span><span class="dv">1</span>]  <span class="co"># Use own prediction</span></span></code></pre></div>
            <p><strong>Interview Q</strong>: ‚ÄúWhat is teacher forcing
            and what‚Äôs its drawback?‚Äù</p>
            <p><strong>A</strong>: Teacher forcing trains sequence
            models by feeding ground-truth tokens as inputs instead of
            model predictions. This speeds up training by preventing
            error accumulation, but creates <strong>exposure
            bias</strong>: the model only sees perfect inputs during
            training but must use its own (potentially wrong)
            predictions at inference. Solutions include scheduled
            sampling (gradually reducing teacher forcing) and beam
            search at inference.</p>
            <hr />
            <h2 id="lstm-and-gru">6.2 LSTM and GRU</h2>
            <h3 id="lstm-long-short-term-memory">LSTM (Long Short-Term
            Memory)</h3>
            <p>The vanilla RNN‚Äôs fatal flaw is that information must
            pass through many multiplicative transformations. Over 50 or
            100 steps, useful gradients either vanish to zero or explode
            to infinity. LSTMs solve this with a elegant architectural
            insight: <strong>create a highway for information to flow
            unchanged</strong>.</p>
            <p><strong>Key idea</strong>: Add a <strong>cell
            state</strong> <span class="math inline">\(c_t\)</span> ‚Äî a
            separate memory channel that can preserve information over
            long distances. Unlike the hidden state which is transformed
            at every step, the cell state flows through time with only
            <strong>additive</strong> modifications, controlled by
            learned <strong>gates</strong>. Think of it as a conveyor
            belt running above the main network: information can be
            placed on the belt, read from the belt, or removed from the
            belt, but the belt itself moves smoothly without the
            repeated squashing that kills gradients.</p>
            <p><img
            src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png"
            alt="LSTM Architecture" /> <em>Figure: LSTM cell
            architecture showing the cell state highway (top), three
            gates (forget, input, output), and hidden state flow. The
            cell state acts as a ‚Äúconveyor belt‚Äù for long-term memory.
            (Source: <a
            href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher
            Olah‚Äôs ‚ÄúUnderstanding LSTMs‚Äù</a>)</em></p>
            <p><strong>The Three Gates</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 15%" />
            <col style="width: 23%" />
            <col style="width: 35%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Gate</th>
            <th>Purpose</th>
            <th>Output Range</th>
            <th>Controls</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Forget Gate</strong> (<span
            class="math inline">\(f_t\)</span>)</td>
            <td>What to <strong>erase</strong> from cell state</td>
            <td>[0, 1]</td>
            <td><span class="math inline">\(f_t = 0\)</span> ‚Üí forget
            everything</td>
            </tr>
            <tr>
            <td><strong>Input Gate</strong> (<span
            class="math inline">\(i_t\)</span>)</td>
            <td>What <strong>new info</strong> to add</td>
            <td>[0, 1]</td>
            <td><span class="math inline">\(i_t = 0\)</span> ‚Üí add
            nothing new</td>
            </tr>
            <tr>
            <td><strong>Output Gate</strong> (<span
            class="math inline">\(o_t\)</span>)</td>
            <td>What to <strong>output</strong> to hidden state</td>
            <td>[0, 1]</td>
            <td><span class="math inline">\(o_t = 0\)</span> ‚Üí output
            nothing</td>
            </tr>
            </tbody>
            </table>
            <h3 id="lstm-equations">LSTM Equations</h3>
            <p><strong>Forget gate</strong>: What to remove from cell
            state <span class="math display">\[f_t = \sigma(W_f
            [h_{t-1}, x_t] + b_f)\]</span></p>
            <p><strong>Input gate</strong>: What new info to add <span
            class="math display">\[i_t = \sigma(W_i [h_{t-1}, x_t] +
            b_i)\]</span></p>
            <p><span class="math display">\[\tilde{c}_t = \tanh(W_c
            [h_{t-1}, x_t] + b_c)\]</span></p>
            <p><strong>Cell state update</strong>: <span
            class="math display">\[c_t = f_t \odot c_{t-1} + i_t \odot
            \tilde{c}_t\]</span></p>
            <p><strong>Output gate</strong>: What to output <span
            class="math display">\[o_t = \sigma(W_o [h_{t-1}, x_t] +
            b_o)\]</span></p>
            <p><span class="math display">\[h_t = o_t \odot
            \tanh(c_t)\]</span></p>
            <h3 id="why-lstm-solves-vanishing-gradients">Why LSTM Solves
            Vanishing Gradients</h3>
            <p>The cell state update is the magic: <span
            class="math display">\[c_t = f_t \odot c_{t-1} + i_t \odot
            \tilde{c}_t\]</span></p>
            <p>Notice this is <strong>addition</strong>, not
            multiplication followed by nonlinearity. The gradient
            flowing backward through time sees: <span
            class="math display">\[\frac{\partial c_t}{\partial c_{t-1}}
            = f_t\]</span></p>
            <p>Compare this to vanilla RNN where <span
            class="math inline">\(\frac{\partial h_t}{\partial h_{t-1}}
            = W_{hh}^T \cdot \text{diag}(\tanh&#39;)\)</span> ‚Äî a matrix
            multiplication followed by element-wise scaling by the tanh
            derivative (which is &lt; 1 and often much smaller).</p>
            <p>In LSTM, if the forget gate <span
            class="math inline">\(f_t \approx 1\)</span> (meaning
            ‚Äúremember everything‚Äù), the gradient flows through almost
            unchanged! The network can learn to keep the forget gate
            high for important long-term information, creating an
            unobstructed gradient highway spanning hundreds of time
            steps.</p>
            <p><img src="figures/gradient_flow.png"
            alt="Gradient Flow Comparison" /> <em>Figure: Comparison of
            gradient flow in vanilla RNN vs LSTM. The LSTM‚Äôs cell state
            provides a highway for gradients.</em></p>
            <h3 id="gru-gated-recurrent-unit">GRU (Gated Recurrent
            Unit)</h3>
            <p>Researchers asked: ‚ÄúDo we really need three separate
            gates and a separate cell state? Can we get similar benefits
            with a simpler design?‚Äù The GRU answers yes ‚Äî it achieves
            comparable performance to LSTM with fewer parameters and
            faster training by combining the forget and input gates into
            a single <strong>update gate</strong>, and merging the cell
            state into the hidden state.</p>
            <p>The design philosophy is minimalism: what‚Äôs the simplest
            gating mechanism that still solves vanishing gradients? GRU
            uses just 2 gates instead of 3:</p>
            <p><img
            src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png"
            alt="GRU Architecture" /> <em>Figure: GRU cell architecture
            showing the two gates (reset and update). GRU combines the
            forget and input gates into a single update gate, making it
            simpler and faster to train. (Source: <a
            href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher
            Olah‚Äôs ‚ÄúUnderstanding LSTMs‚Äù</a>)</em></p>
            <p><strong>The Two Gates</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 24%" />
            <col style="width: 36%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>Gate</th>
            <th>Purpose</th>
            <th>Equation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Update Gate</strong> (<span
            class="math inline">\(z_t\)</span>)</td>
            <td>How much to <strong>update</strong> hidden state</td>
            <td><span class="math inline">\(z_t = \sigma(W_z [h_{t-1},
            x_t])\)</span></td>
            </tr>
            <tr>
            <td><strong>Reset Gate</strong> (<span
            class="math inline">\(r_t\)</span>)</td>
            <td>How much <strong>past</strong> to forget when computing
            candidate</td>
            <td><span class="math inline">\(r_t = \sigma(W_r [h_{t-1},
            x_t])\)</span></td>
            </tr>
            </tbody>
            </table>
            <p><strong>GRU Equations</strong>:</p>
            <p><span class="math display">\[z_t = \sigma(W_z [h_{t-1},
            x_t]) \quad \text{(update gate)}\]</span></p>
            <p><span class="math display">\[r_t = \sigma(W_r [h_{t-1},
            x_t]) \quad \text{(reset gate)}\]</span></p>
            <p><span class="math display">\[\tilde{h}_t = \tanh(W [r_t
            \odot h_{t-1}, x_t]) \quad \text{(candidate)}\]</span></p>
            <p><span class="math display">\[h_t = (1 - z_t) \odot
            h_{t-1} + z_t \odot \tilde{h}_t \quad
            \text{(interpolate)}\]</span></p>
            <p><strong>Intuition</strong>: The update gate <span
            class="math inline">\(z_t\)</span> acts like a ‚Äúslider‚Äù
            between keeping the old hidden state (<span
            class="math inline">\(z_t = 0\)</span>) and using the new
            candidate (<span class="math inline">\(z_t = 1\)</span>).
            The reset gate controls how much history to use when
            computing the candidate.</p>
            <h3 id="lstm-vs-gru">LSTM vs GRU</h3>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>LSTM</th>
            <th>GRU</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Parameters</td>
            <td>More</td>
            <td>Fewer</td>
            </tr>
            <tr>
            <td>Gates</td>
            <td>3 (forget, input, output)</td>
            <td>2 (reset, update)</td>
            </tr>
            <tr>
            <td>Cell state</td>
            <td>Separate <span class="math inline">\(c_t\)</span></td>
            <td>Combined in <span
            class="math inline">\(h_t\)</span></td>
            </tr>
            <tr>
            <td>Performance</td>
            <td>Slightly better on some tasks</td>
            <td>Often comparable</td>
            </tr>
            <tr>
            <td>Training speed</td>
            <td>Slower</td>
            <td>Faster</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Rule of thumb</strong>: Try GRU first (faster),
            switch to LSTM if needed.</p>
            <h3 id="bidirectional-rnns">Bidirectional RNNs</h3>
            <p><strong>Problem</strong>: Standard RNNs only see past
            context. But sometimes future context matters!</p>
            <pre><code>&quot;The man who was wearing a red [MASK] walked into the store&quot;
                                  ‚Üë
                     Need future context to predict &quot;shirt&quot; vs &quot;hat&quot;</code></pre>
            <p><strong>Solution</strong>: Run two RNNs ‚Äî one forward,
            one backward ‚Äî and concatenate:</p>
            <pre><code>Forward:   x‚ÇÅ ‚Üí [‚Üí] ‚Üí h‚ÇÅ‚Üí ‚Üí [‚Üí] ‚Üí h‚ÇÇ‚Üí ‚Üí [‚Üí] ‚Üí h‚ÇÉ‚Üí
                ‚Üì           ‚Üì           ‚Üì
Output:        [h‚ÇÅ‚Üí;h‚ÇÅ‚Üê]  [h‚ÇÇ‚Üí;h‚ÇÇ‚Üê]  [h‚ÇÉ‚Üí;h‚ÇÉ‚Üê]
                ‚Üë           ‚Üë           ‚Üë
Backward:  x‚ÇÅ ‚Üê [‚Üê] ‚Üê h‚ÇÅ‚Üê ‚Üê [‚Üê] ‚Üê h‚ÇÇ‚Üê ‚Üê [‚Üê] ‚Üê h‚ÇÉ‚Üê</code></pre>
            <p><span class="math display">\[h_t = [h_t^{\rightarrow};
            h_t^{\leftarrow}]\]</span></p>
            <p><strong>When to use</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Use Bidirectional</th>
            <th>Use Unidirectional</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Classification (sentiment)</td>
            <td>Generation (text, speech)</td>
            </tr>
            <tr>
            <td>Named Entity Recognition</td>
            <td>Language modeling</td>
            </tr>
            <tr>
            <td>Machine translation (encoder)</td>
            <td>Chatbots</td>
            </tr>
            <tr>
            <td>Question answering</td>
            <td>Real-time streaming</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Connection to BERT</strong>: BERT‚Äôs
            ‚Äúbidirectional‚Äù comes from this concept ‚Äî it sees full
            context in both directions (though via attention, not
            RNN).</p>
            <h3 id="interview-qa-lstm-and-gru">Interview Q&amp;A: LSTM
            and GRU</h3>
            <p><strong>Q</strong>: ‚ÄúHow does LSTM solve the vanishing
            gradient problem?‚Äù</p>
            <p><strong>A</strong>: LSTM adds a <strong>cell
            state</strong> with a linear self-connection: <span
            class="math inline">\(c_t = f_t \odot c_{t-1} + i_t \odot
            \tilde{c}_t\)</span>. The key is the
            <strong>additive</strong> update (not multiplicative).
            Gradients can flow through the cell state without
            exponential decay. The forget gate <span
            class="math inline">\(f_t\)</span> controls what to keep,
            but even with <span class="math inline">\(f_t &lt;
            1\)</span>, the gradient path is much more stable than
            vanilla RNN‚Äôs purely multiplicative path.</p>
            <p><strong>Q</strong>: ‚ÄúWhat‚Äôs the difference between LSTM
            and GRU? When would you use each?‚Äù</p>
            <p><strong>A</strong>: LSTM has 3 gates (forget, input,
            output) and a separate cell state. GRU has 2 gates (reset,
            update) and no separate cell state. GRU is simpler, faster
            to train, and has fewer parameters. Use GRU as default ‚Äî
            it‚Äôs often equally good. Use LSTM for tasks requiring
            fine-grained memory control or when you have more data and
            compute.</p>
            <p><strong>Q</strong>: ‚ÄúWhat does each LSTM gate do?‚Äù</p>
            <p><strong>A</strong>:</p>
            <ul>
            <li><strong>Forget gate</strong> (<span
            class="math inline">\(f_t\)</span>): Decides what to erase
            from cell state. <span class="math inline">\(f_t =
            0\)</span> means forget everything.</li>
            <li><strong>Input gate</strong> (<span
            class="math inline">\(i_t\)</span>): Decides how much new
            info to add. <span class="math inline">\(i_t = 0\)</span>
            means add nothing.</li>
            <li><strong>Output gate</strong> (<span
            class="math inline">\(o_t\)</span>): Decides what part of
            cell state to output. <span class="math inline">\(o_t =
            0\)</span> means output nothing.</li>
            </ul>
            <p>Think of it as: forget old ‚Üí add new ‚Üí decide what to
            share.</p>
            <p><strong>Q</strong>: ‚ÄúWhy might gradients still vanish in
            LSTM?‚Äù</p>
            <p><strong>A</strong>: If the forget gate <span
            class="math inline">\(f_t\)</span> is consistently close to
            0, information gets erased quickly. Also, gradients through
            the output path (not the cell state) can still vanish. The
            cell state helps but doesn‚Äôt completely eliminate the
            problem for very long sequences (1000+ steps).</p>
            <hr />
            <h2 id="seq2seq-and-attention-pre-transformer">6.3 Seq2Seq
            and Attention (Pre-Transformer)</h2>
            <p>The sequence-to-sequence (seq2seq) paradigm, introduced
            by Sutskever et al.¬†in 2014, was revolutionary. Before
            seq2seq, neural approaches to machine translation were
            limited to fixed-length inputs and outputs, or required
            complex architectures for each specific language pair.
            Seq2seq proposed something elegant: use one RNN to
            <strong>encode</strong> the input into a vector, and another
            to <strong>decode</strong> that vector into the output. This
            encoder-decoder framework became the foundation for modern
            neural machine translation and later influenced the
            Transformer architecture.</p>
            <h3 id="the-seq2seq-architecture">The Seq2Seq
            Architecture</h3>
            <p><strong>Problem</strong>: Map variable-length input to
            variable-length output (translation, summarization).</p>
            <pre><code>Encoder:  &quot;I love cats&quot; ‚Üí [LSTM] ‚Üí [LSTM] ‚Üí [LSTM] ‚Üí context vector
                                                          ‚Üì
Decoder:                                    [LSTM] ‚Üí [LSTM] ‚Üí [LSTM]
                                              ‚Üì        ‚Üì        ‚Üì
                                           &quot;J&#39;aime&quot; &quot;les&quot;   &quot;chats&quot;</code></pre>
            <p><strong>The Bottleneck Problem</strong>: The entire input
            must be compressed into a single fixed-size context vector.
            This fails for long sequences!</p>
            <hr />
            <h3 id="attention-the-core-intuition">Attention: The Core
            Intuition</h3>
            <p>Before diving into equations, let‚Äôs understand
            <strong>what attention does intuitively</strong>. The
            fundamental insight is simple: <strong>when generating each
            output word, let the model learn WHERE to look in the
            input</strong>.</p>
            <p>Consider translating ‚ÄúI love cats‚Äù ‚Üí ‚ÄúJ‚Äôaime les
            chats‚Äù:</p>
            <ul>
            <li>When generating ‚ÄúJ‚Äôaime‚Äù (French for ‚ÄúI love‚Äù), the
            model should focus on ‚ÄúI‚Äù and ‚Äúlove‚Äù</li>
            <li>When generating ‚Äúchats‚Äù (French for ‚Äúcats‚Äù), the model
            should focus on ‚Äúcats‚Äù</li>
            <li>The word ‚Äúles‚Äù (the plural article) depends on ‚Äúcats‚Äù
            being plural</li>
            </ul>
            <p>Without attention, all this information must squeeze
            through a single fixed-size vector. With attention, the
            decoder can <strong>dynamically shift its focus</strong> to
            different parts of the input at each generation step.</p>
            <p><strong>The metaphor</strong>: Imagine reading a foreign
            text with a highlighter. For each word you write in the
            translation, you highlight the relevant source words.
            Attention learns to do this highlighting automatically.</p>
            <h3 id="attention-looking-back-at-the-encoder">Attention:
            Looking Back at the Encoder</h3>
            <p><strong>Key insight</strong>: Instead of one context
            vector, let the decoder <strong>look at all encoder
            states</strong> and decide which are relevant.</p>
            <p><strong>Bahdanau Attention</strong> (Additive):</p>
            <p><span class="math display">\[e_{ij} = v^T \tanh(W_s
            s_{i-1} + W_h h_j)\]</span></p>
            <p><span class="math display">\[\alpha_{ij} =
            \frac{\exp(e_{ij})}{\sum_k \exp(e_{ik})}\]</span></p>
            <p><span class="math display">\[c_i = \sum_j \alpha_{ij}
            h_j\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(s_{i-1}\)</span> = decoder
            hidden state at step <span
            class="math inline">\(i-1\)</span></li>
            <li><span class="math inline">\(h_j\)</span> = encoder
            hidden state at position <span
            class="math inline">\(j\)</span></li>
            <li><span class="math inline">\(\alpha_{ij}\)</span> =
            attention weight (how much to attend to encoder position
            <span class="math inline">\(j\)</span>)</li>
            <li><span class="math inline">\(c_i\)</span> = context
            vector for decoder step <span
            class="math inline">\(i\)</span></li>
            </ul>
            <p><strong>Luong Attention</strong> (Multiplicative,
            simpler):</p>
            <p><span class="math display">\[e_{ij} = s_i^T W h_j \quad
            \text{or} \quad e_{ij} = s_i^T h_j\]</span></p>
            <h3 id="step-by-step-attention-in-translation">Step-by-Step:
            Attention in Translation</h3>
            <p>Let‚Äôs trace through attention for translating ‚ÄúI love
            cats‚Äù ‚Üí ‚ÄúJ‚Äôaime les chats‚Äù:</p>
            <p><strong>Step 1: Encoder processes source
            sentence</strong></p>
            <p>The encoder (an LSTM) processes each source word,
            producing hidden states:</p>
            <pre><code>&quot;I&quot;    ‚Üí h‚ÇÅ = [0.2, -0.1, 0.5, ...]   ‚Üê encodes &quot;I&quot; + context
&quot;love&quot; ‚Üí h‚ÇÇ = [0.4, 0.3, -0.2, ...]   ‚Üê encodes &quot;I love&quot;
&quot;cats&quot; ‚Üí h‚ÇÉ = [-0.1, 0.6, 0.3, ...]   ‚Üê encodes &quot;I love cats&quot;</code></pre>
            <p><strong>Step 2: Decoder generates ‚ÄúJ‚Äôaime‚Äù (step
            i=1)</strong></p>
            <p>The decoder has hidden state <span
            class="math inline">\(s_0\)</span> (initialized from
            encoder). Now it must generate the first French word.</p>
            <p><em>Compute attention scores</em>:</p>
            <pre><code>e‚ÇÅ‚ÇÅ = score(s‚ÇÄ, h‚ÇÅ) = 0.8   ‚Üê &quot;I&quot; is relevant
e‚ÇÅ‚ÇÇ = score(s‚ÇÄ, h‚ÇÇ) = 0.9   ‚Üê &quot;love&quot; is very relevant  
e‚ÇÅ‚ÇÉ = score(s‚ÇÄ, h‚ÇÉ) = 0.2   ‚Üê &quot;cats&quot; less relevant for &quot;J&#39;aime&quot;</code></pre>
            <p><em>Apply softmax to get weights</em>:</p>
            <pre><code>Œ±‚ÇÅ = softmax([0.8, 0.9, 0.2]) = [0.35, 0.42, 0.23]</code></pre>
            <p><em>Compute context vector</em> (weighted sum of encoder
            states):</p>
            <pre><code>c‚ÇÅ = 0.35¬∑h‚ÇÅ + 0.42¬∑h‚ÇÇ + 0.23¬∑h‚ÇÉ</code></pre>
            <p><em>Generate output</em>: The decoder uses <span
            class="math inline">\(c_1\)</span> along with <span
            class="math inline">\(s_0\)</span> to generate ‚ÄúJ‚Äôaime‚Äù.</p>
            <p><strong>Step 3: Decoder generates ‚Äúchats‚Äù (step
            i=3)</strong></p>
            <p>Now the decoder is generating the third word. Its current
            state <span class="math inline">\(s_2\)</span> encodes
            ‚ÄúJ‚Äôaime les‚Äù.</p>
            <p><em>Compute attention scores</em>:</p>
            <pre><code>e‚ÇÉ‚ÇÅ = score(s‚ÇÇ, h‚ÇÅ) = 0.1   ‚Üê &quot;I&quot; not relevant now
e‚ÇÉ‚ÇÇ = score(s‚ÇÇ, h‚ÇÇ) = 0.2   ‚Üê &quot;love&quot; not relevant now
e‚ÇÉ‚ÇÉ = score(s‚ÇÇ, h‚ÇÉ) = 0.95  ‚Üê &quot;cats&quot; is exactly what we need!</code></pre>
            <p><em>Apply softmax</em>:</p>
            <pre><code>Œ±‚ÇÉ = softmax([0.1, 0.2, 0.95]) = [0.15, 0.18, 0.67]</code></pre>
            <p>The model has learned to focus heavily on ‚Äúcats‚Äù when
            generating ‚Äúchats‚Äù!</p>
            <hr />
            <h3 id="attention-as-soft-alignment">Attention as Soft
            Alignment</h3>
            <p>One of the most beautiful aspects of attention is its
            <strong>interpretability</strong>. The attention weights
            <span class="math inline">\(\alpha_{ij}\)</span> form an
            alignment matrix that shows which source words the model
            ‚Äúlooks at‚Äù when generating each target word.</p>
            <pre><code>                    Source: I    love   cats
                           ‚Üì      ‚Üì      ‚Üì
Target: J&#39;aime           [0.35  0.42   0.23]  ‚Üê looks at &quot;I&quot; and &quot;love&quot;
        les              [0.15  0.25   0.60]  ‚Üê looks at &quot;cats&quot; (plural marker)
        chats            [0.10  0.15   0.75]  ‚Üê looks at &quot;cats&quot;</code></pre>
            <p>This alignment emerges automatically from training ‚Äî no
            one labeled which words should align! The model discovers
            that: - Subject pronouns align across languages (‚ÄúI‚Äù ‚Üí
            ‚ÄúJ‚Äôaime‚Äù) - Verbs align with their translations (‚Äúlove‚Äù ‚Üí
            ‚ÄúJ‚Äôaime‚Äù) - Nouns align with their translations (‚Äúcats‚Äù ‚Üí
            ‚Äúchats‚Äù) - Grammatical markers can look at content words
            (‚Äúles‚Äù checks ‚Äúcats‚Äù is plural)</p>
            <p><strong>Why this matters</strong>: 1.
            <strong>Debugging</strong>: If translations are wrong,
            inspect attention to see what the model focused on 2.
            <strong>Trust</strong>: Humans can verify the model is
            making decisions for reasonable reasons 3.
            <strong>Linguistic insight</strong>: Attention patterns
            reveal how languages relate</p>
            <h3 id="why-attention-works-summary">Why Attention Works
            (Summary)</h3>
            <ol type="1">
            <li><strong>No bottleneck</strong>: Instead of squeezing
            everything through one vector, the decoder accesses all
            encoder states</li>
            <li><strong>Dynamic focus</strong>: Different output
            positions look at different input positions</li>
            <li><strong>Interpretable alignment</strong>: Attention
            weights show what the model is ‚Äúthinking about‚Äù</li>
            <li><strong>Better gradient flow</strong>: Gradients flow
            directly from decoder to relevant encoder positions</li>
            </ol>
            <hr />
            <h3 id="from-seq2seq-attention-to-self-attention">From
            Seq2Seq Attention to Self-Attention</h3>
            <p>The attention mechanism we‚Äôve described is
            <strong>cross-attention</strong>: the decoder (one sequence)
            attends to the encoder (a different sequence). The
            Transformer‚Äôs key innovation was asking: <em>what if a
            sequence attended to itself?</em></p>
            <p><strong>Cross-attention</strong> (seq2seq): - Query:
            decoder hidden state (‚Äúwhat French word am I generating?‚Äù) -
            Keys/Values: encoder hidden states (‚Äúwhat did the English
            say?‚Äù) - Different sequences interact</p>
            <p><strong>Self-attention</strong> (Transformer): - Query,
            Keys, Values: <strong>all from the same sequence</strong> -
            Each position can attend to every other position - ‚ÄúThe cat
            sat‚Äù ‚Üí ‚Äúcat‚Äù can directly see ‚Äúsat‚Äù and ‚ÄúThe‚Äù</p>
            <p>This seemingly simple change has profound implications: -
            <strong>No recurrence needed</strong>: All positions can be
            processed in parallel - <strong>Direct long-range
            connections</strong>: Position 1 can directly attend to
            position 1000 - <strong>Richer representations</strong>:
            Each position‚Äôs representation incorporates information from
            all other positions</p>
            <p>The Transformer is essentially ‚Äúattention all the way
            down‚Äù ‚Äî self-attention replaces the RNN entirely, with
            attention doing the heavy lifting of modeling dependencies
            between positions.</p>
            <h3 id="this-is-the-foundation-for-transformers">This is the
            Foundation for Transformers!</h3>
            <p>Transformer attention is essentially the same idea,
            but:</p>
            <ul>
            <li>Applied to <strong>self-attention</strong> (each
            position attends to all positions)</li>
            <li>No recurrence ‚Äî purely attention-based</li>
            <li>Parallel computation</li>
            </ul>
            <hr />
            <h2 id="transformers">6.4 Transformers</h2>
            <h3 id="why-transformers">Why Transformers?</h3>
            <p>The Transformer, introduced in the landmark ‚ÄúAttention Is
            All You Need‚Äù paper (Vaswani et al., 2017), represents a
            fundamental paradigm shift: instead of processing sequences
            step-by-step, process all positions
            <strong>simultaneously</strong> and let
            <strong>attention</strong> handle the dependencies between
            positions.</p>
            <p>This shift matters for three reasons:</p>
            <p><strong>RNN limitations</strong>:</p>
            <ul>
            <li><strong>Sequential processing ‚Üí can‚Äôt
            parallelize</strong>: To compute <span
            class="math inline">\(h_{100}\)</span>, you must first
            compute <span class="math inline">\(h_1, h_2, \ldots,
            h_{99}\)</span>. On modern GPUs with thousands of cores,
            this is like having a 1000-lane highway where only 1 car can
            drive at a time.</li>
            <li><strong>Long-range dependencies still
            difficult</strong>: Even LSTMs struggle beyond a few hundred
            tokens. The hidden state is a fixed-size bottleneck through
            which all information must flow.</li>
            <li><strong>Fixed-size hidden state bottleneck</strong>:
            Everything the model ‚Äúknows‚Äù about positions 1-99 must fit
            in a single vector when processing position 100.</li>
            </ul>
            <p><strong>Transformer solution</strong>:</p>
            <ul>
            <li><strong>Attention creates direct connections</strong>:
            Position 100 can directly attend to position 1 ‚Äî no need to
            pass information through 99 intermediate hidden states. It‚Äôs
            like every word can ‚Äúsee‚Äù every other word directly.</li>
            <li><strong>Parallel processing of all positions</strong>:
            All attention computations for positions 1-100 can happen
            simultaneously. On GPUs, this translates to massive speedups
            during training.</li>
            <li><strong>No recurrence needed</strong>: The sequential
            bottleneck is eliminated entirely. Training time scales much
            better with sequence length.</li>
            </ul>
            <p>The result? Transformers train faster, scale to longer
            sequences, and achieve better performance. This is why every
            major language model since 2018 (BERT, GPT, T5, LLaMA) uses
            the Transformer architecture.</p>
            <h3 id="self-attention-vs-cross-attention">Self-Attention vs
            Cross-Attention</h3>
            <table>
            <colgroup>
            <col style="width: 13%" />
            <col style="width: 31%" />
            <col style="width: 35%" />
            <col style="width: 20%" />
            </colgroup>
            <thead>
            <tr>
            <th>Type</th>
            <th>Q comes from</th>
            <th>K, V come from</th>
            <th>Used in</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Self-Attention</strong></td>
            <td>Same sequence</td>
            <td>Same sequence</td>
            <td>Encoder, decoder</td>
            </tr>
            <tr>
            <td><strong>Cross-Attention</strong></td>
            <td>Decoder</td>
            <td>Encoder</td>
            <td>Encoder-decoder models</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Self-attention</strong>: Each token attends to
            <strong>all tokens in the same sequence</strong></p>
            <pre><code>&quot;The cat sat&quot; ‚Üí Q, K, V all from same sentence
                Each word can attend to every other word</code></pre>
            <p><strong>Cross-attention</strong>: Decoder attends to
            <strong>encoder outputs</strong></p>
            <pre><code>Encoder: &quot;I love cats&quot; ‚Üí K, V
Decoder: &quot;J&#39;aime&quot; ‚Üí Q
         Q attends to encoder K,V to find relevant source words</code></pre>
            <h3 id="worked-example-attention-computation">Worked
            Example: Attention Computation</h3>
            <p>Let‚Äôs compute attention for a sequence of 3 tokens with
            <span class="math inline">\(d_k = 4\)</span>:</p>
            <p><strong>Input embeddings</strong> <span
            class="math inline">\(X\)</span> (3√ó4):</p>
            <pre><code>X = [[1, 0, 1, 0],   ‚Üê &quot;The&quot;
     [0, 1, 1, 0],   ‚Üê &quot;cat&quot;
     [1, 1, 0, 1]]   ‚Üê &quot;sat&quot;</code></pre>
            <p>Each row is a token‚Äôs embedding ‚Äî a vector capturing its
            meaning in a learned semantic space.</p>
            <hr />
            <p><strong>Step 1</strong>: Project to Q, K, V (using weight
            matrices)</p>
            <pre><code>Q = X @ W_Q    K = X @ W_K    V = X @ W_V</code></pre>
            <p><strong>Why three separate projections?</strong> The same
            word plays different roles depending on context:</p>
            <ul>
            <li><strong>Query (Q)</strong>: ‚ÄúWhat information am I
            looking for?‚Äù ‚Äî When ‚Äúsat‚Äù is processed, its query might
            emphasize features that help find the subject (who
            sat?).</li>
            <li><strong>Key (K)</strong>: ‚ÄúWhat information do I offer
            to others?‚Äù ‚Äî ‚Äúcat‚Äù might project keys that advertise ‚ÄúI‚Äôm a
            noun, I can be a subject.‚Äù</li>
            <li><strong>Value (V)</strong>: ‚ÄúWhat content do I
            contribute if selected?‚Äù ‚Äî The actual semantic information
            passed forward.</li>
            </ul>
            <p>If Q, K, V were identical, the model couldn‚Äôt distinguish
            between ‚Äúasking a question‚Äù and ‚Äúproviding an answer.‚Äù The
            learned weight matrices <span
            class="math inline">\(W_Q\)</span>, <span
            class="math inline">\(W_K\)</span>, <span
            class="math inline">\(W_V\)</span> let each token project
            itself into these different functional roles.</p>
            <hr />
            <p><strong>Step 2</strong>: Compute attention scores <span
            class="math inline">\(QK^T\)</span></p>
            <pre><code>scores = Q @ K.T / sqrt(4)    # (3√ó3) matrix</code></pre>
            <p><strong>What‚Äôs happening</strong>: The dot product <span
            class="math inline">\(q_i \cdot k_j\)</span> measures how
            well query <span class="math inline">\(i\)</span> matches
            key <span class="math inline">\(j\)</span>: - <strong>High
            dot product</strong> ‚Üí vectors point in similar directions ‚Üí
            token <span class="math inline">\(j\)</span> is relevant to
            token <span class="math inline">\(i\)</span> -
            <strong>Low/negative dot product</strong> ‚Üí vectors point
            away ‚Üí not relevant</p>
            <p>The result is a (3√ó3) matrix where
            <code>scores[i][j]</code> = ‚Äúhow much should token <span
            class="math inline">\(i\)</span> attend to token <span
            class="math inline">\(j\)</span>?‚Äù</p>
            <p><strong>Why divide by <span
            class="math inline">\(\sqrt{d_k}\)</span>?</strong> Without
            scaling, dot products grow with dimension (variance = <span
            class="math inline">\(d_k\)</span>). For <span
            class="math inline">\(d_k = 64\)</span>, scores would be ~8√ó
            larger, causing softmax to saturate (output nearly one-hot,
            gradients vanish). Dividing by <span
            class="math inline">\(\sqrt{d_k}\)</span> keeps scores in a
            reasonable range.</p>
            <hr />
            <p><strong>Step 3</strong>: Apply softmax row-wise</p>
            <pre><code>weights = softmax(scores, dim=-1)    # Each row sums to 1</code></pre>
            <p><strong>Why softmax?</strong> Convert raw scores into a
            probability distribution: - All weights become positive -
            Each row sums to 1 (attention weights are ‚Äúhow much‚Äù to
            attend) - Differentiable for backpropagation</p>
            <p><strong>Row-wise</strong> because each token gets its own
            attention pattern ‚Äî ‚Äúsat‚Äù might attend heavily to ‚Äúcat‚Äù,
            while ‚ÄúThe‚Äù might attend mostly to itself.</p>
            <hr />
            <p><strong>Step 4</strong>: Weighted sum of values</p>
            <pre><code>output = weights @ V    # (3√ó4) output</code></pre>
            <p><strong>The core operation</strong>: Each token‚Äôs new
            representation is a blend of all tokens‚Äô values, weighted by
            relevance:</p>
            <p><span class="math display">\[\text{output}_i = \sum_j
            \text{weights}_{ij} \cdot V_j\]</span></p>
            <p>For ‚Äúsat‚Äù:
            <code>output[2] = 0.2 √ó V["The"] + 0.3 √ó V["cat"] + 0.5 √ó V["sat"]</code></p>
            <p>This means ‚Äúsat‚Äù‚Äôs output now <strong>incorporates
            context</strong> ‚Äî it ‚Äúknows‚Äù that a cat is the subject.
            This context-mixing is what makes attention powerful.</p>
            <hr />
            <p><strong>Concrete numbers</strong> (simplified):</p>
            <pre><code>scores = [[1.2, 0.3, 0.5],    softmax‚Üí   [[0.6, 0.2, 0.2],
          [0.4, 1.1, 0.5],    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí     [0.2, 0.5, 0.3],
          [0.3, 0.6, 1.0]]                [0.2, 0.3, 0.5]]</code></pre>
            <p><strong>Interpreting the attention weights</strong>: -
            Row 1: ‚ÄúThe‚Äù attends mostly to itself (0.6) ‚Äî function words
            often self-attend - Row 2: ‚Äúcat‚Äù attends mostly to itself
            (0.5) ‚Äî nouns carry their own meaning - Row 3: ‚Äúsat‚Äù attends
            to itself (0.5) and ‚Äúcat‚Äù (0.3) ‚Äî the verb finds its
            subject!</p>
            <p>The model learns these patterns from data. After
            training, verbs naturally attend to subjects, pronouns
            attend to their referents, and adjectives attend to nouns
            they modify.</p>
            <h3 id="self-attention-mechanism">Self-Attention
            Mechanism</h3>
            <p><strong>Key idea</strong>: For each position, compute a
            weighted sum of all other positions based on relevance.</p>
            <p><strong>Queries, Keys, Values</strong>: <span
            class="math display">\[Q = XW_Q, \quad K = XW_K, \quad V =
            XW_V\]</span></p>
            <p><strong>Attention scores</strong>: <span
            class="math display">\[\text{Attention}(Q, K, V) =
            \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
            <h3 id="step-by-step-attention">Step-by-Step Attention</h3>
            <ol type="1">
            <li><strong>Query</strong>: ‚ÄúWhat am I looking for?‚Äù</li>
            <li><strong>Key</strong>: ‚ÄúWhat do I contain?‚Äù</li>
            <li><strong>Dot product</strong> <span
            class="math inline">\(QK^T\)</span>: How relevant is each
            key to each query?</li>
            <li><strong>Scale by</strong> <span
            class="math inline">\(\sqrt{d_k}\)</span>: Prevent large
            values from dominating softmax</li>
            <li><strong>Softmax</strong>: Convert to probabilities
            (attention weights)</li>
            <li><strong>Multiply by V</strong>: Weighted sum of
            values</li>
            </ol>
            <p><img src="figures/attention_heatmap.png"
            alt="Attention Heatmap" /> <em>Figure: Visualization of
            attention weights showing how different tokens attend to
            each other in a sentence.</em></p>
            <h3 id="example-the-cat-sat-on-the-mat">Example: ‚ÄúThe cat
            sat on the mat‚Äù</h3>
            <p>For the word ‚Äúsat‚Äù, attention might give high weights
            to:</p>
            <ul>
            <li>‚Äúcat‚Äù (subject performing the action)</li>
            <li>‚Äúmat‚Äù (related location)</li>
            </ul>
            <p>And low weights to:</p>
            <ul>
            <li>‚ÄúThe‚Äù (not semantically important)</li>
            </ul>
            <h3 id="why-scale-by-sqrtd_k-deep-dive">Why Scale by <span
            class="math inline">\(\sqrt{d_k}\)</span>? (Deep Dive)</h3>
            <p>This seemingly minor detail ‚Äî dividing by <span
            class="math inline">\(\sqrt{d_k}\)</span> ‚Äî is actually
            crucial for training stability. Let‚Äôs understand
            <em>why</em> through both math and numerical examples.</p>
            <h4 id="the-problem-softmax-saturation">The Problem: Softmax
            Saturation</h4>
            <p>Softmax has a nasty property: when inputs get large, it
            saturates ‚Äî the output becomes nearly one-hot, and gradients
            become tiny. This is similar to how sigmoid saturates for
            large inputs.</p>
            <p>Consider softmax on two sets of inputs:</p>
            <pre><code>softmax([1, 2, 3]) = [0.09, 0.24, 0.67]    ‚Üê smooth distribution
softmax([10, 20, 30]) = [0.00, 0.00, 1.00] ‚Üê saturated!</code></pre>
            <p>In the saturated case, the gradient with respect to the
            largest input is nearly 0, and training stalls.</p>
            <h4 id="why-do-attention-scores-get-large">Why Do Attention
            Scores Get Large?</h4>
            <p>When we compute <span class="math inline">\(q \cdot k =
            \sum_{i=1}^{d_k} q_i k_i\)</span>, we‚Äôre summing <span
            class="math inline">\(d_k\)</span> terms. If each <span
            class="math inline">\(q_i, k_i \sim \mathcal{N}(0,
            1)\)</span>:</p>
            <ul>
            <li>Each product <span class="math inline">\(q_i
            k_i\)</span> has <strong>mean 0</strong> and
            <strong>variance 1</strong></li>
            <li>The sum of <span class="math inline">\(d_k\)</span>
            independent terms has <strong>variance <span
            class="math inline">\(d_k\)</span></strong> (variances
            add)</li>
            <li>So <span class="math inline">\(q \cdot k\)</span> has
            standard deviation <span
            class="math inline">\(\sqrt{d_k}\)</span></li>
            </ul>
            <p><strong>Numerical example</strong> with <span
            class="math inline">\(d_k = 64\)</span>:</p>
            <pre><code>q = [0.5, -0.3, 0.8, ..., -0.2]   ‚Üê 64 random values
k = [0.2, 0.7, -0.4, ..., 0.6]   ‚Üê 64 random values

q ¬∑ k ‚âà 8.5  ‚Üê typical magnitude is ‚àö64 = 8</code></pre>
            <p>With 64 dimensions, dot products naturally have magnitude
            ~8. After applying softmax to a matrix of such values, we
            get severe saturation.</p>
            <h4 id="the-solution-scale-down">The Solution: Scale
            Down</h4>
            <p>Dividing by <span
            class="math inline">\(\sqrt{d_k}\)</span> normalizes the
            variance back to 1: <span
            class="math display">\[\text{Var}\left(\frac{q \cdot
            k}{\sqrt{d_k}}\right) = \frac{\text{Var}(q \cdot k)}{d_k} =
            \frac{d_k}{d_k} = 1\]</span></p>
            <p><strong>Before scaling</strong> (typical values with
            <span class="math inline">\(d_k = 64\)</span>):</p>
            <pre><code>scores = [8.5, 7.2, -6.1, 9.3, ...]
softmax ‚Üí [0.02, 0.01, 0.00, 0.97, ...]  ‚Üê nearly one-hot!</code></pre>
            <p><strong>After scaling</strong> by <span
            class="math inline">\(\sqrt{64} = 8\)</span>:</p>
            <pre><code>scores/8 = [1.06, 0.90, -0.76, 1.16, ...]
softmax ‚Üí [0.28, 0.24, 0.05, 0.31, ...]  ‚Üê smooth distribution</code></pre>
            <p>The scaled version has gradients that flow well; the
            unscaled version essentially picks one key and ignores the
            rest.</p>
            <h4 id="why-d_k-and-not-some-other-value">Why ‚àöd_k and Not
            Some Other Value?</h4>
            <p>The choice of <span
            class="math inline">\(\sqrt{d_k}\)</span> isn‚Äôt arbitrary ‚Äî
            it‚Äôs derived from the statistics of the dot product:</p>
            <ul>
            <li>If entries are unit variance, sum of <span
            class="math inline">\(n\)</span> terms has variance <span
            class="math inline">\(n\)</span></li>
            <li>Standard deviation is <span
            class="math inline">\(\sqrt{n}\)</span></li>
            <li>Dividing by <span
            class="math inline">\(\sqrt{n}\)</span> normalizes to unit
            variance</li>
            </ul>
            <p>This is the same principle behind Xavier/Glorot
            initialization ‚Äî keeping variances stable across layers.</p>
            <h4
            id="interview-q-why-do-we-divide-by-d_k-in-attention">Interview
            Q: ‚ÄúWhy do we divide by ‚àöd_k in attention?‚Äù</h4>
            <p><strong>A</strong>: The dot product <span
            class="math inline">\(q \cdot k\)</span> sums <span
            class="math inline">\(d_k\)</span> terms. If each term has
            unit variance, the sum has variance <span
            class="math inline">\(d_k\)</span>, meaning typical values
            grow like <span class="math inline">\(\sqrt{d_k}\)</span>.
            For <span class="math inline">\(d_k = 64\)</span>, dot
            products are ~8x larger than expected. These large values
            cause softmax to saturate (become nearly one-hot), killing
            gradients. Dividing by <span
            class="math inline">\(\sqrt{d_k}\)</span> normalizes the
            variance back to 1, keeping softmax in its smooth regime
            where gradients flow properly.</p>
            <hr />
            <h3 id="multi-head-attention">Multi-Head Attention</h3>
            <h4 id="why-multiple-heads-the-orchestra-analogy">Why
            Multiple Heads? The Orchestra Analogy</h4>
            <p>A single attention head is like having one spotlight on a
            stage ‚Äî it can only illuminate one relationship at a time.
            But understanding language requires tracking <em>many</em>
            relationships simultaneously. Consider the sentence:</p>
            <blockquote>
            <p><strong>‚ÄúThe cat that the dog chased ran
            away.‚Äù</strong></p>
            </blockquote>
            <p>To fully understand this, you need to track: -
            <strong>Subject-verb</strong>: ‚Äúcat‚Äù ‚Üí ‚Äúran‚Äù (the cat is the
            one running) - <strong>Object-verb</strong>: ‚Äúcat‚Äù ‚Üí
            ‚Äúchased‚Äù (the cat was chased) - <strong>Relative
            clause</strong>: ‚Äúthat‚Äù ‚Üí ‚Äúchased‚Äù (what the relative clause
            is about) - <strong>Agent</strong>: ‚Äúdog‚Äù ‚Üí ‚Äúchased‚Äù (the
            dog did the chasing)</p>
            <p>A single attention head struggles to capture all these
            patterns at once ‚Äî it might focus on subject-verb and miss
            the agent relationship. Multi-head attention solves this by
            running <strong>multiple attention functions in
            parallel</strong>, each with its own learned
            projections.</p>
            <h4 id="the-formula">The Formula</h4>
            <p><span class="math display">\[\text{MultiHead}(Q, K, V) =
            \text{Concat}(\text{head}_1, \ldots,
            \text{head}_h)W_O\]</span></p>
            <p>where <span class="math inline">\(\text{head}_i =
            \text{Attention}(QW_Q^i, KW_K^i, VW_V^i)\)</span></p>
            <p>Each head <span class="math inline">\(i\)</span> has its
            own projection matrices <span
            class="math inline">\(W_Q^i\)</span>, <span
            class="math inline">\(W_K^i\)</span>, <span
            class="math inline">\(W_V^i\)</span>. This is crucial ‚Äî
            different projections mean each head learns to look for
            different patterns in the same input.</p>
            <h4 id="what-do-different-heads-actually-learn">What Do
            Different Heads Actually Learn?</h4>
            <p>Research analyzing transformer heads has found remarkable
            specialization:</p>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 42%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Head Type</th>
            <th>What It Learns</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Positional Head</strong></td>
            <td>Attends to adjacent tokens</td>
            <td>‚Äúthe‚Äù ‚Üí next word, previous word</td>
            </tr>
            <tr>
            <td><strong>Syntactic Head</strong></td>
            <td>Subject-verb agreement</td>
            <td>‚Äúdogs‚Äù ‚Üí ‚Äúrun‚Äù (plural match)</td>
            </tr>
            <tr>
            <td><strong>Coreference Head</strong></td>
            <td>Pronoun resolution</td>
            <td>‚ÄúJohn said <strong>he</strong>‚Ä¶‚Äù ‚Üí ‚Äúhe‚Äù attends to
            ‚ÄúJohn‚Äù</td>
            </tr>
            <tr>
            <td><strong>Semantic Head</strong></td>
            <td>Related concepts</td>
            <td>‚Äúdoctor‚Äù ‚Üí ‚Äúpatient‚Äù, ‚Äúhospital‚Äù</td>
            </tr>
            <tr>
            <td><strong>Copy Head</strong></td>
            <td>Rare/proper nouns</td>
            <td>Names, technical terms repeated</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Worked Example</strong>: ‚ÄúThe programmer who
            wrote the code debugged it.‚Äù</p>
            <pre><code>Head 1 (Syntactic):   &quot;debugged&quot; strongly attends to &quot;programmer&quot; (subject)
Head 2 (Coreference): &quot;it&quot; strongly attends to &quot;code&quot; (referent)
Head 3 (Local):       &quot;the&quot; attends to &quot;code&quot; (adjacent)
Head 4 (Semantic):    &quot;debugged&quot; attends to &quot;code&quot; (related concept)</code></pre>
            <p>Each head produces a 64-dimensional output. Concatenating
            8 heads gives 512 dimensions, which <span
            class="math inline">\(W_O\)</span> projects back to <span
            class="math inline">\(d_{model}\)</span>.</p>
            <h4 id="why-not-just-use-one-big-head">Why Not Just Use One
            Big Head?</h4>
            <p>You might wonder: ‚ÄúWhy 8 heads of dimension 64 instead of
            1 head of dimension 512?‚Äù</p>
            <ol type="1">
            <li><p><strong>Diversity</strong>: Multiple smaller
            subspaces can learn diverse patterns. One big head tends to
            collapse to a single dominant pattern.</p></li>
            <li><p><strong>Specialization</strong>: Empirically, heads
            specialize. Ablation studies show different heads contribute
            to different tasks.</p></li>
            <li><p><strong>Computational parity</strong>: The total
            computation is the same! <span class="math inline">\(8
            \times 64 \times 64 = 1 \times 512 \times 64\)</span>
            (roughly). But multiple heads give more representational
            power.</p></li>
            <li><p><strong>Redundancy for robustness</strong>: If one
            head learns a noisy pattern, others can compensate. Dropout
            on heads during training encourages this.</p></li>
            </ol>
            <p><strong>Dimensions</strong>:</p>
            <ul>
            <li><span class="math inline">\(d_{model}\)</span> = 512
            (model dimension)</li>
            <li><span class="math inline">\(h\)</span> = 8 (number of
            heads)</li>
            <li><span class="math inline">\(d_k = d_v = d_{model}/h =
            64\)</span> (per-head dimension)</li>
            </ul>
            <h3 id="attention-variants-modern">Attention Variants
            (Modern)</h3>
            <table>
            <thead>
            <tr>
            <th>Variant</th>
            <th>What it does</th>
            <th>Memory</th>
            <th>Speed</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Multi-Head (MHA)</strong></td>
            <td>Separate K,V per head</td>
            <td>High</td>
            <td>Baseline</td>
            </tr>
            <tr>
            <td><strong>Multi-Query (MQA)</strong></td>
            <td>Shared K,V across heads</td>
            <td>Low</td>
            <td>Fast</td>
            </tr>
            <tr>
            <td><strong>Grouped-Query (GQA)</strong></td>
            <td>Groups share K,V</td>
            <td>Medium</td>
            <td>Medium</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why this matters</strong>: K,V caching for
            inference. GQA (used in LLaMA 2) is a good middle
            ground.</p>
            <h3 id="positional-encoding">Positional Encoding</h3>
            <p>Here‚Äôs a subtle but critical problem: attention computes
            <span class="math inline">\(\text{softmax}(QK^T)V\)</span>,
            which is a weighted sum over all positions. But weighted
            sums don‚Äôt know about order ‚Äî if you shuffle the input
            tokens, the attention output changes, but the
            <em>mechanism</em> treats position 1 and position 100
            identically. Mathematically, attention is
            <strong>permutation equivariant</strong>: swapping inputs
            swaps outputs in the same way, but the model can‚Äôt
            distinguish ‚ÄúThe cat sat on the mat‚Äù from ‚Äúmat the on sat
            cat The‚Äù without help.</p>
            <p>This is a problem because word order matters enormously:
            ‚Äúdog bites man‚Äù and ‚Äúman bites dog‚Äù have opposite
            meanings!</p>
            <p><strong>The solution</strong>: Explicitly inject
            positional information by adding a <strong>positional
            encoding</strong> to each token‚Äôs embedding before feeding
            it to the Transformer.</p>
            <p>The original Transformer uses sinusoidal functions: <span
            class="math display">\[PE_{(pos, 2i)} =
            \sin\left(\frac{pos}{10000^{2i/d}}\right)\]</span></p>
            <p><span class="math display">\[PE_{(pos, 2i+1)} =
            \cos\left(\frac{pos}{10000^{2i/d}}\right)\]</span></p>
            <p><strong>Why sinusoidal encodings are clever</strong>:</p>
            <ol type="1">
            <li><p><strong>Extrapolation</strong>: Unlike learned
            embeddings which fail on unseen positions, sinusoids are
            defined for any position. A model trained on sequences of
            length 512 can theoretically process length 1024.</p></li>
            <li><p><strong>Relative positions via linear
            transformation</strong>: For any fixed offset <span
            class="math inline">\(k\)</span>, the encoding at position
            <span class="math inline">\(pos + k\)</span> can be
            expressed as a linear transformation of the encoding at
            <span class="math inline">\(pos\)</span>. This means the
            model can learn to compute ‚Äúwhat‚Äôs 3 positions ahead?‚Äù
            through attention weights.</p></li>
            <li><p><strong>Multi-scale patterns</strong>: Different
            dimensions correspond to different frequencies ‚Äî some
            dimensions change every position (high frequency), others
            change slowly over hundreds of positions (low frequency).
            This creates a rich representation where nearby positions
            are similar but distinguishable.</p></li>
            </ol>
            <hr />
            <h3 id="rotary-position-embedding-rope">Rotary Position
            Embedding (RoPE)</h3>
            <p>Modern transformers (LLaMA, Qwen, Mistral) use
            <strong>RoPE</strong> instead of absolute positional
            encodings. To understand why RoPE is elegant, let‚Äôs build up
            the intuition step by step.</p>
            <h4 id="the-clock-analogy-why-rotation-encodes-position">The
            Clock Analogy: Why Rotation Encodes Position</h4>
            <p>Imagine a clock. When the hour hand points to 3, you
            immediately know 3 hours have passed since midnight. The
            hand‚Äôs <strong>angle</strong> encodes the
            <strong>time</strong>. This is exactly how RoPE works ‚Äî
            position is encoded by rotation angle.</p>
            <p>But a single clock hand can only distinguish 12 positions
            before wrapping around. What if you need to tell apart
            positions 1 vs 1001? This is where RoPE‚Äôs genius lies:
            <strong>multiple clock hands rotating at different
            speeds</strong>.</p>
            <p>Think of it as having:</p>
            <ul>
            <li>A <strong>fast hand</strong> that completes a full
            rotation every 10 positions (fine-grained: distinguishes
            position 1 from position 2)</li>
            <li>A <strong>medium hand</strong> that rotates once per 100
            positions (medium-grained)</li>
            <li>A <strong>slow hand</strong> that rotates once per
            10,000 positions (coarse-grained: distinguishes position 100
            from position 10,000)</li>
            </ul>
            <p>Together, these hands create a unique ‚Äútime signature‚Äù
            for every position ‚Äî just like combining hours, minutes, and
            seconds gives unique times throughout the day.</p>
            <h4 id="d-rotation-the-building-block">2D Rotation: The
            Building Block</h4>
            <p>RoPE operates on <strong>pairs of dimensions</strong>.
            Each pair is a 2D plane that gets rotated. The rotation
            matrix for angle <span class="math inline">\(\theta\)</span>
            is:</p>
            <p><span class="math display">\[R_\theta = \begin{bmatrix}
            \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta
            \end{bmatrix}\]</span></p>
            <p>Multiplying a 2D vector <span class="math inline">\([x,
            y]^T\)</span> by <span
            class="math inline">\(R_\theta\)</span> rotates it
            counterclockwise by angle <span
            class="math inline">\(\theta\)</span>:</p>
            <pre><code>Original vector:  (1, 0)  ‚Üí  points right
Rotate by 90¬∞:    (0, 1)  ‚Üí  points up
Rotate by 180¬∞:   (-1, 0) ‚Üí  points left</code></pre>
            <h4 id="how-rope-applies-this-to-transformers">How RoPE
            Applies This to Transformers</h4>
            <p>For a query/key vector with dimension <span
            class="math inline">\(d\)</span> (e.g., 6), RoPE splits it
            into <span class="math inline">\(d/2\)</span> pairs (e.g., 3
            pairs). Each pair <span class="math inline">\(p\)</span> is
            rotated by a position-dependent angle:</p>
            <p><span class="math display">\[\text{RoPE}(q_t^{(p)}) =
            \begin{bmatrix} \cos(\theta_p t) &amp; -\sin(\theta_p t) \\
            \sin(\theta_p t) &amp; \cos(\theta_p t) \end{bmatrix}
            \begin{bmatrix} q_t^{(2p-1)} \\ q_t^{(2p)}
            \end{bmatrix}\]</span></p>
            <p>The rotation frequency for pair <span
            class="math inline">\(p\)</span> is: <span
            class="math display">\[\theta_p =
            \frac{1}{\Theta^{2(p-1)/d_q}}\]</span></p>
            <p>where <span class="math inline">\(\Theta = 10000\)</span>
            (original) or 500,000+ (for long-context models like LLaMA
            3).</p>
            <p><strong>The key insight</strong>: Earlier pairs (small
            <span class="math inline">\(p\)</span>) have <strong>high
            frequency</strong> rotations (fast clock hands), while later
            pairs (large <span class="math inline">\(p\)</span>) have
            <strong>low frequency</strong> rotations (slow clock hands).
            This creates a multi-scale positional encoding.</p>
            <h4 id="worked-example-6-dim-vector-at-position-100">Worked
            Example: 6-dim Vector at Position 100</h4>
            <p>Let‚Äôs trace through RoPE step by step for <span
            class="math inline">\(\Theta = 10000\)</span>:</p>
            <pre><code>Input: q‚ÇÅ‚ÇÄ‚ÇÄ = [0.8, 0.6, 0.7, 0.3, 0.5, 0.4]
       Split into 3 pairs: (0.8, 0.6), (0.7, 0.3), (0.5, 0.4)

Pair 1: Œ∏‚ÇÅ = 1/10000^(0/6) = 1.0
        Angle = Œ∏‚ÇÅ √ó 100 = 100 radians
        cos(100) ‚âà 0.86, sin(100) ‚âà -0.51
        Rotated: [0.8√ó0.86 - 0.6√ó(-0.51), 0.8√ó(-0.51) + 0.6√ó0.86]
                ‚âà [0.99, 0.11]

Pair 2: Œ∏‚ÇÇ = 1/10000^(2/6) ‚âà 0.0464
        Angle = 0.0464 √ó 100 = 4.64 radians
        cos(4.64) ‚âà -0.07, sin(4.64) ‚âà -1.00
        Rotated: [0.7√ó(-0.07) - 0.3√ó(-1.00), 0.7√ó(-1.00) + 0.3√ó(-0.07)]
                ‚âà [0.25, -0.72]

Pair 3: Œ∏‚ÇÉ = 1/10000^(4/6) ‚âà 0.0022
        Angle = 0.0022 √ó 100 = 0.22 radians (slow rotation!)
        cos(0.22) ‚âà 0.98, sin(0.22) ‚âà 0.21
        Rotated: [0.5√ó0.98 - 0.4√ó0.21, 0.5√ó0.21 + 0.4√ó0.98]
                ‚âà [0.40, 0.50]

Output: RoPE(q‚ÇÅ‚ÇÄ‚ÇÄ) ‚âà [0.99, 0.11, 0.25, -0.72, 0.40, 0.50]</code></pre>
            <p>Notice how Pair 1 rotated dramatically (100 radians = ~16
            full rotations), while Pair 3 barely moved (0.22 radians ‚âà
            13¬∞). This multi-scale behavior is crucial for
            distinguishing both nearby and distant positions.</p>
            <h4 id="why-only-q-and-k-get-rope-not-v">Why Only Q and K
            Get RoPE (Not V)</h4>
            <p>This is a common point of confusion. Think about what
            each vector does:</p>
            <ul>
            <li><strong>Q (Query)</strong>: ‚ÄúWhat am I looking
            for?‚Äù</li>
            <li><strong>K (Key)</strong>: ‚ÄúWhat do I contain?‚Äù</li>
            <li><strong>V (Value)</strong>: ‚ÄúHere‚Äôs my actual
            content‚Äù</li>
            </ul>
            <p>The attention score is <span class="math inline">\(q
            \cdot k\)</span> ‚Äî this is where <strong>positional
            similarity</strong> matters. When <span
            class="math inline">\(q\)</span> at position 5 looks at
            <span class="math inline">\(k\)</span> at position 3, we
            want the dot product to reflect that they‚Äôre 2 positions
            apart.</p>
            <p>RoPE achieves this beautifully: when both <span
            class="math inline">\(q\)</span> and <span
            class="math inline">\(k\)</span> are rotated, the dot
            product <span class="math inline">\(\text{RoPE}(q_m)^T \cdot
            \text{RoPE}(k_n)\)</span> depends only on the
            <strong>relative position</strong> <span
            class="math inline">\((m - n)\)</span>, not the absolute
            positions. It‚Äôs like two clock hands ‚Äî their relative angle
            tells you the time difference, regardless of what time it
            currently is.</p>
            <p><strong>Values, however, are content carriers.</strong>
            Once attention weights determine ‚Äúhow much‚Äù to attend to
            each position, the Values just deliver the information.
            There‚Äôs no reason to rotate them ‚Äî they don‚Äôt participate in
            the ‚Äúwhere to look‚Äù computation.</p>
            <h4 id="why-rope-is-powerful-summary">Why RoPE is Powerful
            (Summary)</h4>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 67%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>How RoPE Achieves It</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Relative position</strong></td>
            <td><span class="math inline">\(q \cdot k\)</span> depends
            on <span class="math inline">\((m-n)\)</span> through
            rotation math</td>
            </tr>
            <tr>
            <td><strong>Extrapolation</strong></td>
            <td>Rotations are defined for any position (no lookup
            table)</td>
            </tr>
            <tr>
            <td><strong>Multi-scale</strong></td>
            <td>Different frequency per dimension pair</td>
            </tr>
            <tr>
            <td><strong>No learnable parameters</strong></td>
            <td>Just trigonometric functions ‚Äî deterministic</td>
            </tr>
            </tbody>
            </table>
            <h3 id="transformer-architecture">Transformer
            Architecture</h3>
            <p>The complete Transformer architecture consists of stacked
            encoder and decoder blocks, each containing the same core
            components arranged in a specific pattern. Let‚Äôs examine
            each component and understand its role.</p>
            <p><img src="figures/transformer_full_architecture.png"
            alt="Transformer Full Architecture" /> <em>Figure: The
            complete Transformer architecture showing the encoder (left)
            and decoder (right) with all components: embeddings,
            positional encoding, multi-head attention, feed-forward
            networks, and layer normalization with residual
            connections.</em></p>
            <p><strong>The Building Blocks</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 34%" />
            <col style="width: 28%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Component</th>
            <th>Purpose</th>
            <th>Key Detail</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Input Embedding</strong></td>
            <td>Convert tokens to vectors</td>
            <td>Learned lookup table, <span
            class="math inline">\(d_{model}\)</span> dimensions</td>
            </tr>
            <tr>
            <td><strong>Positional Encoding</strong></td>
            <td>Inject position information</td>
            <td>Added (not concatenated) to embeddings</td>
            </tr>
            <tr>
            <td><strong>Multi-Head Attention</strong></td>
            <td>Model dependencies between positions</td>
            <td>Parallel attention heads for different patterns</td>
            </tr>
            <tr>
            <td><strong>Add &amp; Norm</strong></td>
            <td>Stabilize training</td>
            <td>Residual connection + LayerNorm</td>
            </tr>
            <tr>
            <td><strong>Feed-Forward Network</strong></td>
            <td>Per-position nonlinear transformation</td>
            <td>Expands then contracts: <span
            class="math inline">\(d_{model} ‚Üí 4d_{model} ‚Üí
            d_{model}\)</span></td>
            </tr>
            <tr>
            <td><strong>Linear + Softmax</strong></td>
            <td>Output probabilities</td>
            <td>Projects to vocabulary size, softmax for
            distribution</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Encoder Block</strong> (processes input):</p>
            <pre><code>Input
  ‚Üì
[Multi-Head Self-Attention]    ‚Üê Each position attends to all input positions
  ‚Üì
[Add &amp; LayerNorm]  ‚Üê‚îÄ‚îÄ Residual connection
  ‚Üì
[Feed-Forward Network]          ‚Üê Same FFN applied independently to each position
  ‚Üì
[Add &amp; LayerNorm]  ‚Üê‚îÄ‚îÄ Residual connection
  ‚Üì
Output</code></pre>
            <p><strong>Decoder Block</strong> (generates output):</p>
            <pre><code>Input (shifted right)
  ‚Üì
[Masked Multi-Head Self-Attention]  ‚Üê Each position attends only to EARLIER positions
  ‚Üì
[Add &amp; LayerNorm]
  ‚Üì
[Multi-Head Cross-Attention]        ‚Üê Attends to encoder outputs (K,V from encoder)
  ‚Üì
[Add &amp; LayerNorm]
  ‚Üì
[Feed-Forward Network]
  ‚Üì
[Add &amp; LayerNorm]
  ‚Üì
Output</code></pre>
            <p><strong>Key differences between encoder and
            decoder</strong>: - Encoder uses
            <strong>bidirectional</strong> self-attention (sees full
            input) - Decoder uses <strong>causal</strong> (masked)
            self-attention (only sees past outputs) - Decoder has an
            extra <strong>cross-attention</strong> layer to attend to
            encoder outputs</p>
            <p><strong>Feed-Forward Network (Deep Dive)</strong>:</p>
            <p>The FFN is applied independently to each position ‚Äî it‚Äôs
            the same two-layer network applied to every position in the
            sequence. This seemingly simple component is actually where
            most of the model‚Äôs parameters live and where much of the
            ‚Äúthinking‚Äù happens.</p>
            <p><span class="math display">\[\text{FFN}(x) =
            \text{ReLU}(xW_1 + b_1)W_2 + b_2\]</span></p>
            <p>Or with GELU (smoother, used in GPT-2 onwards): <span
            class="math display">\[\text{FFN}(x) =
            \text{GELU}(xW_1)W_2\]</span></p>
            <p><strong>Dimensions</strong>: Input <span
            class="math inline">\(x\)</span> has dimension <span
            class="math inline">\(d_{model}\)</span> (e.g., 512). <span
            class="math inline">\(W_1\)</span> projects to <span
            class="math inline">\(4 \times d_{model}\)</span> (e.g.,
            2048), then <span class="math inline">\(W_2\)</span>
            projects back to <span
            class="math inline">\(d_{model}\)</span>. This expansion
            allows the network to learn complex transformations.</p>
            <p><strong>Why the 4√ó expansion?</strong></p>
            <p>This isn‚Äôt arbitrary ‚Äî it‚Äôs one of the most impactful
            design decisions:</p>
            <ol type="1">
            <li><p><strong>Expressiveness</strong>: The intermediate
            layer with 4√ó dimensions creates a higher-dimensional space
            where the network can represent complex transformations
            before projecting back down.</p></li>
            <li><p><strong>Key-Value Memory View</strong>: Research
            suggests FFN acts like a <strong>learned database</strong>.
            Think of <span class="math inline">\(W_1\)</span> as keys
            and <span class="math inline">\(W_2^T\)</span> as
            values:</p>
            <ul>
            <li>Input activates certain ‚Äúkeys‚Äù (rows of <span
            class="math inline">\(W_1\)</span>)</li>
            <li>ReLU selects which keys match</li>
            <li>Output retrieves corresponding ‚Äúvalues‚Äù (columns of
            <span class="math inline">\(W_2\)</span>)</li>
            </ul></li>
            <li><p><strong>Where factual knowledge lives</strong>:
            Studies show that editing specific rows of <span
            class="math inline">\(W_2\)</span> can change what the model
            ‚Äúknows‚Äù ‚Äî like updating a fact in a database.</p></li>
            </ol>
            <p><strong>Parameter distribution</strong>: In a typical
            transformer: - Attention: ~33% of parameters - FFN: ~67% of
            parameters (!)</p>
            <p>The FFN is deceptively important ‚Äî it‚Äôs the workhorse
            storing and transforming knowledge.</p>
            <p><strong>Residual Connections</strong>:</p>
            <p>Every sub-layer (attention, FFN) is wrapped with a
            residual connection: <span
            class="math display">\[\text{output} = x +
            \text{Sublayer}(x)\]</span></p>
            <p>This enables: 1. <strong>Gradient flow</strong>:
            Gradients can skip sublayers via the identity path 2.
            <strong>Easy identity learning</strong>: If a sublayer isn‚Äôt
            useful, it can learn to output zero 3. <strong>Deep
            networks</strong>: Stack dozens of layers without vanishing
            gradients</p>
            <hr />
            <h3
            id="why-residual-connections-prevent-vanishing-gradients-math">Why
            Residual Connections Prevent Vanishing Gradients (Math)</h3>
            <p>Before diving into the math, let‚Äôs understand
            <em>why</em> multiplication causes vanishing gradients.
            Think of it like a game of telephone where each person
            whispers at half volume ‚Äî after 10 people, the message is
            nearly inaudible.</p>
            <h4 id="the-highway-analogy">The Highway Analogy</h4>
            <p>Imagine two routes from your house to downtown:</p>
            <p><strong>Route A (No residuals)</strong>: Take local roads
            through 32 traffic lights. At each light, there‚Äôs a 50%
            chance you lose 10 minutes. After 32 lights, you‚Äôre almost
            certainly massively delayed.</p>
            <p><strong>Route B (With residuals)</strong>: A toll-free
            express highway runs parallel to Route A. You can hop on
            anytime, bypass all the lights, and arrive quickly. Even if
            some segments of Route A are slow, you always have the
            highway option.</p>
            <p>Residual connections are the highway ‚Äî gradients can flow
            through the ‚Äúshortcut‚Äù path even when the main path is
            congested.</p>
            <h4 id="the-math-step-by-step">The Math: Step by Step</h4>
            <p>Consider a simple 3-layer network without nonlinearities
            (to isolate the core issue):</p>
            <p><span class="math display">\[z = f_1(x) = w_1 x\]</span>
            <span class="math display">\[r = f_2(z) = w_2 z\]</span>
            <span class="math display">\[y = f_3(r) = w_3 r\]</span></p>
            <p>So <span class="math inline">\(y = w_3 w_2 w_1 x\)</span>
            ‚Äî just multiplication!</p>
            <p><strong>The chain rule</strong> gives us: <span
            class="math display">\[\frac{\partial L}{\partial w_1} =
            \frac{\partial L}{\partial y} \cdot \frac{\partial
            y}{\partial r} \cdot \frac{\partial r}{\partial z} \cdot
            \frac{\partial z}{\partial w_1}\]</span></p>
            <p><span class="math display">\[= \frac{\partial L}{\partial
            y} \cdot w_3 \cdot w_2 \cdot x\]</span></p>
            <p>If <span class="math inline">\(w_2 = w_3 = 0.5\)</span>:
            <span class="math display">\[\frac{\partial L}{\partial w_1}
            = \frac{\partial L}{\partial y} \cdot 0.5 \cdot 0.5 \cdot x
            = \frac{\partial L}{\partial y} \cdot 0.25 \cdot
            x\]</span></p>
            <p>Not too bad for 2 layers. But for 32 layers: <span
            class="math display">\[\frac{\partial L}{\partial w_1} =
            \frac{\partial L}{\partial y} \cdot (0.5)^{32} \cdot x
            \approx \frac{\partial L}{\partial y} \cdot 2.3 \times
            10^{-10} \cdot x\]</span></p>
            <p><strong>The gradient is essentially zero!</strong> The
            first layer never learns.</p>
            <h4 id="now-add-residual-connections">Now Add Residual
            Connections</h4>
            <p>With residuals, each layer becomes: <span
            class="math inline">\(f(z) = w \cdot z + z\)</span> (the
            <span class="math inline">\(+z\)</span> is the residual)</p>
            <p>For our 3-layer network: <span class="math display">\[z =
            w_1 x\]</span> <span class="math display">\[r = w_2 z + z =
            (w_2 + 1) z\]</span> <span class="math display">\[y = w_3 r
            + r = (w_3 + 1) r\]</span></p>
            <p>Expanding: <span class="math inline">\(y = (w_3 + 1)(w_2
            + 1)w_1 x\)</span></p>
            <p>When we compute <span
            class="math inline">\(\frac{\partial y}{\partial
            w_1}\)</span> and apply the chain rule: <span
            class="math display">\[\frac{\partial L}{\partial w_1} =
            \frac{\partial L}{\partial y} \cdot (w_3 + 1)(w_2 + 1) \cdot
            x\]</span></p>
            <p>Expand <span class="math inline">\((w_3 + 1)(w_2 +
            1)\)</span>: <span class="math display">\[= w_3 w_2 + w_3 +
            w_2 + \mathbf{1}\]</span></p>
            <p><strong>The +1 term is the key!</strong> Even if <span
            class="math inline">\(w_2 = w_3 = 0\)</span>, we still have:
            <span class="math display">\[\frac{\partial L}{\partial w_1}
            = \frac{\partial L}{\partial y} \cdot 1 \cdot x\]</span></p>
            <p>The gradient flows through unchanged via the residual
            path!</p>
            <h4 id="numerical-comparison">Numerical Comparison</h4>
            <p>With <span class="math inline">\(w_2 = w_3 =
            0.5\)</span>:</p>
            <table>
            <thead>
            <tr>
            <th>Configuration</th>
            <th>Gradient Factor</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Without residuals</td>
            <td><span class="math inline">\(0.5 \times 0.5 =
            0.25\)</span></td>
            </tr>
            <tr>
            <td>With residuals</td>
            <td><span class="math inline">\(0.25 + 0.5 + 0.5 + 1 =
            \mathbf{2.25}\)</span></td>
            </tr>
            </tbody>
            </table>
            <p>The gradient is <strong>9√ó stronger</strong> with
            residuals! And this advantage compounds exponentially with
            depth. This is why we can train 100+ layer transformers
            without vanishing gradients.</p>
            <hr />
            <h3 id="rmsnorm-root-mean-square-normalization">RMSNorm
            (Root Mean Square Normalization)</h3>
            <h4 id="why-normalize-at-all">Why Normalize at All?</h4>
            <p>Deep networks face a problem called <strong>internal
            covariate shift</strong>: as earlier layers update during
            training, the distribution of inputs to later layers
            constantly changes. Imagine trying to hit a moving target ‚Äî
            every time you adjust, the target moves again.</p>
            <p><strong>Normalization</strong> stabilizes this by
            ensuring each layer receives inputs with consistent
            statistical properties. But the question becomes: <em>what
            statistics should we normalize?</em></p>
            <h4 id="layernorm-the-kitchen-sink-approach">LayerNorm: The
            Kitchen Sink Approach</h4>
            <p>LayerNorm (the original) does two things: 1.
            <strong>Centers</strong> the distribution (subtracts the
            mean) 2. <strong>Scales</strong> the distribution (divides
            by standard deviation)</p>
            <p><span class="math display">\[\text{LayerNorm}(x) = \gamma
            \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} +
            \beta\]</span></p>
            <p>This gives you a distribution with mean = 0 and variance
            = 1 (before the learnable <span
            class="math inline">\(\gamma\)</span> and <span
            class="math inline">\(\beta\)</span>).</p>
            <h4 id="rmsnorm-simpler-is-better">RMSNorm: Simpler is
            Better</h4>
            <p>Researchers at Meta (then Facebook) asked: <em>do we
            really need to subtract the mean?</em> It turns out, for
            transformers, the answer is <strong>no</strong>.</p>
            <p>RMSNorm only scales ‚Äî no centering:</p>
            <p><span class="math display">\[\text{RMS}(x) =
            \sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2}\]</span> <span
            class="math display">\[\text{RMSNorm}(x) = \gamma \odot
            \frac{x}{\text{RMS}(x)}\]</span></p>
            <h4 id="worked-example-rmsnorm-on-a-3-element-vector">Worked
            Example: RMSNorm on a 3-Element Vector</h4>
            <p>Let‚Äôs normalize <span class="math inline">\(x = [3, 4,
            0]\)</span> with <span class="math inline">\(\gamma = [1, 1,
            1]\)</span>:</p>
            <p><strong>Step 1</strong>: Compute RMS <span
            class="math display">\[\text{RMS}(x) = \sqrt{\frac{3^2 + 4^2
            + 0^2}{3}} = \sqrt{\frac{9 + 16 + 0}{3}} =
            \sqrt{\frac{25}{3}} \approx 2.89\]</span></p>
            <p><strong>Step 2</strong>: Normalize by RMS <span
            class="math display">\[\frac{x}{\text{RMS}(x)} = \frac{[3,
            4, 0]}{2.89} = [1.04, 1.38, 0]\]</span></p>
            <p><strong>Step 3</strong>: Apply learnable scale <span
            class="math inline">\(\gamma\)</span> <span
            class="math display">\[\text{RMSNorm}(x) = [1, 1, 1] \odot
            [1.04, 1.38, 0] = [1.04, 1.38, 0]\]</span></p>
            <p>Compare to LayerNorm (which would first subtract <span
            class="math inline">\(\mu = 7/3 \approx 2.33\)</span>): -
            LayerNorm would shift the values: <span
            class="math inline">\(x - \mu = [0.67, 1.67, -2.33]\)</span>
            - Then scale by standard deviation</p>
            <p><strong>The key insight</strong>: For transformers, the
            <em>relative magnitudes</em> of activations matter more than
            their absolute values. RMSNorm preserves this while being
            computationally cheaper.</p>
            <h4 id="why-rmsnorm-works-for-transformers">Why RMSNorm
            Works for Transformers</h4>
            <table>
            <thead>
            <tr>
            <th>Property</th>
            <th>LayerNorm</th>
            <th>RMSNorm</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Centers data (subtracts mean)</td>
            <td>‚úì</td>
            <td>‚úó</td>
            </tr>
            <tr>
            <td>Scales data (divides by spread)</td>
            <td>‚úì</td>
            <td>‚úì</td>
            </tr>
            <tr>
            <td>Learnable scale (<span
            class="math inline">\(\gamma\)</span>)</td>
            <td>‚úì</td>
            <td>‚úì</td>
            </tr>
            <tr>
            <td>Learnable shift (<span
            class="math inline">\(\beta\)</span>)</td>
            <td>‚úì</td>
            <td>‚úó</td>
            </tr>
            <tr>
            <td>Computational cost</td>
            <td>Higher</td>
            <td><strong>Lower</strong></td>
            </tr>
            <tr>
            <td>Performance on transformers</td>
            <td>Good</td>
            <td><strong>Equal</strong></td>
            </tr>
            </tbody>
            </table>
            <p>The shift (<span class="math inline">\(\beta\)</span>)
            and centering aren‚Äôt needed because: 1. Attention is
            <strong>translation-invariant</strong> ‚Äî adding a constant
            to all values doesn‚Äôt change softmax outputs 2. The
            subsequent linear layer can learn any necessary shift</p>
            <h4 id="placement-pre-ln-vs-post-ln">Placement: Pre-LN vs
            Post-LN</h4>
            <p>Modern transformers use <strong>Pre-LN</strong>
            (normalize <em>before</em> the sublayer):</p>
            <pre><code>x ‚Üí RMSNorm ‚Üí Self-Attention ‚Üí + ‚Üê x (residual)
                               ‚Üì
                           RMSNorm ‚Üí FFN ‚Üí + ‚Üê (residual)</code></pre>
            <p>This is more stable than Post-LN (normalize
            <em>after</em> adding residual) because: - Gradients flow
            through the residual path unimpeded - No need for learning
            rate warmup - More predictable training dynamics</p>
            <h3 id="causal-masking-deep-dive">Causal Masking (Deep
            Dive)</h3>
            <p>For decoder/language models, we must prevent attending to
            future tokens ‚Äî the model can‚Äôt ‚Äúcheat‚Äù by looking ahead
            during training.</p>
            <h4 id="why-causal-masking-is-necessary">Why Causal Masking
            is Necessary</h4>
            <p>During language model training, we predict the next token
            given previous tokens. If position <span
            class="math inline">\(i\)</span> could attend to position
            <span class="math inline">\(j &gt; i\)</span> (a future
            position), it would be learning to copy the answer rather
            than predict it!</p>
            <p><strong>Visual: The Causal Mask Matrix</strong></p>
            <p>For a sequence of 5 tokens, the mask looks like:</p>
            <pre><code>           Keys (j)
         1    2    3    4    5
    1 [  0   -‚àû   -‚àû   -‚àû   -‚àû ]   ‚Üê &quot;The&quot; can only see itself
Q   2 [  0    0   -‚àû   -‚àû   -‚àû ]   ‚Üê &quot;cat&quot; sees &quot;The&quot;, itself
u   3 [  0    0    0   -‚àû   -‚àû ]   ‚Üê &quot;sat&quot; sees &quot;The&quot;, &quot;cat&quot;, itself
e   4 [  0    0    0    0   -‚àû ]   ‚Üê &quot;on&quot; sees everything before
r   5 [  0    0    0    0    0 ]   ‚Üê &quot;mat&quot; sees full history
y
(i)</code></pre>
            <p>The lower triangle is 0 (allowed), the upper triangle is
            <span class="math inline">\(-\infty\)</span> (blocked).</p>
            <p><span class="math display">\[\text{mask}_{ij} =
            \begin{cases} 0 &amp; i \geq j \\ -\infty &amp; i &lt; j
            \end{cases}\]</span></p>
            <h4 id="how-it-works-with-attention">How It Works with
            Attention</h4>
            <p>The mask is <strong>added</strong> to the attention
            scores before softmax: <span
            class="math display">\[\text{Attention} =
            \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} +
            \text{mask}\right)V\]</span></p>
            <p><strong>Why <span class="math inline">\(-\infty\)</span>
            and not 0?</strong></p>
            <ul>
            <li><span class="math inline">\(e^{-\infty} = 0\)</span> ‚Üí
            After softmax, masked positions have <strong>zero
            weight</strong></li>
            <li>If we used 0: <span class="math inline">\(e^0 =
            1\)</span> ‚Üí Masked positions would still contribute!</li>
            </ul>
            <p><strong>Example</strong>: Token 3 attending</p>
            <pre><code>Raw scores:      [2.1, 3.0, 1.5, 4.2, 0.8]
After mask:      [2.1, 3.0, 1.5, -‚àû, -‚àû]
After softmax:   [0.24, 0.59, 0.17, 0.00, 0.00]
                              ‚Üë blocked positions</code></pre>
            <h4 id="implementation-detail">Implementation Detail</h4>
            <div class="sourceCode" id="cb242"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create causal mask (upper triangular of True values)</span></span>
<span id="cb242-2"><a href="#cb242-2" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> torch.triu(torch.ones(seq_len, seq_len), diagonal<span class="op">=</span><span class="dv">1</span>).<span class="bu">bool</span>()</span>
<span id="cb242-3"><a href="#cb242-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb242-4"><a href="#cb242-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to attention scores</span></span>
<span id="cb242-5"><a href="#cb242-5" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> scores.masked_fill(mask, <span class="bu">float</span>(<span class="st">&#39;-inf&#39;</span>))</span>
<span id="cb242-6"><a href="#cb242-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb242-7"><a href="#cb242-7" aria-hidden="true" tabindex="-1"></a><span class="co"># After softmax, masked positions become 0</span></span>
<span id="cb242-8"><a href="#cb242-8" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span></code></pre></div>
            <hr />
            <h3
            id="decoder-only-vs-encoder-decoder-deep-dive">Decoder-Only
            vs Encoder-Decoder (Deep Dive)</h3>
            <p>This architectural choice has become one of the most
            important in modern LLMs. Let‚Äôs understand why
            <strong>decoder-only won</strong> for large-scale language
            models.</p>
            <h4
            id="encoder-decoder-original-transformer-t5-bart">Encoder-Decoder
            (Original Transformer, T5, BART)</h4>
            <pre><code>Input: &quot;Translate: I love cats&quot;
                ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ      ENCODER         ‚îÇ
    ‚îÇ   (bidirectional)    ‚îÇ  ‚Üê sees full input
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì K, V
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ      DECODER         ‚îÇ
    ‚îÇ   (causal + cross)   ‚îÇ  ‚Üê generates output
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
Output: &quot;J&#39;aime les chats&quot;</code></pre>
            <p><strong>Components</strong>:</p>
            <ul>
            <li><strong>Encoder</strong>: Bidirectional self-attention
            (sees full input at once)</li>
            <li><strong>Decoder</strong>: Causal self-attention +
            cross-attention to encoder</li>
            <li><strong>Cross-attention</strong>: Keys/Values from
            encoder, Queries from decoder</li>
            </ul>
            <p><strong>Use cases</strong>: Translation, summarization ‚Äî
            tasks with clear input/output separation.</p>
            <h4 id="decoder-only-gpt-llama-claude">Decoder-Only (GPT,
            LLaMA, Claude)</h4>
            <pre><code>Input + Output: &quot;The capital of France is [GENERATE‚Üí]&quot;
                         ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ        DECODER           ‚îÇ
         ‚îÇ    (causal attention)    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
               &quot;Paris&quot;</code></pre>
            <p><strong>Single component</strong>:</p>
            <ul>
            <li>Only causal (masked) self-attention</li>
            <li>Each position attends only to previous positions</li>
            <li>Input and output are concatenated in the same
            sequence</li>
            </ul>
            <h4 id="why-decoder-only-won-for-llms">Why Decoder-Only Won
            for LLMs</h4>
            <table>
            <colgroup>
            <col style="width: 20%" />
            <col style="width: 43%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Factor</th>
            <th>Encoder-Decoder</th>
            <th>Decoder-Only</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Simplicity</strong></td>
            <td>Two separate modules</td>
            <td>One module</td>
            </tr>
            <tr>
            <td><strong>Parameter efficiency</strong></td>
            <td>Params split between enc/dec</td>
            <td>All params in one stack</td>
            </tr>
            <tr>
            <td><strong>Scaling</strong></td>
            <td>Cross-attention adds overhead</td>
            <td>Scales smoothly</td>
            </tr>
            <tr>
            <td><strong>Flexibility</strong></td>
            <td>Best for seq2seq tasks</td>
            <td>Handles any text task</td>
            </tr>
            <tr>
            <td><strong>Training</strong></td>
            <td>Needs paired data</td>
            <td>Trains on raw text</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The key insight</strong>: With enough scale and
            prompting, decoder-only models can do <em>everything</em>
            encoder-decoder models can do ‚Äî plus more. For
            translation:</p>
            <ul>
            <li>Encoder-decoder: Requires parallel corpus, separate
            encodings</li>
            <li>Decoder-only: Just prompt ‚ÄúTranslate to French: I love
            cats ‚Üí‚Äù</li>
            </ul>
            <p><strong>Scaling properties</strong>: When you double the
            compute budget:</p>
            <ul>
            <li>Encoder-decoder: Need to decide how to split between
            encoder and decoder</li>
            <li>Decoder-only: Just add more decoder layers ‚Äî simpler
            hyperparameter tuning</li>
            </ul>
            <p><strong>Modern consensus</strong>: Decoder-only for
            general-purpose LLMs, encoder-decoder for specific seq2seq
            tasks where you have supervised data.</p>
            <h3 id="layernorm-placement">LayerNorm Placement</h3>
            <p><strong>Post-LN</strong> (Original): <span
            class="math display">\[x&#39; = \text{LayerNorm}(x +
            \text{Sublayer}(x))\]</span></p>
            <p><strong>Pre-LN</strong> (Modern, more stable): <span
            class="math display">\[x&#39; = x +
            \text{Sublayer}(\text{LayerNorm}(x))\]</span></p>
            <p>Pre-LN is more stable for training deep transformers
            without warmup.</p>
            <h3
            id="interview-q-walk-me-through-how-attention-works">Interview
            Q: ‚ÄúWalk me through how attention works‚Äù</h3>
            <p><strong>A</strong>:</p>
            <ol type="1">
            <li>Each input token is projected into Query, Key, and Value
            vectors via learned linear transformations</li>
            <li>For each query, compute dot product with all keys to get
            relevance scores</li>
            <li>Scale by <span class="math inline">\(\sqrt{d_k}\)</span>
            to prevent gradient issues in softmax</li>
            <li>Apply softmax to get attention weights
            (probabilities)</li>
            <li>Weighted sum of value vectors gives the output</li>
            <li>Multi-head attention does this h times in parallel with
            different projections</li>
            <li>Results are concatenated and projected back</li>
            </ol>
            <p>The key insight is that attention allows direct
            connections between any positions, unlike RNNs which must
            pass information sequentially.</p>
            <hr />
            <h2 id="decoding-strategies">6.5 Decoding Strategies</h2>
            <p>Once we‚Äôve trained a language model, we face a
            deceptively complex question: <strong>how do we actually
            generate text?</strong> The model gives us a probability
            distribution over the next token at each step, but turning
            those distributions into coherent text requires careful
            choices.</p>
            <p>The fundamental tension is between
            <strong>quality</strong> (finding high-probability
            sequences) and <strong>diversity</strong> (producing varied,
            creative outputs). Deterministic methods like greedy
            decoding and beam search optimize for probability but can
            produce repetitive, boring text. Stochastic methods like
            sampling introduce randomness but can produce incoherent
            text. The right strategy depends on the application: you
            want beam search for machine translation (correctness
            matters) but sampling with temperature for creative writing
            (diversity matters).</p>
            <h3 id="greedy-decoding">Greedy Decoding</h3>
            <p>At each step, pick the token with highest
            probability:</p>
            <p><span class="math display">\[y_t = \arg\max P(y |
            y_{&lt;t})\]</span></p>
            <p><strong>Problem</strong>: Locally optimal ‚â† globally
            optimal!</p>
            <pre><code>&quot;The dog&quot; ‚Üí P(&quot;ran&quot;) = 0.4, P(&quot;quickly&quot;) = 0.3
            Choose &quot;ran&quot;... but &quot;quickly ran&quot; might have been better!</code></pre>
            <h3 id="beam-search">Beam Search</h3>
            <p>Keep track of <strong>top-k</strong> (beam width)
            candidates at each step:</p>
            <pre><code>Step 1: Start with &lt;BOS&gt;
        ‚Üí Beam: [&quot;The&quot; (0.4), &quot;A&quot; (0.3), &quot;My&quot; (0.2)]

Step 2: Expand each candidate
        &quot;The&quot; ‚Üí [&quot;The dog&quot; (0.32), &quot;The cat&quot; (0.28), ...]
        &quot;A&quot;   ‚Üí [&quot;A bird&quot; (0.21), ...]
        &quot;My&quot;  ‚Üí [&quot;My friend&quot; (0.15), ...]
        ‚Üí Keep top-k: [&quot;The dog&quot; (0.32), &quot;The cat&quot; (0.28), &quot;A bird&quot; (0.21)]

Step 3: Continue until &lt;EOS&gt;</code></pre>
            <p><strong>Algorithm</strong>:</p>
            <div class="sourceCode" id="cb247"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beam_search(model, start_token, beam_width<span class="op">=</span><span class="dv">5</span>, max_len<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb247-2"><a href="#cb247-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize with start token</span></span>
<span id="cb247-3"><a href="#cb247-3" aria-hidden="true" tabindex="-1"></a>    beams <span class="op">=</span> [(start_token, <span class="fl">0.0</span>)]  <span class="co"># (sequence, log_prob)</span></span>
<span id="cb247-4"><a href="#cb247-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb247-5"><a href="#cb247-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_len):</span>
<span id="cb247-6"><a href="#cb247-6" aria-hidden="true" tabindex="-1"></a>        all_candidates <span class="op">=</span> []</span>
<span id="cb247-7"><a href="#cb247-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> seq, score <span class="kw">in</span> beams:</span>
<span id="cb247-8"><a href="#cb247-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> seq[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> EOS:</span>
<span id="cb247-9"><a href="#cb247-9" aria-hidden="true" tabindex="-1"></a>                all_candidates.append((seq, score))</span>
<span id="cb247-10"><a href="#cb247-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb247-11"><a href="#cb247-11" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb247-12"><a href="#cb247-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get next token probabilities</span></span>
<span id="cb247-13"><a href="#cb247-13" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> model(seq)</span>
<span id="cb247-14"><a href="#cb247-14" aria-hidden="true" tabindex="-1"></a>            top_k <span class="op">=</span> probs.topk(beam_width)</span>
<span id="cb247-15"><a href="#cb247-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb247-16"><a href="#cb247-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> token, log_p <span class="kw">in</span> top_k:</span>
<span id="cb247-17"><a href="#cb247-17" aria-hidden="true" tabindex="-1"></a>                new_seq <span class="op">=</span> seq <span class="op">+</span> [token]</span>
<span id="cb247-18"><a href="#cb247-18" aria-hidden="true" tabindex="-1"></a>                new_score <span class="op">=</span> score <span class="op">+</span> log_p</span>
<span id="cb247-19"><a href="#cb247-19" aria-hidden="true" tabindex="-1"></a>                all_candidates.append((new_seq, new_score))</span>
<span id="cb247-20"><a href="#cb247-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb247-21"><a href="#cb247-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep top beam_width candidates</span></span>
<span id="cb247-22"><a href="#cb247-22" aria-hidden="true" tabindex="-1"></a>        beams <span class="op">=</span> <span class="bu">sorted</span>(all_candidates, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[:beam_width]</span>
<span id="cb247-23"><a href="#cb247-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb247-24"><a href="#cb247-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Early stopping if all beams ended</span></span>
<span id="cb247-25"><a href="#cb247-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">all</span>(b[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> EOS <span class="cf">for</span> b <span class="kw">in</span> beams):</span>
<span id="cb247-26"><a href="#cb247-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb247-27"><a href="#cb247-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb247-28"><a href="#cb247-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beams[<span class="dv">0</span>][<span class="dv">0</span>]  <span class="co"># Return best sequence</span></span></code></pre></div>
            <p><strong>Trade-offs</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Beam Width</th>
            <th>Quality</th>
            <th>Speed</th>
            <th>Diversity</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1 (greedy)</td>
            <td>Low</td>
            <td>Fast</td>
            <td>None</td>
            </tr>
            <tr>
            <td>5-10</td>
            <td>Good</td>
            <td>Medium</td>
            <td>Medium</td>
            </tr>
            <tr>
            <td>50+</td>
            <td>Diminishing returns</td>
            <td>Slow</td>
            <td>Low (mode collapse)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Length Normalization</strong>: Longer sequences
            have lower probabilities (product of many &lt;1 values).
            Normalize:</p>
            <p><span class="math display">\[\text{score} = \frac{\log
            P(y)}{|y|^\alpha}\]</span></p>
            <p>where <span class="math inline">\(\alpha \in [0.6,
            0.7]\)</span> works well.</p>
            <h3 id="other-decoding-methods">Other Decoding Methods</h3>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 43%" />
            <col style="width: 31%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>How it works</th>
            <th>Use case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Top-k Sampling</strong></td>
            <td>Sample from top-k tokens</td>
            <td>Creative text</td>
            </tr>
            <tr>
            <td><strong>Top-p (Nucleus)</strong></td>
            <td>Sample from smallest set with cumulative prob ‚â• p</td>
            <td>Better diversity</td>
            </tr>
            <tr>
            <td><strong>Temperature</strong></td>
            <td>Scale logits by T before softmax</td>
            <td>T&lt;1: confident, T&gt;1: diverse</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Temperature effect</strong>: <span
            class="math display">\[P(y_i) = \frac{\exp(z_i / T)}{\sum_j
            \exp(z_j / T)}\]</span></p>
            <ul>
            <li><span class="math inline">\(T ‚Üí 0\)</span>: Greedy
            (argmax)</li>
            <li><span class="math inline">\(T = 1\)</span>: Standard
            softmax</li>
            <li><span class="math inline">\(T ‚Üí ‚àû\)</span>: Uniform
            distribution</li>
            </ul>
            <hr />
            <h2 id="efficient-inference-kv-cache">6.6 Efficient
            Inference: KV Cache</h2>
            <p>Transformer inference for text generation has a hidden
            inefficiency that becomes critical at scale. When generating
            text autoregressively (one token at a time), naive
            implementations redo enormous amounts of redundant
            computation. Understanding and fixing this inefficiency ‚Äî
            the <strong>KV cache</strong> ‚Äî is essential knowledge for
            anyone deploying large language models.</p>
            <h3 id="the-problem-redundant-computation">The Problem:
            Redundant Computation</h3>
            <p>In autoregressive generation, each new token requires
            attending to <strong>all previous tokens</strong>. The naive
            approach recomputes K and V for ALL tokens at every
            step:</p>
            <h4 id="without-kv-cache-naive-approach">Without KV Cache
            (Naive Approach)</h4>
            <p>Let‚Äôs trace through generating ‚ÄúThe cat sat‚Äù:</p>
            <p><strong>Step 1: Generate ‚ÄúThe‚Äù</strong></p>
            <pre><code>Input: [BOS]
Compute: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ for &quot;BOS&quot;
Attention: Q‚ÇÅ attends to K‚ÇÅ ‚Üí output hidden state ‚Üí predict &quot;The&quot;</code></pre>
            <p><strong>Step 2: Generate ‚Äúcat‚Äù</strong></p>
            <pre><code>Input: [BOS, The]
Compute: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ for &quot;BOS&quot;      ‚Üê RECOMPUTED! (wasteful)
Compute: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ for &quot;The&quot;       ‚Üê RECOMPUTED! (wasteful)
Attention: Q‚ÇÇ attends to [K‚ÇÅ, K‚ÇÇ] ‚Üí output ‚Üí predict &quot;cat&quot;</code></pre>
            <p><strong>Step 3: Generate ‚Äúsat‚Äù</strong></p>
            <pre><code>Input: [BOS, The, cat]
Compute: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ for &quot;BOS&quot;      ‚Üê RECOMPUTED AGAIN!
Compute: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ for &quot;The&quot;       ‚Üê RECOMPUTED AGAIN!
Compute: Q‚ÇÉ, K‚ÇÉ, V‚ÇÉ for &quot;cat&quot;       ‚Üê RECOMPUTED!
Attention: Q‚ÇÉ attends to [K‚ÇÅ, K‚ÇÇ, K‚ÇÉ] ‚Üí output ‚Üí predict &quot;sat&quot;</code></pre>
            <p><strong>The waste</strong>: At step <span
            class="math inline">\(n\)</span>, we recompute K and V for
            all <span class="math inline">\(n-1\)</span> previous
            tokens, even though those K,V values haven‚Äôt changed!</p>
            <h3 id="the-key-insight-k-and-v-dont-change">The Key
            Insight: K and V Don‚Äôt Change</h3>
            <p>Here‚Äôs the crucial observation:</p>
            <p><span class="math display">\[K_i = x_i \cdot W_K, \quad
            V_i = x_i \cdot W_V\]</span></p>
            <p>The K and V for token <span
            class="math inline">\(i\)</span> depend
            <strong>only</strong> on: - The token embedding <span
            class="math inline">\(x_i\)</span> (fixed once the token is
            known) - The weight matrices <span
            class="math inline">\(W_K, W_V\)</span> (fixed during
            inference)</p>
            <p>So <span class="math inline">\(K_1\)</span> computed at
            step 1 is <strong>identical</strong> to <span
            class="math inline">\(K_1\)</span> computed at step 100! Why
            redo the work?</p>
            <h3 id="the-solution-kv-cache">The Solution: KV Cache</h3>
            <p><strong>Key insight</strong>: K and V for past tokens
            don‚Äôt change. Cache them!</p>
            <h4 id="with-kv-cache-efficient">With KV Cache
            (Efficient)</h4>
            <p><strong>Step 1: Generate ‚ÄúThe‚Äù</strong></p>
            <pre><code>Input: [BOS]
Compute: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ for &quot;BOS&quot;
CACHE: Store K‚ÇÅ, V‚ÇÅ in memory
Attention: Q‚ÇÅ @ K‚ÇÅ·µÄ ‚Üí softmax ‚Üí multiply by V‚ÇÅ ‚Üí predict &quot;The&quot;</code></pre>
            <p><strong>Step 2: Generate ‚Äúcat‚Äù</strong></p>
            <pre><code>Input: &quot;The&quot; (just the new token!)
Compute: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ for &quot;The&quot; ONLY (one token, not two)
CACHE: Append K‚ÇÇ, V‚ÇÇ to cache ‚Üí Cache now has [K‚ÇÅ,K‚ÇÇ], [V‚ÇÅ,V‚ÇÇ]
Attention: Q‚ÇÇ @ [K‚ÇÅ,K‚ÇÇ]·µÄ ‚Üí softmax ‚Üí multiply by [V‚ÇÅ,V‚ÇÇ] ‚Üí predict &quot;cat&quot;</code></pre>
            <p><strong>Step 3: Generate ‚Äúsat‚Äù</strong></p>
            <pre><code>Input: &quot;cat&quot; (just the new token!)
Compute: Q‚ÇÉ, K‚ÇÉ, V‚ÇÉ for &quot;cat&quot; ONLY
CACHE: Append K‚ÇÉ, V‚ÇÉ ‚Üí Cache now has [K‚ÇÅ,K‚ÇÇ,K‚ÇÉ], [V‚ÇÅ,V‚ÇÇ,V‚ÇÉ]
Attention: Q‚ÇÉ @ [K‚ÇÅ,K‚ÇÇ,K‚ÇÉ]·µÄ ‚Üí softmax ‚Üí multiply by [V‚ÇÅ,V‚ÇÇ,V‚ÇÉ] ‚Üí predict &quot;sat&quot;</code></pre>
            <h3 id="visual-comparison">Visual Comparison</h3>
            <pre><code>WITHOUT CACHE (step 4):          WITH CACHE (step 4):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Compute K‚ÇÅ,V‚ÇÅ        ‚îÇ         ‚îÇ Read K‚ÇÅ,V‚ÇÅ from cache ‚îÇ (free!)
‚îÇ Compute K‚ÇÇ,V‚ÇÇ        ‚îÇ         ‚îÇ Read K‚ÇÇ,V‚ÇÇ from cache ‚îÇ (free!)
‚îÇ Compute K‚ÇÉ,V‚ÇÉ        ‚îÇ         ‚îÇ Read K‚ÇÉ,V‚ÇÉ from cache ‚îÇ (free!)
‚îÇ Compute K‚ÇÑ,V‚ÇÑ        ‚îÇ         ‚îÇ Compute K‚ÇÑ,V‚ÇÑ         ‚îÇ (1 token)
‚îÇ Q‚ÇÑ @ [K‚ÇÅ,K‚ÇÇ,K‚ÇÉ,K‚ÇÑ]·µÄ  ‚îÇ         ‚îÇ Q‚ÇÑ @ [K‚ÇÅ,K‚ÇÇ,K‚ÇÉ,K‚ÇÑ]·µÄ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   4 K,V computations                1 K,V computation</code></pre>
            <h3 id="why-only-cache-k-and-v-not-q">Why Only Cache K and V
            (Not Q)?</h3>
            <p><strong>Q (Query)</strong> is different: - We only need
            the Query for the <strong>current</strong> token - The
            current token‚Äôs Query asks ‚Äúwhat should I attend to?‚Äù - Past
            queries are irrelevant ‚Äî we already used them</p>
            <p><strong>K and V</strong> accumulate: - All past Keys are
            needed for the current token to find relevant positions -
            All past Values are needed to actually retrieve the
            information</p>
            <h3 id="complexity-improvement">Complexity Improvement</h3>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 12%" />
            <col style="width: 35%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Approach</th>
            <th>Per-token K,V computation</th>
            <th>Per-token attention</th>
            <th>Total for n tokens</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>No cache</td>
            <td><span class="math inline">\(O(n)\)</span> (all
            tokens)</td>
            <td><span class="math inline">\(O(n)\)</span></td>
            <td><span class="math inline">\(O(n^2)\)</span> K,V + <span
            class="math inline">\(O(n^2)\)</span> attention</td>
            </tr>
            <tr>
            <td>With cache</td>
            <td><span class="math inline">\(O(1)\)</span> (one
            token)</td>
            <td><span class="math inline">\(O(n)\)</span></td>
            <td><span class="math inline">\(O(n)\)</span> K,V + <span
            class="math inline">\(O(n^2)\)</span> attention</td>
            </tr>
            </tbody>
            </table>
            <p>The attention operation is still <span
            class="math inline">\(O(n)\)</span> per token (attending to
            <span class="math inline">\(n\)</span> past positions), but
            we eliminate the redundant K,V recomputations, which is a
            huge win.</p>
            <h3 id="memory-cost">Memory Cost</h3>
            <p>For a model with:</p>
            <ul>
            <li><span class="math inline">\(L\)</span> layers</li>
            <li><span class="math inline">\(d_{model}\)</span>
            dimension</li>
            <li>Sequence length <span
            class="math inline">\(n\)</span></li>
            <li>Batch size <span class="math inline">\(B\)</span></li>
            </ul>
            <p><strong>KV Cache size</strong>: <span
            class="math inline">\(2 \times L \times n \times d_{model}
            \times B \times \text{dtype\_size}\)</span></p>
            <p>For LLaMA 7B (<span class="math inline">\(L=32\)</span>,
            <span class="math inline">\(d=4096\)</span>), 2048 tokens,
            batch 1, fp16: <span class="math display">\[2 \times 32
            \times 2048 \times 4096 \times 2 \text{ bytes} = 1 \text{
            GB}\]</span></p>
            <h3 id="why-gqamqa-matters-for-kv-cache">Why GQA/MQA Matters
            for KV Cache</h3>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 21%" />
            <col style="width: 26%" />
            <col style="width: 19%" />
            </colgroup>
            <thead>
            <tr>
            <th>Attention Type</th>
            <th>KV heads</th>
            <th>Cache Size</th>
            <th>Quality</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Multi-Head (MHA)</td>
            <td><span class="math inline">\(h\)</span> heads</td>
            <td><span class="math inline">\(2 \times L \times n \times h
            \times d_k\)</span></td>
            <td>Best</td>
            </tr>
            <tr>
            <td>Grouped-Query (GQA)</td>
            <td><span class="math inline">\(h/g\)</span> heads</td>
            <td>Reduced by factor <span
            class="math inline">\(g\)</span></td>
            <td>Near MHA</td>
            </tr>
            <tr>
            <td>Multi-Query (MQA)</td>
            <td>1 head</td>
            <td>Reduced by factor <span
            class="math inline">\(h\)</span></td>
            <td>Good</td>
            </tr>
            </tbody>
            </table>
            <p>GQA (LLaMA 2) is a good middle ground: ~8x smaller cache
            with minimal quality loss.</p>
            <h3
            id="interview-q-how-does-kv-caching-speed-up-transformer-inference">Interview
            Q: ‚ÄúHow does KV caching speed up transformer
            inference?‚Äù</h3>
            <p><strong>A</strong>: In autoregressive generation, each
            new token needs to attend to all previous tokens. Without
            caching, we‚Äôd recompute Keys and Values for all past tokens
            at each step ‚Äî <span class="math inline">\(O(n^2)\)</span>
            per token. KV caching stores the K,V projections of past
            tokens, so we only compute K,V for the new token and reuse
            the cache ‚Äî <span class="math inline">\(O(n)\)</span> per
            token. The trade-off is memory: cache grows linearly with
            sequence length. GQA and MQA reduce cache size by sharing
            K,V across attention heads.</p>
            <hr />
            <h1 id="part-7-llm-training-pipeline">Part 7: LLM Training
            Pipeline</h1>
            <h2 id="overview-the-three-stage-pipeline">7.1 Overview: The
            Three-Stage Pipeline</h2>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM Training Pipeline                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    Stage 1          ‚îÇ    Stage 2          ‚îÇ    Stage 3              ‚îÇ
‚îÇ    PRETRAINING      ‚îÇ    SFT              ‚îÇ    ALIGNMENT            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Objective:          ‚îÇ Objective:          ‚îÇ Objective:              ‚îÇ
‚îÇ Next token          ‚îÇ Follow              ‚îÇ Align with human        ‚îÇ
‚îÇ prediction          ‚îÇ instructions        ‚îÇ preferences             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Data:               ‚îÇ Data:               ‚îÇ Data:                   ‚îÇ
‚îÇ Web crawl,          ‚îÇ Instruction-        ‚îÇ Human preference        ‚îÇ
‚îÇ books, code         ‚îÇ response pairs      ‚îÇ comparisons             ‚îÇ
‚îÇ (trillions tokens)  ‚îÇ (100k-1M examples)  ‚îÇ (10k-100k pairs)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Compute:            ‚îÇ Compute:            ‚îÇ Compute:                ‚îÇ
‚îÇ Massive             ‚îÇ Moderate            ‚îÇ Small                   ‚îÇ
‚îÇ (weeks on clusters) ‚îÇ (hours to days)     ‚îÇ (hours)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Method:             ‚îÇ Method:             ‚îÇ Methods:                ‚îÇ
‚îÇ Standard LM loss    ‚îÇ Supervised          ‚îÇ RLHF (PPO)              ‚îÇ
‚îÇ                     ‚îÇ fine-tuning         ‚îÇ DPO, GRPO               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="why-three-stages">Why Three Stages?</h3>
            <p>Modern LLM training follows a carefully designed
            progression that transforms a randomly initialized neural
            network into a helpful, safe AI assistant. Each stage builds
            on the previous one, addressing specific limitations:</p>
            <ol type="1">
            <li><strong>Pretraining</strong>: Learn language, world
            knowledge, reasoning from massive text</li>
            <li><strong>SFT</strong>: Learn to be helpful, follow
            instructions</li>
            <li><strong>Alignment</strong>: Learn human preferences
            (helpful, harmless, honest)</li>
            </ol>
            <p><strong>The conceptual progression</strong>: Think of it
            like educating a person. Pretraining is like reading every
            book in every library‚Äîyou learn facts, language patterns,
            and reasoning, but you don‚Äôt know how to have a
            conversation. SFT is like job training‚Äîyou learn the format
            of being an assistant, how to respond to questions, and what
            tasks you should help with. Alignment is like learning
            social norms and ethics‚Äîyou learn what humans actually
            prefer, what‚Äôs helpful vs.¬†harmful, and how to be genuinely
            useful rather than just technically correct.</p>
            <p><strong>Why not train everything at once?</strong> The
            stages have fundamentally different data requirements and
            objectives. Pretraining needs trillions of tokens but only
            predicts the next word. SFT needs carefully curated
            instruction-response pairs but only hundreds of thousands of
            examples. Alignment needs human preference comparisons,
            which are expensive to collect. Separating the stages allows
            each to be optimized independently with appropriate data and
            compute budgets.</p>
            <hr />
            <h2 id="pretraining">7.2 Pretraining</h2>
            <p>Pretraining is the foundation of modern LLMs‚Äîthe
            computationally intensive process where a model learns to
            understand and generate language by processing enormous
            amounts of text. This stage consumes 99%+ of total training
            compute and determines the model‚Äôs core capabilities: its
            vocabulary, world knowledge, reasoning abilities, and even
            emergent skills like in-context learning.</p>
            <p>The goal sounds deceptively simple: predict the next
            word. But to do this well across trillions of tokens
            spanning every domain of human knowledge, the model must
            develop rich internal representations of language, facts,
            logic, and even some degree of common-sense reasoning. What
            emerges from this simple objective is remarkable‚Äîa
            general-purpose language model that can be adapted to almost
            any downstream task.</p>
            <h3 id="objective-causal-language-modeling">Objective:
            Causal Language Modeling</h3>
            <p>Predict the next token given all previous tokens:</p>
            <p><span class="math display">\[P(x_1, x_2, \ldots, x_T) =
            \prod_{t=1}^{T} P(x_t | x_1, \ldots, x_{t-1})\]</span></p>
            <h3 id="loss-function-causal-language-modeling-clm">Loss
            Function: Causal Language Modeling (CLM)</h3>
            <p><strong>Cross-entropy loss</strong> over vocabulary for
            autoregressive (GPT-style) models:</p>
            <p><span class="math display">\[\mathcal{L}_{CLM} =
            -\frac{1}{T} \sum_{t=1}^{T} \log P_\theta(x_t |
            x_{&lt;t})\]</span></p>
            <p><strong>The autoregressive factorization</strong>: <span
            class="math display">\[P(x_1, x_2, \ldots, x_T) =
            \prod_{t=1}^{T} P(x_t | x_{&lt;t})\]</span></p>
            <p>Each token is predicted based only on <strong>previous
            tokens</strong> (left-to-right constraint).</p>
            <p>Equivalently, minimize <strong>perplexity</strong>: <span
            class="math display">\[\text{PPL} =
            \exp(\mathcal{L})\]</span></p>
            <h3 id="masked-language-modeling-mlm-loss">Masked Language
            Modeling (MLM) Loss</h3>
            <p><strong>MLM</strong> (BERT-style) uses bidirectional
            context by masking random tokens:</p>
            <p><strong>Masking Strategy</strong> (BERT‚Äôs 15%
            masking):</p>
            <ul>
            <li>80% ‚Üí Replace with <code>[MASK]</code> token</li>
            <li>10% ‚Üí Replace with random word</li>
            <li>10% ‚Üí Keep unchanged (prevents model from only learning
            masked positions)</li>
            </ul>
            <p><strong>MLM Loss</strong>: <span
            class="math display">\[\mathcal{L}_{MLM} = -\sum_{i \in M}
            \log P(x_i | x_{\backslash i})\]</span></p>
            <p>where <span class="math inline">\(M\)</span> is the set
            of masked positions and <span
            class="math inline">\(x_{\backslash i}\)</span> is the
            context (all tokens except position <span
            class="math inline">\(i\)</span>).</p>
            <h3 id="clm-vs-mlm-key-differences">CLM vs MLM: Key
            Differences</h3>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 35%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>CLM (GPT)</th>
            <th>MLM (BERT)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Context</strong></td>
            <td>Left-to-right only</td>
            <td>Bidirectional</td>
            </tr>
            <tr>
            <td><strong>Training</strong></td>
            <td>Predict next token</td>
            <td>Predict masked tokens</td>
            </tr>
            <tr>
            <td><strong>Generation</strong></td>
            <td>Natural (autoregressive)</td>
            <td>Difficult (need iterative)</td>
            </tr>
            <tr>
            <td><strong>Understanding</strong></td>
            <td>Good</td>
            <td>Better (sees full context)</td>
            </tr>
            <tr>
            <td><strong>Use case</strong></td>
            <td>Text generation, chatbots</td>
            <td>Classification, NER, QA</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why CLM for generation?</strong> Autoregressive
            models naturally generate token-by-token. BERT can‚Äôt easily
            generate because it needs the full sequence to predict any
            position.</p>
            <p><strong>Why MLM for understanding?</strong> Bidirectional
            context helps capture dependencies in both directions,
            improving performance on tasks that require full sentence
            understanding.</p>
            <h3
            id="interview-q-whats-the-difference-between-mlm-and-clm-loss">Interview
            Q: ‚ÄúWhat‚Äôs the difference between MLM and CLM loss?‚Äù</h3>
            <p><strong>A</strong>: CLM (Causal Language Modeling)
            predicts the next token given only previous tokens: <span
            class="math inline">\(P(x_t|x_{&lt;t})\)</span>. It‚Äôs used
            in GPT-style models and enables natural text generation. MLM
            (Masked Language Modeling) randomly masks ~15% of tokens and
            predicts them using bidirectional context: <span
            class="math inline">\(P(x_i|x_{\backslash i})\)</span>. BERT
            uses MLM with a specific masking strategy (80% [MASK], 10%
            random, 10% unchanged) to prevent the model from only
            learning masked positions. CLM is better for generation; MLM
            is better for understanding tasks because it sees context
            from both directions.</p>
            <h3 id="evaluating-language-models-perplexity">Evaluating
            Language Models: Perplexity</h3>
            <p>Perplexity (PPL) is the <strong>primary metric</strong>
            for evaluating language model quality during pretraining. It
            directly measures what the model is optimizing: how well it
            predicts the next token.</p>
            <p><strong>Definition</strong>: Perplexity is the
            exponential of the average cross-entropy loss:</p>
            <p><span class="math display">\[\text{PPL} =
            \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P(x_i |
            x_{&lt;i})\right) = \exp(\mathcal{L}_{CLM})\]</span></p>
            <p><strong>Intuitive Interpretation</strong>: Perplexity
            measures the model‚Äôs ‚Äúconfusion‚Äù ‚Äî the effective number of
            equally likely tokens the model is choosing from at each
            step.</p>
            <table>
            <thead>
            <tr>
            <th>Perplexity</th>
            <th>Interpretation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>PPL = 1</strong></td>
            <td>Perfect prediction (knows exactly what comes next)</td>
            </tr>
            <tr>
            <td><strong>PPL = 10</strong></td>
            <td>As uncertain as choosing uniformly from 10 options</td>
            </tr>
            <tr>
            <td><strong>PPL = 50,000</strong></td>
            <td>Random guessing over entire vocabulary</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why Perplexity Matters for
            Pretraining</strong>:</p>
            <ol type="1">
            <li><strong>Directly tied to training objective</strong>:
            Lower perplexity = lower cross-entropy loss = better
            next-token prediction</li>
            <li><strong>Comparable across model sizes</strong>: A 7B
            model with PPL=15 is better than a 70B model with PPL=20 on
            the same data</li>
            <li><strong>Tracks training progress</strong>: Plot PPL
            vs.¬†training steps to monitor convergence</li>
            <li><strong>Scaling law target</strong>: Chinchilla-style
            scaling laws predict PPL as a function of compute, data, and
            model size</li>
            </ol>
            <p><strong>Typical Perplexity Values</strong> (on common
            benchmarks):</p>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>WikiText-103 PPL</th>
            <th>Notes</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>GPT-2 Small (124M)</td>
            <td>~29</td>
            <td>Baseline</td>
            </tr>
            <tr>
            <td>GPT-2 Large (1.5B)</td>
            <td>~18</td>
            <td>Scaling helps</td>
            </tr>
            <tr>
            <td>LLaMA 7B</td>
            <td>~12-15</td>
            <td>Modern architecture + data</td>
            </tr>
            <tr>
            <td>LLaMA 70B</td>
            <td>~8-10</td>
            <td>Near state-of-the-art</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Important Caveats</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>Limitation</th>
            <th>Why It Matters</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Tokenizer-dependent</strong></td>
            <td>Can‚Äôt compare PPL across different tokenizers</td>
            </tr>
            <tr>
            <td><strong>Domain-specific</strong></td>
            <td>PPL on code ‚â† PPL on prose</td>
            </tr>
            <tr>
            <td><strong>Doesn‚Äôt measure utility</strong></td>
            <td>Low PPL ‚â† good at following instructions</td>
            </tr>
            <tr>
            <td><strong>Train/test overlap</strong></td>
            <td>Contamination inflates results</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Beyond Perplexity</strong>: While perplexity is
            the gold standard for pretraining evaluation, it doesn‚Äôt
            capture: - Instruction-following ability (measured by
            benchmarks like MT-Bench) - Factuality (measured by
            TruthfulQA) - Reasoning (measured by GSM8K, MATH) - Safety
            (measured by red-teaming)</p>
            <p>This is why the three-stage pipeline exists: perplexity
            optimizes prediction, but we need SFT and alignment to
            optimize for actual usefulness.</p>
            <hr />
            <h3 id="training-data">Training Data</h3>
            <table>
            <thead>
            <tr>
            <th>Source</th>
            <th>Tokens</th>
            <th>Content</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Common Crawl</td>
            <td>~1T+</td>
            <td>Web pages</td>
            </tr>
            <tr>
            <td>Books</td>
            <td>~100B</td>
            <td>Literature</td>
            </tr>
            <tr>
            <td>Wikipedia</td>
            <td>~10B</td>
            <td>Encyclopedic</td>
            </tr>
            <tr>
            <td>Code (GitHub)</td>
            <td>~100B+</td>
            <td>Programming</td>
            </tr>
            <tr>
            <td>Academic papers</td>
            <td>~50B</td>
            <td>Research</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Data quality matters more than
            quantity!</strong></p>
            <p><strong>The Data Curation Challenge</strong>: Raw web
            data is messy. Common Crawl alone contains spam, porn, hate
            speech, duplicate content, boilerplate HTML, and
            machine-generated text. Turning this into high-quality
            training data requires extensive preprocessing:</p>
            <ul>
            <li><strong>Deduplication</strong>: Removing exact and
            near-duplicate documents prevents the model from memorizing
            repeated content and improves compute efficiency. Both exact
            hash-based and fuzzy (MinHash/LSH) deduplication are
            used.</li>
            <li><strong>Quality filtering</strong>: Heuristics like
            perplexity scoring (using a smaller trained model), length
            thresholds, character/word ratios, and presence of stopwords
            help identify high-quality text.</li>
            <li><strong>Toxicity filtering</strong>: Classifiers remove
            hate speech, explicit content, and other harmful
            material‚Äîthough this is imperfect and models still learn
            some toxic patterns.</li>
            <li><strong>Domain balancing</strong>: Simply training on
            ‚Äúall available data‚Äù would over-represent web text.
            Deliberate upsampling of high-quality sources (Wikipedia,
            books, code) improves model quality.</li>
            <li><strong>Personally Identifiable Information (PII)
            removal</strong>: Email addresses, phone numbers, and other
            PII are scrubbed to protect privacy.</li>
            </ul>
            <p><strong>The ‚Äúdata wall‚Äù</strong>: There‚Äôs a concern that
            we‚Äôre approaching the limits of high-quality human-generated
            text. Synthetic data generation (using LLMs to create
            training data for other LLMs) is an active research area,
            but carries risks of ‚Äúmodel collapse‚Äù where errors compound
            across generations.</p>
            <h3 id="chinchilla-scaling-laws">Chinchilla Scaling
            Laws</h3>
            <p>DeepMind‚Äôs finding: <strong>Train smaller models on more
            data</strong></p>
            <p>Optimal compute allocation: <span
            class="math display">\[N \propto C^{0.5}, \quad D \propto
            C^{0.5}\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Parameters</th>
            <th>Tokens</th>
            <th>Compute-Optimal?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>GPT-3</td>
            <td>175B</td>
            <td>300B</td>
            <td>Undertrained</td>
            </tr>
            <tr>
            <td>Chinchilla</td>
            <td>70B</td>
            <td>1.4T</td>
            <td>‚úì Optimal</td>
            </tr>
            <tr>
            <td>LLaMA</td>
            <td>7-65B</td>
            <td>1-1.4T</td>
            <td>‚úì Optimal</td>
            </tr>
            </tbody>
            </table>
            <p><strong>What this means practically</strong>: Before
            Chinchilla (2022), the prevailing wisdom was ‚Äúbigger models
            are better‚Äù‚Äîleading to the 175B parameter GPT-3 trained on
            ‚Äúonly‚Äù 300B tokens. Chinchilla showed this was wildly
            inefficient: for the same compute budget, a 70B model
            trained on 1.4T tokens significantly outperforms a 175B
            model trained on 300B tokens.</p>
            <p><strong>The implication</strong>: Most early LLMs were
            <em>undertrained</em>‚Äîthey had more parameters than their
            training data could effectively fill. This explains why
            LLaMA (2023) matched GPT-3 performance with only 7-65B
            parameters: it was trained on the ‚ÄúChinchilla-optimal‚Äù
            amount of data.</p>
            <p><strong>Beyond Chinchilla</strong>: For inference-heavy
            deployment (where you serve the model billions of times), it
            may be worth training a smaller model even longer than
            Chinchilla-optimal, trading training compute for inference
            efficiency. LLaMA-2 and later models often train on 2T+
            tokens even for smaller model sizes.</p>
            <p><strong>The formula explained</strong>: <span
            class="math inline">\(N \propto C^{0.5}\)</span> and <span
            class="math inline">\(D \propto C^{0.5}\)</span> mean that
            if you double your compute budget, you should increase both
            model size and data by <span class="math inline">\(\sqrt{2}
            \approx 1.4\times\)</span>. Parameters and tokens should
            scale together‚Äîneither should dominate.</p>
            <h3 id="interview-q-how-do-you-pretrain-an-llm">Interview Q:
            ‚ÄúHow do you pretrain an LLM?‚Äù</h3>
            <p><strong>A</strong>:</p>
            <ol type="1">
            <li><strong>Data</strong>: Collect diverse text (web, books,
            code), clean and deduplicate</li>
            <li><strong>Tokenization</strong>: BPE or SentencePiece to
            convert text to tokens</li>
            <li><strong>Architecture</strong>: Decoder-only transformer
            with causal attention</li>
            <li><strong>Objective</strong>: Next token prediction
            (cross-entropy loss)</li>
            <li><strong>Optimization</strong>: AdamW with warmup +
            cosine decay</li>
            <li><strong>Scale</strong>: Distributed training (DDP,
            tensor/pipeline parallelism)</li>
            <li><strong>Duration</strong>: Weeks to months on
            hundreds/thousands of GPUs</li>
            </ol>
            <hr />
            <h2 id="supervised-fine-tuning-sft">7.3 Supervised
            Fine-Tuning (SFT)</h2>
            <h3 id="what-is-sft">What is SFT?</h3>
            <p>After pretraining, the model can complete text but
            doesn‚Äôt follow instructions well. SFT teaches it to be a
            helpful assistant.</p>
            <p><strong>The problem with pretrained models</strong>: A
            pretrained LLM is essentially a sophisticated autocomplete
            system. Ask it ‚ÄúWhat is the capital of France?‚Äù and it might
            continue with ‚Äú‚Ä¶ is a common geography question‚Äù or ‚ÄúThe
            capital of France is Paris. The capital of Germany is
            Berlin. The capital of‚Ä¶‚Äù rather than simply answering
            ‚ÄúParis.‚Äù It has the <em>knowledge</em> but not the
            <em>behavior</em> of an assistant.</p>
            <p><strong>What SFT teaches</strong>: SFT exposes the model
            to thousands of examples of (instruction, response) pairs,
            demonstrating the <em>format</em> of being helpful: - How to
            start and end responses appropriately - When to be concise
            vs.¬†elaborate - How to handle different types of requests
            (questions, tasks, creative prompts) - The ‚Äúvoice‚Äù of a
            helpful assistant</p>
            <p><strong>What SFT doesn‚Äôt change</strong>: The model‚Äôs
            core knowledge and capabilities come from pretraining. SFT
            doesn‚Äôt teach new facts‚Äîit teaches how to <em>express</em>
            existing knowledge in a helpful format. A model that doesn‚Äôt
            know Python after pretraining won‚Äôt learn it from SFT; but a
            model that knows Python will learn to <em>write code when
            asked</em>.</p>
            <p><strong>SFT is surprisingly efficient</strong>: While
            pretraining requires trillions of tokens, SFT works with
            just 10k-1M high-quality examples. The model already ‚Äúknows‚Äù
            language; it just needs to learn the task format. This is
            sometimes called ‚Äúinstruction tuning‚Äù or ‚Äúalignment tuning‚Äù
            (though the latter term is increasingly reserved for
            RLHF/DPO).</p>
            <h3 id="data-format">Data Format</h3>
            <pre><code>&lt;|system|&gt;You are a helpful assistant.&lt;/s&gt;
&lt;|user|&gt;What is the capital of France?&lt;/s&gt;
&lt;|assistant|&gt;The capital of France is Paris.&lt;/s&gt;</code></pre>
            <h3 id="loss-function">Loss Function</h3>
            <p>Same as pretraining (cross-entropy), but <strong>only on
            assistant responses</strong>:</p>
            <p><span class="math display">\[\mathcal{L}_{SFT} = -\sum_{t
            \in \text{response}} \log P_\theta(x_t |
            x_{&lt;t})\]</span></p>
            <p>Don‚Äôt backprop through user prompts ‚Äî we want to learn to
            respond, not to ask.</p>
            <h4
            id="understanding-sft-loss-a-worked-example">Understanding
            SFT Loss: A Worked Example</h4>
            <p>The key insight is <strong>selective
            backpropagation</strong>. Let‚Äôs trace through a concrete
            example:</p>
            <p><strong>Example conversation</strong>:</p>
            <pre><code>&lt;|user|&gt;What is 2 + 2?&lt;/s&gt;
&lt;|assistant|&gt;The answer is 4.&lt;/s&gt;</code></pre>
            <p><strong>During pretraining</strong>, we compute loss on
            EVERY token:</p>
            <pre><code>Position:    0      1     2    3     4      5       6      7     8        9     10     11    12
Tokens:     &lt;user&gt; What  is   2     +      2       ?     &lt;/s&gt;  &lt;assistant&gt; The  answer  is    4
Loss:        ‚úì      ‚úì     ‚úì    ‚úì     ‚úì      ‚úì       ‚úì      ‚úì      ‚úì         ‚úì      ‚úì     ‚úì     ‚úì
             ‚Üë‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ compute loss on everything ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üë</code></pre>
            <p><strong>During SFT</strong>, we only compute loss on the
            RESPONSE part:</p>
            <pre><code>Position:    0      1     2    3     4      5       6      7     8        9     10     11    12
Tokens:     &lt;user&gt; What  is   2     +      2       ?     &lt;/s&gt;  &lt;assistant&gt; The  answer  is    4
Loss:        ‚úó      ‚úó     ‚úó    ‚úó     ‚úó      ‚úó       ‚úó      ‚úó      ‚úó         ‚úì      ‚úì     ‚úì     ‚úì
             ‚Üë‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NO gradient (masked) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üë       ‚Üë‚îÄ‚îÄ compute loss ‚îÄ‚îÄ‚Üë</code></pre>
            <p><strong>What this means mathematically</strong>:</p>
            <p><strong>Pretraining loss</strong> (all 12 tokens): <span
            class="math display">\[\mathcal{L}_{pretrain} =
            -\frac{1}{12} \sum_{t=1}^{12} \log P(x_t |
            x_{&lt;t})\]</span></p>
            <p><strong>SFT loss</strong> (only response tokens 9-12):
            <span class="math display">\[\mathcal{L}_{SFT} =
            -\frac{1}{4} \sum_{t=9}^{12} \log P(x_t |
            x_{&lt;t})\]</span></p>
            <p>The model still <strong>sees</strong> the prompt (it‚Äôs in
            <span class="math inline">\(x_{&lt;t}\)</span>), but we
            don‚Äôt penalize it for ‚Äúpredicting the prompt wrong.‚Äù</p>
            <h4 id="why-mask-the-prompt">Why Mask the Prompt?</h4>
            <ol type="1">
            <li><p><strong>We want to teach responses, not
            questions</strong>: Training on the prompt would teach the
            model to generate questions like ‚ÄúWhat is 2+2?‚Äù ‚Äî but we
            want it to answer, not ask.</p></li>
            <li><p><strong>The prompt is given at inference</strong>: At
            inference time, the user provides the prompt. The model
            doesn‚Äôt need to predict it.</p></li>
            <li><p><strong>Focus the learning signal</strong>: By only
            backpropagating through the response, all gradient signal is
            directed toward improving answer quality.</p></li>
            </ol>
            <h4 id="implementation-with-label-masking">Implementation
            with Label Masking</h4>
            <div class="sourceCode" id="cb260"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The input sequence (all tokens)</span></span>
<span id="cb260-2"><a href="#cb260-2" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> [user_token, <span class="st">&quot;What&quot;</span>, <span class="st">&quot;is&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;+&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;?&quot;</span>, eos, </span>
<span id="cb260-3"><a href="#cb260-3" aria-hidden="true" tabindex="-1"></a>             assistant_token, <span class="st">&quot;The&quot;</span>, <span class="st">&quot;answer&quot;</span>, <span class="st">&quot;is&quot;</span>, <span class="st">&quot;4&quot;</span>, eos]</span>
<span id="cb260-4"><a href="#cb260-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-5"><a href="#cb260-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Labels: -100 means &quot;don&#39;t compute loss here&quot;</span></span>
<span id="cb260-6"><a href="#cb260-6" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>, <span class="op">-</span><span class="dv">100</span>,  <span class="co"># prompt: masked</span></span>
<span id="cb260-7"><a href="#cb260-7" aria-hidden="true" tabindex="-1"></a>          <span class="op">-</span><span class="dv">100</span>,  <span class="co"># assistant token: masked  </span></span>
<span id="cb260-8"><a href="#cb260-8" aria-hidden="true" tabindex="-1"></a>          token_id(<span class="st">&quot;The&quot;</span>), token_id(<span class="st">&quot;answer&quot;</span>), token_id(<span class="st">&quot;is&quot;</span>), token_id(<span class="st">&quot;4&quot;</span>), token_id(eos)]  <span class="co"># response: compute loss</span></span>
<span id="cb260-9"><a href="#cb260-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb260-10"><a href="#cb260-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-entropy loss automatically ignores -100 positions</span></span>
<span id="cb260-11"><a href="#cb260-11" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size), labels.view(<span class="op">-</span><span class="dv">1</span>), ignore_index<span class="op">=-</span><span class="dv">100</span>)</span></code></pre></div>
            <p>The <code>-100</code> is PyTorch‚Äôs convention for ‚Äúignore
            this position when computing loss.‚Äù</p>
            <h4 id="sft-vs-pretraining-loss-comparison">SFT vs
            Pretraining Loss Comparison</h4>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Pretraining</th>
            <th>SFT</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Tokens trained on</strong></td>
            <td>All tokens</td>
            <td>Response only</td>
            </tr>
            <tr>
            <td><strong>Loss positions</strong></td>
            <td>Every position</td>
            <td>Assistant turns only</td>
            </tr>
            <tr>
            <td><strong>What model learns</strong></td>
            <td>Predict any text</td>
            <td>Predict helpful responses</td>
            </tr>
            <tr>
            <td><strong>Gradient flow</strong></td>
            <td>Through everything</td>
            <td>Only through responses</td>
            </tr>
            </tbody>
            </table>
            <h3 id="key-considerations">Key Considerations</h3>
            <table>
            <thead>
            <tr>
            <th>Factor</th>
            <th>Importance</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Data quality</strong></td>
            <td>More important than quantity</td>
            </tr>
            <tr>
            <td><strong>Diversity</strong></td>
            <td>Cover many tasks (QA, code, math, creative)</td>
            </tr>
            <tr>
            <td><strong>Format consistency</strong></td>
            <td>Same template throughout</td>
            </tr>
            <tr>
            <td><strong>Length</strong></td>
            <td>Mix short and long responses</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why quality over quantity</strong>: Unlike
            pretraining where you need trillions of tokens, SFT benefits
            more from having 10k excellent examples than 1M mediocre
            ones. Each example teaches the model a behavior
            pattern‚Äîlow-quality examples teach bad habits. Models
            trained on human-written responses consistently outperform
            those trained on synthetic data of the same size.</p>
            <p><strong>The diversity imperative</strong>: A model SFT‚Äôd
            only on QA will struggle with coding tasks, even if it knew
            how to code after pretraining. The fine-tuning distribution
            shapes what behaviors the model exhibits. Including diverse
            tasks (QA, summarization, coding, math, creative writing,
            multi-turn dialogue) ensures the model remains generally
            capable rather than overfitting to one task type.</p>
            <p><strong>Format consistency</strong>: Using a consistent
            prompt template (system message, user turn, assistant turn)
            helps the model learn the structure. Mixing formats confuses
            the model about when it should respond and how. Most modern
            models use chat templates like ChatML or similar.</p>
            <p><strong>The length trap</strong>: If all training
            examples have short responses, the model learns to give
            terse answers even when elaboration is needed. If all are
            long, it becomes verbose. A mix of lengths‚Äîwith length
            roughly matching what‚Äôs appropriate for each task‚Äîproduces a
            model that adapts its response length to the situation.</p>
            <p><strong>SFT is necessary but not sufficient</strong>:
            While SFT teaches the format of helpfulness, it doesn‚Äôt
            teach the model to <em>prefer</em> better responses. Given
            two valid ways to answer, SFT doesn‚Äôt tell the model which
            is better. This is why we need the alignment stage
            (RLHF/DPO).</p>
            <h3 id="lora-efficient-fine-tuning">LoRA: Efficient
            Fine-Tuning</h3>
            <p><strong>Problem</strong>: Full fine-tuning requires
            storing full gradient/optimizer states for all
            parameters.</p>
            <p><strong>Solution</strong>: Low-Rank Adaptation ‚Äî only
            train small adapter matrices.</p>
            <p><span class="math display">\[W&#39; = W + BA\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(W \in \mathbb{R}^{d \times
            k}\)</span>: Frozen pretrained weight</li>
            <li><span class="math inline">\(B \in \mathbb{R}^{d \times
            r}\)</span>: Trainable (small rank <span
            class="math inline">\(r\)</span>)</li>
            <li><span class="math inline">\(A \in \mathbb{R}^{r \times
            k}\)</span>: Trainable (small rank <span
            class="math inline">\(r\)</span>)</li>
            </ul>
            <p><strong>Typical</strong>: <span class="math inline">\(r =
            8\)</span> to <span class="math inline">\(64\)</span>, vs
            <span class="math inline">\(d = 4096+\)</span></p>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li>~1000√ó fewer trainable parameters</li>
            <li>Same inference speed (merge <span
            class="math inline">\(BA\)</span> into <span
            class="math inline">\(W\)</span>)</li>
            <li>Can train on single GPU</li>
            </ul>
            <h3 id="interview-q-what-is-lora-and-why-use-it">Interview
            Q: ‚ÄúWhat is LoRA and why use it?‚Äù</h3>
            <p><strong>A</strong>: LoRA (Low-Rank Adaptation) freezes
            the pretrained model and injects small trainable
            rank-decomposition matrices into each layer. Instead of
            updating a <span class="math inline">\(d \times k\)</span>
            weight matrix, we train two smaller matrices of rank <span
            class="math inline">\(r\)</span> (typically 8-64). This
            reduces trainable parameters by 1000√ó, enables fine-tuning
            on limited hardware, and produces adapters that can be
            merged back for efficient inference. It‚Äôs based on the
            hypothesis that weight updates during fine-tuning lie in a
            low-rank subspace.</p>
            <h3
            id="why-low-rank-works-the-deep-dive-interview-topic">Why
            Low Rank Works: The Deep Dive (Interview Topic)</h3>
            <p>A common interview question probes deeper: ‚ÄúWhy does
            low-rank adaptation work? What‚Äôs happening
            mathematically?‚Äù</p>
            <p><strong>The Core Insight: Fine-Tuning Updates Are
            Naturally Low-Rank</strong></p>
            <p>During full fine-tuning, we learn a weight change <span
            class="math inline">\(\Delta W = W_{\text{finetuned}} -
            W_{\text{pretrained}}\)</span>.</p>
            <p>Empirical observation: <strong><span
            class="math inline">\(\Delta W\)</span> has low intrinsic
            rank</strong> ‚Äî most of its eigenvalues are near zero!</p>
            <div class="sourceCode" id="cb261"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb261-1"><a href="#cb261-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Empirical evidence: measure the rank of fine-tuning updates</span></span>
<span id="cb261-2"><a href="#cb261-2" aria-hidden="true" tabindex="-1"></a>delta_W <span class="op">=</span> W_finetuned <span class="op">-</span> W_pretrained  <span class="co"># Full fine-tuning update</span></span>
<span id="cb261-3"><a href="#cb261-3" aria-hidden="true" tabindex="-1"></a>U, S, V <span class="op">=</span> torch.svd(delta_W)</span>
<span id="cb261-4"><a href="#cb261-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb261-5"><a href="#cb261-5" aria-hidden="true" tabindex="-1"></a><span class="co"># S contains singular values ‚Äî most are tiny!</span></span>
<span id="cb261-6"><a href="#cb261-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Top 8-64 singular values capture most of the &quot;meaningful&quot; change</span></span>
<span id="cb261-7"><a href="#cb261-7" aria-hidden="true" tabindex="-1"></a>effective_rank <span class="op">=</span> (S <span class="op">&gt;</span> <span class="fl">0.01</span> <span class="op">*</span> S[<span class="dv">0</span>]).<span class="bu">sum</span>()  <span class="co"># Often &lt;&lt; full dimension</span></span></code></pre></div>
            <p><strong>Why Is This True? Three
            Perspectives</strong>:</p>
            <ol type="1">
            <li><p><strong>Task Specificity</strong>: Fine-tuning adapts
            a general model to a specific task. The ‚Äúcorrection‚Äù needed
            is much simpler than the original knowledge ‚Äî it lives in a
            low-dimensional subspace of weight space.</p></li>
            <li><p><strong>Over-parameterization</strong>: Neural
            networks are massively over-parameterized. There are many
            equivalent ways to solve the same task. The gradient descent
            path happens to find solutions with low-rank
            updates.</p></li>
            <li><p><strong>Regularization View</strong>: Low-rank
            updates are implicitly regularized. With fewer degrees of
            freedom, the model is less likely to overfit to small
            fine-tuning datasets.</p></li>
            </ol>
            <p><strong>Information Bottleneck Connection</strong>:</p>
            <p>A weight matrix <span class="math inline">\(W \in
            \mathbb{R}^{d \times k}\)</span> can encode <span
            class="math inline">\(d \times k\)</span> parameters of
            information.</p>
            <p>A low-rank factorization <span class="math inline">\(W =
            BA\)</span> with rank <span
            class="math inline">\(r\)</span>:</p>
            <ul>
            <li><span class="math inline">\(B \in \mathbb{R}^{d \times
            r}\)</span> + <span class="math inline">\(A \in
            \mathbb{R}^{r \times k}\)</span> = only <span
            class="math inline">\((d + k) \times r\)</span>
            parameters</li>
            <li>For <span class="math inline">\(d = k = 4096\)</span>
            and <span class="math inline">\(r = 8\)</span>: <span
            class="math inline">\(16.7M\)</span> ‚Üí <span
            class="math inline">\(65K\)</span> parameters (256√ó
            reduction!)</li>
            </ul>
            <p><strong>Interview Q</strong>: ‚ÄúWhat happens if your
            weight matrix is low rank?‚Äù</p>
            <p><strong>A</strong>: A low-rank weight matrix projects
            data into a lower-dimensional subspace, creating an
            <strong>information bottleneck</strong>. For an <span
            class="math inline">\(d \times k\)</span> matrix with rank
            <span class="math inline">\(r &lt; \min(d, k)\)</span>:</p>
            <ul>
            <li>Output lives in at most an <span
            class="math inline">\(r\)</span>-dimensional subspace</li>
            <li>Information is compressed through this bottleneck</li>
            <li>Some input information is irreversibly lost</li>
            </ul>
            <p>In <strong>LoRA</strong>, this is actually desirable!
            We‚Äôre not replacing <span class="math inline">\(W\)</span>
            with a low-rank matrix ‚Äî we‚Äôre adding a low-rank
            <strong>update</strong>: <span class="math inline">\(W&#39;
            = W + BA\)</span>. The pretrained <span
            class="math inline">\(W\)</span> retains full rank; <span
            class="math inline">\(BA\)</span> captures the task-specific
            adaptation in a low-dimensional subspace.</p>
            <p>In <strong>model compression</strong>, low-rank
            approximation intentionally loses information to reduce
            parameters. We accept some accuracy loss for efficiency.</p>
            <p><strong>Connection to SVD</strong>:</p>
            <p>Any matrix can be decomposed via SVD: <span
            class="math inline">\(W = U \Sigma V^T\)</span></p>
            <p>Low-rank approximation keeps only top <span
            class="math inline">\(r\)</span> singular values: <span
            class="math display">\[W_r = U_r \Sigma_r V_r^T\]</span></p>
            <p>This is the <strong>optimal</strong> rank-<span
            class="math inline">\(r\)</span> approximation (minimizes
            Frobenius norm error). LoRA implicitly learns a similar
            decomposition but optimized for the downstream task, not
            reconstruction.</p>
            <p><strong>Why Rank <span class="math inline">\(r =
            8\)</span> to <span class="math inline">\(64\)</span> is
            Enough</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Task Type</th>
            <th>Typical Rank Needed</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Single task (sentiment)</td>
            <td>4-8</td>
            <td>Very specific adaptation</td>
            </tr>
            <tr>
            <td>Instruction following</td>
            <td>16-32</td>
            <td>More diverse but still constrained</td>
            </tr>
            <tr>
            <td>Multi-task</td>
            <td>32-64</td>
            <td>More subspace needed</td>
            </tr>
            <tr>
            <td>Approaching full FT</td>
            <td>128+</td>
            <td>Diminishing returns vs full FT</td>
            </tr>
            </tbody>
            </table>
            <p><strong>LoRA vs Full Fine-Tuning: When Each
            Wins</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 30%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Winner</th>
            <th>Reason</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Small fine-tuning dataset</td>
            <td>LoRA</td>
            <td>Implicit regularization prevents overfitting</td>
            </tr>
            <tr>
            <td>Single-GPU training</td>
            <td>LoRA</td>
            <td>Memory efficient</td>
            </tr>
            <tr>
            <td>Multiple task adapters</td>
            <td>LoRA</td>
            <td>Can swap adapters without reloading base model</td>
            </tr>
            <tr>
            <td>Large dataset, max quality</td>
            <td>Full FT</td>
            <td>Slightly higher ceiling</td>
            </tr>
            <tr>
            <td>Compute budget unlimited</td>
            <td>Full FT</td>
            <td>Marginal quality gain</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Interview Q</strong>: ‚ÄúWhy are fine-tuning
            updates low-rank? Explain intuitively.‚Äù</p>
            <p><strong>A</strong>: Fine-tuning adapts a pretrained model
            to a specific task. The pretrained model already ‚Äúknows‚Äù
            language and world knowledge ‚Äî we just need to teach it the
            task-specific behavior. This correction is much simpler than
            the original knowledge, so it lives in a low-dimensional
            subspace.</p>
            <p>Think of it like adjusting a GPS route: the base map
            (pretrained weights) has billions of details about roads;
            your destination change (fine-tuning) just tweaks a few
            high-level direction choices. The ‚Äúdelta‚Äù is low-rank
            because most of the network‚Äôs capabilities remain
            unchanged.</p>
            <p>Empirically, when we measure <span
            class="math inline">\(\Delta W = W_{\text{finetuned}} -
            W_{\text{pretrained}}\)</span> from full fine-tuning, its
            singular value spectrum decays rapidly ‚Äî top 8-64 values
            capture most of the meaningful change.</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúHow do you choose the rank
            <span class="math inline">\(r\)</span> in LoRA?‚Äù</p>
            <p><strong>A</strong>: Start with <span
            class="math inline">\(r = 8\)</span> or <span
            class="math inline">\(r = 16\)</span> and increase if
            performance is insufficient. Key considerations:</p>
            <ul>
            <li><strong>Task complexity</strong>: Simple tasks
            (classification) need lower rank than complex tasks
            (instruction following)</li>
            <li><strong>Dataset size</strong>: Larger datasets can
            support higher ranks without overfitting</li>
            <li><strong>Base model size</strong>: Larger models may need
            higher absolute rank, but rank/dimension ratio often stays
            similar</li>
            <li><strong>Practical</strong>: <span
            class="math inline">\(r = 16\)</span> is a good default for
            most instruction-following tasks</li>
            </ul>
            <p>Trade-off: Higher <span class="math inline">\(r\)</span>
            = more capacity but more parameters and slower training.
            Usually diminishing returns above <span
            class="math inline">\(r = 64\)</span>.</p>
            <hr />
            <h2 id="rl-foundations-for-llm-alignment">7.4 RL Foundations
            for LLM Alignment</h2>
            <p>Before diving into RLHF, DPO, and GRPO, it helps to
            understand the core RL concepts they build upon. This
            section provides the minimal background needed to understand
            how these alignment methods work.</p>
            <h3 id="policy-gradient-the-foundation">Policy Gradient: The
            Foundation</h3>
            <p>In RL, a <strong>policy</strong> <span
            class="math inline">\(\pi_\theta(a|s)\)</span> is a
            distribution over actions given a state. For LLMs:</p>
            <ul>
            <li><strong>State</strong> <span
            class="math inline">\(s\)</span> = prompt + tokens generated
            so far</li>
            <li><strong>Action</strong> <span
            class="math inline">\(a\)</span> = next token to
            generate</li>
            <li><strong>Policy</strong> <span
            class="math inline">\(\pi_\theta(y|x)\)</span> = the LLM
            itself!</li>
            </ul>
            <p><strong>The goal</strong>: Find parameters <span
            class="math inline">\(\theta\)</span> that maximize expected
            reward: <span class="math display">\[J(\theta) =
            \mathbb{E}_{y \sim \pi_\theta}[R(x, y)]\]</span></p>
            <p><strong>REINFORCE (vanilla policy gradient)</strong>:
            <span class="math display">\[\nabla_\theta J =
            \mathbb{E}\left[\nabla_\theta \log \pi_\theta(y|x) \cdot
            R(x, y)\right]\]</span></p>
            <p><strong>Intuition</strong>: If a response <span
            class="math inline">\(y\)</span> gets high reward <span
            class="math inline">\(R\)</span>, increase its probability.
            The gradient <span class="math inline">\(\nabla_\theta \log
            \pi_\theta\)</span> tells us <em>how</em> to increase
            probability; <span class="math inline">\(R\)</span> tells us
            <em>how much</em>.</p>
            <p><strong>The problem</strong>: REINFORCE has <strong>high
            variance</strong>. Two episodes with the same state might
            get rewards of 0.8 and 0.2 ‚Äî the gradient estimates swing
            wildly. This makes training slow and unstable.</p>
            <h3 id="baselines-reducing-variance">Baselines: Reducing
            Variance</h3>
            <p><strong>Key insight</strong>: We don‚Äôt care about
            absolute rewards, only <em>relative</em> rewards.</p>
            <p>Instead of using raw reward <span
            class="math inline">\(R\)</span>, use the
            <strong>advantage</strong>: <span
            class="math display">\[A(s, a) = Q(s, a) - V(s)\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(Q(s, a)\)</span> = expected
            return from taking action <span
            class="math inline">\(a\)</span> in state <span
            class="math inline">\(s\)</span></li>
            <li><span class="math inline">\(V(s)\)</span> = expected
            return from state <span class="math inline">\(s\)</span>
            (averaging over actions)</li>
            <li><span class="math inline">\(A(s, a)\)</span> = ‚Äúhow much
            better is this action than average?‚Äù</li>
            </ul>
            <p><strong>Why this helps</strong>: If all rewards are
            positive (e.g., 0.7, 0.8, 0.9), vanilla policy gradient
            increases probability of <em>all</em> actions. With
            advantage, we increase above-average actions and decrease
            below-average ones ‚Äî much clearer signal!</p>
            <p><strong>Mathematical guarantee</strong>: Subtracting any
            baseline <span class="math inline">\(b(s)\)</span> that
            doesn‚Äôt depend on the action leaves the gradient
            <strong>unbiased</strong> while reducing variance: <span
            class="math display">\[\nabla_\theta J =
            \mathbb{E}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot (R
            - b(s))\right]\]</span></p>
            <p>The optimal baseline is <span class="math inline">\(b(s)
            = V(s)\)</span>, which gives us the advantage.</p>
            <h3 id="actor-critic-learning-the-baseline">Actor-Critic:
            Learning the Baseline</h3>
            <p><strong>The problem</strong>: We need <span
            class="math inline">\(V(s)\)</span> to compute advantages,
            but we don‚Äôt know it!</p>
            <p><strong>Solution</strong>: Learn it! Train a
            <strong>critic</strong> network <span
            class="math inline">\(V_\phi(s)\)</span> to estimate
            expected return.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Actor-Critic Architecture                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇ   ACTOR     ‚îÇ                    ‚îÇ   CRITIC    ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ   œÄ_Œ∏(a|s)  ‚îÇ                    ‚îÇ   V_œÜ(s)    ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ                    ‚îÇ             ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ  &quot;What      ‚îÇ                    ‚îÇ  &quot;How good  ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ   action    ‚îÇ                    ‚îÇ   is this   ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ   to take?&quot; ‚îÇ                    ‚îÇ   state?&quot;   ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ         ‚îÇ                                  ‚îÇ                        ‚îÇ
‚îÇ         ‚îÇ generates action a               ‚îÇ estimates V(s)         ‚îÇ
‚îÇ         ‚îÇ                                  ‚îÇ                        ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                        ‚îÇ                                            ‚îÇ
‚îÇ                        ‚Üì                                            ‚îÇ
‚îÇ              Advantage = r + Œ≥V(s&#39;) - V(s)                          ‚îÇ
‚îÇ                        ‚îÇ                                            ‚îÇ
‚îÇ                        ‚Üì                                            ‚îÇ
‚îÇ         Actor update: ‚àá_Œ∏ log œÄ_Œ∏(a|s) ¬∑ Advantage                  ‚îÇ
‚îÇ         Critic update: minimize (V_œÜ(s) - target)¬≤                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>The two networks</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 45%" />
            <col style="width: 19%" />
            </colgroup>
            <thead>
            <tr>
            <th>Component</th>
            <th>What it does</th>
            <th>Loss</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Actor</strong> <span
            class="math inline">\(\pi_\theta\)</span></td>
            <td>Generates actions (tokens)</td>
            <td>Policy gradient with advantage</td>
            </tr>
            <tr>
            <td><strong>Critic</strong> <span
            class="math inline">\(V_\phi\)</span></td>
            <td>Estimates expected return</td>
            <td>MSE between prediction and actual return</td>
            </tr>
            </tbody>
            </table>
            <p><strong>TD advantage estimate</strong> (used in
            practice): <span class="math display">\[\hat{A}_t = r_t +
            \gamma V_\phi(s_{t+1}) - V_\phi(s_t)\]</span></p>
            <p>This is a one-step estimate. GAE (Generalized Advantage
            Estimation) uses a weighted average of multi-step estimates
            for better bias-variance tradeoff.</p>
            <h3 id="connection-to-llm-alignment">Connection to LLM
            Alignment</h3>
            <p>In RLHF, the Actor-Critic framework maps directly to LLM
            training:</p>
            <table>
            <thead>
            <tr>
            <th>RL Concept</th>
            <th>LLM Alignment</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Actor</strong> <span
            class="math inline">\(\pi_\theta\)</span></td>
            <td>The LLM (policy model)</td>
            </tr>
            <tr>
            <td><strong>Critic</strong> <span
            class="math inline">\(V_\phi\)</span></td>
            <td>Value network (estimates response quality)</td>
            </tr>
            <tr>
            <td><strong>State</strong> <span
            class="math inline">\(s\)</span></td>
            <td>Prompt + partial response</td>
            </tr>
            <tr>
            <td><strong>Action</strong> <span
            class="math inline">\(a\)</span></td>
            <td>Next token</td>
            </tr>
            <tr>
            <td><strong>Reward</strong> <span
            class="math inline">\(R\)</span></td>
            <td>Reward model score (at end of response)</td>
            </tr>
            <tr>
            <td><strong>Advantage</strong> <span
            class="math inline">\(A\)</span></td>
            <td>‚ÄúHow much better is this token than average?‚Äù</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The full RLHF setup</strong> (PPO) requires: 1.
            <strong>Policy model</strong> (actor) ‚Äî the LLM being
            trained 2. <strong>Reference model</strong> ‚Äî frozen copy
            for KL penalty 3. <strong>Reward model</strong> ‚Äî scores
            complete responses 4. <strong>Value model</strong> (critic)
            ‚Äî estimates expected reward</p>
            <p>That‚Äôs <strong>4 models</strong> to manage! This
            complexity motivates simpler alternatives.</p>
            <h3 id="how-dpo-and-grpo-simplify-this">How DPO and GRPO
            Simplify This</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Actor</th>
            <th>Critic</th>
            <th>Reward Model</th>
            <th>Reference</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>RLHF (PPO)</strong></td>
            <td>‚úì</td>
            <td>‚úì</td>
            <td>‚úì</td>
            <td>‚úì</td>
            </tr>
            <tr>
            <td><strong>DPO</strong></td>
            <td>‚úì</td>
            <td>‚úó</td>
            <td>‚úó (implicit)</td>
            <td>‚úì</td>
            </tr>
            <tr>
            <td><strong>GRPO</strong></td>
            <td>‚úì</td>
            <td>‚úó</td>
            <td>‚úì</td>
            <td>‚úì</td>
            </tr>
            </tbody>
            </table>
            <p><strong>DPO eliminates both</strong> the reward model and
            critic by deriving a supervised loss that‚Äôs equivalent to
            RLHF.</p>
            <p><strong>GRPO eliminates the critic</strong> by using
            group statistics as a baseline instead of a learned value
            function.</p>
            <h3
            id="interview-q-what-is-actor-critic-and-how-does-it-relate-to-rlhf">Interview
            Q: ‚ÄúWhat is Actor-Critic and how does it relate to
            RLHF?‚Äù</h3>
            <p><strong>A</strong>: Actor-Critic combines policy gradient
            (actor) with value function estimation (critic). The actor
            <span class="math inline">\(\pi_\theta(a|s)\)</span>
            generates actions; the critic <span
            class="math inline">\(V_\phi(s)\)</span> estimates expected
            returns. We compute advantages <span class="math inline">\(A
            = Q - V\)</span> to reduce variance ‚Äî ‚Äúhow much better is
            this action than average?‚Äù ‚Äî then update the actor with
            policy gradient.</p>
            <p>In RLHF, the LLM is the actor, and we add a critic (value
            network) to estimate expected reward for partial responses.
            The advantage tells us which tokens are better than
            expected. However, this requires training a separate value
            network with the same size as the LLM ‚Äî doubling memory. DPO
            avoids this by deriving a supervised loss, and GRPO replaces
            the learned critic with group statistics computed from
            multiple samples.</p>
            <hr />
            <h2 id="rlhf-reinforcement-learning-from-human-feedback">7.5
            RLHF: Reinforcement Learning from Human Feedback</h2>
            <h3 id="why-rlhf">Why RLHF?</h3>
            <p>SFT teaches format, but not necessarily what humans
            prefer:</p>
            <ul>
            <li>Multiple valid responses exist</li>
            <li>Some are more helpful/safe than others</li>
            <li>Cross-entropy loss doesn‚Äôt capture preference
            ranking</li>
            </ul>
            <p><strong>The fundamental limitation of SFT</strong>:
            Supervised learning optimizes for <em>maximum
            likelihood</em>‚Äîmaking the model‚Äôs output distribution match
            the training data distribution. But ‚Äúmatching the
            distribution‚Äù and ‚Äúbeing preferred by humans‚Äù are different
            objectives.</p>
            <p>Consider a question with two valid answers: one is
            technically correct but terse, another is correct and also
            explains the reasoning clearly. SFT treats both as equally
            good training targets. But humans consistently prefer the
            explanatory answer. SFT has no mechanism to capture this
            preference‚Äîit just learns to produce <em>some</em> valid
            response, not the <em>best</em> response.</p>
            <p><strong>Why not just use better SFT data?</strong> You
            could try to curate only the ‚Äúbest‚Äù responses for SFT, but:
            (1) it‚Äôs hard to define ‚Äúbest‚Äù without explicit preference
            comparisons, (2) you‚Äôd throw away valid data that‚Äôs just
            slightly suboptimal, and (3) you still can‚Äôt distinguish
            degrees of quality. RLHF directly optimizes for preference,
            which is what we actually care about.</p>
            <p><strong>The key insight</strong>: Humans are better at
            <em>comparing</em> responses than <em>generating</em>
            perfect responses. RLHF exploits this by collecting pairwise
            comparisons and training the model to produce responses that
            would win such comparisons.</p>
            <h3 id="the-rlhf-pipeline">The RLHF Pipeline</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        RLHF Pipeline                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 1: Train Reward Model                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Prompt ‚îÄ‚îÄ‚Üí [Model] ‚îÄ‚îÄ‚Üí Response A                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÄ‚îÄ‚Üí [Model] ‚îÄ‚îÄ‚Üí Response B                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Human annotator: &quot;A is better than B&quot;                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Train reward model: R(prompt, response) ‚Üí scalar          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Step 2: Optimize Policy with PPO                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Prompt ‚îÄ‚îÄ‚Üí [Policy œÄ] ‚îÄ‚îÄ‚Üí Response ‚îÄ‚îÄ‚Üí [Reward R] ‚îÄ‚îÄ‚Üí r   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Maximize: E[R(response)] - Œ≤¬∑KL(œÄ || œÄ_ref)               ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="step-1-reward-model-training">Step 1: Reward Model
            Training</h3>
            <p>Collect preference data: <span class="math inline">\((x,
            y_w, y_l)\)</span> where <span
            class="math inline">\(y_w\)</span> is preferred over <span
            class="math inline">\(y_l\)</span>.</p>
            <p><strong>Bradley-Terry model</strong>: <span
            class="math display">\[P(y_w \succ y_l | x) = \sigma(R(x,
            y_w) - R(x, y_l))\]</span></p>
            <p><strong>Loss</strong>: <span
            class="math display">\[\mathcal{L}_{RM} = -\mathbb{E}[\log
            \sigma(R(x, y_w) - R(x, y_l))]\]</span></p>
            <h3 id="step-2-ppo-proximal-policy-optimization">Step 2: PPO
            (Proximal Policy Optimization)</h3>
            <p><strong>Objective</strong>: <span
            class="math display">\[\max_\theta \mathbb{E}_{x \sim D, y
            \sim \pi_\theta}[R(x, y)] - \beta \cdot \text{KL}(\pi_\theta
            || \pi_{\text{ref}})\]</span></p>
            <p><strong>Why KL penalty?</strong></p>
            <ul>
            <li>Prevents policy from deviating too far from SFT
            model</li>
            <li>Avoids reward hacking (gaming the reward model)</li>
            <li>Maintains language quality</li>
            </ul>
            <p><strong>PPO Update</strong>: <span
            class="math display">\[\mathcal{L}_{PPO} =
            \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t,
            \text{clip}(r_t(\theta), 1-\epsilon,
            1+\epsilon)\hat{A}_t\right)\right]\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(r_t(\theta) =
            \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}\)</span>
            ‚Äî probability ratio</li>
            <li><span class="math inline">\(\hat{A}_t\)</span> ‚Äî
            advantage estimate</li>
            <li><span class="math inline">\(\epsilon \approx
            0.2\)</span> ‚Äî clipping parameter</li>
            </ul>
            <p><strong>Intuition behind PPO clipping</strong>: The core
            problem in policy gradient methods is that large updates can
            be catastrophic‚Äîif you change the policy too much in one
            step, you might move to a bad region and never recover.
            PPO‚Äôs solution is elegant: allow updates that improve the
            objective, but <em>clip</em> them if they try to change the
            policy too much.</p>
            <p>The probability ratio <span
            class="math inline">\(r_t(\theta)\)</span> measures how much
            more (or less) likely an action is under the new policy
            compared to the old. When the advantage <span
            class="math inline">\(\hat{A}_t\)</span> is positive (good
            action), we want to increase <span
            class="math inline">\(r_t\)</span>‚Äîbut only up to <span
            class="math inline">\(1 + \epsilon\)</span>. When advantage
            is negative, we want to decrease <span
            class="math inline">\(r_t\)</span>‚Äîbut only down to <span
            class="math inline">\(1 - \epsilon\)</span>. This creates a
            ‚Äútrust region‚Äù where updates are safe.</p>
            <p><strong>Why this matters for LLMs</strong>: During RLHF,
            each token generation is an ‚Äúaction.‚Äù Without clipping, the
            model might dramatically change its token probabilities in
            pursuit of higher reward, potentially destroying its
            language capabilities. The clipping keeps updates
            conservative, ensuring the model remains coherent while
            improving alignment.</p>
            <h3 id="challenges-with-rlhf">Challenges with RLHF</h3>
            <table>
            <thead>
            <tr>
            <th>Challenge</th>
            <th>Issue</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Reward hacking</strong></td>
            <td>Model finds exploits in reward model</td>
            </tr>
            <tr>
            <td><strong>Complexity</strong></td>
            <td>Need reward model + policy + value network</td>
            </tr>
            <tr>
            <td><strong>Instability</strong></td>
            <td>RL training is notoriously unstable</td>
            </tr>
            <tr>
            <td><strong>Cost</strong></td>
            <td>Human annotation is expensive</td>
            </tr>
            <tr>
            <td><strong>Distribution shift</strong></td>
            <td>Policy generates OOD samples</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Understanding these challenges in
            depth</strong>:</p>
            <p><strong>Reward hacking</strong> is perhaps the most
            insidious problem. The reward model is a <em>proxy</em> for
            human preferences, not human preferences themselves. It‚Äôs
            trained on finite data and has blind spots. A clever policy
            can find inputs where the reward model gives high scores but
            humans would actually disapprove. Classic examples:
            responses that are confidently wrong (confidence is often
            rewarded), excessively verbose responses (longer often
            correlates with more helpful in training data), or responses
            that pattern-match to high-reward examples without
            substance. The KL penalty helps but doesn‚Äôt fully solve
            this.</p>
            <p><strong>Complexity</strong> is a practical headache. At
            any time during training, you need: (1) the policy model
            generating responses, (2) the reference model for KL
            computation, (3) the reward model scoring responses, and (4)
            the value/critic network estimating expected returns. For a
            70B parameter model, this means managing 200B+ parameters
            across multiple GPUs with careful memory orchestration.</p>
            <p><strong>Instability</strong> stems from RL‚Äôs fundamental
            nature‚Äîyou‚Äôre optimizing a moving target (the value
            estimates depend on the policy, which is changing). Small
            hyperparameter changes can lead to mode collapse (policy
            degenerates to a single type of response) or reward hacking.
            Unlike supervised learning where you can often ‚Äúset and
            forget,‚Äù RLHF requires careful monitoring and
            adjustment.</p>
            <p><strong>Distribution shift</strong> is subtle but
            important. The reward model was trained on responses from
            the SFT model. As PPO updates the policy, it generates
            responses the reward model has never seen. The reward
            model‚Äôs scores on these out-of-distribution (OOD) samples
            become unreliable, potentially leading to reward hacking.
            This creates a cat-and-mouse dynamic that‚Äôs hard to fully
            escape.</p>
            <h3 id="interview-q-explain-the-rlhf-pipeline">Interview Q:
            ‚ÄúExplain the RLHF pipeline‚Äù</h3>
            <p><strong>A</strong>:</p>
            <ol type="1">
            <li><strong>Preference Collection</strong>: Show humans
            pairs of model responses, ask which is better</li>
            <li><strong>Reward Model</strong>: Train a model to predict
            human preferences (Bradley-Terry model)</li>
            <li><strong>Policy Optimization</strong>: Use PPO to
            maximize expected reward while staying close to the SFT
            model (KL penalty)</li>
            </ol>
            <p>The KL penalty is crucial ‚Äî without it, the model would
            exploit weaknesses in the reward model. RLHF is effective
            but complex, requiring 3 separate models (policy, reward,
            reference) and careful hyperparameter tuning.</p>
            <hr />
            <h2 id="dpo-direct-preference-optimization">7.6 DPO: Direct
            Preference Optimization</h2>
            <h3 id="the-key-insight">The Key Insight</h3>
            <p><strong>Problem</strong>: RLHF requires training a
            separate reward model and doing unstable RL.</p>
            <p><strong>DPO insight</strong>: The optimal policy under
            RLHF has a closed-form solution! We can derive a loss that
            directly optimizes for preferences.</p>
            <p><strong>What DPO accomplishes intuitively</strong>:
            Imagine you want to teach a model to prefer response A over
            response B. RLHF does this indirectly: first learn a reward
            function that scores A higher than B, then use RL to
            maximize that reward. DPO asks: ‚ÄúCan we skip the reward
            model and directly adjust the policy to prefer A over B?‚Äù
            The answer is yes‚Äîand it turns out to be mathematically
            equivalent to RLHF, just expressed differently.</p>
            <p><strong>The core idea</strong>: Instead of learning ‚Äúhow
            good is each response?‚Äù (reward modeling) and then ‚Äúgenerate
            responses that are good‚Äù (RL), DPO directly learns ‚Äúmake the
            preferred response more likely and the rejected response
            less likely.‚Äù This is a supervised learning problem, not an
            RL problem, making it much simpler to implement and
            train.</p>
            <h3 id="the-math">The Math</h3>
            <p>Under the KL-constrained RLHF objective: <span
            class="math display">\[\max_\pi \mathbb{E}[R(x, y)] - \beta
            \cdot \text{KL}(\pi || \pi_{\text{ref}})\]</span></p>
            <p><strong>What this means</strong>: We want to maximize
            reward while not drifting too far from the reference policy
            (usually the SFT model). The KL term acts as a regularizer,
            controlled by <span
            class="math inline">\(\beta\)</span>.</p>
            <p>The optimal policy is: <span
            class="math display">\[\pi^*(y|x) = \frac{1}{Z(x)}
            \pi_{\text{ref}}(y|x) \exp\left(\frac{R(x,
            y)}{\beta}\right)\]</span></p>
            <p><strong>What this means</strong>: The optimal policy is
            the reference policy <em>reweighted</em> by the
            exponentiated reward. High-reward responses get
            exponentially higher probability; low-reward responses get
            exponentially lower probability. The <span
            class="math inline">\(\beta\)</span> controls how
            aggressively we reweight‚Äîlarge <span
            class="math inline">\(\beta\)</span> means we stay closer to
            the reference.</p>
            <p>Rearranging to express reward in terms of policies: <span
            class="math display">\[R(x, y) = \beta \log
            \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log
            Z(x)\]</span></p>
            <p><strong>The key insight</strong>: This equation says that
            the reward can be expressed entirely in terms of
            log-probability ratios! The <span
            class="math inline">\(Z(x)\)</span> term (partition
            function) is a normalizing constant that depends only on the
            prompt, not the response. When we compare two responses,
            this term cancels out‚Äîwhich is exactly what happens in the
            Bradley-Terry preference model.</p>
            <h3 id="dpo-loss">DPO Loss</h3>
            <p>Substituting the implicit reward into the Bradley-Terry
            model:</p>
            <p><span class="math display">\[\mathcal{L}_{DPO} =
            -\mathbb{E}\left[\log \sigma\left(\beta \log
            \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta
            \log
            \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]\]</span></p>
            <p><strong>Breaking down this formula</strong>:</p>
            <ul>
            <li><p><span class="math inline">\(\log
            \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}\)</span> ‚Äî
            How much more likely is the preferred response under our
            policy vs.¬†the reference? Positive means we‚Äôve increased its
            probability.</p></li>
            <li><p><span class="math inline">\(\log
            \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\)</span> ‚Äî
            Same for the rejected response.</p></li>
            <li><p>The difference of these log-ratios measures the
            <em>margin</em>: how much more have we increased the
            preferred response compared to the rejected one?</p></li>
            <li><p><span class="math inline">\(\sigma(\cdot)\)</span> is
            the sigmoid function, converting this margin into a
            probability (0 to 1).</p></li>
            <li><p>The negative log sigmoid <span
            class="math inline">\(-\log \sigma(\cdot)\)</span> is
            minimized when the margin is large and positive‚Äîi.e., when
            we strongly prefer the winning response.</p></li>
            </ul>
            <p><strong>Intuition</strong>: Increase probability of
            preferred response relative to reference, decrease
            probability of rejected response. The loss pushes the model
            to widen the gap between preferred and rejected responses,
            while the reference model anchors us to prevent
            collapse.</p>
            <p><strong>Why the reference model matters</strong>: Without
            the reference, the model could trivially minimize the loss
            by making <span class="math inline">\(\pi_\theta(y_w|x) =
            1\)</span> for all preferred responses‚Äîessentially
            memorizing the training data. The reference model ensures we
            measure <em>relative</em> changes, keeping the model
            grounded.</p>
            <h3 id="dpo-vs-rlhf">DPO vs RLHF</h3>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>RLHF (PPO)</th>
            <th>DPO</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Reward model</td>
            <td>Required</td>
            <td>Implicit</td>
            </tr>
            <tr>
            <td>Training</td>
            <td>RL (unstable)</td>
            <td>Supervised (stable)</td>
            </tr>
            <tr>
            <td>Models needed</td>
            <td>3+</td>
            <td>2</td>
            </tr>
            <tr>
            <td>Hyperparameters</td>
            <td>Many</td>
            <td>Few</td>
            </tr>
            <tr>
            <td>Performance</td>
            <td>Strong</td>
            <td>Comparable</td>
            </tr>
            <tr>
            <td>Implementation</td>
            <td>Complex</td>
            <td>Simple</td>
            </tr>
            </tbody>
            </table>
            <h3 id="dpo-implementation">DPO Implementation</h3>
            <div class="sourceCode" id="cb264"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dpo_loss(policy_logps_w, policy_logps_l, ref_logps_w, ref_logps_l, beta<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb264-2"><a href="#cb264-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb264-3"><a href="#cb264-3" aria-hidden="true" tabindex="-1"></a><span class="co">    policy_logps_w: log œÄ(y_w|x) from current policy</span></span>
<span id="cb264-4"><a href="#cb264-4" aria-hidden="true" tabindex="-1"></a><span class="co">    policy_logps_l: log œÄ(y_l|x) from current policy</span></span>
<span id="cb264-5"><a href="#cb264-5" aria-hidden="true" tabindex="-1"></a><span class="co">    ref_logps_w: log œÄ_ref(y_w|x) from reference model</span></span>
<span id="cb264-6"><a href="#cb264-6" aria-hidden="true" tabindex="-1"></a><span class="co">    ref_logps_l: log œÄ_ref(y_l|x) from reference model</span></span>
<span id="cb264-7"><a href="#cb264-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb264-8"><a href="#cb264-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Log ratios</span></span>
<span id="cb264-9"><a href="#cb264-9" aria-hidden="true" tabindex="-1"></a>    log_ratio_w <span class="op">=</span> policy_logps_w <span class="op">-</span> ref_logps_w</span>
<span id="cb264-10"><a href="#cb264-10" aria-hidden="true" tabindex="-1"></a>    log_ratio_l <span class="op">=</span> policy_logps_l <span class="op">-</span> ref_logps_l</span>
<span id="cb264-11"><a href="#cb264-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb264-12"><a href="#cb264-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># DPO loss</span></span>
<span id="cb264-13"><a href="#cb264-13" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> (log_ratio_w <span class="op">-</span> log_ratio_l))</span>
<span id="cb264-14"><a href="#cb264-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses.mean()</span></code></pre></div>
            <h3 id="interview-q-how-does-dpo-differ-from-rlhf">Interview
            Q: ‚ÄúHow does DPO differ from RLHF?‚Äù</h3>
            <p><strong>A</strong>: DPO (Direct Preference Optimization)
            eliminates the need for a separate reward model by showing
            that the optimal RLHF policy has a closed-form relationship
            with the reward. Instead of: (1) train reward model, (2) run
            PPO, DPO directly optimizes the policy using preference
            pairs with a supervised loss. The loss increases the log
            probability ratio of preferred over rejected responses,
            relative to a reference model. DPO is simpler, more stable,
            and achieves comparable results with fewer
            hyperparameters.</p>
            <hr />
            <h2 id="grpo-group-relative-policy-optimization">7.7 GRPO:
            Group Relative Policy Optimization</h2>
            <p>GRPO represents a significant simplification of the RLHF
            pipeline, introduced by DeepSeek in their R1 model training.
            While PPO has been the standard for LLM alignment, its
            complexity‚Äîrequiring a separate value network, careful
            advantage estimation, and managing multiple model copies‚Äîhas
            motivated the search for simpler alternatives. GRPO
            elegantly sidesteps these issues by leveraging a simple
            insight: in language model alignment, we care about
            <em>relative</em> quality of responses, not absolute
            values.</p>
            <h3 id="the-deepseek-innovation">The DeepSeek
            Innovation</h3>
            <p><strong>Problem</strong>: PPO requires a critic (value)
            network to estimate advantages. This doubles memory and
            compute.</p>
            <p><strong>GRPO solution</strong>: Estimate advantages from
            a group of sampled responses, no critic needed!</p>
            <p><strong>Why this is a big deal</strong>: In standard PPO,
            the advantage <span class="math inline">\(A(s, a) = Q(s, a)
            - V(s)\)</span> tells us ‚Äúhow much better is this action
            than average?‚Äù Computing this requires a value network <span
            class="math inline">\(V\)</span> that must be trained
            alongside the policy. This value network has the same size
            as the policy (~70B parameters), needs its own optimizer
            states, and must be carefully synchronized. For LLMs, this
            is a massive overhead.</p>
            <p>GRPO‚Äôs insight: if we sample multiple responses for the
            same prompt, we can compute ‚Äúhow much better is this
            response than average‚Äù directly from the reward scores of
            the group‚Äîno learned value function needed!</p>
            <h3 id="how-grpo-works">How GRPO Works</h3>
            <p>For each prompt, sample multiple responses and use their
            rewards to compute group-relative advantages:</p>
            <p><span class="math display">\[A_i = \frac{R_i -
            \text{mean}(\{R_j\})}{\text{std}(\{R_j\})}\]</span></p>
            <p>Then apply policy gradient with this advantage.</p>
            <p><strong>Understanding this formula</strong>:</p>
            <ul>
            <li><strong><span
            class="math inline">\(R_i\)</span></strong> is the reward
            for response <span class="math inline">\(i\)</span> (from a
            reward model)</li>
            <li><strong><span
            class="math inline">\(\text{mean}(\{R_j\})\)</span></strong>
            is the average reward across all <span
            class="math inline">\(G\)</span> responses to this
            prompt</li>
            <li><strong><span
            class="math inline">\(\text{std}(\{R_j\})\)</span></strong>
            is the standard deviation of rewards</li>
            </ul>
            <p>The normalized advantage <span
            class="math inline">\(A_i\)</span> tells us: ‚ÄúHow many
            standard deviations above (or below) average is this
            response?‚Äù A response with <span class="math inline">\(A_i =
            1\)</span> is one standard deviation better than average;
            <span class="math inline">\(A_i = -1\)</span> is one
            standard deviation worse.</p>
            <p><strong>Why normalize?</strong> Without normalization,
            the magnitude of gradients would depend on the reward scale,
            which varies across prompts and training stages.
            Normalization ensures stable, consistent gradient magnitudes
            regardless of absolute reward values.</p>
            <p><strong>Comparison with PPO‚Äôs advantage</strong>: In PPO,
            the advantage is <span class="math inline">\(A_t =
            \sum_{t&#39;=t}^T \gamma^{t&#39;-t} r_{t&#39;} -
            V(s_t)\)</span>, which requires temporal credit assignment
            over tokens and a learned value baseline. GRPO‚Äôs advantage
            is simpler: no temporal component, no learned baseline‚Äîjust
            direct comparison with peer responses.</p>
            <h3 id="grpo-algorithm">GRPO Algorithm</h3>
            <pre><code>For each prompt x:

    1. Sample G responses: {y_1, ..., y_G} ~ œÄ_Œ∏(¬∑|x)
    2. Get rewards: {R_1, ..., R_G} from reward model
    3. Compute advantages: A_i = normalize(R_i - mean(R))
    4. Policy gradient update with clipping (like PPO)</code></pre>
            <h3 id="grpo-vs-ppo">GRPO vs PPO</h3>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>PPO</th>
            <th>GRPO</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Critic network</td>
            <td>Required</td>
            <td>None</td>
            </tr>
            <tr>
            <td>Memory</td>
            <td>2√ó policy</td>
            <td>1√ó policy</td>
            </tr>
            <tr>
            <td>Samples per prompt</td>
            <td>1-2</td>
            <td>G (e.g., 8)</td>
            </tr>
            <tr>
            <td>Advantage estimate</td>
            <td>TD/GAE</td>
            <td>Group statistics</td>
            </tr>
            <tr>
            <td>Complexity</td>
            <td>High</td>
            <td>Lower</td>
            </tr>
            </tbody>
            </table>
            <h3 id="why-it-works-2">Why It Works</h3>
            <ul>
            <li><strong>Law of large numbers</strong>: With enough
            samples, mean reward is a good baseline</li>
            <li><strong>Relative ranking</strong>: What matters is being
            better than alternatives</li>
            <li><strong>No value function bias</strong>: Critic networks
            can have their own errors</li>
            </ul>
            <p><strong>Deeper intuition behind each point</strong>:</p>
            <p><strong>Law of large numbers</strong>: The group mean
            <span class="math inline">\(\frac{1}{G}\sum_j R_j\)</span>
            is an unbiased estimate of the expected reward <span
            class="math inline">\(\mathbb{E}[R]\)</span> for that prompt
            under the current policy. With <span class="math inline">\(G
            = 8\)</span> or more samples, this estimate is reasonably
            accurate. It‚Äôs not perfect‚Äîthere‚Äôs variance‚Äîbut it‚Äôs good
            enough, and importantly, it‚Äôs <em>free</em> (no additional
            model needed).</p>
            <p><strong>Relative ranking</strong>: In alignment, we don‚Äôt
            care if a response scores 0.7 vs.¬†0.8 in absolute terms. We
            care: ‚ÄúIs response A better than response B?‚Äù GRPO directly
            optimizes for this by computing advantages relative to the
            group. If one response scores 0.8 when others score 0.5,
            0.6, 0.7, it gets a high advantage. If it scores 0.8 when
            others score 0.78, 0.82, 0.81, it gets a small advantage.
            This is exactly the signal we want.</p>
            <p><strong>No value function bias</strong>: A learned value
            function <span class="math inline">\(V_\phi(s)\)</span> is
            itself a model with its own approximation errors. If <span
            class="math inline">\(V\)</span> is inaccurate (which it
            often is, especially early in training), the advantages will
            be biased, leading to suboptimal policy updates. GRPO avoids
            this entirely‚Äîthe ‚Äúbaseline‚Äù is computed directly from data,
            not approximated.</p>
            <p><strong>The trade-off</strong>: GRPO requires <span
            class="math inline">\(G\)</span> forward passes per prompt
            instead of 1-2, increasing compute per training step. But
            this is usually cheaper than maintaining a separate value
            network, and the simplicity gains are substantial.</p>
            <h3 id="when-to-use-grpo-vs.-ppo-vs.-dpo">When to Use GRPO
            vs.¬†PPO vs.¬†DPO</h3>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 41%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Best Choice</th>
            <th>Reason</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Limited GPU memory</td>
            <td>GRPO or DPO</td>
            <td>No critic network needed</td>
            </tr>
            <tr>
            <td>Offline preference data</td>
            <td>DPO</td>
            <td>Doesn‚Äôt require online sampling</td>
            </tr>
            <tr>
            <td>Online learning / exploration</td>
            <td>PPO or GRPO</td>
            <td>Can sample fresh responses</td>
            </tr>
            <tr>
            <td>Maximum simplicity</td>
            <td>DPO</td>
            <td>Supervised learning only</td>
            </tr>
            <tr>
            <td>Reward model available</td>
            <td>GRPO</td>
            <td>Leverages RM without RL complexity</td>
            </tr>
            <tr>
            <td>Highest quality ceiling</td>
            <td>PPO</td>
            <td>Most mature, most tunable</td>
            </tr>
            </tbody>
            </table>
            <h3 id="interview-q-whats-new-in-grpo">Interview Q: ‚ÄúWhat‚Äôs
            new in GRPO?‚Äù</h3>
            <p><strong>A</strong>: GRPO (Group Relative Policy
            Optimization), introduced by DeepSeek, removes the
            critic/value network from PPO. Instead of learning a value
            function to estimate advantages, GRPO samples multiple
            responses per prompt and computes advantages relative to the
            group mean and standard deviation. This halves memory
            requirements and removes the complexity of training a stable
            value function. The key insight is that for LLM alignment,
            relative preferences within a batch are more important than
            absolute value estimates.</p>
            <p><strong>Follow-up Q</strong>: ‚ÄúWhat are the trade-offs of
            GRPO vs.¬†PPO?‚Äù</p>
            <p><strong>A</strong>: GRPO trades the cost of maintaining a
            value network for the cost of sampling more responses per
            prompt. With PPO, you sample 1-2 responses per prompt but
            need to train and store a critic network (~2√ó memory). With
            GRPO, you sample 8+ responses but only need the policy
            model. GRPO is simpler to implement, more stable (no critic
            to destabilize training), and often achieves comparable
            results. The main downside is increased sampling cost during
            training‚Äîbut for many setups, the memory savings and
            simplicity are worth it.</p>
            <hr />
            <h2 id="quantization">7.8 Quantization</h2>
            <p>Quantization is the process of reducing the numerical
            precision of model weights (and sometimes activations) from
            high-precision floating point (FP32 or FP16) to
            lower-precision formats (INT8, INT4, or even lower). This is
            primarily a <em>deployment</em> optimization‚Äîmaking models
            smaller and faster for inference‚Äîthough quantization-aware
            training can be part of the training process.</p>
            <p>The fundamental insight is that neural networks are
            remarkably robust to precision loss. While training requires
            high precision for stable gradient updates, trained models
            often work nearly as well with much lower precision weights.
            This is because: (1) neural networks are over-parameterized,
            so small weight errors don‚Äôt dramatically affect outputs,
            and (2) the loss landscape around good solutions is
            typically flat, meaning small perturbations don‚Äôt change
            behavior much.</p>
            <h3 id="why-quantize">Why Quantize?</h3>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>FP32 Size</th>
            <th>INT8 Size</th>
            <th>INT4 Size</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>7B</td>
            <td>28 GB</td>
            <td>7 GB</td>
            <td>3.5 GB</td>
            </tr>
            <tr>
            <td>70B</td>
            <td>280 GB</td>
            <td>70 GB</td>
            <td>35 GB</td>
            </tr>
            <tr>
            <td>405B</td>
            <td>1.6 TB</td>
            <td>405 GB</td>
            <td>202 GB</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li><strong>Memory</strong>: 4√ó smaller with INT8, 8√ó with
            INT4</li>
            <li><strong>Speed</strong>: Faster memory bandwidth,
            specialized INT8 kernels</li>
            <li><strong>Deployment</strong>: Run larger models on
            smaller hardware</li>
            </ul>
            <p><strong>The practical reality</strong>: A 70B parameter
            model in FP16 requires ~140GB of memory just for weights.
            This exceeds the memory of even high-end GPUs (A100 has
            80GB). Quantization to INT4 reduces this to ~35GB‚Äîsuddenly
            deployable on a single GPU. For edge deployment or consumer
            hardware, quantization is often the difference between
            ‚Äúruns‚Äù and ‚Äúdoesn‚Äôt run.‚Äù</p>
            <h3 id="number-formats">Number Formats</h3>
            <table>
            <thead>
            <tr>
            <th>Format</th>
            <th>Bits</th>
            <th>Range</th>
            <th>Precision</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>FP32</td>
            <td>32</td>
            <td>¬±3.4√ó10¬≥‚Å∏</td>
            <td>High</td>
            </tr>
            <tr>
            <td>FP16</td>
            <td>16</td>
            <td>¬±65504</td>
            <td>Medium</td>
            </tr>
            <tr>
            <td>BF16</td>
            <td>16</td>
            <td>¬±3.4√ó10¬≥‚Å∏</td>
            <td>Lower</td>
            </tr>
            <tr>
            <td>INT8</td>
            <td>8</td>
            <td>-128 to 127</td>
            <td>Discrete</td>
            </tr>
            <tr>
            <td>INT4</td>
            <td>4</td>
            <td>-8 to 7</td>
            <td>Very low</td>
            </tr>
            </tbody>
            </table>
            <h3 id="quantization-formula">Quantization Formula</h3>
            <p><span class="math display">\[x_q =
            \text{round}\left(\frac{x}{\text{scale}}\right) +
            \text{zero\_point}\]</span></p>
            <p><span class="math display">\[x_{\text{dequant}} =
            \text{scale} \cdot (x_q - \text{zero\_point})\]</span></p>
            <p><strong>Understanding this formula</strong>:</p>
            <ul>
            <li><p><strong>Scale</strong> maps the floating-point range
            to the integer range. For INT8 with values in [-128, 127],
            if your weights range from -1.0 to 1.0, the scale would be
            <span class="math inline">\(1.0/127 \approx
            0.0079\)</span>.</p></li>
            <li><p><strong>Zero point</strong> handles asymmetric
            distributions. If your weights range from 0.0 to 2.0 (not
            centered at zero), the zero point shifts the integer range
            to match.</p></li>
            <li><p><strong>Rounding</strong> is where precision is lost.
            The continuous value 0.123 might round to the same integer
            as 0.127‚Äîthis quantization error is the price we
            pay.</p></li>
            </ul>
            <p><strong>Symmetric vs.¬†Asymmetric quantization</strong>:
            Symmetric quantization sets zero_point = 0 and only uses
            scale, which is simpler but wastes representation range for
            asymmetric distributions. Asymmetric quantization uses both,
            better utilizing the integer range but with slightly more
            compute overhead.</p>
            <h3 id="post-training-quantization-ptq">Post-Training
            Quantization (PTQ)</h3>
            <p>PTQ quantizes a model after training is complete, without
            any retraining. This is the simplest approach‚Äîyou take a
            trained FP16/FP32 model and convert it to lower
            precision.</p>
            <div class="sourceCode" id="cb266"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple per-tensor quantization</span></span>
<span id="cb266-2"><a href="#cb266-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> x.<span class="bu">abs</span>().<span class="bu">max</span>() <span class="op">/</span> <span class="dv">127</span></span>
<span id="cb266-3"><a href="#cb266-3" aria-hidden="true" tabindex="-1"></a>x_quant <span class="op">=</span> torch.<span class="bu">round</span>(x <span class="op">/</span> scale).clamp(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).to(torch.int8)</span>
<span id="cb266-4"><a href="#cb266-4" aria-hidden="true" tabindex="-1"></a>x_dequant <span class="op">=</span> x_quant.<span class="bu">float</span>() <span class="op">*</span> scale</span></code></pre></div>
            <p><strong>How PTQ works in practice</strong>:</p>
            <ol type="1">
            <li><strong>Calibration</strong>: Run the model on a small
            calibration dataset to measure the range of activations at
            each layer.</li>
            <li><strong>Scale computation</strong>: For each tensor (or
            channel, or group of weights), compute the optimal scale
            that minimizes quantization error.</li>
            <li><strong>Weight conversion</strong>: Apply quantization
            to all weights using the computed scales.</li>
            <li><strong>Inference</strong>: At runtime, dequantize
            weights on-the-fly (or use specialized INT8 kernels).</li>
            </ol>
            <p><strong>Granularity matters</strong>: Per-tensor
            quantization uses one scale for an entire weight
            matrix‚Äîsimple but loses precision if the matrix has
            outliers. Per-channel quantization uses different scales for
            each output channel‚Äîbetter accuracy but more overhead.
            Per-group quantization (used in GPTQ) divides each row into
            groups with their own scales‚Äîa balance between accuracy and
            complexity.</p>
            <p><strong>The outlier problem</strong>: LLM weights often
            have outliers‚Äîa few values much larger than the rest. These
            outliers force the scale to be large, reducing effective
            precision for normal values. Advanced methods like AWQ
            specifically address this by protecting important
            weights.</p>
            <h3 id="quantization-aware-training-qat">Quantization-Aware
            Training (QAT)</h3>
            <p>QAT incorporates quantization into the training process,
            allowing the model to learn to be robust to precision loss.
            During training, weights are quantized and immediately
            dequantized (‚Äúfake quantization‚Äù), so the model ‚Äúsees‚Äù the
            quantization errors and adapts.</p>
            <div class="sourceCode" id="cb267"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb267-1"><a href="#cb267-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FakeQuantize(nn.Module):</span>
<span id="cb267-2"><a href="#cb267-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb267-3"><a href="#cb267-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward: quantize then dequantize</span></span>
<span id="cb267-4"><a href="#cb267-4" aria-hidden="true" tabindex="-1"></a>        x_q <span class="op">=</span> quantize(x)</span>
<span id="cb267-5"><a href="#cb267-5" aria-hidden="true" tabindex="-1"></a>        x_deq <span class="op">=</span> dequantize(x_q)</span>
<span id="cb267-6"><a href="#cb267-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward: straight-through estimator</span></span>
<span id="cb267-7"><a href="#cb267-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> (x_deq <span class="op">-</span> x).detach()</span></code></pre></div>
            <p><strong>Why ‚Äústraight-through estimator‚Äù?</strong>: The
            rounding operation in quantization has zero gradient almost
            everywhere (it‚Äôs a step function). We can‚Äôt backpropagate
            through it directly. The straight-through estimator pretends
            the gradient of rounding is 1‚Äîit passes gradients through
            unchanged. This is a hack, but it works surprisingly
            well.</p>
            <p><strong>PTQ vs.¬†QAT tradeoffs</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>PTQ</th>
            <th>QAT</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Compute cost</td>
            <td>Low (just inference)</td>
            <td>High (full training)</td>
            </tr>
            <tr>
            <td>Quality at INT8</td>
            <td>Good (99%+)</td>
            <td>Slightly better</td>
            </tr>
            <tr>
            <td>Quality at INT4</td>
            <td>Moderate (95-99%)</td>
            <td>Better (97-99%)</td>
            </tr>
            <tr>
            <td>Ease of use</td>
            <td>Simple</td>
            <td>Complex</td>
            </tr>
            <tr>
            <td>When to use</td>
            <td>Most cases</td>
            <td>Aggressive quantization (INT4/INT2)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>When to use which</strong>: For INT8
            quantization, PTQ is usually sufficient. The quality loss is
            minimal and doesn‚Äôt justify retraining. For INT4 or lower,
            QAT can recover 1-3% quality. QLoRA is a popular QAT
            approach: keep the base model in 4-bit, train LoRA adapters
            in full precision, and the adapters learn to compensate for
            quantization errors.</p>
            <h3 id="popular-quantization-methods">Popular Quantization
            Methods</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Type</th>
            <th>Bits</th>
            <th>Approach</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>GPTQ</strong></td>
            <td>PTQ</td>
            <td>4</td>
            <td>Layer-wise quantization with Hessian</td>
            </tr>
            <tr>
            <td><strong>AWQ</strong></td>
            <td>PTQ</td>
            <td>4</td>
            <td>Activation-aware weight quantization</td>
            </tr>
            <tr>
            <td><strong>GGML/GGUF</strong></td>
            <td>PTQ</td>
            <td>2-8</td>
            <td>CPU-optimized, various quant types</td>
            </tr>
            <tr>
            <td><strong>QLoRA</strong></td>
            <td>QAT</td>
            <td>4</td>
            <td>Quantized base + LoRA adapters</td>
            </tr>
            </tbody>
            </table>
            <h3 id="gptq-how-it-works">GPTQ: How It Works</h3>
            <p>GPTQ (GPT-Quantization) is a sophisticated PTQ method
            that achieves remarkably good 4-bit quantization by using
            second-order information.</p>
            <ol type="1">
            <li>Quantize weights layer by layer</li>
            <li>Use second-order information (Hessian) to minimize
            quantization error</li>
            <li>Update remaining weights to compensate</li>
            </ol>
            <p><strong>The key insight</strong>: When you quantize one
            weight, you introduce error. But you can partially
            compensate by slightly adjusting other weights that haven‚Äôt
            been quantized yet. GPTQ uses the Hessian (second
            derivatives of the loss) to figure out the optimal
            adjustment.</p>
            <p><strong>Why layer-by-layer?</strong>: Processing the
            entire model at once would be computationally intractable.
            By going layer by layer, GPTQ keeps the problem manageable
            while still achieving good results.</p>
            <p><strong>Hessian intuition</strong>: The Hessian tells us
            how the loss changes with small weight perturbations.
            Weights with large Hessian entries are
            ‚Äúsensitive‚Äù‚Äîquantizing them badly hurts more. GPTQ
            prioritizes quantizing less sensitive weights first and uses
            the remaining weights to compensate for errors in sensitive
            weights.</p>
            <p><strong>Practical consideration</strong>: GPTQ requires a
            calibration dataset (usually 128-1024 samples) to estimate
            the Hessian. The quality of quantization depends on this
            calibration set being representative of actual usage.</p>
            <h3 id="awq-activation-aware">AWQ: Activation-Aware</h3>
            <p>AWQ (Activation-aware Weight Quantization) takes a
            different approach: instead of compensating after
            quantization, it protects important weights <em>before</em>
            quantization.</p>
            <p><strong>Key insight</strong>: Not all weights are equally
            important. Weights connected to larger activations matter
            more.</p>
            <ol type="1">
            <li>Measure activation magnitudes on calibration data</li>
            <li>Scale important weights up before quantization</li>
            <li>Scale activations down to compensate</li>
            </ol>
            <p><strong>Why this works</strong>: Consider a weight <span
            class="math inline">\(w\)</span> that connects to activation
            <span class="math inline">\(a\)</span>. The output is <span
            class="math inline">\(wa\)</span>. If <span
            class="math inline">\(a\)</span> is large, quantization
            error in <span class="math inline">\(w\)</span> gets
            amplified. AWQ identifies these high-impact weights by
            measuring activation magnitudes, then scales them up (say,
            by 2√ó) so they get more bits of precision after
            quantization. The activations are scaled down (by 0.5√ó) to
            compensate, preserving the original <span
            class="math inline">\(wa\)</span> product.</p>
            <p><strong>AWQ vs.¬†GPTQ</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 31%" />
            <col style="width: 26%" />
            </colgroup>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>GPTQ</th>
            <th>AWQ</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Approach</td>
            <td>Compensate after quantization</td>
            <td>Protect before quantization</td>
            </tr>
            <tr>
            <td>Compute</td>
            <td>More expensive (Hessian)</td>
            <td>Cheaper (activation stats)</td>
            </tr>
            <tr>
            <td>Quality</td>
            <td>Excellent</td>
            <td>Excellent (often slightly better)</td>
            </tr>
            <tr>
            <td>Speed</td>
            <td>Slower calibration</td>
            <td>Faster calibration</td>
            </tr>
            </tbody>
            </table>
            <p>Both achieve comparable quality at 4-bit. AWQ tends to be
            faster to apply and slightly more robust. In practice, both
            are excellent choices.</p>
            <h3 id="quality-vs-size-tradeoff">Quality vs Size
            Tradeoff</h3>
            <table>
            <thead>
            <tr>
            <th>Quantization</th>
            <th>Relative Quality</th>
            <th>Size Reduction</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>FP16</td>
            <td>100%</td>
            <td>2√ó</td>
            </tr>
            <tr>
            <td>INT8</td>
            <td>99%+</td>
            <td>4√ó</td>
            </tr>
            <tr>
            <td>INT4 (GPTQ)</td>
            <td>95-99%</td>
            <td>8√ó</td>
            </tr>
            <tr>
            <td>INT4 (AWQ)</td>
            <td>96-99%</td>
            <td>8√ó</td>
            </tr>
            <tr>
            <td>INT3</td>
            <td>90-95%</td>
            <td>10√ó</td>
            </tr>
            <tr>
            <td>INT2</td>
            <td>80-90%</td>
            <td>16√ó</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-the-tradeoff-of-4-bit-quantization">Interview
            Q: ‚ÄúWhat‚Äôs the tradeoff of 4-bit quantization?‚Äù</h3>
            <p><strong>A</strong>: 4-bit quantization reduces model size
            by 8√ó and improves inference speed, but introduces some
            quality degradation. Modern methods like GPTQ and AWQ
            minimize this by using calibration data ‚Äî GPTQ uses Hessian
            information to optimally quantize each layer, while AWQ
            preserves important weights (those connected to large
            activations). Typical quality loss is 1-5% on benchmarks.
            The tradeoff is worthwhile for deployment when the
            alternative is not running the model at all. Key
            consideration: quantize weights but often keep activations
            in higher precision (W4A16).</p>
            <hr />
            <h2 id="sampling-techniques-for-text-generation">7.9
            Sampling Techniques for Text Generation</h2>
            <h3 id="what-this-means-for-beginners-8">What This Means
            (For Beginners)</h3>
            <p>When an LLM generates text, it predicts a
            <strong>probability distribution</strong> over all possible
            next tokens. But how do we pick which token to actually
            output? This is where <strong>decoding strategies</strong>
            (sampling techniques) come in.</p>
            <p>Think of it like choosing what to eat:</p>
            <ul>
            <li><strong>Greedy</strong>: Always pick your favorite food
            (predictable, boring)</li>
            <li><strong>Random sampling</strong>: Spin a wheel with all
            options (chaotic, might pick something weird)</li>
            <li><strong>Top-k</strong>: Choose from your top 5 favorites
            randomly (more variety, still sensible)</li>
            <li><strong>Top-p</strong>: Choose from foods that together
            make up 90% of your preference (adaptive variety)</li>
            </ul>
            <h3 id="greedy-decoding-1">Greedy Decoding</h3>
            <p><strong>Always pick the most probable token</strong>:</p>
            <p><span class="math display">\[x_t = \arg\max_x P(x |
            x_{&lt;t})\]</span></p>
            <div class="sourceCode" id="cb268"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb268-1"><a href="#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> greedy_decode(model, prompt, max_length):</span>
<span id="cb268-2"><a href="#cb268-2" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenize(prompt)</span>
<span id="cb268-3"><a href="#cb268-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb268-4"><a href="#cb268-4" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(tokens)</span>
<span id="cb268-5"><a href="#cb268-5" aria-hidden="true" tabindex="-1"></a>        next_token <span class="op">=</span> logits[<span class="op">-</span><span class="dv">1</span>].argmax()  <span class="co"># Pick highest probability</span></span>
<span id="cb268-6"><a href="#cb268-6" aria-hidden="true" tabindex="-1"></a>        tokens.append(next_token)</span>
<span id="cb268-7"><a href="#cb268-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> next_token <span class="op">==</span> EOS:</span>
<span id="cb268-8"><a href="#cb268-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb268-9"><a href="#cb268-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokens</span></code></pre></div>
            <p><strong>Pros</strong>: Deterministic, fast, coherent
            <strong>Cons</strong>: Repetitive, boring, can get stuck in
            loops</p>
            <h3 id="temperature-scaling">Temperature Scaling</h3>
            <p><strong>Control randomness by scaling logits before
            softmax</strong>:</p>
            <p><span class="math display">\[P(x_i) = \frac{\exp(z_i /
            T)}{\sum_j \exp(z_j / T)}\]</span></p>
            <table>
            <thead>
            <tr>
            <th>Temperature <span class="math inline">\(T\)</span></th>
            <th>Effect</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(T \to 0\)</span></td>
            <td>‚Üí Greedy (picks max)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(T = 1\)</span></td>
            <td>Original distribution</td>
            </tr>
            <tr>
            <td><span class="math inline">\(T &gt; 1\)</span></td>
            <td>Flatter distribution (more random)</td>
            </tr>
            </tbody>
            </table>
            <div class="sourceCode" id="cb269"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb269-1"><a href="#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_with_temperature(logits, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb269-2"><a href="#cb269-2" aria-hidden="true" tabindex="-1"></a>    scaled_logits <span class="op">=</span> logits <span class="op">/</span> temperature</span>
<span id="cb269-3"><a href="#cb269-3" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> softmax(scaled_logits)</span>
<span id="cb269-4"><a href="#cb269-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sample_from_distribution(probs)</span></code></pre></div>
            <h3 id="top-k-sampling">Top-k Sampling</h3>
            <p><strong>Sample only from the k most likely
            tokens</strong>:</p>
            <div class="sourceCode" id="cb270"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb270-1"><a href="#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> top_k_sampling(logits, k<span class="op">=</span><span class="dv">50</span>, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb270-2"><a href="#cb270-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply temperature</span></span>
<span id="cb270-3"><a href="#cb270-3" aria-hidden="true" tabindex="-1"></a>    scaled_logits <span class="op">=</span> logits <span class="op">/</span> temperature</span>
<span id="cb270-4"><a href="#cb270-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb270-5"><a href="#cb270-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep only top k</span></span>
<span id="cb270-6"><a href="#cb270-6" aria-hidden="true" tabindex="-1"></a>    top_k_logits, top_k_indices <span class="op">=</span> torch.topk(scaled_logits, k)</span>
<span id="cb270-7"><a href="#cb270-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb270-8"><a href="#cb270-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Zero out everything else</span></span>
<span id="cb270-9"><a href="#cb270-9" aria-hidden="true" tabindex="-1"></a>    filtered_logits <span class="op">=</span> torch.full_like(logits, <span class="op">-</span><span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>))</span>
<span id="cb270-10"><a href="#cb270-10" aria-hidden="true" tabindex="-1"></a>    filtered_logits[top_k_indices] <span class="op">=</span> top_k_logits</span>
<span id="cb270-11"><a href="#cb270-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb270-12"><a href="#cb270-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample</span></span>
<span id="cb270-13"><a href="#cb270-13" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> softmax(filtered_logits)</span>
<span id="cb270-14"><a href="#cb270-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sample_from_distribution(probs)</span></code></pre></div>
            <p><strong>Problem</strong>: Fixed k doesn‚Äôt adapt to the
            distribution. If the model is very confident (one token has
            99% probability), we‚Äôre still sampling from 50 tokens. If
            uncertain, 50 might not be enough.</p>
            <h3 id="top-p-nucleus-sampling">Top-p (Nucleus)
            Sampling</h3>
            <p><strong>Sample from smallest set of tokens whose
            cumulative probability ‚â• p</strong>:</p>
            <div class="sourceCode" id="cb271"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb271-1"><a href="#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> top_p_sampling(logits, p<span class="op">=</span><span class="fl">0.9</span>, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb271-2"><a href="#cb271-2" aria-hidden="true" tabindex="-1"></a>    scaled_logits <span class="op">=</span> logits <span class="op">/</span> temperature</span>
<span id="cb271-3"><a href="#cb271-3" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> softmax(scaled_logits)</span>
<span id="cb271-4"><a href="#cb271-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb271-5"><a href="#cb271-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by probability</span></span>
<span id="cb271-6"><a href="#cb271-6" aria-hidden="true" tabindex="-1"></a>    sorted_probs, sorted_indices <span class="op">=</span> torch.sort(probs, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb271-7"><a href="#cb271-7" aria-hidden="true" tabindex="-1"></a>    cumulative_probs <span class="op">=</span> torch.cumsum(sorted_probs, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb271-8"><a href="#cb271-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb271-9"><a href="#cb271-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find cutoff where cumsum &gt; p</span></span>
<span id="cb271-10"><a href="#cb271-10" aria-hidden="true" tabindex="-1"></a>    cutoff_idx <span class="op">=</span> (cumulative_probs <span class="op">&gt;</span> p).nonzero()[<span class="dv">0</span>]</span>
<span id="cb271-11"><a href="#cb271-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb271-12"><a href="#cb271-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep only tokens up to cutoff</span></span>
<span id="cb271-13"><a href="#cb271-13" aria-hidden="true" tabindex="-1"></a>    top_p_probs <span class="op">=</span> sorted_probs[:cutoff_idx <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb271-14"><a href="#cb271-14" aria-hidden="true" tabindex="-1"></a>    top_p_indices <span class="op">=</span> sorted_indices[:cutoff_idx <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb271-15"><a href="#cb271-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb271-16"><a href="#cb271-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Renormalize and sample</span></span>
<span id="cb271-17"><a href="#cb271-17" aria-hidden="true" tabindex="-1"></a>    top_p_probs <span class="op">=</span> top_p_probs <span class="op">/</span> top_p_probs.<span class="bu">sum</span>()</span>
<span id="cb271-18"><a href="#cb271-18" aria-hidden="true" tabindex="-1"></a>    selected_idx <span class="op">=</span> sample_from_distribution(top_p_probs)</span>
<span id="cb271-19"><a href="#cb271-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> top_p_indices[selected_idx]</span></code></pre></div>
            <p><strong>Advantage</strong>: Adapts to confidence
            level:</p>
            <ul>
            <li>High confidence ‚Üí small nucleus (few tokens)</li>
            <li>Low confidence ‚Üí large nucleus (many tokens)</li>
            </ul>
            <h3 id="beam-search-1">Beam Search</h3>
            <p><strong>Maintain k best partial sequences, expand
            each</strong>:</p>
            <pre><code>Step 1: &quot;The&quot; ‚Üí P=0.8
         
Step 2: &quot;The cat&quot; ‚Üí P=0.3    &quot;The dog&quot; ‚Üí P=0.25    (keep top k=2)
         
Step 3: &quot;The cat sat&quot; ‚Üí P=0.2    &quot;The cat ran&quot; ‚Üí P=0.15    (keep top k=2)</code></pre>
            <div class="sourceCode" id="cb273"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb273-1"><a href="#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> beam_search(model, prompt, beam_width<span class="op">=</span><span class="dv">5</span>, max_length<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb273-2"><a href="#cb273-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize with prompt</span></span>
<span id="cb273-3"><a href="#cb273-3" aria-hidden="true" tabindex="-1"></a>    beams <span class="op">=</span> [(tokenize(prompt), <span class="fl">0.0</span>)]  <span class="co"># (tokens, log_probability)</span></span>
<span id="cb273-4"><a href="#cb273-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb273-5"><a href="#cb273-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_length):</span>
<span id="cb273-6"><a href="#cb273-6" aria-hidden="true" tabindex="-1"></a>        all_candidates <span class="op">=</span> []</span>
<span id="cb273-7"><a href="#cb273-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb273-8"><a href="#cb273-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> tokens, score <span class="kw">in</span> beams:</span>
<span id="cb273-9"><a href="#cb273-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> tokens[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> EOS:</span>
<span id="cb273-10"><a href="#cb273-10" aria-hidden="true" tabindex="-1"></a>                all_candidates.append((tokens, score))</span>
<span id="cb273-11"><a href="#cb273-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb273-12"><a href="#cb273-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb273-13"><a href="#cb273-13" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(tokens)</span>
<span id="cb273-14"><a href="#cb273-14" aria-hidden="true" tabindex="-1"></a>            log_probs <span class="op">=</span> log_softmax(logits[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb273-15"><a href="#cb273-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb273-16"><a href="#cb273-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Expand with all possible next tokens</span></span>
<span id="cb273-17"><a href="#cb273-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> token_id, log_prob <span class="kw">in</span> <span class="bu">enumerate</span>(log_probs):</span>
<span id="cb273-18"><a href="#cb273-18" aria-hidden="true" tabindex="-1"></a>                new_tokens <span class="op">=</span> tokens <span class="op">+</span> [token_id]</span>
<span id="cb273-19"><a href="#cb273-19" aria-hidden="true" tabindex="-1"></a>                new_score <span class="op">=</span> score <span class="op">+</span> log_prob</span>
<span id="cb273-20"><a href="#cb273-20" aria-hidden="true" tabindex="-1"></a>                all_candidates.append((new_tokens, new_score))</span>
<span id="cb273-21"><a href="#cb273-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb273-22"><a href="#cb273-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep top beam_width candidates</span></span>
<span id="cb273-23"><a href="#cb273-23" aria-hidden="true" tabindex="-1"></a>        all_candidates.sort(key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb273-24"><a href="#cb273-24" aria-hidden="true" tabindex="-1"></a>        beams <span class="op">=</span> all_candidates[:beam_width]</span>
<span id="cb273-25"><a href="#cb273-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb273-26"><a href="#cb273-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stop if all beams ended</span></span>
<span id="cb273-27"><a href="#cb273-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">all</span>(b[<span class="dv">0</span>][<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> EOS <span class="cf">for</span> b <span class="kw">in</span> beams):</span>
<span id="cb273-28"><a href="#cb273-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb273-29"><a href="#cb273-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb273-30"><a href="#cb273-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beams[<span class="dv">0</span>][<span class="dv">0</span>]  <span class="co"># Return best beam</span></span></code></pre></div>
            <p><strong>Length normalization</strong>: Longer sequences
            have lower probabilities (more terms multiplied). Normalize
            by length:</p>
            <p><span class="math display">\[\text{score} = \frac{\log
            P(y|x)}{|y|^\alpha}\]</span></p>
            <p>where <span class="math inline">\(\alpha \in [0.6,
            1.0]\)</span> is a hyperparameter.</p>
            <h3 id="comparison-table-3">Comparison Table</h3>
            <table>
            <colgroup>
            <col style="width: 13%" />
            <col style="width: 25%" />
            <col style="width: 15%" />
            <col style="width: 18%" />
            <col style="width: 11%" />
            <col style="width: 16%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Deterministic?</th>
            <th>Quality</th>
            <th>Diversity</th>
            <th>Speed</th>
            <th>Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Greedy</strong></td>
            <td>Yes</td>
            <td>Good</td>
            <td>None</td>
            <td>Fast</td>
            <td>Factual QA</td>
            </tr>
            <tr>
            <td><strong>Beam Search</strong></td>
            <td>Yes</td>
            <td>Best</td>
            <td>Low</td>
            <td>Slow</td>
            <td>Translation, summarization</td>
            </tr>
            <tr>
            <td><strong>Top-k</strong></td>
            <td>No</td>
            <td>Good</td>
            <td>Medium</td>
            <td>Fast</td>
            <td>Creative writing</td>
            </tr>
            <tr>
            <td><strong>Top-p</strong></td>
            <td>No</td>
            <td>Good</td>
            <td>Medium</td>
            <td>Fast</td>
            <td>General generation</td>
            </tr>
            <tr>
            <td><strong>Temperature</strong></td>
            <td>No</td>
            <td>Varies</td>
            <td>Adjustable</td>
            <td>Fast</td>
            <td>Control randomness</td>
            </tr>
            </tbody>
            </table>
            <h3 id="common-combinations">Common Combinations</h3>
            <p>In practice, combine techniques:</p>
            <div class="sourceCode" id="cb274"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(model, prompt, temperature<span class="op">=</span><span class="fl">0.7</span>, top_p<span class="op">=</span><span class="fl">0.9</span>, top_k<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb274-2"><a href="#cb274-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Typical LLM generation with combined sampling&quot;&quot;&quot;</span></span>
<span id="cb274-3"><a href="#cb274-3" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> model(prompt)</span>
<span id="cb274-4"><a href="#cb274-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb274-5"><a href="#cb274-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Apply temperature</span></span>
<span id="cb274-6"><a href="#cb274-6" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> logits <span class="op">/</span> temperature</span>
<span id="cb274-7"><a href="#cb274-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb274-8"><a href="#cb274-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Top-k filtering</span></span>
<span id="cb274-9"><a href="#cb274-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> top_k <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb274-10"><a href="#cb274-10" aria-hidden="true" tabindex="-1"></a>        top_k_logits, top_k_idx <span class="op">=</span> torch.topk(logits, top_k)</span>
<span id="cb274-11"><a href="#cb274-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> torch.full_like(logits, <span class="op">-</span><span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>))</span>
<span id="cb274-12"><a href="#cb274-12" aria-hidden="true" tabindex="-1"></a>        logits[top_k_idx] <span class="op">=</span> top_k_logits</span>
<span id="cb274-13"><a href="#cb274-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb274-14"><a href="#cb274-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Top-p filtering  </span></span>
<span id="cb274-15"><a href="#cb274-15" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> softmax(logits)</span>
<span id="cb274-16"><a href="#cb274-16" aria-hidden="true" tabindex="-1"></a>    sorted_probs, sorted_idx <span class="op">=</span> torch.sort(probs, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb274-17"><a href="#cb274-17" aria-hidden="true" tabindex="-1"></a>    cumsum <span class="op">=</span> torch.cumsum(sorted_probs, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb274-18"><a href="#cb274-18" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> cumsum <span class="op">&gt;</span> top_p</span>
<span id="cb274-19"><a href="#cb274-19" aria-hidden="true" tabindex="-1"></a>    mask[<span class="dv">0</span>] <span class="op">=</span> <span class="va">False</span>  <span class="co"># Keep at least one token</span></span>
<span id="cb274-20"><a href="#cb274-20" aria-hidden="true" tabindex="-1"></a>    sorted_probs[mask] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb274-21"><a href="#cb274-21" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> sorted_probs[torch.argsort(sorted_idx)]  <span class="co"># Unsort</span></span>
<span id="cb274-22"><a href="#cb274-22" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> probs <span class="op">/</span> probs.<span class="bu">sum</span>()</span>
<span id="cb274-23"><a href="#cb274-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb274-24"><a href="#cb274-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Sample</span></span>
<span id="cb274-25"><a href="#cb274-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sample_from_distribution(probs)</span></code></pre></div>
            <h3
            id="interview-q-when-would-you-use-beam-search-vs-nucleus-sampling">Interview
            Q: ‚ÄúWhen would you use beam search vs nucleus
            sampling?‚Äù</h3>
            <p><strong>A</strong>: <strong>Beam search</strong> for
            tasks where there‚Äôs a ‚Äúcorrect‚Äù answer and you want the
            highest quality output: translation, summarization,
            structured data generation. It‚Äôs deterministic and finds
            high-probability sequences, but produces repetitive, ‚Äúsafe‚Äù
            outputs.</p>
            <p><strong>Nucleus (top-p) sampling</strong> for creative
            tasks where diversity matters: story generation, dialogue,
            brainstorming. It produces varied outputs by sampling from
            the probability distribution, with the nucleus adapting to
            model confidence.</p>
            <p>For chat applications, typically use <strong>temperature
            + top-p</strong> (e.g., temp=0.7, top_p=0.9) for a balance
            of coherence and variety. For factual QA, use low
            temperature or greedy.</p>
            <h3
            id="practical-guidelines-choosing-your-sampling-strategy">Practical
            Guidelines: Choosing Your Sampling Strategy</h3>
            <p>Choosing the right sampling strategy is more art than
            science, but here are practical guidelines based on common
            use cases:</p>
            <p><strong>For factual question answering and information
            retrieval</strong>: - Use <strong>greedy decoding</strong>
            or <strong>low temperature (0.1-0.3)</strong> with top-p -
            You want the single most likely answer, not creative
            variations - Example: ‚ÄúWhat year did World War II end?‚Äù ‚Üí
            You want ‚Äú1945‚Äù, not creative alternatives</p>
            <p><strong>For code generation</strong>: - Use <strong>low
            to medium temperature (0.2-0.5)</strong> with top-p
            (0.9-0.95) - Code needs to be syntactically correct, so you
            want high-probability tokens - Some variation helps explore
            different valid implementations - Example: ‚ÄúWrite a Python
            function to sort a list‚Äù ‚Üí Multiple valid approaches exist,
            but syntax must be correct</p>
            <p><strong>For creative writing and brainstorming</strong>:
            - Use <strong>medium to high temperature (0.7-1.0)</strong>
            with top-p (0.9) - Diversity and surprise are features, not
            bugs - Example: ‚ÄúWrite a poem about autumn‚Äù ‚Üí You want
            creative, unexpected word choices</p>
            <p><strong>For general-purpose chat assistants</strong>: -
            Use <strong>temperature 0.7</strong> with <strong>top-p
            0.9</strong> as a balanced default - This is why most API
            defaults are in this range - Produces coherent, helpful
            responses with some personality</p>
            <p><strong>For structured output (JSON, XML)</strong>: - Use
            <strong>greedy or very low temperature</strong> with strict
            top-p - Structure violations are catastrophic‚Äîyou need
            high-probability tokens - Consider constrained decoding
            (forcing valid tokens only)</p>
            <p><strong>Common pitfalls to avoid</strong>: -
            <strong>Temperature &gt; 1.2</strong>: Often produces
            gibberish; the distribution becomes too flat - <strong>Top-k
            alone</strong>: Fixed k doesn‚Äôt adapt to model confidence;
            prefer top-p - <strong>No length penalty in beam
            search</strong>: Will prefer shorter outputs; always use
            length normalization - <strong>Forgetting repetition
            penalties</strong>: Especially for long generation, consider
            adding penalties for repeated n-grams</p>
            <p><strong>The meta-lesson</strong>: There‚Äôs no universally
            optimal strategy. The right choice depends on your task,
            your tolerance for errors vs.¬†blandness, and empirical
            testing with your specific model and use case.</p>
            <hr />
            <h1 id="part-8-distributed-training">Part 8: Distributed
            Training</h1>
            <p>Training large models requires distributing computation
            across multiple GPUs. This chapter covers the three main
            parallelism strategies and how they complement each
            other.</p>
            <h2
            id="understanding-the-distributed-training-landscape">Understanding
            the Distributed Training Landscape</h2>
            <p>Modern deep learning faces two fundamental bottlenecks
            when scaling: <strong>compute</strong> and
            <strong>memory</strong>. A single GPU can only perform so
            many floating-point operations per second (compute-bound),
            and it can only store so many parameters and activations
            (memory-bound). Different parallelism strategies address
            these bottlenecks differently:</p>
            <p><strong>Data Parallelism</strong> (Section 8.1) addresses
            the compute bottleneck by replicating the model across GPUs
            and splitting the data. Each GPU processes different samples
            but holds the full model. This scales throughput linearly
            but doesn‚Äôt help if the model itself doesn‚Äôt fit.</p>
            <p><strong>Tensor Parallelism</strong> (Section 8.2)
            addresses the memory bottleneck by splitting individual
            weight matrices across GPUs. Each GPU holds a fraction of
            each layer. This reduces per-GPU memory but requires
            frequent communication during every forward/backward
            pass.</p>
            <p><strong>Pipeline Parallelism</strong> (Section 8.3)
            addresses the memory bottleneck by assigning different
            layers to different GPUs. Each GPU holds a subset of layers.
            Communication only happens between adjacent stages, but
            introduces ‚Äúbubbles‚Äù of idle time.</p>
            <p><strong>ZeRO/FSDP</strong> (Section 8.4) is a hybrid
            approach that partitions optimizer states, gradients, and
            optionally parameters across GPUs while maintaining data
            parallelism semantics. It achieves memory efficiency without
            the communication patterns of tensor parallelism.</p>
            <p>The choice between strategies depends on your hardware
            topology (how GPUs are connected), model size, and scaling
            requirements. In practice, large models use combinations of
            all three‚Äîcalled ‚Äú3D parallelism.‚Äù</p>
            <h2 id="distributed-data-parallelism-ddp">8.1 Distributed
            Data Parallelism (DDP)</h2>
            <h3 id="the-core-problem-single-gpu-bottleneck">The Core
            Problem: Single GPU Bottleneck</h3>
            <p>A single GPU can only process so much data per second. If
            you have:</p>
            <ul>
            <li>1 GPU processing 32 samples/second</li>
            <li>10 trillion tokens to train on</li>
            </ul>
            <p>That‚Äôs <strong>10 years</strong> of training! We need
            parallelism.</p>
            <h3 id="the-key-insight-batch-splitting">The Key Insight:
            Batch Splitting</h3>
            <p><strong>Data parallelism</strong> is the simplest form of
            parallelism: replicate the model on multiple GPUs, split the
            batch, and average gradients.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Data Parallelism                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                   ‚îÇ
‚îÇ  Global Batch (128 samples)                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Samples 0-31 ‚îÇ Samples 32-63 ‚îÇ Samples 64-95 ‚îÇ 96-127 ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ         ‚Üì               ‚Üì               ‚Üì            ‚Üì            ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ     ‚îÇ GPU 0 ‚îÇ       ‚îÇ GPU 1 ‚îÇ       ‚îÇ GPU 2 ‚îÇ    ‚îÇ GPU 3 ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ Model ‚îÇ       ‚îÇ Model ‚îÇ       ‚îÇ Model ‚îÇ    ‚îÇ Model ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ Copy  ‚îÇ       ‚îÇ Copy  ‚îÇ       ‚îÇ Copy  ‚îÇ    ‚îÇ Copy  ‚îÇ        ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ         ‚îÇ               ‚îÇ               ‚îÇ            ‚îÇ            ‚îÇ
‚îÇ         ‚Üì               ‚Üì               ‚Üì            ‚Üì            ‚îÇ
‚îÇ      Grad‚ÇÄ           Grad‚ÇÅ           Grad‚ÇÇ        Grad‚ÇÉ           ‚îÇ
‚îÇ         ‚îÇ               ‚îÇ               ‚îÇ            ‚îÇ            ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                              ‚Üì                                    ‚îÇ
‚îÇ                      AllReduce (Average)                          ‚îÇ
‚îÇ                              ‚Üì                                    ‚îÇ
‚îÇ                  All GPUs get same Avg Grad                       ‚îÇ
‚îÇ                              ‚Üì                                    ‚îÇ
‚îÇ                  All GPUs update identically                      ‚îÇ
‚îÇ                    (Models stay in sync!)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="why-its-mathematically-equivalent">Why It‚Äôs
            Mathematically Equivalent</h3>
            <p>For SGD, the gradient of a batch is the average of
            per-sample gradients:</p>
            <p><span class="math display">\[\nabla L_{batch} =
            \frac{1}{B} \sum_{i=1}^{B} \nabla L_i\]</span></p>
            <p>If we split batch B into K parts:</p>
            <p><span class="math display">\[\nabla L_{batch} =
            \frac{1}{K} \sum_{k=1}^{K} \left( \frac{1}{B/K} \sum_{i \in
            \text{part}_k} \nabla L_i \right) = \frac{1}{K}
            \sum_{k=1}^{K} \nabla L_k\]</span></p>
            <p>Each GPU computes <span class="math inline">\(\nabla
            L_k\)</span>, then AllReduce averages them ‚Üí
            <strong>identical</strong> to single-GPU training!</p>
            <h3
            id="allreduce-the-critical-communication-operation">AllReduce:
            The Critical Communication Operation</h3>
            <p><strong>AllReduce</strong> combines values from all
            processes and distributes the result back to all:</p>
            <pre><code>Before AllReduce:           After AllReduce (sum):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

GPU 0: [1, 2, 3]           GPU 0: [6, 8, 10]
GPU 1: [2, 2, 3]    ‚îÄ‚îÄ‚îÄ‚Üí   GPU 1: [6, 8, 10]
GPU 2: [3, 4, 4]           GPU 2: [6, 8, 10]</code></pre>
            <p><strong>Ring AllReduce</strong> (efficient
            implementation):</p>
            <pre><code>Step 1: Each GPU sends chunk to next, receives from previous
Step 2: Repeat N-1 times (reduce-scatter phase)
Step 3: Each GPU has 1/N of final result
Step 4: Repeat N-1 times to broadcast (all-gather phase)

Total data transferred per GPU: 2 √ó (N-1)/N √ó data_size ‚âà 2 √ó data_size
Communication time: O(data_size), NOT O(N √ó data_size)!</code></pre>
            <p><strong>Why Ring AllReduce achieves O(data_size) instead
            of O(N √ó data_size)</strong>: The naive approach would have
            every GPU send its full gradient to a central server and
            receive the full result back‚Äîrequiring O(N) data movement as
            you add GPUs. Ring AllReduce avoids this by organizing GPUs
            in a ring topology. Each GPU splits its data into N chunks
            and only ever sends/receives one chunk at a time to/from its
            neighbors. After N-1 send-receive steps (reduce-scatter),
            each GPU holds 1/N of the fully-reduced result. After
            another N-1 steps (all-gather), every GPU has the complete
            result. The total data each GPU sends is (N-1)/N √ó data_size
            per phase, or ~2√ó data_size total‚Äîregardless of how many
            GPUs participate. This makes Ring AllReduce
            <strong>bandwidth-optimal</strong>: scaling to more GPUs
            doesn‚Äôt increase per-GPU communication overhead.</p>
            <h3
            id="overlapping-computation-and-communication">Overlapping
            Computation and Communication</h3>
            <p>Modern DDP overlaps gradient computation with
            AllReduce:</p>
            <pre><code>Timeline for GPU 0:

Layer 5 backward:  [compute grad‚ÇÖ]
                            ‚Üì start AllReduce for grad‚ÇÖ
Layer 4 backward:  [compute grad‚ÇÑ]        [AllReduce‚ÇÖ running]
                            ‚Üì start AllReduce for grad‚ÇÑ
Layer 3 backward:  [compute grad‚ÇÉ]        [AllReduce‚ÇÑ running]
...

Communication is (mostly) hidden behind computation!</code></pre>
            <h3 id="pytorch-ddp-implementation">PyTorch DDP
            Implementation</h3>
            <div class="sourceCode" id="cb279"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb279-2"><a href="#cb279-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb279-3"><a href="#cb279-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span>
<span id="cb279-4"><a href="#cb279-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-5"><a href="#cb279-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize process group (one process per GPU)</span></span>
<span id="cb279-6"><a href="#cb279-6" aria-hidden="true" tabindex="-1"></a>dist.init_process_group(</span>
<span id="cb279-7"><a href="#cb279-7" aria-hidden="true" tabindex="-1"></a>    backend<span class="op">=</span><span class="st">&#39;nccl&#39;</span>,  <span class="co"># Use NCCL for GPU communication (fastest)</span></span>
<span id="cb279-8"><a href="#cb279-8" aria-hidden="true" tabindex="-1"></a>    init_method<span class="op">=</span><span class="st">&#39;env://&#39;</span>,  <span class="co"># Get rank/world_size from environment</span></span>
<span id="cb279-9"><a href="#cb279-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb279-10"><a href="#cb279-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-11"><a href="#cb279-11" aria-hidden="true" tabindex="-1"></a>local_rank <span class="op">=</span> <span class="bu">int</span>(os.environ[<span class="st">&#39;LOCAL_RANK&#39;</span>])</span>
<span id="cb279-12"><a href="#cb279-12" aria-hidden="true" tabindex="-1"></a>torch.cuda.set_device(local_rank)</span>
<span id="cb279-13"><a href="#cb279-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-14"><a href="#cb279-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap model in DDP</span></span>
<span id="cb279-15"><a href="#cb279-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyModel().cuda()</span>
<span id="cb279-16"><a href="#cb279-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DDP(model, device_ids<span class="op">=</span>[local_rank])</span>
<span id="cb279-17"><a href="#cb279-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-18"><a href="#cb279-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create distributed sampler (ensures non-overlapping data)</span></span>
<span id="cb279-19"><a href="#cb279-19" aria-hidden="true" tabindex="-1"></a>sampler <span class="op">=</span> torch.utils.data.distributed.DistributedSampler(dataset)</span>
<span id="cb279-20"><a href="#cb279-20" aria-hidden="true" tabindex="-1"></a>dataloader <span class="op">=</span> DataLoader(dataset, sampler<span class="op">=</span>sampler, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb279-21"><a href="#cb279-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb279-22"><a href="#cb279-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop ‚Äî looks almost identical to single GPU!</span></span>
<span id="cb279-23"><a href="#cb279-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb279-24"><a href="#cb279-24" aria-hidden="true" tabindex="-1"></a>    sampler.set_epoch(epoch)  <span class="co"># Important: shuffle differently each epoch</span></span>
<span id="cb279-25"><a href="#cb279-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb279-26"><a href="#cb279-26" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb279-27"><a href="#cb279-27" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> model(batch)</span>
<span id="cb279-28"><a href="#cb279-28" aria-hidden="true" tabindex="-1"></a>        loss.backward()  <span class="co"># ‚Üê AllReduce happens here automatically</span></span>
<span id="cb279-29"><a href="#cb279-29" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div>
            <h3 id="scaling-efficiency-1">Scaling Efficiency</h3>
            <p><strong>Ideal speedup</strong>: 4 GPUs ‚Üí 4√ó faster
            <strong>Real speedup</strong>: 4 GPUs ‚Üí ~3.6√ó faster (90%
            efficiency)</p>
            <p>Where does the 10% go? 1. <strong>Communication
            overhead</strong>: AllReduce takes time 2.
            <strong>Synchronization</strong>: GPUs wait for slowest one
            3. <strong>Batch size effects</strong>: Larger effective
            batch may need LR adjustment</p>
            <pre><code>Scaling efficiency:

GPUs    Speedup    Efficiency
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1       1.0√ó       100%
2       1.9√ó       95%
4       3.6√ó       90%
8       6.8√ó       85%
16      12.0√ó      75%
64      40.0√ó      62%</code></pre>
            <p><strong>Rule of thumb</strong>: DDP scales well to ~8
            GPUs on same node, degrades across nodes due to network
            bandwidth.</p>
            <h3
            id="interview-q-walk-me-through-how-ddp-synchronizes-gradients">Interview
            Q: ‚ÄúWalk me through how DDP synchronizes gradients‚Äù</h3>
            <p><strong>A</strong>:</p>
            <ol type="1">
            <li>Each GPU has a full copy of the model and processes a
            different mini-batch</li>
            <li>Forward pass happens independently on each GPU</li>
            <li>During backward pass, as each layer‚Äôs gradients are
            computed, DDP <strong>hooks</strong> trigger AllReduce</li>
            <li>AllReduce (Ring algorithm) averages gradients across all
            GPUs efficiently ‚Äî each GPU sends/receives ~2√ó its gradient
            size total, regardless of GPU count</li>
            <li>Once AllReduce completes for a layer, all GPUs have
            identical averaged gradients</li>
            <li>Optimizer step happens independently but identically on
            each GPU</li>
            <li>Since they started with same weights and got same
            gradients, they stay synchronized</li>
            </ol>
            <p>The key optimization is <strong>overlapping</strong>:
            AllReduce for layer N runs while layer N-1 backward is
            computed.</p>
            <hr />
            <h2 id="tensor-parallelism">8.2 Tensor Parallelism</h2>
            <h3 id="when-data-parallelism-isnt-enough">When Data
            Parallelism Isn‚Äôt Enough</h3>
            <p>Data parallelism requires each GPU to hold the
            <strong>full model</strong>. For a 70B parameter model: -
            Parameters: 70B √ó 4 bytes = 280 GB (FP32) - Even in FP16:
            140 GB - Single GPU (A100): 80 GB max</p>
            <p><strong>The model doesn‚Äôt fit!</strong> We need to split
            the model itself.</p>
            <h3 id="the-core-idea-split-weight-matrices">The Core Idea:
            Split Weight Matrices</h3>
            <p>Instead of one GPU doing: <span class="math inline">\(Y =
            XW\)</span></p>
            <p>Two GPUs can do: <span class="math inline">\(Y = X[W_0 |
            W_1] = [Y_0 | Y_1]\)</span></p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Tensor Parallelism                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Standard (single GPU):                                            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  X [batch, hidden] √ó W [hidden, 4√óhidden] = Y [batch, 4√óhidden]   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Tensor Parallel (2 GPUs):                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  GPU 0: X √ó W‚ÇÄ [hidden, 2√óhidden] = Y‚ÇÄ [batch, 2√óhidden]          ‚îÇ
‚îÇ  GPU 1: X √ó W‚ÇÅ [hidden, 2√óhidden] = Y‚ÇÅ [batch, 2√óhidden]          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  No communication needed for this matmul!                          ‚îÇ
‚îÇ  Y = [Y‚ÇÄ | Y‚ÇÅ] (concatenate)                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="column-parallel-vs-row-parallel">Column Parallel vs
            Row Parallel</h3>
            <p><strong>Column Parallel</strong>: Split weight
            columns</p>
            <pre><code>Y = X @ [W_col0 | W_col1]
    = [X @ W_col0 | X @ W_col1]
    = [Y0 | Y1]

- Input X: broadcast to all GPUs
- Output Y: partitioned across GPUs
- No communication during compute!</code></pre>
            <p><strong>Row Parallel</strong>: Split weight rows</p>
            <pre><code>Y = X @ [[W_row0], [W_row1]]
    = X0 @ W_row0 + X1 @ W_row1  (where X = [X0 | X1])
    = Y0 + Y1

- Input X: partitioned across GPUs
- Output Y: requires AllReduce (sum)!</code></pre>
            <h3 id="transformer-ffn-with-tensor-parallelism">Transformer
            FFN with Tensor Parallelism</h3>
            <p>The transformer FFN has two linear layers:</p>
            <p><span class="math display">\[\text{FFN}(x) =
            \text{GELU}(xW_1)W_2\]</span></p>
            <p><strong>Megatron-LM strategy</strong>: Column-parallel
            first, row-parallel second:</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Tensor Parallel FFN (Megatron-style)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Input x (on all GPUs)                                             ‚îÇ
‚îÇ         ‚Üì                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ W‚ÇÅ Column Parallel (no communication)   ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ  GPU 0: x @ W‚ÇÅ‚ÇÄ ‚Üí h‚ÇÄ                    ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ  GPU 1: x @ W‚ÇÅ‚ÇÅ ‚Üí h‚ÇÅ                    ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ         ‚Üì                                                          ‚îÇ
‚îÇ  GELU activation (local, no comm)                                  ‚îÇ
‚îÇ         ‚Üì                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ W‚ÇÇ Row Parallel (AllReduce at end)      ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ  GPU 0: h‚ÇÄ @ W‚ÇÇ‚ÇÄ ‚Üí y‚ÇÄ                   ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ  GPU 1: h‚ÇÅ @ W‚ÇÇ‚ÇÅ ‚Üí y‚ÇÅ                   ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ                                          ‚îÇ                      ‚îÇ
‚îÇ  ‚îÇ  AllReduce: y = y‚ÇÄ + y‚ÇÅ                 ‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ         ‚Üì                                                          ‚îÇ
‚îÇ  Output y (on all GPUs)                                            ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Total communication: ONE AllReduce per FFN layer!                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Why this pairing works</strong>: Column-parallel
            produces partitioned output, which is exactly what
            row-parallel needs as input! This is the key insight behind
            Megatron-style tensor parallelism. If we used
            column-parallel for both W‚ÇÅ and W‚ÇÇ, we‚Äôd need an AllGather
            between them (to reconstruct the full hidden state) and then
            another AllReduce at the end‚Äîtwo communication operations.
            By pairing column‚Üírow, we avoid the intermediate
            communication entirely. The column-parallel W‚ÇÅ naturally
            partitions the hidden dimension across GPUs, and
            row-parallel W‚ÇÇ consumes that partitioned input directly.
            Only at the very end do we need one AllReduce to sum the
            partial outputs. This halves the communication cost per FFN
            layer.</p>
            <h3 id="attention-with-tensor-parallelism">Attention with
            Tensor Parallelism</h3>
            <p>Multi-head attention naturally parallelizes ‚Äî just put
            different heads on different GPUs:</p>
            <pre><code>8-head attention with TP=4:

GPU 0: Heads 0, 1  ‚Üí  Compute attention  ‚Üí  Output partition 0
GPU 1: Heads 2, 3  ‚Üí  Compute attention  ‚Üí  Output partition 1
GPU 2: Heads 4, 5  ‚Üí  Compute attention  ‚Üí  Output partition 2
GPU 3: Heads 6, 7  ‚Üí  Compute attention  ‚Üí  Output partition 3

AllReduce after output projection (or AllGather + linear)</code></pre>
            <h3 id="communication-analysis">Communication Analysis</h3>
            <p><strong>Per transformer layer with TP=N</strong>: - FFN:
            2 AllReduce (forward) + 2 AllReduce (backward) = 4 AllReduce
            - Attention: Similar, ~4 AllReduce</p>
            <p><strong>Total communication volume per layer</strong>:
            <span class="math inline">\(O(B \times S \times H)\)</span>
            where B=batch, S=seq_len, H=hidden</p>
            <p>This is <strong>independent of model size</strong> ‚Äî only
            depends on activation size!</p>
            <h3 id="why-tp-is-usually-2-8">Why TP is Usually 2-8</h3>
            <p><strong>Intra-node</strong> (GPUs on same machine):
            NVLink provides 600+ GB/s <strong>Inter-node</strong>
            (across machines): InfiniBand provides ~50 GB/s</p>
            <p>TP needs <strong>low latency, high bandwidth</strong>
            communication ‚Üí keep within one node.</p>
            <table>
            <thead>
            <tr>
            <th>TP Degree</th>
            <th>Typical Setup</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>TP=2</td>
            <td>Small models, 2 GPUs</td>
            </tr>
            <tr>
            <td>TP=4</td>
            <td>Medium models, single node</td>
            </tr>
            <tr>
            <td>TP=8</td>
            <td>Large models, 8-GPU node (A100/H100)</td>
            </tr>
            <tr>
            <td>TP=16</td>
            <td>Very rare, 2 nodes (communication hurts)</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-why-is-tensor-parallelism-usually-limited-to-2-8-gpus">Interview
            Q: ‚ÄúWhy is tensor parallelism usually limited to 2-8
            GPUs?‚Äù</h3>
            <p><strong>A</strong>: Tensor parallelism requires
            <strong>many small AllReduce operations</strong> (one per
            layer, per micro-batch). Unlike data parallelism where one
            large AllReduce can hide behind computation, TP‚Äôs small
            frequent communications are latency-sensitive.</p>
            <p>Within a node, NVLink provides 600+ GB/s with microsecond
            latency ‚Äî TP works well. Across nodes, even InfiniBand‚Äôs 50
            GB/s with higher latency creates bottlenecks. That‚Äôs why TP
            is typically kept within a single 8-GPU node, and scaling
            beyond uses data parallelism or pipeline parallelism
            instead.</p>
            <hr />
            <h2 id="pipeline-parallelism">8.3 Pipeline Parallelism</h2>
            <h3 id="the-problem-model-too-large-tp-not-enough">The
            Problem: Model Too Large, TP Not Enough</h3>
            <p>For a 175B model: - Even TP=8 means 22B params per GPU =
            88GB (FP32) - Still doesn‚Äôt fit!</p>
            <p><strong>Solution</strong>: Split by layers instead of by
            weight matrices.</p>
            <h3 id="the-core-idea-layer-partitioning">The Core Idea:
            Layer Partitioning</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Pipeline Parallelism                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  48-layer transformer split across 4 GPUs:                         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  GPU 0   ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  GPU 1   ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  GPU 2   ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  GPU 3   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ Layers   ‚îÇ    ‚îÇ Layers   ‚îÇ    ‚îÇ Layers   ‚îÇ    ‚îÇ Layers   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  0-11    ‚îÇ    ‚îÇ  12-23   ‚îÇ    ‚îÇ  24-35   ‚îÇ    ‚îÇ  36-47   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Each GPU only stores 12 layers ‚Üí 4√ó memory reduction!             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="the-bubble-problem">The Bubble Problem</h3>
            <p><strong>Naive pipeline</strong> has GPUs sitting
            idle:</p>
            <pre><code>Time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí

GPU 0: [Forward]                              [Backward]
GPU 1:          [Forward]              [Backward]
GPU 2:                   [Forward][Backward]
GPU 3:                           [F][B]

        ‚Üë GPU 3 idle!    ‚Üë GPU 0 idle!
        
&quot;Bubble&quot; = wasted compute time</code></pre>
            <p><strong>Bubble fraction with P pipeline stages, 1
            micro-batch</strong>:</p>
            <p><span class="math display">\[\text{Bubble} =
            \frac{P-1}{P} = \frac{3}{4} = 75\% \text{
            wasted!}\]</span></p>
            <h3 id="micro-batching-the-solution">Micro-batching: The
            Solution</h3>
            <p>Split the batch into M <strong>micro-batches</strong> and
            pipeline them:</p>
            <pre><code>Time ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí

M=4 micro-batches, P=4 pipeline stages:

GPU 0: [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][F‚ÇÑ]          [B‚ÇÑ][B‚ÇÉ][B‚ÇÇ][B‚ÇÅ]
GPU 1:     [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][F‚ÇÑ]   [B‚ÇÑ][B‚ÇÉ][B‚ÇÇ][B‚ÇÅ]
GPU 2:         [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][F‚ÇÑ][B‚ÇÑ][B‚ÇÉ][B‚ÇÇ][B‚ÇÅ]
GPU 3:             [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][F‚ÇÑ][B‚ÇÑ][B‚ÇÉ][B‚ÇÇ][B‚ÇÅ]

                   ‚Üë Much smaller bubble!</code></pre>
            <p><strong>Bubble fraction with M
            micro-batches</strong>:</p>
            <p><span class="math display">\[\text{Bubble} = \frac{P-1}{M
            + P - 1}\]</span></p>
            <p>With M=32, P=4: Bubble = 3/35 ‚âà 9% ‚Äî much better!</p>
            <h3 id="gpipe-vs-pipedream">GPipe vs PipeDream</h3>
            <p><strong>GPipe</strong> (synchronous): - All forward
            micro-batches, then all backward - Simpler, deterministic -
            Larger memory (store all activations)</p>
            <p><strong>PipeDream</strong> (asynchronous, 1F1B schedule):
            - Interleave forward and backward - Lower memory (process
            backward immediately) - More complex scheduling</p>
            <pre><code>PipeDream 1F1B schedule:

GPU 0: [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][F‚ÇÑ][B‚ÇÅ][F‚ÇÖ][B‚ÇÇ][F‚ÇÜ][B‚ÇÉ]...
GPU 1:     [F‚ÇÅ][F‚ÇÇ][F‚ÇÉ][B‚ÇÅ][F‚ÇÑ][B‚ÇÇ][F‚ÇÖ][B‚ÇÉ]...
GPU 2:         [F‚ÇÅ][F‚ÇÇ][B‚ÇÅ][F‚ÇÉ][B‚ÇÇ][F‚ÇÑ][B‚ÇÉ]...
GPU 3:             [F‚ÇÅ][B‚ÇÅ][F‚ÇÇ][B‚ÇÇ][F‚ÇÉ][B‚ÇÉ]...

Backward starts as soon as possible ‚Üí lower peak memory!</code></pre>
            <p><strong>Why 1F1B reduces peak memory</strong>: In GPipe,
            all M micro-batches complete their forward passes before any
            backward pass begins. This means each GPU must store
            activations for all M micro-batches simultaneously‚Äîpeak
            memory scales with M. In 1F1B, once micro-batch 1 completes
            forward on all stages, its backward pass starts immediately.
            As soon as B‚ÇÅ finishes on a stage, those activations are
            freed. The key insight is that at steady state, each GPU
            only holds activations for ~P micro-batches (the pipeline
            depth), not M micro-batches. For M &gt;&gt; P, this is a
            substantial memory reduction. The trade-off is more complex
            scheduling logic and potential for weight staleness in
            asynchronous variants.</p>
            <h3 id="communication-in-pipeline-parallelism">Communication
            in Pipeline Parallelism</h3>
            <p><strong>Between stages</strong>: Only activations, not
            weights!</p>
            <pre><code>Communication per micro-batch:

GPU k ‚Üí GPU k+1: Activations [batch, seq_len, hidden_dim]

For hidden_dim=4096, seq_len=2048, micro_batch=1:
Size = 1 √ó 2048 √ó 4096 √ó 2 bytes (FP16) = 16 MB

Compare to TP AllReduce: Same 16 MB but P2P is easier to pipeline</code></pre>
            <p><strong>Point-to-point</strong> communication (not
            AllReduce) ‚Üí easier to hide behind compute.</p>
            <h3 id="memory-analysis">Memory Analysis</h3>
            <p><strong>Without PP</strong> (all layers on one GPU): -
            Parameters: <span class="math inline">\(L \times P\)</span>
            (all layers) - Activations: <span class="math inline">\(B
            \times S \times H \times L\)</span> (all layers)</p>
            <p><strong>With PP</strong> (L/K layers per GPU): -
            Parameters: <span class="math inline">\(L \times P /
            K\)</span> - Activations: <span class="math inline">\(B
            \times S \times H \times L / K\)</span> (but √ó M for
            micro-batches!)</p>
            <p>Trade-off: PP saves parameter memory but micro-batching
            increases activation memory.</p>
            <h3
            id="interview-q-explain-the-bubble-problem-in-pipeline-parallelism">Interview
            Q: ‚ÄúExplain the bubble problem in pipeline parallelism‚Äù</h3>
            <p><strong>A</strong>: In pipeline parallelism, GPUs are
            partitioned by layers. GPU 1 can‚Äôt start until GPU 0
            finishes its forward pass and sends activations. Similarly,
            GPU 0 can‚Äôt start backward until GPU N finishes and sends
            gradients back. This creates <strong>idle time</strong>
            called the ‚Äúbubble.‚Äù</p>
            <p>With P pipeline stages and 1 micro-batch, bubble fraction
            is (P-1)/P ‚Äî for P=4, that‚Äôs 75% wasted! The solution is
            <strong>micro-batching</strong>: split the batch into M
            pieces and pipeline them. This reduces bubble to
            (P-1)/(M+P-1). With M=32, P=4, bubble is only ~9%.</p>
            <p>The trade-off: more micro-batches means more activations
            stored (or recomputed). Modern schedulers like 1F1B (one
            forward, one backward) interleave forward and backward
            passes to reduce peak memory.</p>
            <hr />
            <h2 id="zero-zero-redundancy-optimizer-1">8.4 ZeRO: Zero
            Redundancy Optimizer</h2>
            <h3 id="the-memory-problem-in-data-parallelism">The Memory
            Problem in Data Parallelism</h3>
            <p>With standard DDP, <strong>every GPU stores
            everything</strong>:</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Memory per GPU in Data Parallelism                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  For a 7B parameter model with Adam optimizer in FP32:             ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Parameters:           7B √ó 4 bytes =  28 GB                       ‚îÇ
‚îÇ  Gradients:            7B √ó 4 bytes =  28 GB                       ‚îÇ
‚îÇ  Optimizer states:                                                 ‚îÇ
‚îÇ    - m (momentum):     7B √ó 4 bytes =  28 GB                       ‚îÇ
‚îÇ    - v (variance):     7B √ó 4 bytes =  28 GB                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                         ‚îÇ
‚îÇ  Total per GPU:                        112 GB                      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  With 8 GPUs: 8 √ó 112 GB = 896 GB total                           ‚îÇ
‚îÇ  But we only NEED 112 GB! (8√ó redundancy)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3
            id="zero-key-insight-partition-instead-of-replicate">ZeRO
            Key Insight: Partition Instead of Replicate</h3>
            <p>Instead of every GPU having everything,
            <strong>partition</strong> state across GPUs and
            <strong>gather</strong> when needed.</p>
            <h3 id="zero-stage-1-partition-optimizer-states">ZeRO Stage
            1: Partition Optimizer States</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ZeRO Stage 1                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  8 GPUs, each stores 1/8 of optimizer states:                      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  GPU 0: Full params, full grads, optimizer for params 0-0.875B    ‚îÇ
‚îÇ  GPU 1: Full params, full grads, optimizer for params 0.875-1.75B ‚îÇ
‚îÇ  ...                                                               ‚îÇ
‚îÇ  GPU 7: Full params, full grads, optimizer for params 6.125-7B    ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Memory per GPU:                                                   ‚îÇ
‚îÇ    Parameters:    28 GB (full)                                     ‚îÇ
‚îÇ    Gradients:     28 GB (full)                                     ‚îÇ
‚îÇ    Optimizer:      7 GB (1/8th) ‚Üê 8√ó reduction!                   ‚îÇ
‚îÇ    Total:         63 GB                                            ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  After optimizer step, AllGather updated parameters.               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="zero-stage-2-partition-gradients">ZeRO Stage 2: +
            Partition Gradients</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ZeRO Stage 2                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  GPU 0: Full params, grads 0-0.875B, optimizer for 0-0.875B       ‚îÇ
‚îÇ  GPU 1: Full params, grads 0.875-1.75B, optimizer for 0.875-1.75B ‚îÇ
‚îÇ  ...                                                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Memory per GPU:                                                   ‚îÇ
‚îÇ    Parameters:    28 GB (full)                                     ‚îÇ
‚îÇ    Gradients:    3.5 GB (1/8th) ‚Üê Additional 8√ó reduction!        ‚îÇ
‚îÇ    Optimizer:      7 GB (1/8th)                                    ‚îÇ
‚îÇ    Total:       38.5 GB                                            ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Communication change: Use ReduceScatter instead of AllReduce     ‚îÇ
‚îÇ  (Each GPU gets only the gradients it needs)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="zero-stage-3-partition-parameters">ZeRO Stage 3: +
            Partition Parameters</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ZeRO Stage 3                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  GPU 0: params 0-0.875B, grads 0-0.875B, optimizer 0-0.875B       ‚îÇ
‚îÇ  GPU 1: params 0.875-1.75B, grads 0.875-1.75B, opt 0.875-1.75B    ‚îÇ
‚îÇ  ...                                                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Memory per GPU:                                                   ‚îÇ
‚îÇ    Parameters:   3.5 GB (1/8th) ‚Üê Another 8√ó reduction!           ‚îÇ
‚îÇ    Gradients:    3.5 GB (1/8th)                                    ‚îÇ
‚îÇ    Optimizer:      7 GB (1/8th)                                    ‚îÇ
‚îÇ    Total:         14 GB ‚Üê Fits on any modern GPU!                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Trade-off: Must AllGather parameters BEFORE each layer&#39;s forward  ‚îÇ
‚îÇ  and backward pass.                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="communication-trade-offs">Communication
            Trade-offs</h3>
            <table>
            <thead>
            <tr>
            <th>Stage</th>
            <th>Memory/GPU</th>
            <th>Communication Volume</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>DDP (baseline)</td>
            <td>4√ó params</td>
            <td>2 √ó params (AllReduce grads)</td>
            </tr>
            <tr>
            <td><strong>ZeRO-1</strong></td>
            <td>4√ó ‚Üí ~1.5√ó</td>
            <td>Same as DDP</td>
            </tr>
            <tr>
            <td><strong>ZeRO-2</strong></td>
            <td>~1.5√ó ‚Üí ~1.2√ó</td>
            <td>Same as DDP (ReduceScatter + AllGather)</td>
            </tr>
            <tr>
            <td><strong>ZeRO-3</strong></td>
            <td>~1.2√ó ‚Üí 1/N</td>
            <td>3√ó DDP (AllGather params twice per layer)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why ZeRO-3 has 3√ó the communication of
            DDP</strong>: In standard DDP, communication happens once
            per training step‚Äîan AllReduce on gradients (2√ó model_size
            total: reduce-scatter + all-gather). ZeRO-3 partitions
            parameters, so before each layer‚Äôs forward pass, we must
            AllGather the full parameters for that layer (1√ó
            layer_size). Then during backward, we AllGather again
            (another 1√ó layer_size) because we discarded them after
            forward. Finally, we do the gradient sync (ReduceScatter for
            1√ó layer_size, then AllGather updated params for 1√ó
            layer_size). Summed across all layers, this is roughly 3√ó
            the communication of DDP. The trade-off is worthwhile when
            the model simply doesn‚Äôt fit in memory otherwise‚Äîyou‚Äôre
            trading bandwidth for the ability to train at all.</p>
            <h3 id="fsdp-pytorchs-native-zero">FSDP: PyTorch‚Äôs Native
            ZeRO</h3>
            <p><strong>FSDP (Fully Sharded Data Parallel)</strong> is
            PyTorch‚Äôs built-in implementation of ZeRO, introduced in
            PyTorch 1.11.</p>
            <p><strong>ZeRO vs FSDP equivalence</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>ZeRO Stage</th>
            <th>FSDP Equivalent</th>
            <th>What‚Äôs Sharded</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>ZeRO-1</strong></td>
            <td><code>NO_SHARD</code> + manual</td>
            <td>Optimizer states</td>
            </tr>
            <tr>
            <td><strong>ZeRO-2</strong></td>
            <td><code>SHARD_GRAD_OP</code></td>
            <td>Optimizer + Gradients</td>
            </tr>
            <tr>
            <td><strong>ZeRO-3</strong></td>
            <td><code>FULL_SHARD</code></td>
            <td>Optimizer + Gradients + Parameters</td>
            </tr>
            </tbody>
            </table>
            <p><strong>FSDP Implementation</strong>:</p>
            <div class="sourceCode" id="cb295"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb295-1"><a href="#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb295-2"><a href="#cb295-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed.fsdp <span class="im">import</span> (</span>
<span id="cb295-3"><a href="#cb295-3" aria-hidden="true" tabindex="-1"></a>    FullyShardedDataParallel <span class="im">as</span> FSDP,</span>
<span id="cb295-4"><a href="#cb295-4" aria-hidden="true" tabindex="-1"></a>    ShardingStrategy,</span>
<span id="cb295-5"><a href="#cb295-5" aria-hidden="true" tabindex="-1"></a>    CPUOffload,</span>
<span id="cb295-6"><a href="#cb295-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb295-7"><a href="#cb295-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed.fsdp.wrap <span class="im">import</span> transformer_auto_wrap_policy</span>
<span id="cb295-8"><a href="#cb295-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-9"><a href="#cb295-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define which modules to wrap (important for transformers)</span></span>
<span id="cb295-10"><a href="#cb295-10" aria-hidden="true" tabindex="-1"></a>auto_wrap_policy <span class="op">=</span> transformer_auto_wrap_policy(</span>
<span id="cb295-11"><a href="#cb295-11" aria-hidden="true" tabindex="-1"></a>    transformer_layer_cls<span class="op">=</span>{TransformerBlock},  <span class="co"># Your layer class</span></span>
<span id="cb295-12"><a href="#cb295-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb295-13"><a href="#cb295-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-14"><a href="#cb295-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Wrap model with FSDP</span></span>
<span id="cb295-15"><a href="#cb295-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> FSDP(</span>
<span id="cb295-16"><a href="#cb295-16" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb295-17"><a href="#cb295-17" aria-hidden="true" tabindex="-1"></a>    sharding_strategy<span class="op">=</span>ShardingStrategy.FULL_SHARD,  <span class="co"># = ZeRO-3</span></span>
<span id="cb295-18"><a href="#cb295-18" aria-hidden="true" tabindex="-1"></a>    cpu_offload<span class="op">=</span>CPUOffload(offload_params<span class="op">=</span><span class="va">True</span>),    <span class="co"># Optional CPU offload</span></span>
<span id="cb295-19"><a href="#cb295-19" aria-hidden="true" tabindex="-1"></a>    auto_wrap_policy<span class="op">=</span>auto_wrap_policy,</span>
<span id="cb295-20"><a href="#cb295-20" aria-hidden="true" tabindex="-1"></a>    device_id<span class="op">=</span>torch.cuda.current_device(),</span>
<span id="cb295-21"><a href="#cb295-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb295-22"><a href="#cb295-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-23"><a href="#cb295-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop is standard!</span></span>
<span id="cb295-24"><a href="#cb295-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb295-25"><a href="#cb295-25" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb295-26"><a href="#cb295-26" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch)</span>
<span id="cb295-27"><a href="#cb295-27" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb295-28"><a href="#cb295-28" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
            <p><strong>FSDP Sharding Strategies</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Strategy</th>
            <th>Memory</th>
            <th>Communication</th>
            <th>Use Case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>NO_SHARD</code></td>
            <td>High (like DDP)</td>
            <td>Low</td>
            <td>Small models</td>
            </tr>
            <tr>
            <td><code>SHARD_GRAD_OP</code></td>
            <td>Medium</td>
            <td>Medium</td>
            <td>Medium models</td>
            </tr>
            <tr>
            <td><code>FULL_SHARD</code></td>
            <td>Low (= ZeRO-3)</td>
            <td>High</td>
            <td>Large models</td>
            </tr>
            <tr>
            <td><code>HYBRID_SHARD</code></td>
            <td>Configurable</td>
            <td>Medium</td>
            <td>Multi-node</td>
            </tr>
            </tbody>
            </table>
            <p><strong>FSDP vs DeepSpeed ZeRO</strong>:</p>
            <table>
            <thead>
            <tr>
            <th></th>
            <th><strong>DeepSpeed ZeRO</strong></th>
            <th><strong>PyTorch FSDP</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Dependencies</strong></td>
            <td>Needs DeepSpeed library</td>
            <td>Native PyTorch</td>
            </tr>
            <tr>
            <td><strong>Maturity</strong></td>
            <td>More mature, optimized</td>
            <td>Catching up rapidly</td>
            </tr>
            <tr>
            <td><strong>Ease of use</strong></td>
            <td>Config-file driven</td>
            <td>Pythonic API</td>
            </tr>
            <tr>
            <td><strong>Integration</strong></td>
            <td>Works with HF Trainer</td>
            <td>Works with any PyTorch</td>
            </tr>
            <tr>
            <td><strong>Debugging</strong></td>
            <td>Harder (separate library)</td>
            <td>Easier (native)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>When to use which</strong>: -
            <strong>FSDP</strong>: New projects, want native PyTorch,
            simpler debugging - <strong>DeepSpeed</strong>: Need
            specific features (ZeRO++, inference optimizations),
            existing DeepSpeed codebase</p>
            <hr />
            <h3 id="zero-offload-use-cpu-memory-too">ZeRO-Offload: Use
            CPU Memory Too</h3>
            <p>When GPU memory still isn‚Äôt enough:</p>
            <p><strong>DeepSpeed Offload</strong>:</p>
            <div class="sourceCode" id="cb296"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb296-1"><a href="#cb296-1" aria-hidden="true" tabindex="-1"></a>ds_config <span class="op">=</span> {</span>
<span id="cb296-2"><a href="#cb296-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;zero_optimization&quot;</span>: {</span>
<span id="cb296-3"><a href="#cb296-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;stage&quot;</span>: <span class="dv">3</span>,</span>
<span id="cb296-4"><a href="#cb296-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;offload_param&quot;</span>: {</span>
<span id="cb296-5"><a href="#cb296-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;device&quot;</span>: <span class="st">&quot;cpu&quot;</span>,       <span class="co"># Store params on CPU</span></span>
<span id="cb296-6"><a href="#cb296-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;pin_memory&quot;</span>: <span class="va">True</span>     <span class="co"># Faster CPU-GPU transfer</span></span>
<span id="cb296-7"><a href="#cb296-7" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb296-8"><a href="#cb296-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;offload_optimizer&quot;</span>: {</span>
<span id="cb296-9"><a href="#cb296-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;device&quot;</span>: <span class="st">&quot;cpu&quot;</span>        <span class="co"># Store optimizer on CPU</span></span>
<span id="cb296-10"><a href="#cb296-10" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb296-11"><a href="#cb296-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb296-12"><a href="#cb296-12" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
            <p><strong>FSDP Offload</strong>:</p>
            <div class="sourceCode" id="cb297"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb297-1"><a href="#cb297-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed.fsdp <span class="im">import</span> CPUOffload</span>
<span id="cb297-2"><a href="#cb297-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb297-3"><a href="#cb297-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> FSDP(</span>
<span id="cb297-4"><a href="#cb297-4" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb297-5"><a href="#cb297-5" aria-hidden="true" tabindex="-1"></a>    sharding_strategy<span class="op">=</span>ShardingStrategy.FULL_SHARD,</span>
<span id="cb297-6"><a href="#cb297-6" aria-hidden="true" tabindex="-1"></a>    cpu_offload<span class="op">=</span>CPUOffload(offload_params<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb297-7"><a href="#cb297-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
            <p><strong>Trade-off</strong>: PCIe bandwidth (~32 GB/s) is
            much slower than HBM (~2 TB/s), but enables training models
            that otherwise wouldn‚Äôt fit.</p>
            <h3 id="when-to-use-zerofsdp-vs-tensor-parallelism">When to
            Use ZeRO/FSDP vs Tensor Parallelism</h3>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 46%" />
            <col style="width: 17%" />
            </colgroup>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Best Choice</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Model fits with TP=8</td>
            <td><strong>Tensor Parallel</strong></td>
            <td>Lower communication overhead</td>
            </tr>
            <tr>
            <td>Model needs &gt;8-way split</td>
            <td><strong>ZeRO-3</strong></td>
            <td>Scales to any # of GPUs</td>
            </tr>
            <tr>
            <td>Training across nodes</td>
            <td><strong>ZeRO-2/3</strong></td>
            <td>AllReduce more network-friendly than TP</td>
            </tr>
            <tr>
            <td>Single node, memory tight</td>
            <td><strong>ZeRO-3</strong></td>
            <td>Maximum memory efficiency</td>
            </tr>
            <tr>
            <td>Inference</td>
            <td><strong>Neither</strong></td>
            <td>No optimizer states!</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-when-would-you-use-zero-3-vs-tensor-parallelism">Interview
            Q: ‚ÄúWhen would you use ZeRO-3 vs Tensor Parallelism?‚Äù</h3>
            <p><strong>A</strong>:</p>
            <p><strong>Use Tensor Parallelism when</strong>: - Model
            fits with TP ‚â§ 8 (single node) - You want lowest latency (TP
            has fewer communication rounds) - You‚Äôre already maxing out
            DP across nodes</p>
            <p><strong>Use ZeRO-3 when</strong>: - Model too large for
            TP=8 - Training across multiple nodes (ZeRO‚Äôs AllReduce is
            more network-friendly than TP‚Äôs frequent small messages) -
            You want simpler code (ZeRO is a drop-in with DeepSpeed)</p>
            <p><strong>Often use both</strong>: TP within node (8-way) +
            ZeRO-3 across nodes. This is ‚Äú3D parallelism‚Äù with DP (via
            ZeRO) + TP + PP.</p>
            <hr />
            <h2 id="memory-optimization-techniques">8.5 Memory
            Optimization Techniques</h2>
            <h3 id="gradient-accumulation-1">Gradient Accumulation</h3>
            <p><strong>Problem</strong>: Want large batch size, but GPU
            memory is limited.</p>
            <p><strong>Solution</strong>: Accumulate gradients over
            multiple mini-batches, then update.</p>
            <pre><code>Standard Training (batch_size=32):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[Batch of 32] ‚Üí Forward ‚Üí Backward ‚Üí Update weights

Gradient Accumulation (effective_batch=32, micro_batch=8):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[Micro-batch 8] ‚Üí Forward ‚Üí Backward ‚Üí Accumulate
[Micro-batch 8] ‚Üí Forward ‚Üí Backward ‚Üí Accumulate
[Micro-batch 8] ‚Üí Forward ‚Üí Backward ‚Üí Accumulate
[Micro-batch 8] ‚Üí Forward ‚Üí Backward ‚Üí Accumulate ‚Üí Update weights</code></pre>
            <p><strong>Implementation</strong>:</p>
            <div class="sourceCode" id="cb299"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb299-1"><a href="#cb299-1" aria-hidden="true" tabindex="-1"></a>accumulation_steps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb299-2"><a href="#cb299-2" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb299-3"><a href="#cb299-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb299-4"><a href="#cb299-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb299-5"><a href="#cb299-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch) <span class="op">/</span> accumulation_steps  <span class="co"># Scale loss!</span></span>
<span id="cb299-6"><a href="#cb299-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()  <span class="co"># Gradients accumulate (they add up in .grad)</span></span>
<span id="cb299-7"><a href="#cb299-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb299-8"><a href="#cb299-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb299-9"><a href="#cb299-9" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb299-10"><a href="#cb299-10" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div>
            <p><strong>Key points</strong>:</p>
            <ul>
            <li>Divide loss by <code>accumulation_steps</code> to get
            correct average</li>
            <li>Mathematically equivalent to larger batch (for SGD)</li>
            <li>Memory stays constant (only one micro-batch at a
            time)</li>
            <li>Compute time increases (no parallelism benefit)</li>
            </ul>
            <p><strong>When to use</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Use Gradient Accumulation</th>
            <th>Don‚Äôt Use</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Single GPU, need large batch</td>
            <td>Already using data parallelism</td>
            </tr>
            <tr>
            <td>Memory constrained</td>
            <td>Latency matters (adds overhead)</td>
            </tr>
            <tr>
            <td>Batch norm issues at small batch</td>
            <td>Very small datasets</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h3 id="mixed-precision-training-1">Mixed Precision
            Training</h3>
            <p><strong>Problem</strong>: FP32 (32-bit floats) use too
            much memory and bandwidth.</p>
            <p><strong>Solution</strong>: Use FP16/BF16 for most
            operations, FP32 for sensitive ones.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Mixed Precision Training                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  FP32 Master Weights ‚îÄ‚îÄ‚Üí Cast to FP16 ‚îÄ‚îÄ‚Üí Forward Pass (FP16)      ‚îÇ
‚îÇ           ‚Üë                                      ‚Üì                  ‚îÇ
‚îÇ           ‚îÇ                              Loss (FP32)               ‚îÇ
‚îÇ           ‚îÇ                                      ‚Üì                  ‚îÇ
‚îÇ      Update FP32 ‚Üê‚îÄ‚îÄ Scale Down ‚Üê‚îÄ‚îÄ Backward Pass (FP16)           ‚îÇ
‚îÇ                              ‚Üë                                      ‚îÇ
‚îÇ                       Loss Scaling                                  ‚îÇ
‚îÇ                    (prevent underflow)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="fp16-vs-bf16-vs-fp32">FP16 vs BF16 vs FP32</h3>
            <table>
            <thead>
            <tr>
            <th>Format</th>
            <th>Bits</th>
            <th>Exponent</th>
            <th>Mantissa</th>
            <th>Range</th>
            <th>Precision</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>FP32</strong></td>
            <td>32</td>
            <td>8</td>
            <td>23</td>
            <td>¬±3.4√ó10¬≥‚Å∏</td>
            <td>High</td>
            </tr>
            <tr>
            <td><strong>FP16</strong></td>
            <td>16</td>
            <td>5</td>
            <td>10</td>
            <td>¬±65504</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>BF16</strong></td>
            <td>16</td>
            <td>8</td>
            <td>7</td>
            <td>¬±3.4√ó10¬≥‚Å∏</td>
            <td>Medium</td>
            </tr>
            </tbody>
            </table>
            <p><strong>BF16 advantage</strong>: Same range as FP32 ‚Üí no
            loss scaling needed!</p>
            <h3 id="loss-scaling-fp16-only">Loss Scaling (FP16
            only)</h3>
            <p><strong>Problem</strong>: FP16 gradients can underflow
            (become 0) for small values.</p>
            <p><strong>Solution</strong>: Scale loss up before backward,
            scale gradients down after.</p>
            <div class="sourceCode" id="cb301"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb301-1"><a href="#cb301-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual loss scaling</span></span>
<span id="cb301-2"><a href="#cb301-2" aria-hidden="true" tabindex="-1"></a>scale <span class="op">=</span> <span class="fl">1024.0</span></span>
<span id="cb301-3"><a href="#cb301-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> model(x)</span>
<span id="cb301-4"><a href="#cb301-4" aria-hidden="true" tabindex="-1"></a>scaled_loss <span class="op">=</span> loss <span class="op">*</span> scale</span>
<span id="cb301-5"><a href="#cb301-5" aria-hidden="true" tabindex="-1"></a>scaled_loss.backward()</span>
<span id="cb301-6"><a href="#cb301-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-7"><a href="#cb301-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Unscale gradients before optimizer step</span></span>
<span id="cb301-8"><a href="#cb301-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb301-9"><a href="#cb301-9" aria-hidden="true" tabindex="-1"></a>    p.grad <span class="op">/=</span> scale</span>
<span id="cb301-10"><a href="#cb301-10" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span></code></pre></div>
            <p><strong>Dynamic loss scaling</strong>: Start high, reduce
            if inf/nan gradients occur.</p>
            <h3 id="which-operations-use-fp32">Which Operations Use
            FP32?</h3>
            <table>
            <thead>
            <tr>
            <th>Operation</th>
            <th>Precision</th>
            <th>Why</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MatMuls, Convs</strong></td>
            <td>FP16/BF16</td>
            <td>Safe, main compute</td>
            </tr>
            <tr>
            <td><strong>Activations</strong></td>
            <td>FP16/BF16</td>
            <td>Safe</td>
            </tr>
            <tr>
            <td><strong>LayerNorm, BatchNorm</strong></td>
            <td>FP32</td>
            <td>Accumulation needs precision</td>
            </tr>
            <tr>
            <td><strong>Softmax</strong></td>
            <td>FP32</td>
            <td>Numerical stability</td>
            </tr>
            <tr>
            <td><strong>Loss computation</strong></td>
            <td>FP32</td>
            <td>Small values matter</td>
            </tr>
            <tr>
            <td><strong>Master weights</strong></td>
            <td>FP32</td>
            <td>Accumulate small updates</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why these specific operations need
            FP32</strong>:</p>
            <ul>
            <li><p><strong>LayerNorm/BatchNorm</strong>: These compute
            running means and variances by summing many values. In FP16,
            summing thousands of small numbers causes catastrophic
            cancellation‚Äîsmall differences get rounded away. FP32‚Äôs
            23-bit mantissa preserves these differences. Additionally,
            the variance calculation involves squaring (amplifies
            errors) and division (sensitive to small
            denominators).</p></li>
            <li><p><strong>Softmax</strong>: Computing <span
            class="math inline">\(e^{x_i} / \sum e^{x_j}\)</span> is
            numerically treacherous. Large logits overflow FP16‚Äôs range
            (~65504), and the exponential amplifies any precision loss.
            The standard trick subtracts max(x), but even then, the sum
            of exponentials needs precision. Getting softmax wrong means
            attention weights are wrong, breaking the entire
            model.</p></li>
            <li><p><strong>Loss computation</strong>: Cross-entropy loss
            involves <span class="math inline">\(-\log(p)\)</span> where
            p can be very small (e.g., 1e-7). In FP16, small
            probabilities get rounded to zero, making log undefined.
            Even near-zero values lose precision. Since the loss
            directly drives gradients, errors here corrupt the entire
            training signal.</p></li>
            <li><p><strong>Master weights</strong>: Weight updates are
            often tiny: learning_rate √ó gradient might be 1e-5. Adding
            1e-5 to a weight of 1.0 in FP16 gives‚Ä¶ 1.0 (the small update
            is lost). Master weights in FP32 accumulate these tiny
            updates correctly, then cast to FP16 for forward/backward
            passes.</p></li>
            </ul>
            <h3 id="pytorch-amp-automatic-mixed-precision">PyTorch AMP
            (Automatic Mixed Precision)</h3>
            <div class="sourceCode" id="cb302"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb302-1"><a href="#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb302-2"><a href="#cb302-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-3"><a href="#cb302-3" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> GradScaler()  <span class="co"># For loss scaling (FP16 only)</span></span>
<span id="cb302-4"><a href="#cb302-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb302-5"><a href="#cb302-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb302-6"><a href="#cb302-6" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb302-7"><a href="#cb302-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb302-8"><a href="#cb302-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> autocast(dtype<span class="op">=</span>torch.float16):  <span class="co"># or torch.bfloat16</span></span>
<span id="cb302-9"><a href="#cb302-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch)</span>
<span id="cb302-10"><a href="#cb302-10" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb302-11"><a href="#cb302-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb302-12"><a href="#cb302-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># FP16: Use scaler</span></span>
<span id="cb302-13"><a href="#cb302-13" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb302-14"><a href="#cb302-14" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb302-15"><a href="#cb302-15" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb302-16"><a href="#cb302-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb302-17"><a href="#cb302-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BF16: No scaling needed</span></span>
<span id="cb302-18"><a href="#cb302-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss.backward()</span></span>
<span id="cb302-19"><a href="#cb302-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optimizer.step()</span></span></code></pre></div>
            <h3 id="memory-savings">Memory Savings</h3>
            <table>
            <thead>
            <tr>
            <th>Precision</th>
            <th>Model Memory</th>
            <th>Activation Memory</th>
            <th>Speedup</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>FP32</strong></td>
            <td>1√ó</td>
            <td>1√ó</td>
            <td>1√ó</td>
            </tr>
            <tr>
            <td><strong>FP16/BF16</strong></td>
            <td>0.5√ó</td>
            <td>0.5√ó</td>
            <td>1.5-2√ó</td>
            </tr>
            </tbody>
            </table>
            <p>For a 7B model: FP32 = 28GB weights ‚Üí FP16 = 14GB
            weights</p>
            <hr />
            <h3
            id="gradient-checkpointing-activation-checkpointing">Gradient
            Checkpointing (Activation Checkpointing)</h3>
            <p><strong>Problem</strong>: Activations consume most memory
            during training.</p>
            <p>During forward pass, we must store activations for
            backward:</p>
            <pre><code>Layer 1: Store activation‚ÇÅ (for backward)
Layer 2: Store activation‚ÇÇ (for backward)
...
Layer N: Store activation_N (for backward)

Total: O(N √ó batch_size √ó hidden_dim) ‚Üê Can be HUGE!</code></pre>
            <p><strong>Solution</strong>: Don‚Äôt store all activations.
            Recompute them during backward.</p>
            <pre><code>Standard Training:                  Gradient Checkpointing:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Forward: Store ALL activations     Forward: Store SOME activations
                                            (checkpoints)

Backward: Use stored activations   Backward: Recompute between 
                                            checkpoints

Memory: O(N)                       Memory: O(‚àöN) optimal
Compute: 1√ó forward                Compute: ~1.3√ó forward</code></pre>
            <h3 id="how-it-works-2">How It Works</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Gradient Checkpointing (‚àöN strategy)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Forward Pass:                                                      ‚îÇ
‚îÇ  [L1]‚Üí[L2]‚Üí[L3]‚Üí[L4]‚Üí[L5]‚Üí[L6]‚Üí[L7]‚Üí[L8]‚Üí[L9]                     ‚îÇ
‚îÇ    ‚Üì         ‚Üì              ‚Üì              ‚Üì                        ‚îÇ
‚îÇ  Save     Save           Save           Save    (checkpoints)      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Backward Pass (for segment L4-L6):                                ‚îÇ
‚îÇ  1. Load checkpoint at L4                                          ‚îÇ
‚îÇ  2. Recompute forward: L4‚ÜíL5‚ÜíL6                                   ‚îÇ
‚îÇ  3. Now have activations, compute gradients                        ‚îÇ
‚îÇ  4. Free activations, move to next segment                         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="implementation">Implementation</h3>
            <div class="sourceCode" id="cb306"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb306-1"><a href="#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.utils.checkpoint <span class="im">as</span> checkpoint</span>
<span id="cb306-2"><a href="#cb306-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-3"><a href="#cb306-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb306-4"><a href="#cb306-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb306-5"><a href="#cb306-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attention(<span class="va">self</span>.norm1(x))</span>
<span id="cb306-6"><a href="#cb306-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.norm2(x))</span>
<span id="cb306-7"><a href="#cb306-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb306-8"><a href="#cb306-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-9"><a href="#cb306-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CheckpointedTransformer(nn.Module):</span>
<span id="cb306-10"><a href="#cb306-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_layers):</span>
<span id="cb306-11"><a href="#cb306-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb306-12"><a href="#cb306-12" aria-hidden="true" tabindex="-1"></a>            TransformerBlock() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb306-13"><a href="#cb306-13" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb306-14"><a href="#cb306-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb306-15"><a href="#cb306-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb306-16"><a href="#cb306-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb306-17"><a href="#cb306-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Checkpoint each layer (or every N layers)</span></span>
<span id="cb306-18"><a href="#cb306-18" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> checkpoint.checkpoint(layer, x, use_reentrant<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb306-19"><a href="#cb306-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
            <h3 id="checkpointing-strategies">Checkpointing
            Strategies</h3>
            <table>
            <thead>
            <tr>
            <th>Strategy</th>
            <th>Memory</th>
            <th>Compute Overhead</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Every layer</strong></td>
            <td>O(1) per layer</td>
            <td>~33% more</td>
            </tr>
            <tr>
            <td><strong>Every ‚àöN layers</strong></td>
            <td>O(‚àöN)</td>
            <td>~15-20% more</td>
            </tr>
            <tr>
            <td><strong>Every N layers</strong></td>
            <td>O(N/k)</td>
            <td>k√ó recompute</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Why ‚àöN is optimal</strong>: The memory-compute
            trade-off has a beautiful mathematical optimum. Let‚Äôs say we
            have N layers and place checkpoints every K layers. Memory
            usage has two components: (1) storing K checkpoints, and (2)
            storing activations for the current segment being recomputed
            (at most N/K layers). Total memory ‚âà K + N/K. To minimize,
            take derivative and set to zero: d/dK(K + N/K) = 1 - N/K¬≤ =
            0, giving K = ‚àöN. At this optimum, both terms equal ‚àöN, so
            total memory is O(2‚àöN) = O(‚àöN). The compute overhead is also
            balanced: we recompute ‚àöN segments of ‚àöN layers each, adding
            one extra forward pass worth of compute (~33% overhead for
            backward being ~2√ó forward).</p>
            <hr />
            <h3 id="combining-memory-optimizations">Combining Memory
            Optimizations</h3>
            <p><strong>Recipe for training large models on limited
            hardware</strong>:</p>
            <div class="sourceCode" id="cb307"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb307-1"><a href="#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Mixed Precision (2√ó memory saving)</span></span>
<span id="cb307-2"><a href="#cb307-2" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb307-3"><a href="#cb307-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-4"><a href="#cb307-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Gradient Checkpointing (‚àöN memory for activations)</span></span>
<span id="cb307-5"><a href="#cb307-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CheckpointedTransformer(num_layers<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb307-6"><a href="#cb307-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-7"><a href="#cb307-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Gradient Accumulation (larger effective batch)</span></span>
<span id="cb307-8"><a href="#cb307-8" aria-hidden="true" tabindex="-1"></a>accumulation_steps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb307-9"><a href="#cb307-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb307-10"><a href="#cb307-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Optional: ZeRO (Part 8.4) for even larger models</span></span></code></pre></div>
            <p><strong>Memory breakdown for 7B model
            training</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Component</th>
            <th>FP32</th>
            <th>With Optimizations</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Parameters</td>
            <td>28 GB</td>
            <td>14 GB (FP16)</td>
            </tr>
            <tr>
            <td>Gradients</td>
            <td>28 GB</td>
            <td>14 GB (FP16)</td>
            </tr>
            <tr>
            <td>Optimizer (Adam)</td>
            <td>56 GB</td>
            <td>56 GB (always FP32)</td>
            </tr>
            <tr>
            <td>Activations</td>
            <td>50+ GB</td>
            <td>~10 GB (checkpointing)</td>
            </tr>
            <tr>
            <td><strong>Total</strong></td>
            <td><strong>162 GB</strong></td>
            <td><strong>~94 GB</strong></td>
            </tr>
            </tbody>
            </table>
            <p>Add ZeRO-3: Distribute optimizer states ‚Üí fits on 4√ó 40GB
            GPUs!</p>
            <hr />
            <h3
            id="interview-q-how-does-gradient-checkpointing-trade-compute-for-memory">Interview
            Q: ‚ÄúHow does gradient checkpointing trade compute for
            memory?‚Äù</h3>
            <p><strong>A</strong>: Gradient checkpointing reduces memory
            by <strong>not storing</strong> intermediate activations
            during forward pass. Instead, it saves only periodic
            ‚Äúcheckpoints.‚Äù During backward pass, it
            <strong>recomputes</strong> activations between checkpoints
            as needed. This trades ~30% more compute for O(‚àöN) memory
            instead of O(N). The optimal strategy checkpoints every ‚àöN
            layers. It‚Äôs essential for training transformers with long
            sequences where activation memory dominates.</p>
            <h3
            id="interview-q-why-does-mixed-precision-training-need-loss-scaling-for-fp16-but-not-bf16">Interview
            Q: ‚ÄúWhy does mixed precision training need loss scaling for
            FP16 but not BF16?‚Äù</h3>
            <p><strong>A</strong>: FP16 has only 5 exponent bits ‚Üí
            limited range (max ~65504). Small gradients can
            <strong>underflow</strong> to zero, losing training signal.
            Loss scaling multiplies the loss by a large factor (e.g.,
            1024) before backward, making gradients larger, then scales
            down after.</p>
            <p>BF16 has 8 exponent bits (same as FP32) ‚Üí same range.
            Small values don‚Äôt underflow. The tradeoff is less mantissa
            precision (7 vs 10 bits), but for deep learning this rarely
            matters. That‚Äôs why BF16 is preferred when hardware supports
            it ‚Äî simpler training with no scaling needed.</p>
            <hr />
            <h2 id="d-parallelism-putting-it-all-together">8.6 3D
            Parallelism: Putting It All Together</h2>
            <p>For the largest models (100B+), you need to combine all
            three parallelism types:</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      3D Parallelism                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  128 GPUs training a 175B model:                                   ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Pipeline Parallel (PP=4):                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                         ‚îÇ
‚îÇ  Split 96 layers into 4 stages (24 layers each)                    ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Tensor Parallel (TP=8):                                           ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                           ‚îÇ
‚îÇ  Each pipeline stage uses 8 GPUs for tensor parallelism            ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Data Parallel (DP=4):                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                             ‚îÇ
‚îÇ  4 replica groups, each handling different data                    ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Total: PP √ó TP √ó DP = 4 √ó 8 √ó 4 = 128 GPUs                        ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    Data Parallel Replica 0                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Stage 0  ‚îÇ‚Üí ‚îÇ Stage 1  ‚îÇ‚Üí ‚îÇ Stage 2  ‚îÇ‚Üí ‚îÇ Stage 3  ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (8 GPUs) ‚îÇ  ‚îÇ (8 GPUs) ‚îÇ  ‚îÇ (8 GPUs) ‚îÇ  ‚îÇ (8 GPUs) ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   TP=8   ‚îÇ  ‚îÇ   TP=8   ‚îÇ  ‚îÇ   TP=8   ‚îÇ  ‚îÇ   TP=8   ‚îÇ     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    Data Parallel Replica 1                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ... (same structure, different data) ...                   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ... (Replicas 2 and 3)                                            ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="which-dimension-first">Which Dimension First?</h3>
            <ol type="1">
            <li><strong>Tensor Parallelism</strong>: Within a node
            (NVLink bandwidth)</li>
            <li><strong>Pipeline Parallelism</strong>: Across nodes in
            same ‚Äúcolumn‚Äù</li>
            <li><strong>Data Parallelism</strong>: Across pipeline
            replicas</li>
            </ol>
            <h3 id="hardware-topology-why-it-matters">Hardware Topology:
            Why It Matters</h3>
            <p>The choice of parallelism strategy is fundamentally
            constrained by how GPUs are physically connected.
            Understanding this hierarchy is crucial for efficient
            distributed training:</p>
            <p><strong>Within a node (8 GPUs on same machine)</strong>:
            - <strong>NVLink/NVSwitch</strong>: 600-900 GB/s
            bidirectional bandwidth, ~1Œºs latency - All 8 GPUs can
            communicate with any other GPU at full speed - This is where
            <strong>tensor parallelism</strong> thrives‚Äîfrequent small
            AllReduces complete in microseconds</p>
            <p><strong>Across nodes (different machines)</strong>: -
            <strong>InfiniBand HDR</strong>: ~50 GB/s per connection,
            ~1-5Œºs latency - <strong>Ethernet (RoCE)</strong>: ~25-100
            GB/s, higher latency - Network bandwidth is 10-20√ó lower
            than NVLink - This is where <strong>data
            parallelism</strong> works well‚Äîone large AllReduce per step
            can saturate the link efficiently</p>
            <p><strong>The topology rule</strong>: Match communication
            patterns to network capabilities. Tensor parallelism‚Äôs many
            small AllReduces would bottleneck on inter-node latency.
            Data parallelism‚Äôs single large AllReduce amortizes latency.
            Pipeline parallelism‚Äôs point-to-point sends are easier to
            overlap with compute.</p>
            <p><strong>Practical implication for 3D
            parallelism</strong>: The standard recipe is TP=8 (within
            node), PP across nodes (point-to-point is latency-tolerant),
            DP across node groups (large AllReduce is
            bandwidth-efficient). This matches each parallelism type to
            the network tier where it performs best.</p>
            <h3 id="real-world-configurations">Real-World
            Configurations</h3>
            <table>
            <thead>
            <tr>
            <th>Model</th>
            <th>Size</th>
            <th>Configuration</th>
            <th>Hardware</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>LLaMA-65B</strong></td>
            <td>65B</td>
            <td>TP=8, PP=1</td>
            <td>8√ó A100 80GB</td>
            </tr>
            <tr>
            <td><strong>GPT-3</strong></td>
            <td>175B</td>
            <td>TP=8, PP=8, DP=8</td>
            <td>512√ó V100</td>
            </tr>
            <tr>
            <td><strong>LLaMA-405B</strong></td>
            <td>405B</td>
            <td>TP=8, PP=16, DP=~16</td>
            <td>~2000√ó H100</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h2 id="summary-parallelism-decision-guide">Summary:
            Parallelism Decision Guide</h2>
            <table>
            <thead>
            <tr>
            <th>Situation</th>
            <th>Recommendation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Model fits on 1 GPU</td>
            <td>No parallelism</td>
            </tr>
            <tr>
            <td>Model fits, want faster</td>
            <td><strong>Data Parallel</strong></td>
            </tr>
            <tr>
            <td>Model doesn‚Äôt fit</td>
            <td><strong>ZeRO-3</strong> or <strong>Tensor
            Parallel</strong></td>
            </tr>
            <tr>
            <td>Very large model (100B+)</td>
            <td><strong>3D Parallelism</strong> (DP + TP + PP)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="quick-reference-communication-patterns">Quick
            Reference: Communication Patterns</h3>
            <table>
            <colgroup>
            <col style="width: 14%" />
            <col style="width: 33%" />
            <col style="width: 14%" />
            <col style="width: 37%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Communication Type</th>
            <th>Volume</th>
            <th>Latency Sensitivity</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Data Parallel</strong></td>
            <td>AllReduce (gradients)</td>
            <td>2 √ó model_size</td>
            <td>Low</td>
            </tr>
            <tr>
            <td><strong>Tensor Parallel</strong></td>
            <td>AllReduce (activations)</td>
            <td>batch √ó seq √ó hidden</td>
            <td>High</td>
            </tr>
            <tr>
            <td><strong>Pipeline Parallel</strong></td>
            <td>P2P (activations)</td>
            <td>batch √ó seq √ó hidden</td>
            <td>Medium</td>
            </tr>
            <tr>
            <td><strong>ZeRO-3</strong></td>
            <td>AllGather (params)</td>
            <td>3 √ó model_size</td>
            <td>Medium</td>
            </tr>
            </tbody>
            </table>
            <h3 id="typical-modern-configurations">Typical Modern
            Configurations</h3>
            <table>
            <thead>
            <tr>
            <th>Model Size</th>
            <th>Configuration</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>7B</td>
            <td>1-8 GPUs with DDP</td>
            </tr>
            <tr>
            <td>70B</td>
            <td>8 GPUs with TP=8, or ZeRO-3</td>
            </tr>
            <tr>
            <td>405B</td>
            <td>128+ GPUs with TP=8, PP=4, DP=4</td>
            </tr>
            </tbody>
            </table>
            <hr />
            <h1 id="part-9-reinforcement-learning">Part 9: Reinforcement
            Learning</h1>
            <blockquote>
            <p><strong>Interview Priority Guide</strong>: For ML
            fundamentals interviews, focus on these sections in
            order:</p>
            <p>üî¥ <strong>Must Know</strong> (Core concepts tested
            frequently):</p>
            <ul>
            <li><strong>9.1 MDPs</strong> ‚Äî Markov property,
            states/actions/rewards, discount factor</li>
            <li><strong>9.2 Value Functions &amp; Bellman
            Equations</strong> ‚Äî V(s), Q(s,a), Bellman optimality</li>
            <li><strong>9.4 TD Learning</strong> ‚Äî TD vs MC tradeoff,
            bias-variance, TD(Œª)</li>
            <li><strong>9.5 Q-Learning</strong> ‚Äî Off-policy learning,
            DQN innovations (replay, target network)</li>
            <li><strong>9.6 Policy Gradient</strong> ‚Äî REINFORCE,
            log-derivative trick, baselines</li>
            </ul>
            <p>üü° <strong>Important</strong> (Frequently asked for ML/RL
            roles):</p>
            <ul>
            <li><strong>9.7 Actor-Critic</strong> ‚Äî Advantage function,
            A2C, GAE</li>
            <li><strong>9.8 PPO</strong> ‚Äî Clipping mechanism, why it‚Äôs
            used in RLHF</li>
            <li><strong>9.9 Exploration vs Exploitation</strong> ‚Äî
            Œµ-greedy, UCB, intrinsic motivation</li>
            <li><strong>9.10 Connection to LLM Alignment</strong> ‚Äî RLHF
            framing, KL constraints</li>
            </ul>
            <p>üü¢ <strong>Advanced</strong> (For specialized RL
            roles):</p>
            <ul>
            <li><strong>9.11 Model-Based vs Model-Free</strong> ‚Äî Dyna,
            Dreamer, sample efficiency</li>
            <li><strong>9.12 Offline RL</strong> ‚Äî Distribution shift,
            CQL, Decision Transformer</li>
            <li><strong>9.13 Multi-Agent RL</strong> ‚Äî CTDE, self-play,
            Nash equilibrium</li>
            <li><strong>9.14 MCTS</strong> ‚Äî AlphaGo/AlphaZero, UCB
            selection</li>
            <li><strong>9.15 Distributional RL</strong> ‚Äî C51, quantile
            regression</li>
            </ul>
            </blockquote>
            <hr />
            <h2 id="markov-decision-processes-mdps">9.1 Markov Decision
            Processes (MDPs)</h2>
            <h3 id="what-is-an-mdp">What is an MDP?</h3>
            <p>An MDP is a mathematical framework for sequential
            decision-making where outcomes are partly random and partly
            under the control of a decision maker (agent).</p>
            <p><strong>Formal Definition</strong>: An MDP is a tuple
            <span class="math inline">\((S, A, P, R,
            \gamma)\)</span>:</p>
            <table>
            <colgroup>
            <col style="width: 34%" />
            <col style="width: 25%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>Component</th>
            <th>Symbol</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>States</strong></td>
            <td><span class="math inline">\(S\)</span></td>
            <td>Set of all possible situations</td>
            </tr>
            <tr>
            <td><strong>Actions</strong></td>
            <td><span class="math inline">\(A\)</span></td>
            <td>Set of all possible actions</td>
            </tr>
            <tr>
            <td><strong>Transition</strong></td>
            <td><span class="math inline">\(P(s&#39;|s,a)\)</span></td>
            <td>Probability of reaching <span
            class="math inline">\(s&#39;\)</span> from <span
            class="math inline">\(s\)</span> via action <span
            class="math inline">\(a\)</span></td>
            </tr>
            <tr>
            <td><strong>Reward</strong></td>
            <td><span class="math inline">\(R(s,a,s&#39;)\)</span></td>
            <td>Immediate reward for transition</td>
            </tr>
            <tr>
            <td><strong>Discount</strong></td>
            <td><span class="math inline">\(\gamma \in
            [0,1]\)</span></td>
            <td>How much to value future rewards</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-markov-property">The Markov Property</h3>
            <p>The key assumption: <strong>The future depends only on
            the present, not the past.</strong></p>
            <p><span class="math display">\[P(s_{t+1} | s_t, a_t,
            s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1} | s_t,
            a_t)\]</span></p>
            <p><strong>Intuition</strong>: The current state contains
            all relevant information for decision-making.</p>
            <h3 id="example-grid-world">Example: Grid World</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ S ‚îÇ   ‚îÇ   ‚îÇ G ‚îÇ   S = Start, G = Goal (+1)
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§   X = Pit (-1)
‚îÇ   ‚îÇ X ‚îÇ   ‚îÇ   ‚îÇ   
‚îú‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§   Actions: Up, Down, Left, Right
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <ul>
            <li><strong>States</strong>: Each grid cell</li>
            <li><strong>Actions</strong>: {Up, Down, Left, Right}</li>
            <li><strong>Transitions</strong>: Deterministic (or
            stochastic with slip probability)</li>
            <li><strong>Rewards</strong>: +1 at goal, -1 at pit, -0.04
            per step</li>
            </ul>
            <h3 id="trajectory-and-return">Trajectory and Return</h3>
            <p>A <strong>trajectory</strong> (episode) is a sequence of
            states, actions, rewards:</p>
            <p><span class="math display">\[\tau = (s_0, a_0, r_1, s_1,
            a_1, r_2, \ldots, s_T)\]</span></p>
            <p>The <strong>return</strong> is the cumulative discounted
            reward:</p>
            <p><span class="math display">\[G_t = r_{t+1} + \gamma
            r_{t+2} + \gamma^2 r_{t+3} + \ldots = \sum_{k=0}^{\infty}
            \gamma^k r_{t+k+1}\]</span></p>
            <h3 id="why-discount-gamma">Why Discount (<span
            class="math inline">\(\gamma\)</span>)?</h3>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(\gamma\)</span></th>
            <th>Behavior</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\gamma = 0\)</span></td>
            <td>Myopic: Only care about immediate reward</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\gamma = 1\)</span></td>
            <td>Far-sighted: All rewards equally important</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\gamma = 0.99\)</span></td>
            <td>Typical: Balance immediate and future</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Practical reasons</strong>:</p>
            <ul>
            <li>Mathematical convenience (ensures finite returns)</li>
            <li>Models uncertainty about the future</li>
            <li>Encourages faster solutions</li>
            </ul>
            <h3 id="policy">Policy</h3>
            <p>A <strong>policy</strong> <span
            class="math inline">\(\pi\)</span> maps states to
            actions:</p>
            <ul>
            <li><strong>Deterministic</strong>: <span
            class="math inline">\(a = \pi(s)\)</span></li>
            <li><strong>Stochastic</strong>: <span
            class="math inline">\(a \sim \pi(a|s)\)</span></li>
            </ul>
            <p><strong>Goal</strong>: Find the optimal policy <span
            class="math inline">\(\pi^*\)</span> that maximizes expected
            return.</p>
            <h3
            id="interview-q-whats-the-markov-property-and-why-is-it-important">Interview
            Q: ‚ÄúWhat‚Äôs the Markov property and why is it
            important?‚Äù</h3>
            <p><strong>A</strong>: The Markov property states that the
            future is conditionally independent of the past given the
            present state: <span class="math inline">\(P(s_{t+1}|s_t,
            a_t, s_{t-1}, \ldots) = P(s_{t+1}|s_t, a_t)\)</span>. This
            is crucial because it means the state contains all relevant
            information for decision-making, enabling recursive value
            function definitions (Bellman equations) and making RL
            computationally tractable. Without it, we‚Äôd need to track
            entire histories.</p>
            <hr />
            <h2 id="value-functions-and-bellman-equations">9.2 Value
            Functions and Bellman Equations</h2>
            <h3 id="state-value-function-vpis">State Value Function
            <span class="math inline">\(V^\pi(s)\)</span></h3>
            <p><strong>Expected return starting from state <span
            class="math inline">\(s\)</span> and following policy <span
            class="math inline">\(\pi\)</span></strong>:</p>
            <p><span class="math display">\[V^\pi(s) =
            \mathbb{E}_\pi\left[G_t | s_t = s\right] =
            \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
            \bigg| s_t = s\right]\]</span></p>
            <h3 id="action-value-function-qpis-a">Action Value Function
            <span class="math inline">\(Q^\pi(s, a)\)</span></h3>
            <p><strong>Expected return starting from state <span
            class="math inline">\(s\)</span>, taking action <span
            class="math inline">\(a\)</span>, then following policy
            <span class="math inline">\(\pi\)</span></strong>:</p>
            <p><span class="math display">\[Q^\pi(s, a) =
            \mathbb{E}_\pi\left[G_t | s_t = s, a_t =
            a\right]\]</span></p>
            <h3 id="relationship-between-v-and-q">Relationship Between V
            and Q</h3>
            <p><span class="math display">\[V^\pi(s) = \sum_a \pi(a|s)
            Q^\pi(s, a)\]</span></p>
            <p><span class="math display">\[Q^\pi(s, a) = R(s, a) +
            \gamma \sum_{s&#39;} P(s&#39;|s,a)
            V^\pi(s&#39;)\]</span></p>
            <h3 id="bellman-expectation-equation">Bellman Expectation
            Equation</h3>
            <p>The recursive structure of value functions:</p>
            <p><strong>For <span
            class="math inline">\(V^\pi\)</span></strong>: <span
            class="math display">\[V^\pi(s) = \sum_a \pi(a|s) \left[
            R(s,a) + \gamma \sum_{s&#39;} P(s&#39;|s,a) V^\pi(s&#39;)
            \right]\]</span></p>
            <p><strong>For <span
            class="math inline">\(Q^\pi\)</span></strong>: <span
            class="math display">\[Q^\pi(s, a) = R(s,a) + \gamma
            \sum_{s&#39;} P(s&#39;|s,a) \sum_{a&#39;} \pi(a&#39;|s&#39;)
            Q^\pi(s&#39;, a&#39;)\]</span></p>
            <p><strong>Intuition</strong>: Value = Immediate reward +
            Discounted value of next state</p>
            <h3 id="optimal-value-functions">Optimal Value
            Functions</h3>
            <p>The <strong>optimal value function</strong> is the
            maximum over all policies:</p>
            <p><span class="math display">\[V^*(s) = \max_\pi
            V^\pi(s)\]</span></p>
            <p><span class="math display">\[Q^*(s, a) = \max_\pi
            Q^\pi(s, a)\]</span></p>
            <h3 id="bellman-optimality-equation">Bellman Optimality
            Equation</h3>
            <p><span class="math display">\[V^*(s) = \max_a \left[
            R(s,a) + \gamma \sum_{s&#39;} P(s&#39;|s,a) V^*(s&#39;)
            \right]\]</span></p>
            <p><span class="math display">\[Q^*(s, a) = R(s,a) + \gamma
            \sum_{s&#39;} P(s&#39;|s,a) \max_{a&#39;} Q^*(s&#39;,
            a&#39;)\]</span></p>
            <p><strong>Key insight</strong>: The optimal policy is
            greedy with respect to <span
            class="math inline">\(Q^*\)</span>: <span
            class="math display">\[\pi^*(s) = \arg\max_a Q^*(s,
            a)\]</span></p>
            <h3 id="example-simple-mdp">Example: Simple MDP</h3>
            <pre><code>State A ‚îÄ‚îÄ(action: go)‚îÄ‚îÄ‚Üí State B ‚îÄ‚îÄ(reward: +10)‚îÄ‚îÄ‚Üí Terminal
   ‚îÇ
   ‚îî‚îÄ‚îÄ(action: stay, reward: +1)‚îÄ‚îÄ‚Üí State A</code></pre>
            <p>With <span class="math inline">\(\gamma =
            0.9\)</span>:</p>
            <ul>
            <li>If <span class="math inline">\(V^*(B) = 10\)</span>
            (terminal)</li>
            <li>Then <span class="math inline">\(V^*(A) = \max(1 + 0.9
            \cdot V^*(A), 0 + 0.9 \cdot 10)\)</span></li>
            <li>Solving: <span class="math inline">\(V^*(A) = \max(1 +
            0.9V^*(A), 9)\)</span></li>
            <li>If go: <span class="math inline">\(V^*(A) =
            9\)</span></li>
            <li>If stay: <span class="math inline">\(V^*(A) = 1 +
            0.9V^*(A) \Rightarrow V^*(A) = 10\)</span></li>
            </ul>
            <p>So staying is optimal! (Infinite loop of +1 rewards beats
            finite +10)</p>
            <h3 id="interview-q-explain-the-bellman-equation">Interview
            Q: ‚ÄúExplain the Bellman equation‚Äù</h3>
            <p><strong>A</strong>: The Bellman equation expresses the
            recursive relationship of value functions: the value of a
            state equals the immediate reward plus the discounted value
            of the next state. For the optimal value function: <span
            class="math inline">\(V^*(s) = \max_a [R(s,a) + \gamma
            \sum_{s&#39;} P(s&#39;|s,a) V^*(s&#39;)]\)</span>. This
            decomposition is the foundation of dynamic programming and
            most RL algorithms. It allows us to bootstrap ‚Äî estimate
            value of a state from estimated values of successor
            states.</p>
            <hr />
            <h2 id="monte-carlo-methods">9.3 Monte Carlo Methods</h2>
            <h3 id="the-idea-2">The Idea</h3>
            <p><strong>Learn from complete episodes</strong> by
            averaging returns.</p>
            <p>No model needed ‚Äî learn directly from experience!</p>
            <h3 id="first-visit-mc-prediction">First-Visit MC
            Prediction</h3>
            <p><strong>Why iterate backwards?</strong> We compute
            returns by working backwards through the episode because the
            return at time <span class="math inline">\(t\)</span>
            depends on all future rewards: <span
            class="math inline">\(G_t = r_{t+1} + \gamma
            G_{t+1}\)</span>. Starting from the end (where <span
            class="math inline">\(G_T = 0\)</span>) and moving backwards
            lets us efficiently compute each return using the previously
            computed value.</p>
            <p><strong>First-visit vs Every-visit</strong>: First-visit
            MC only uses the return from the <em>first</em> time a state
            is visited in an episode. This ensures statistical
            independence between samples ‚Äî if a state is visited
            multiple times, later visits might be correlated with
            earlier ones (since they follow from the same trajectory).
            Every-visit MC uses all visits, which can be more sample
            efficient but with potentially correlated samples. In
            practice, both converge to the true value, and every-visit
            often works just as well.</p>
            <div class="sourceCode" id="cb311"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb311-1"><a href="#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> first_visit_mc(policy, episodes, gamma<span class="op">=</span><span class="fl">0.99</span>):</span>
<span id="cb311-2"><a href="#cb311-2" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> defaultdict(<span class="bu">float</span>)</span>
<span id="cb311-3"><a href="#cb311-3" aria-hidden="true" tabindex="-1"></a>    returns <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb311-4"><a href="#cb311-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb311-5"><a href="#cb311-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> episodes:</span>
<span id="cb311-6"><a href="#cb311-6" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb311-7"><a href="#cb311-7" aria-hidden="true" tabindex="-1"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb311-8"><a href="#cb311-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb311-9"><a href="#cb311-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Work backwards through episode</span></span>
<span id="cb311-10"><a href="#cb311-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(episode))):</span>
<span id="cb311-11"><a href="#cb311-11" aria-hidden="true" tabindex="-1"></a>            s, a, r <span class="op">=</span> episode[t]</span>
<span id="cb311-12"><a href="#cb311-12" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> G</span>
<span id="cb311-13"><a href="#cb311-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb311-14"><a href="#cb311-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> s <span class="kw">not</span> <span class="kw">in</span> visited:  <span class="co"># First visit only</span></span>
<span id="cb311-15"><a href="#cb311-15" aria-hidden="true" tabindex="-1"></a>                visited.add(s)</span>
<span id="cb311-16"><a href="#cb311-16" aria-hidden="true" tabindex="-1"></a>                returns[s].append(G)</span>
<span id="cb311-17"><a href="#cb311-17" aria-hidden="true" tabindex="-1"></a>                V[s] <span class="op">=</span> np.mean(returns[s])</span>
<span id="cb311-18"><a href="#cb311-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb311-19"><a href="#cb311-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V</span></code></pre></div>
            <h3 id="every-visit-mc">Every-Visit MC</h3>
            <p>Count all visits, not just first:</p>
            <div class="sourceCode" id="cb312"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb312-1"><a href="#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same as above, but without the visited check</span></span>
<span id="cb312-2"><a href="#cb312-2" aria-hidden="true" tabindex="-1"></a>returns[s].append(G)</span></code></pre></div>
            <h3 id="mc-for-q-values-control">MC for Q-Values
            (Control)</h3>
            <p>To learn <span class="math inline">\(Q(s, a)\)</span>, we
            need to explore all state-action pairs:</p>
            <div class="sourceCode" id="cb313"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb313-1"><a href="#cb313-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mc_control_epsilon_greedy(episodes, gamma<span class="op">=</span><span class="fl">0.99</span>, epsilon<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb313-2"><a href="#cb313-2" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: defaultdict(<span class="bu">float</span>))</span>
<span id="cb313-3"><a href="#cb313-3" aria-hidden="true" tabindex="-1"></a>    returns <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb313-4"><a href="#cb313-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb313-5"><a href="#cb313-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> episodes:</span>
<span id="cb313-6"><a href="#cb313-6" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb313-7"><a href="#cb313-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb313-8"><a href="#cb313-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(episode))):</span>
<span id="cb313-9"><a href="#cb313-9" aria-hidden="true" tabindex="-1"></a>            s, a, r <span class="op">=</span> episode[t]</span>
<span id="cb313-10"><a href="#cb313-10" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> G</span>
<span id="cb313-11"><a href="#cb313-11" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb313-12"><a href="#cb313-12" aria-hidden="true" tabindex="-1"></a>            returns[(s, a)].append(G)</span>
<span id="cb313-13"><a href="#cb313-13" aria-hidden="true" tabindex="-1"></a>            Q[s][a] <span class="op">=</span> np.mean(returns[(s, a)])</span>
<span id="cb313-14"><a href="#cb313-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb313-15"><a href="#cb313-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Policy is implicit: epsilon-greedy w.r.t. Q</span></span>
<span id="cb313-16"><a href="#cb313-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb313-17"><a href="#cb313-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q</span></code></pre></div>
            <h3 id="mc-properties">MC Properties</h3>
            <table>
            <thead>
            <tr>
            <th>Property</th>
            <th>Monte Carlo</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Bias</strong></td>
            <td>Unbiased (uses true returns)</td>
            </tr>
            <tr>
            <td><strong>Variance</strong></td>
            <td>High (full episode randomness)</td>
            </tr>
            <tr>
            <td><strong>Bootstrap</strong></td>
            <td>No (waits for episode end)</td>
            </tr>
            <tr>
            <td><strong>Works with</strong></td>
            <td>Episodic tasks only</td>
            </tr>
            <tr>
            <td><strong>Model needed</strong></td>
            <td>No (model-free)</td>
            </tr>
            </tbody>
            </table>
            <h3 id="importance-sampling">Importance Sampling</h3>
            <p>When the behavior policy <span
            class="math inline">\(b\)</span> differs from target policy
            <span class="math inline">\(\pi\)</span>:</p>
            <p><span class="math display">\[V^\pi(s) =
            \mathbb{E}_b\left[\prod_{t=0}^{T-1}
            \frac{\pi(a_t|s_t)}{b(a_t|s_t)} G_t \bigg| s_0 =
            s\right]\]</span></p>
            <p>The product of ratios corrects for the policy
            mismatch.</p>
            <hr />
            <h2 id="temporal-difference-learning">9.4 Temporal
            Difference Learning</h2>
            <h3 id="the-key-insight-1">The Key Insight</h3>
            <p><strong>Don‚Äôt wait for episode end ‚Äî update after each
            step!</strong></p>
            <p>Use the <strong>TD error</strong> to bootstrap:</p>
            <p><span class="math display">\[\delta_t = r_{t+1} + \gamma
            V(s_{t+1}) - V(s_t)\]</span></p>
            <h3 id="td0-update">TD(0) Update</h3>
            <p><span class="math display">\[V(s_t) \leftarrow V(s_t) +
            \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
            \right]\]</span></p>
            <div class="sourceCode" id="cb314"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> td_0(episodes, alpha<span class="op">=</span><span class="fl">0.1</span>, gamma<span class="op">=</span><span class="fl">0.99</span>):</span>
<span id="cb314-2"><a href="#cb314-2" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> defaultdict(<span class="bu">float</span>)</span>
<span id="cb314-3"><a href="#cb314-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb314-4"><a href="#cb314-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> episodes:</span>
<span id="cb314-5"><a href="#cb314-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(episode) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb314-6"><a href="#cb314-6" aria-hidden="true" tabindex="-1"></a>            s, a, r <span class="op">=</span> episode[t]</span>
<span id="cb314-7"><a href="#cb314-7" aria-hidden="true" tabindex="-1"></a>            s_next <span class="op">=</span> episode[t <span class="op">+</span> <span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb314-8"><a href="#cb314-8" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb314-9"><a href="#cb314-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># TD update</span></span>
<span id="cb314-10"><a href="#cb314-10" aria-hidden="true" tabindex="-1"></a>            td_error <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> V[s_next] <span class="op">-</span> V[s]</span>
<span id="cb314-11"><a href="#cb314-11" aria-hidden="true" tabindex="-1"></a>            V[s] <span class="op">+=</span> alpha <span class="op">*</span> td_error</span>
<span id="cb314-12"><a href="#cb314-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb314-13"><a href="#cb314-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V</span></code></pre></div>
            <h3 id="td-vs-mc">TD vs MC</h3>
            <table>
            <thead>
            <tr>
            <th>Property</th>
            <th>Monte Carlo</th>
            <th>TD(0)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Update timing</strong></td>
            <td>End of episode</td>
            <td>Every step</td>
            </tr>
            <tr>
            <td><strong>Bias</strong></td>
            <td>Unbiased</td>
            <td>Biased (bootstrap)</td>
            </tr>
            <tr>
            <td><strong>Variance</strong></td>
            <td>High</td>
            <td>Lower</td>
            </tr>
            <tr>
            <td><strong>Convergence</strong></td>
            <td>Slower</td>
            <td>Faster</td>
            </tr>
            <tr>
            <td><strong>Continuous tasks</strong></td>
            <td>No</td>
            <td>Yes</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-bias-variance-tradeoff">The Bias-Variance
            Tradeoff</h3>
            <pre><code>MC:  G_t = r_{t+1} + Œ≥r_{t+2} + Œ≥¬≤r_{t+3} + ...  (many random variables)
     High variance, unbiased

TD:  r_{t+1} + Œ≥V(s_{t+1})  (one random variable + estimate)
     Lower variance, biased (V might be wrong)</code></pre>
            <h3 id="tdlambda-blending-mc-and-td">TD(<span
            class="math inline">\(\lambda\)</span>): Blending MC and
            TD</h3>
            <p><strong>Eligibility traces</strong> provide a continuum
            between TD(0) and MC:</p>
            <p><span class="math display">\[G_t^{(\lambda)} =
            (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1}
            G_t^{(n)}\]</span></p>
            <p>where <span class="math inline">\(G_t^{(n)}\)</span> is
            the n-step return.</p>
            <p><strong>What are eligibility traces?</strong> An
            eligibility trace is a temporary record of which states (or
            state-action pairs) have been visited recently. When a TD
            error occurs, instead of updating only the current state, we
            update <em>all</em> recently visited states in proportion to
            their eligibility. A state‚Äôs eligibility decays
            exponentially over time (by factor <span
            class="math inline">\(\gamma\lambda\)</span>), so recent
            states receive larger updates than states visited long ago.
            This bridges TD and MC: with <span
            class="math inline">\(\lambda=0\)</span>, only the current
            state is updated (pure TD); with <span
            class="math inline">\(\lambda=1\)</span>, all states from
            the episode receive credit (like MC). The trace acts as a
            ‚Äúmemory‚Äù that allows credit to flow backwards through time
            without waiting for the episode to end.</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline">\(\lambda\)</span></th>
            <th>Behavior</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\lambda = 0\)</span></td>
            <td>TD(0)</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\lambda = 1\)</span></td>
            <td>Monte Carlo</td>
            </tr>
            <tr>
            <td><span class="math inline">\(\lambda = 0.9\)</span></td>
            <td>Typical (blend)</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-the-difference-between-td-and-monte-carlo">Interview
            Q: ‚ÄúWhat‚Äôs the difference between TD and Monte Carlo?‚Äù</h3>
            <p><strong>A</strong>: Monte Carlo waits until episode end
            and uses the actual return <span
            class="math inline">\(G_t\)</span>, giving unbiased but
            high-variance estimates. TD updates after every step using
            <span class="math inline">\(r + \gamma V(s&#39;)\)</span>,
            bootstrapping from current value estimates. This introduces
            bias (estimates may be wrong) but reduces variance (only one
            random reward). TD can also handle continuing tasks and
            learns online. In practice, TD often converges faster due to
            lower variance, and TD(<span
            class="math inline">\(\lambda\)</span>) provides a spectrum
            between the two.</p>
            <hr />
            <h2 id="q-learning">9.5 Q-Learning</h2>
            <h3 id="the-algorithm">The Algorithm</h3>
            <p><strong>Off-policy TD control</strong> ‚Äî learn <span
            class="math inline">\(Q^*\)</span> regardless of behavior
            policy.</p>
            <p><span class="math display">\[Q(s_t, a_t) \leftarrow
            Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_{a&#39;}
            Q(s_{t+1}, a&#39;) - Q(s_t, a_t) \right]\]</span></p>
            <p><strong>Key</strong>: Uses <span
            class="math inline">\(\max_{a&#39;}\)</span> for the next
            state, not the action actually taken!</p>
            <h3 id="q-learning-implementation">Q-Learning
            Implementation</h3>
            <div class="sourceCode" id="cb316"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb316-1"><a href="#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_learning(env, episodes<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, gamma<span class="op">=</span><span class="fl">0.99</span>, epsilon<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb316-2"><a href="#cb316-2" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: defaultdict(<span class="bu">float</span>))</span>
<span id="cb316-3"><a href="#cb316-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb316-4"><a href="#cb316-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb316-5"><a href="#cb316-5" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> env.reset()</span>
<span id="cb316-6"><a href="#cb316-6" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb316-7"><a href="#cb316-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb316-8"><a href="#cb316-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb316-9"><a href="#cb316-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb316-10"><a href="#cb316-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random.random() <span class="op">&lt;</span> epsilon:</span>
<span id="cb316-11"><a href="#cb316-11" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> env.action_space.sample()</span>
<span id="cb316-12"><a href="#cb316-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb316-13"><a href="#cb316-13" aria-hidden="true" tabindex="-1"></a>                a <span class="op">=</span> <span class="bu">max</span>(Q[s], key<span class="op">=</span>Q[s].get, default<span class="op">=</span>env.action_space.sample())</span>
<span id="cb316-14"><a href="#cb316-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb316-15"><a href="#cb316-15" aria-hidden="true" tabindex="-1"></a>            s_next, r, done, _ <span class="op">=</span> env.step(a)</span>
<span id="cb316-16"><a href="#cb316-16" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb316-17"><a href="#cb316-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Q-learning update (off-policy)</span></span>
<span id="cb316-18"><a href="#cb316-18" aria-hidden="true" tabindex="-1"></a>            best_next <span class="op">=</span> <span class="bu">max</span>(Q[s_next].values(), default<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb316-19"><a href="#cb316-19" aria-hidden="true" tabindex="-1"></a>            td_error <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> best_next <span class="op">-</span> Q[s][a]</span>
<span id="cb316-20"><a href="#cb316-20" aria-hidden="true" tabindex="-1"></a>            Q[s][a] <span class="op">+=</span> alpha <span class="op">*</span> td_error</span>
<span id="cb316-21"><a href="#cb316-21" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb316-22"><a href="#cb316-22" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> s_next</span>
<span id="cb316-23"><a href="#cb316-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb316-24"><a href="#cb316-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q</span></code></pre></div>
            <h3 id="sarsa-on-policy-alternative">SARSA: On-Policy
            Alternative</h3>
            <p>Uses the actual next action, not the max:</p>
            <p><span class="math display">\[Q(s_t, a_t) \leftarrow
            Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma Q(s_{t+1},
            a_{t+1}) - Q(s_t, a_t) \right]\]</span></p>
            <p><strong>SARSA</strong> = <strong>S</strong>tate,
            <strong>A</strong>ction, <strong>R</strong>eward,
            <strong>S</strong>tate, <strong>A</strong>ction</p>
            <h3 id="q-learning-vs-sarsa">Q-Learning vs SARSA</h3>
            <table>
            <colgroup>
            <col style="width: 34%" />
            <col style="width: 41%" />
            <col style="width: 24%" />
            </colgroup>
            <thead>
            <tr>
            <th>Property</th>
            <th>Q-Learning</th>
            <th>SARSA</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Type</strong></td>
            <td>Off-policy</td>
            <td>On-policy</td>
            </tr>
            <tr>
            <td><strong>Update target</strong></td>
            <td><span class="math inline">\(\max_{a&#39;} Q(s&#39;,
            a&#39;)\)</span></td>
            <td><span class="math inline">\(Q(s&#39;, a&#39;)\)</span>
            (actual action)</td>
            </tr>
            <tr>
            <td><strong>Learns</strong></td>
            <td>Optimal Q</td>
            <td>Q for current policy</td>
            </tr>
            <tr>
            <td><strong>Exploration impact</strong></td>
            <td>Ignores</td>
            <td>Accounts for</td>
            </tr>
            <tr>
            <td><strong>Risk-awareness</strong></td>
            <td>No</td>
            <td>Yes (safer)</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Example</strong>: Cliff-walking ‚Äî Q-learning
            finds optimal (risky) path, SARSA finds safer path because
            it accounts for exploration mistakes.</p>
            <h3 id="deep-q-network-dqn">Deep Q-Network (DQN)</h3>
            <p>Use a neural network to approximate <span
            class="math inline">\(Q(s, a; \theta)\)</span>:</p>
            <p><strong>Key innovations</strong>:</p>
            <ol type="1">
            <li><p><strong>Experience replay</strong>: Store transitions
            <span class="math inline">\((s, a, r, s&#39;)\)</span> in a
            replay buffer and sample random mini-batches for training.
            This breaks the temporal correlation between consecutive
            samples ‚Äî without it, the network sees highly correlated
            sequences (state at time <span
            class="math inline">\(t\)</span> is similar to state at
            <span class="math inline">\(t+1\)</span>), which violates
            the i.i.d. assumption of SGD and causes unstable learning.
            Replay also improves sample efficiency since each experience
            can be used multiple times.</p></li>
            <li><p><strong>Target network</strong>: Use a separate,
            slowly-updated copy of the Q-network to compute targets. The
            problem: in <span class="math inline">\(Q(s,a) \leftarrow r
            + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)\)</span>, both
            sides depend on the same network parameters. When we update
            <span class="math inline">\(Q\)</span>, the target also
            changes, creating a ‚Äúmoving target‚Äù that destabilizes
            training. The target network <span
            class="math inline">\(Q_{target}\)</span> is frozen and only
            updated periodically (e.g., every 10,000 steps), providing
            stable targets for the main network to chase.</p></li>
            </ol>
            <div class="sourceCode" id="cb317"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DQN loss</span></span>
<span id="cb317-2"><a href="#cb317-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> <span class="bu">max</span>(Q_target(s_next, a<span class="st">&#39;))  # Target network</span></span>
<span id="cb317-3"><a href="#cb317-3" aria-hidden="true" tabindex="-1"></a><span class="er">loss = </span>(Q(s, a<span class="op">;</span> theta) <span class="op">-</span> target)<span class="op">^</span><span class="dv">2</span></span></code></pre></div>
            <h3
            id="interview-q-explain-q-learning-and-why-its-off-policy">Interview
            Q: ‚ÄúExplain Q-learning and why it‚Äôs off-policy‚Äù</h3>
            <p><strong>A</strong>: Q-learning updates Q-values using:
            <span class="math inline">\(Q(s,a) \leftarrow Q(s,a) +
            \alpha[r + \gamma \max_{a&#39;} Q(s&#39;,a&#39;) -
            Q(s,a)]\)</span>. It‚Äôs off-policy because the update uses
            <span class="math inline">\(\max_{a&#39;}\)</span> ‚Äî the
            best possible action ‚Äî regardless of which action was
            actually taken. This means we can learn the optimal policy
            while following an exploratory policy. The tradeoff is that
            Q-learning can overestimate values due to the max operator
            (addressed by Double DQN). SARSA is the on-policy
            alternative that uses the actual next action.</p>
            <hr />
            <h2 id="policy-gradient-methods">9.6 Policy Gradient
            Methods</h2>
            <h3 id="the-problem-with-value-based-methods">The Problem
            with Value-Based Methods</h3>
            <ul>
            <li><strong>Discrete actions only</strong>: Can‚Äôt easily
            handle continuous actions</li>
            <li><strong>Deterministic policies</strong>: No natural way
            to model stochastic policies</li>
            <li><strong>Indirect</strong>: Learn value, then derive
            policy</li>
            </ul>
            <h3 id="policy-gradient-idea">Policy Gradient Idea</h3>
            <p><strong>Learn the policy directly!</strong></p>
            <p>Parameterize policy as <span
            class="math inline">\(\pi_\theta(a|s)\)</span> and
            optimize:</p>
            <p><span class="math display">\[J(\theta) = \mathbb{E}_{\tau
            \sim \pi_\theta}\left[\sum_t r_t\right]\]</span></p>
            <h3 id="the-policy-gradient-theorem">The Policy Gradient
            Theorem</h3>
            <p><span class="math display">\[\nabla_\theta J(\theta) =
            \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T}
            \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot
            G_t\right]\]</span></p>
            <p><strong>The log-derivative trick</strong>: <span
            class="math display">\[\nabla_\theta \pi_\theta = \pi_\theta
            \cdot \nabla_\theta \log \pi_\theta\]</span></p>
            <h3 id="reinforce-algorithm">REINFORCE Algorithm</h3>
            <div class="sourceCode" id="cb318"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb318-1"><a href="#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reinforce(env, policy_net, optimizer, episodes<span class="op">=</span><span class="dv">1000</span>, gamma<span class="op">=</span><span class="fl">0.99</span>):</span>
<span id="cb318-2"><a href="#cb318-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb318-3"><a href="#cb318-3" aria-hidden="true" tabindex="-1"></a>        states, actions, rewards <span class="op">=</span> [], [], []</span>
<span id="cb318-4"><a href="#cb318-4" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb318-5"><a href="#cb318-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Collect episode</span></span>
<span id="cb318-6"><a href="#cb318-6" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> env.reset()</span>
<span id="cb318-7"><a href="#cb318-7" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb318-8"><a href="#cb318-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb318-9"><a href="#cb318-9" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> policy_net(s)</span>
<span id="cb318-10"><a href="#cb318-10" aria-hidden="true" tabindex="-1"></a>            a <span class="op">=</span> sample(probs)</span>
<span id="cb318-11"><a href="#cb318-11" aria-hidden="true" tabindex="-1"></a>            s_next, r, done, _ <span class="op">=</span> env.step(a)</span>
<span id="cb318-12"><a href="#cb318-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb318-13"><a href="#cb318-13" aria-hidden="true" tabindex="-1"></a>            states.append(s)</span>
<span id="cb318-14"><a href="#cb318-14" aria-hidden="true" tabindex="-1"></a>            actions.append(a)</span>
<span id="cb318-15"><a href="#cb318-15" aria-hidden="true" tabindex="-1"></a>            rewards.append(r)</span>
<span id="cb318-16"><a href="#cb318-16" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> s_next</span>
<span id="cb318-17"><a href="#cb318-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb318-18"><a href="#cb318-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute returns</span></span>
<span id="cb318-19"><a href="#cb318-19" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb318-20"><a href="#cb318-20" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb318-21"><a href="#cb318-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">reversed</span>(rewards):</span>
<span id="cb318-22"><a href="#cb318-22" aria-hidden="true" tabindex="-1"></a>            G <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> G</span>
<span id="cb318-23"><a href="#cb318-23" aria-hidden="true" tabindex="-1"></a>            returns.insert(<span class="dv">0</span>, G)</span>
<span id="cb318-24"><a href="#cb318-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb318-25"><a href="#cb318-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Policy gradient update</span></span>
<span id="cb318-26"><a href="#cb318-26" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb318-27"><a href="#cb318-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s, a, G <span class="kw">in</span> <span class="bu">zip</span>(states, actions, returns):</span>
<span id="cb318-28"><a href="#cb318-28" aria-hidden="true" tabindex="-1"></a>            log_prob <span class="op">=</span> log(policy_net(s)[a])</span>
<span id="cb318-29"><a href="#cb318-29" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">-=</span> log_prob <span class="op">*</span> G  <span class="co"># Negative for gradient ascent</span></span>
<span id="cb318-30"><a href="#cb318-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb318-31"><a href="#cb318-31" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb318-32"><a href="#cb318-32" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb318-33"><a href="#cb318-33" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div>
            <h3 id="variance-reduction-baseline">Variance Reduction:
            Baseline</h3>
            <p>Subtract a baseline <span
            class="math inline">\(b(s)\)</span> that doesn‚Äôt depend on
            action:</p>
            <p><span class="math display">\[\nabla_\theta J(\theta) =
            \mathbb{E}\left[\sum_t \nabla_\theta \log
            \pi_\theta(a_t|s_t) \cdot (G_t - b(s_t))\right]\]</span></p>
            <p><strong>Why does subtracting a baseline reduce variance
            without adding bias?</strong> The gradient <span
            class="math inline">\(\nabla_\theta \log
            \pi_\theta(a|s)\)</span> tells us how to adjust the policy,
            and the return <span class="math inline">\(G_t\)</span>
            scales this adjustment ‚Äî positive returns increase the
            action‚Äôs probability, negative returns decrease it. The
            problem is that <span class="math inline">\(G_t\)</span> can
            vary wildly from episode to episode, causing high variance
            in gradient estimates.</p>
            <p>Subtracting a baseline <span
            class="math inline">\(b(s)\)</span> shifts the scale:
            instead of asking ‚Äúwas this return good?‚Äù we ask ‚Äúwas this
            return better than average?‚Äù Mathematically, subtracting a
            baseline doesn‚Äôt change the expected gradient (it‚Äôs
            unbiased) because <span
            class="math inline">\(\mathbb{E}_a[\nabla_\theta \log
            \pi_\theta(a|s) \cdot b(s)] = b(s) \cdot \nabla_\theta
            \sum_a \pi_\theta(a|s) = b(s) \cdot \nabla_\theta 1 =
            0\)</span>. The baseline can be anything that doesn‚Äôt depend
            on the action ‚Äî but choosing <span
            class="math inline">\(b(s) = V(s)\)</span> (expected return
            from state <span class="math inline">\(s\)</span>) is
            optimal because it minimizes variance by centering the
            returns around their mean.</p>
            <p><strong>Common choice</strong>: <span
            class="math inline">\(b(s) = V(s)\)</span> ‚Üí gives us the
            <strong>advantage</strong>:</p>
            <p><span class="math display">\[A(s, a) = Q(s, a) - V(s) =
            G_t - V(s_t)\]</span></p>
            <h3 id="why-policy-gradients">Why Policy Gradients?</h3>
            <table>
            <thead>
            <tr>
            <th>Advantage</th>
            <th>Explanation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Continuous actions</strong></td>
            <td>Natural for continuous control</td>
            </tr>
            <tr>
            <td><strong>Stochastic policies</strong></td>
            <td>Can learn exploration</td>
            </tr>
            <tr>
            <td><strong>Direct optimization</strong></td>
            <td>No value function indirection</td>
            </tr>
            <tr>
            <td><strong>Better convergence</strong></td>
            <td>Small policy change = small behavior change</td>
            </tr>
            </tbody>
            </table>
            <h3 id="drawbacks">Drawbacks</h3>
            <table>
            <thead>
            <tr>
            <th>Drawback</th>
            <th>Explanation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>High variance</strong></td>
            <td>Full returns are noisy</td>
            </tr>
            <tr>
            <td><strong>Sample inefficient</strong></td>
            <td>On-policy (discard data after update)</td>
            </tr>
            <tr>
            <td><strong>Local optima</strong></td>
            <td>Gradient ascent on non-convex</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-derive-the-policy-gradient-theorem">Interview
            Q: ‚ÄúDerive the policy gradient theorem‚Äù</h3>
            <p><strong>A</strong>: We want <span
            class="math inline">\(\nabla_\theta J(\theta) =
            \nabla_\theta \mathbb{E}_{\tau \sim
            \pi_\theta}[R(\tau)]\)</span>.</p>
            <p>Using the log-derivative trick: <span
            class="math inline">\(\nabla_\theta \pi_\theta(\tau) =
            \pi_\theta(\tau) \nabla_\theta \log
            \pi_\theta(\tau)\)</span></p>
            <p><span class="math display">\[\nabla_\theta J = \int
            \nabla_\theta \pi_\theta(\tau) R(\tau) d\tau = \int
            \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) R(\tau)
            d\tau\]</span></p>
            <p><span class="math display">\[= \mathbb{E}_{\tau \sim
            \pi_\theta}[\nabla_\theta \log \pi_\theta(\tau) \cdot
            R(\tau)]\]</span></p>
            <p>Since <span class="math inline">\(\log \pi_\theta(\tau) =
            \sum_t \log \pi_\theta(a_t|s_t)\)</span> and future actions
            don‚Äôt affect past rewards:</p>
            <p><span class="math display">\[\nabla_\theta J =
            \mathbb{E}\left[\sum_t \nabla_\theta \log
            \pi_\theta(a_t|s_t) \cdot G_t\right]\]</span></p>
            <p>This allows us to estimate gradients from sampled
            trajectories.</p>
            <hr />
            <h2 id="actor-critic-methods">9.7 Actor-Critic Methods</h2>
            <h3 id="the-idea-3">The Idea</h3>
            <p><strong>Combine policy gradient (actor) with value
            function (critic)</strong>:</p>
            <ul>
            <li><strong>Actor</strong>: Policy <span
            class="math inline">\(\pi_\theta(a|s)\)</span> ‚Äî decides
            actions</li>
            <li><strong>Critic</strong>: Value function <span
            class="math inline">\(V_\phi(s)\)</span> ‚Äî evaluates
            actions</li>
            </ul>
            <h3 id="why-actor-critic">Why Actor-Critic?</h3>
            <table>
            <colgroup>
            <col style="width: 39%" />
            <col style="width: 25%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Component</th>
            <th>Alone</th>
            <th>Combined</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Actor</strong> (Policy Gradient)</td>
            <td>High variance</td>
            <td>Uses critic for lower variance</td>
            </tr>
            <tr>
            <td><strong>Critic</strong> (Value-based)</td>
            <td>Can‚Äôt do continuous</td>
            <td>Actor handles actions</td>
            </tr>
            </tbody>
            </table>
            <h3 id="advantage-actor-critic-a2c">Advantage Actor-Critic
            (A2C)</h3>
            <p><strong>Advantage</strong>: <span
            class="math inline">\(A(s, a) = Q(s, a) - V(s) \approx r +
            \gamma V(s&#39;) - V(s)\)</span></p>
            <div class="sourceCode" id="cb319"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb319-1"><a href="#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> a2c_update(states, actions, rewards, next_states, dones,</span>
<span id="cb319-2"><a href="#cb319-2" aria-hidden="true" tabindex="-1"></a>               actor, critic, actor_optim, critic_optim, gamma<span class="op">=</span><span class="fl">0.99</span>):</span>
<span id="cb319-3"><a href="#cb319-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb319-4"><a href="#cb319-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute advantage</span></span>
<span id="cb319-5"><a href="#cb319-5" aria-hidden="true" tabindex="-1"></a>    values <span class="op">=</span> critic(states)</span>
<span id="cb319-6"><a href="#cb319-6" aria-hidden="true" tabindex="-1"></a>    next_values <span class="op">=</span> critic(next_states)</span>
<span id="cb319-7"><a href="#cb319-7" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> rewards <span class="op">+</span> gamma <span class="op">*</span> next_values <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> dones)</span>
<span id="cb319-8"><a href="#cb319-8" aria-hidden="true" tabindex="-1"></a>    advantages <span class="op">=</span> targets <span class="op">-</span> values</span>
<span id="cb319-9"><a href="#cb319-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb319-10"><a href="#cb319-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Critic update (minimize TD error)</span></span>
<span id="cb319-11"><a href="#cb319-11" aria-hidden="true" tabindex="-1"></a>    critic_loss <span class="op">=</span> F.mse_loss(values, targets.detach())</span>
<span id="cb319-12"><a href="#cb319-12" aria-hidden="true" tabindex="-1"></a>    critic_optim.zero_grad()</span>
<span id="cb319-13"><a href="#cb319-13" aria-hidden="true" tabindex="-1"></a>    critic_loss.backward()</span>
<span id="cb319-14"><a href="#cb319-14" aria-hidden="true" tabindex="-1"></a>    critic_optim.step()</span>
<span id="cb319-15"><a href="#cb319-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb319-16"><a href="#cb319-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Actor update (policy gradient with advantage)</span></span>
<span id="cb319-17"><a href="#cb319-17" aria-hidden="true" tabindex="-1"></a>    log_probs <span class="op">=</span> actor(states).log_prob(actions)</span>
<span id="cb319-18"><a href="#cb319-18" aria-hidden="true" tabindex="-1"></a>    actor_loss <span class="op">=</span> <span class="op">-</span>(log_probs <span class="op">*</span> advantages.detach()).mean()</span>
<span id="cb319-19"><a href="#cb319-19" aria-hidden="true" tabindex="-1"></a>    actor_optim.zero_grad()</span>
<span id="cb319-20"><a href="#cb319-20" aria-hidden="true" tabindex="-1"></a>    actor_loss.backward()</span>
<span id="cb319-21"><a href="#cb319-21" aria-hidden="true" tabindex="-1"></a>    actor_optim.step()</span></code></pre></div>
            <h3 id="a3c-asynchronous-advantage-actor-critic">A3C:
            Asynchronous Advantage Actor-Critic</h3>
            <p>Multiple workers train in parallel:</p>
            <ul>
            <li>Each worker has its own environment copy</li>
            <li>Computes gradients independently</li>
            <li>Updates shared parameters asynchronously</li>
            </ul>
            <p><strong>Benefits</strong>: More exploration, better GPU
            utilization</p>
            <h3 id="generalized-advantage-estimation-gae">Generalized
            Advantage Estimation (GAE)</h3>
            <p>Like TD(<span class="math inline">\(\lambda\)</span>) for
            advantages:</p>
            <p><span class="math display">\[\hat{A}_t^{GAE(\gamma,
            \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l
            \delta_{t+l}\]</span></p>
            <p>where <span class="math inline">\(\delta_t = r_t + \gamma
            V(s_{t+1}) - V(s_t)\)</span></p>
            <p><strong>Intuition behind GAE</strong>: Just as TD(<span
            class="math inline">\(\lambda\)</span>) provides a smooth
            interpolation between TD(0) and Monte Carlo for value
            estimation, GAE does the same for advantage estimation. The
            one-step advantage estimate <span
            class="math inline">\(\delta_t = r_t + \gamma V(s_{t+1}) -
            V(s_t)\)</span> has low variance (only one reward is
            stochastic) but high bias (relies on potentially inaccurate
            <span class="math inline">\(V\)</span> estimates). The Monte
            Carlo advantage <span class="math inline">\(G_t -
            V(s_t)\)</span> is unbiased but has high variance (sums many
            random rewards). GAE computes a weighted average of n-step
            advantage estimates, with <span
            class="math inline">\(\lambda\)</span> controlling the decay
            of weights. Each TD error <span
            class="math inline">\(\delta_t\)</span> represents ‚Äúlocal‚Äù
            credit, and GAE spreads this credit backwards through time
            with exponentially decaying weights <span
            class="math inline">\((\gamma\lambda)^l\)</span>. In
            practice, <span class="math inline">\(\lambda \approx
            0.95\)</span> works well ‚Äî getting most of the variance
            reduction of TD while retaining much of MC‚Äôs lower bias.</p>
            <ul>
            <li><span class="math inline">\(\lambda = 0\)</span>: TD
            estimate (low variance, high bias)</li>
            <li><span class="math inline">\(\lambda = 1\)</span>: MC
            estimate (high variance, low bias)</li>
            </ul>
            <h3
            id="interview-q-whats-the-advantage-function-and-why-use-it">Interview
            Q: ‚ÄúWhat‚Äôs the advantage function and why use it?‚Äù</h3>
            <p><strong>A</strong>: The advantage <span
            class="math inline">\(A(s,a) = Q(s,a) - V(s)\)</span>
            measures how much better an action is compared to average.
            Using advantage instead of raw returns reduces variance
            without adding bias (since <span
            class="math inline">\(V(s)\)</span> doesn‚Äôt depend on the
            action). In actor-critic methods, the critic estimates <span
            class="math inline">\(V(s)\)</span>, and the advantage <span
            class="math inline">\(A \approx r + \gamma V(s&#39;) -
            V(s)\)</span> guides the actor update. Positive advantage
            means the action was better than expected, so increase its
            probability. This is crucial for stable training.</p>
            <hr />
            <h2 id="proximal-policy-optimization-ppo">9.8 Proximal
            Policy Optimization (PPO)</h2>
            <h3 id="the-problem-with-vanilla-policy-gradient">The
            Problem with Vanilla Policy Gradient</h3>
            <p>Large policy updates can be catastrophic:</p>
            <ul>
            <li>Policy might suddenly become bad</li>
            <li>Can‚Äôt recover (on-policy data is now useless)</li>
            <li>Training is unstable</li>
            </ul>
            <h3 id="trust-region-policy-optimization-trpo">Trust Region
            Policy Optimization (TRPO)</h3>
            <p><strong>Idea</strong>: Constrain how much the policy can
            change.</p>
            <p><span class="math display">\[\max_\theta
            \mathbb{E}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}
            \hat{A}(s,a)\right]\]</span></p>
            <p><span class="math display">\[\text{subject to }
            D_{KL}(\pi_{\theta_{old}} || \pi_\theta) \leq
            \delta\]</span></p>
            <p>Problem: The constraint requires expensive second-order
            optimization.</p>
            <h3 id="ppo-simpler-alternative">PPO: Simpler
            Alternative</h3>
            <p><strong>Clipped objective</strong> ‚Äî no constraint, just
            clip the ratio:</p>
            <p><span class="math display">\[L^{CLIP}(\theta) =
            \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t,
            \text{clip}(r_t(\theta), 1-\epsilon,
            1+\epsilon)\hat{A}_t\right)\right]\]</span></p>
            <p>where <span class="math inline">\(r_t(\theta) =
            \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\)</span></p>
            <h3 id="how-clipping-works">How Clipping Works</h3>
            <pre><code>If A &gt; 0 (good action):
    Want to increase œÄ(a|s)
    But clip ratio at 1+Œµ to prevent too large increase
    
If A &lt; 0 (bad action):
    Want to decrease œÄ(a|s)
    But clip ratio at 1-Œµ to prevent too large decrease</code></pre>
            <p><strong>The min ensures we take the more pessimistic
            bound.</strong></p>
            <h3 id="ppo-implementation">PPO Implementation</h3>
            <div class="sourceCode" id="cb321"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb321-1"><a href="#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo_update(actor, critic, states, actions, old_log_probs, returns, advantages,</span>
<span id="cb321-2"><a href="#cb321-2" aria-hidden="true" tabindex="-1"></a>               epochs<span class="op">=</span><span class="dv">10</span>, epsilon<span class="op">=</span><span class="fl">0.2</span>, clip_value<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb321-3"><a href="#cb321-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb321-4"><a href="#cb321-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb321-5"><a href="#cb321-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Current policy</span></span>
<span id="cb321-6"><a href="#cb321-6" aria-hidden="true" tabindex="-1"></a>        new_log_probs <span class="op">=</span> actor(states).log_prob(actions)</span>
<span id="cb321-7"><a href="#cb321-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb321-8"><a href="#cb321-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Probability ratio</span></span>
<span id="cb321-9"><a href="#cb321-9" aria-hidden="true" tabindex="-1"></a>        ratio <span class="op">=</span> torch.exp(new_log_probs <span class="op">-</span> old_log_probs)</span>
<span id="cb321-10"><a href="#cb321-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb321-11"><a href="#cb321-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Clipped objective</span></span>
<span id="cb321-12"><a href="#cb321-12" aria-hidden="true" tabindex="-1"></a>        surr1 <span class="op">=</span> ratio <span class="op">*</span> advantages</span>
<span id="cb321-13"><a href="#cb321-13" aria-hidden="true" tabindex="-1"></a>        surr2 <span class="op">=</span> torch.clamp(ratio, <span class="dv">1</span><span class="op">-</span>epsilon, <span class="dv">1</span><span class="op">+</span>epsilon) <span class="op">*</span> advantages</span>
<span id="cb321-14"><a href="#cb321-14" aria-hidden="true" tabindex="-1"></a>        actor_loss <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">min</span>(surr1, surr2).mean()</span>
<span id="cb321-15"><a href="#cb321-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb321-16"><a href="#cb321-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Value loss (optionally clipped too)</span></span>
<span id="cb321-17"><a href="#cb321-17" aria-hidden="true" tabindex="-1"></a>        values <span class="op">=</span> critic(states)</span>
<span id="cb321-18"><a href="#cb321-18" aria-hidden="true" tabindex="-1"></a>        critic_loss <span class="op">=</span> F.mse_loss(values, returns)</span>
<span id="cb321-19"><a href="#cb321-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb321-20"><a href="#cb321-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update</span></span>
<span id="cb321-21"><a href="#cb321-21" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> actor_loss <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> critic_loss</span>
<span id="cb321-22"><a href="#cb321-22" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb321-23"><a href="#cb321-23" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb321-24"><a href="#cb321-24" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div>
            <h3 id="ppo-hyperparameters">PPO Hyperparameters</h3>
            <table>
            <thead>
            <tr>
            <th>Hyperparameter</th>
            <th>Typical Value</th>
            <th>Purpose</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">\(\epsilon\)</span>
            (clip)</td>
            <td>0.1-0.2</td>
            <td>How much ratio can change</td>
            </tr>
            <tr>
            <td>Epochs per update</td>
            <td>3-10</td>
            <td>Reuse data multiple times</td>
            </tr>
            <tr>
            <td>GAE <span class="math inline">\(\lambda\)</span></td>
            <td>0.95</td>
            <td>Advantage estimation</td>
            </tr>
            <tr>
            <td>Learning rate</td>
            <td>3e-4</td>
            <td>Optimization</td>
            </tr>
            </tbody>
            </table>
            <h3 id="why-ppo-for-rlhf">Why PPO for RLHF?</h3>
            <ol type="1">
            <li><strong>Stable</strong>: Clipping prevents catastrophic
            updates</li>
            <li><strong>Sample efficient</strong>: Multiple epochs per
            batch</li>
            <li><strong>Simple</strong>: First-order optimization, easy
            to implement</li>
            <li><strong>Proven</strong>: Works well empirically</li>
            </ol>
            <h3 id="interview-q-why-does-ppo-use-clipping">Interview Q:
            ‚ÄúWhy does PPO use clipping?‚Äù</h3>
            <p><strong>A</strong>: PPO clips the probability ratio <span
            class="math inline">\(r(\theta) =
            \pi_\theta(a|s)/\pi_{old}(a|s)\)</span> to [1-Œµ, 1+Œµ]. This
            prevents the policy from changing too drastically in a
            single update, which could be catastrophic for on-policy
            learning. The clipped objective is pessimistic ‚Äî it takes
            the minimum of clipped and unclipped, so we only get credit
            for policy improvement up to the clip boundary. This
            achieves similar stability to TRPO‚Äôs KL constraint but with
            simpler first-order optimization. For RLHF, this stability
            is crucial because we‚Äôre optimizing against a learned reward
            model.</p>
            <hr />
            <h2 id="exploration-vs-exploitation">9.9 Exploration vs
            Exploitation</h2>
            <h3 id="the-dilemma">The Dilemma</h3>
            <ul>
            <li><strong>Exploitation</strong>: Use current knowledge to
            maximize reward</li>
            <li><strong>Exploration</strong>: Try new actions to
            discover potentially better strategies</li>
            </ul>
            <p><strong>Too much exploitation</strong>: Get stuck in
            local optima <strong>Too much exploration</strong>: Never
            capitalize on good strategies</p>
            <h3 id="exploration-methods">Exploration Methods</h3>
            <h4 id="Œµ-greedy">1. Œµ-Greedy</h4>
            <div class="sourceCode" id="cb322"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> epsilon_greedy(Q, s, epsilon):</span>
<span id="cb322-2"><a href="#cb322-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> random.random() <span class="op">&lt;</span> epsilon:</span>
<span id="cb322-3"><a href="#cb322-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random_action()  <span class="co"># Explore</span></span>
<span id="cb322-4"><a href="#cb322-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb322-5"><a href="#cb322-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> argmax(Q[s])     <span class="co"># Exploit</span></span></code></pre></div>
            <ul>
            <li>Simple but can be inefficient</li>
            <li>Explores uniformly (doesn‚Äôt target uncertainty)</li>
            </ul>
            <h4 id="boltzmann-softmax-exploration">2. Boltzmann
            (Softmax) Exploration</h4>
            <p><span class="math display">\[P(a|s) =
            \frac{\exp(Q(s,a)/\tau)}{\sum_{a&#39;}
            \exp(Q(s,a&#39;)/\tau)}\]</span></p>
            <ul>
            <li><span class="math inline">\(\tau\)</span> high: More
            uniform (exploration)</li>
            <li><span class="math inline">\(\tau\)</span> low: More
            greedy (exploitation)</li>
            </ul>
            <h4 id="upper-confidence-bound-ucb">3. Upper Confidence
            Bound (UCB)</h4>
            <p><span class="math display">\[a_t = \arg\max_a \left[ Q(s,
            a) + c\sqrt{\frac{\ln t}{N(s, a)}} \right]\]</span></p>
            <ul>
            <li>Bonus for less-tried actions</li>
            <li>Theoretically motivated (regret bounds)</li>
            <li>Used in MCTS (AlphaGo)</li>
            </ul>
            <h4 id="intrinsic-motivation">4. Intrinsic Motivation</h4>
            <p>Add <strong>curiosity</strong> reward for novel
            states:</p>
            <p><span class="math display">\[r_{total} = r_{extrinsic} +
            \beta \cdot r_{intrinsic}\]</span></p>
            <p><strong>Examples</strong>:</p>
            <ul>
            <li><strong>Prediction error</strong>: Reward = how
            surprising the next state is</li>
            <li><strong>Count-based</strong>: Reward = 1/‚àö(visit
            count)</li>
            <li><strong>Random Network Distillation (RND)</strong>:
            Compare random and learned networks</li>
            </ul>
            <h3 id="exploration-in-deep-rl">Exploration in Deep RL</h3>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>Approach</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>NoisyNets</strong></td>
            <td>Learnable noise in network weights</td>
            </tr>
            <tr>
            <td><strong>Parameter Space Noise</strong></td>
            <td>Add noise to parameters</td>
            </tr>
            <tr>
            <td><strong>Entropy Regularization</strong></td>
            <td>Encourage stochastic policy</td>
            </tr>
            <tr>
            <td><strong>ICM</strong></td>
            <td>Intrinsic curiosity module</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-multi-armed-bandit">The Multi-Armed Bandit</h3>
            <p>Simplest exploration-exploitation setting:</p>
            <ul>
            <li><span class="math inline">\(K\)</span> slot machines
            (arms) with unknown reward distributions</li>
            <li>Goal: Maximize total reward over <span
            class="math inline">\(T\)</span> pulls</li>
            </ul>
            <p><strong>Regret</strong>: How much worse than optimal?</p>
            <p><span class="math display">\[\text{Regret}_T = T \cdot
            \mu^* - \sum_{t=1}^{T} r_t\]</span></p>
            <p><strong>Optimal algorithms</strong>: UCB achieves <span
            class="math inline">\(O(\sqrt{KT \ln T})\)</span>
            regret.</p>
            <h3
            id="interview-q-whats-the-exploration-exploitation-tradeoff">Interview
            Q: ‚ÄúWhat‚Äôs the exploration-exploitation tradeoff?‚Äù</h3>
            <p><strong>A</strong>: Exploration means trying new actions
            to gather information; exploitation means using current best
            knowledge. Too much exploration wastes time on suboptimal
            actions; too little means you might miss better strategies.
            Common solutions include Œµ-greedy (random with probability
            Œµ), UCB (optimism under uncertainty ‚Äî bonus for untried
            actions), and entropy regularization (encourage diverse
            policies). In deep RL, methods like NoisyNets add learnable
            noise, and intrinsic motivation provides curiosity rewards
            for novel states. The right balance depends on the horizon:
            explore more early, exploit more later.</p>
            <hr />
            <h2 id="connection-to-llm-alignment-1">9.10 Connection to
            LLM Alignment</h2>
            <h3 id="why-rl-matters-for-llms">Why RL Matters for
            LLMs</h3>
            <p>The RLHF/DPO/GRPO methods covered in Part 5 are direct
            applications of RL:</p>
            <table>
            <colgroup>
            <col style="width: 46%" />
            <col style="width: 53%" />
            </colgroup>
            <thead>
            <tr>
            <th>LLM Concept</th>
            <th>RL Foundation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Policy</strong> <span
            class="math inline">\(\pi_\theta(y\|x)\)</span></td>
            <td>LLM generating response given prompt</td>
            </tr>
            <tr>
            <td><strong>State</strong></td>
            <td>Prompt + generated tokens so far</td>
            </tr>
            <tr>
            <td><strong>Action</strong></td>
            <td>Next token to generate</td>
            </tr>
            <tr>
            <td><strong>Reward</strong></td>
            <td>Human preference / reward model score</td>
            </tr>
            <tr>
            <td><strong>KL penalty</strong></td>
            <td>Keeps policy close to reference (prevents reward
            hacking)</td>
            </tr>
            <tr>
            <td><strong>PPO in RLHF</strong></td>
            <td>Section 7.8 ‚Äî stable policy updates</td>
            </tr>
            <tr>
            <td><strong>Advantage in GRPO</strong></td>
            <td>Section 7.7 ‚Äî group-relative baseline</td>
            </tr>
            </tbody>
            </table>
            <h3 id="key-insights-from-rl-for-llm-alignment">Key Insights
            from RL for LLM Alignment</h3>
            <ol type="1">
            <li><strong>Why KL constraint?</strong> ‚Äî From trust region
            methods: large updates destabilize training</li>
            <li><strong>Why advantage estimation?</strong> ‚Äî Reduces
            variance (same as actor-critic with baseline)</li>
            <li><strong>Why PPO clipping?</strong> ‚Äî Prevents
            catastrophic policy updates</li>
            <li><strong>Why GRPO works?</strong> ‚Äî Group mean is an
            unbiased baseline (like REINFORCE with baseline)</li>
            </ol>
            <h3
            id="interview-q-how-does-rlhf-relate-to-standard-rl">Interview
            Q: ‚ÄúHow does RLHF relate to standard RL?‚Äù</h3>
            <p><strong>A</strong>: RLHF treats the LLM as a policy that
            maps prompts (states) to responses (action sequences). The
            reward model provides the reward signal. We use PPO because
            it‚Äôs stable ‚Äî the KL constraint prevents the policy from
            drifting too far from the SFT model, which would lead to
            reward hacking. The key difference from game RL: the
            ‚Äúenvironment‚Äù is deterministic (text generation), rewards
            are sparse (one score per full response), and we must
            prevent distribution shift from the language prior.</p>
            <hr />
            <h2 id="model-based-vs-model-free-rl">9.11 Model-Based vs
            Model-Free RL</h2>
            <h3 id="the-fundamental-distinction">The Fundamental
            Distinction</h3>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 40%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Approach</th>
            <th>What It Learns</th>
            <th>How It Plans</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Model-Free</strong></td>
            <td>Policy or value function directly</td>
            <td>No explicit planning ‚Äî act from learned policy</td>
            </tr>
            <tr>
            <td><strong>Model-Based</strong></td>
            <td>Environment dynamics model</td>
            <td>Plan using the learned model</td>
            </tr>
            </tbody>
            </table>
            <h3 id="model-free-rl-what-weve-covered">Model-Free RL (What
            We‚Äôve Covered)</h3>
            <p>All previous sections (Q-learning, Policy Gradient, PPO)
            are <strong>model-free</strong>:</p>
            <pre><code>Model-Free:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Environment (unknown) ‚Üê‚îÄ‚îÄ‚Üí Agent
                            ‚îÇ
                            ‚Üì
                    Learn œÄ(a|s) or Q(s,a) directly
                            ‚îÇ
                            ‚Üì
                    No internal model of world</code></pre>
            <p><strong>Algorithms</strong>: Q-learning, SARSA,
            REINFORCE, A2C, PPO, SAC</p>
            <p><strong>Pros</strong>:</p>
            <ul>
            <li>No model bias ‚Äî learns directly from real
            experience</li>
            <li>Works when dynamics are complex/unknown</li>
            <li>Often simpler to implement</li>
            </ul>
            <p><strong>Cons</strong>:</p>
            <ul>
            <li>Sample inefficient ‚Äî needs many environment
            interactions</li>
            <li>Can‚Äôt ‚Äúimagine‚Äù or plan ahead</li>
            </ul>
            <h3 id="model-based-rl">Model-Based RL</h3>
            <p>Learn a <strong>world model</strong>: <span
            class="math inline">\(\hat{P}(s&#39;|s,a)\)</span> and <span
            class="math inline">\(\hat{R}(s,a)\)</span></p>
            <pre><code>Model-Based:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Environment (unknown) ‚Üê‚îÄ‚îÄ‚Üí Agent
                            ‚îÇ
                            ‚Üì
                    Learn world model:
                    ≈ù&#39; = f(s, a)
                    rÃÇ = g(s, a)
                            ‚îÇ
                            ‚Üì
                    Plan using model:

                    - Simulate trajectories
                    - Search for best actions
                    - Model Predictive Control</code></pre>
            <p><strong>Algorithms</strong>: Dyna-Q, PILCO, PETS,
            Dreamer, MuZero</p>
            <h3 id="the-dyna-architecture">The Dyna Architecture</h3>
            <p><strong>Key idea</strong>: Use both real AND simulated
            experience!</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Dyna-Q Architecture                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Real Experience:                                                   ‚îÇ
‚îÇ  Environment ‚Üí (s, a, r, s&#39;) ‚Üí Update Q                            ‚îÇ
‚îÇ                     ‚îÇ                                               ‚îÇ
‚îÇ                     ‚Üì                                               ‚îÇ
‚îÇ  Learn Model: (s, a) ‚Üí Model ‚Üí (rÃÇ, ≈ù&#39;)                            ‚îÇ
‚îÇ                                   ‚îÇ                                 ‚îÇ
‚îÇ                                   ‚Üì                                 ‚îÇ
‚îÇ  Simulated Experience:                                              ‚îÇ
‚îÇ  Random (s, a) ‚Üí Model ‚Üí (rÃÇ, ≈ù&#39;) ‚Üí Update Q                       ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  n planning steps per real step!                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <div class="sourceCode" id="cb326"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb326-1"><a href="#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dyna_q(env, n_planning_steps<span class="op">=</span><span class="dv">5</span>, episodes<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb326-2"><a href="#cb326-2" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> defaultdict(<span class="kw">lambda</span>: defaultdict(<span class="bu">float</span>))</span>
<span id="cb326-3"><a href="#cb326-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> {}  <span class="co"># (s, a) ‚Üí (r, s&#39;)</span></span>
<span id="cb326-4"><a href="#cb326-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb326-5"><a href="#cb326-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb326-6"><a href="#cb326-6" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> env.reset()</span>
<span id="cb326-7"><a href="#cb326-7" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb326-8"><a href="#cb326-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb326-9"><a href="#cb326-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb326-10"><a href="#cb326-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 1. Act in real environment</span></span>
<span id="cb326-11"><a href="#cb326-11" aria-hidden="true" tabindex="-1"></a>            a <span class="op">=</span> epsilon_greedy(Q, s)</span>
<span id="cb326-12"><a href="#cb326-12" aria-hidden="true" tabindex="-1"></a>            s_next, r, done, _ <span class="op">=</span> env.step(a)</span>
<span id="cb326-13"><a href="#cb326-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb326-14"><a href="#cb326-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 2. Direct RL update</span></span>
<span id="cb326-15"><a href="#cb326-15" aria-hidden="true" tabindex="-1"></a>            Q[s][a] <span class="op">+=</span> alpha <span class="op">*</span> (r <span class="op">+</span> gamma <span class="op">*</span> <span class="bu">max</span>(Q[s_next].values()) <span class="op">-</span> Q[s][a])</span>
<span id="cb326-16"><a href="#cb326-16" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb326-17"><a href="#cb326-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 3. Learn model</span></span>
<span id="cb326-18"><a href="#cb326-18" aria-hidden="true" tabindex="-1"></a>            model[(s, a)] <span class="op">=</span> (r, s_next)</span>
<span id="cb326-19"><a href="#cb326-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb326-20"><a href="#cb326-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 4. Planning: simulate from model</span></span>
<span id="cb326-21"><a href="#cb326-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_planning_steps):</span>
<span id="cb326-22"><a href="#cb326-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Sample previously seen (s, a)</span></span>
<span id="cb326-23"><a href="#cb326-23" aria-hidden="true" tabindex="-1"></a>                s_sim, a_sim <span class="op">=</span> random.choice(<span class="bu">list</span>(model.keys()))</span>
<span id="cb326-24"><a href="#cb326-24" aria-hidden="true" tabindex="-1"></a>                r_sim, s_next_sim <span class="op">=</span> model[(s_sim, a_sim)]</span>
<span id="cb326-25"><a href="#cb326-25" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb326-26"><a href="#cb326-26" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Q-learning update on simulated experience</span></span>
<span id="cb326-27"><a href="#cb326-27" aria-hidden="true" tabindex="-1"></a>                Q[s_sim][a_sim] <span class="op">+=</span> alpha <span class="op">*</span> (</span>
<span id="cb326-28"><a href="#cb326-28" aria-hidden="true" tabindex="-1"></a>                    r_sim <span class="op">+</span> gamma <span class="op">*</span> <span class="bu">max</span>(Q[s_next_sim].values()) <span class="op">-</span> Q[s_sim][a_sim]</span>
<span id="cb326-29"><a href="#cb326-29" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb326-30"><a href="#cb326-30" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb326-31"><a href="#cb326-31" aria-hidden="true" tabindex="-1"></a>            s <span class="op">=</span> s_next</span>
<span id="cb326-32"><a href="#cb326-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb326-33"><a href="#cb326-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q</span></code></pre></div>
            <h3 id="world-models-learning-to-dream">World Models:
            Learning to Dream</h3>
            <p><strong>Modern approach</strong>: Learn a latent dynamics
            model with neural networks.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    World Model Architecture                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Observation o‚Çú ‚Üí [Encoder] ‚Üí Latent state z‚Çú                      ‚îÇ
‚îÇ                                     ‚îÇ                               ‚îÇ
‚îÇ                                     ‚Üì                               ‚îÇ
‚îÇ                      z‚Çú, a‚Çú ‚Üí [Dynamics] ‚Üí ·∫ë‚Çú‚Çä‚ÇÅ                    ‚îÇ
‚îÇ                                     ‚îÇ                               ‚îÇ
‚îÇ                                     ‚Üì                               ‚îÇ
‚îÇ                         ·∫ë‚Çú‚Çä‚ÇÅ ‚Üí [Decoder] ‚Üí √¥‚Çú‚Çä‚ÇÅ                    ‚îÇ
‚îÇ                              ‚Üí [Reward] ‚Üí rÃÇ‚Çú‚Çä‚ÇÅ                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  &quot;Dream&quot; in latent space ‚Üí Plan ‚Üí Execute in real world            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Dreamer</strong> (Hafner et al.):</p>
            <ol type="1">
            <li>Learn world model from experience</li>
            <li>‚ÄúImagine‚Äù trajectories in latent space</li>
            <li>Train policy on imagined trajectories</li>
            <li>Execute in real world</li>
            </ol>
            <p><strong>Why learn in latent space?</strong> Raw
            observations (e.g., images) are high-dimensional and contain
            much irrelevant information. Learning dynamics directly on
            pixels is wasteful ‚Äî predicting every pixel of the next
            frame when most are background. The latent space compresses
            observations into a compact representation that captures
            only the task-relevant information. The encoder learns to
            extract features that are <em>predictable</em> (dynamics
            model needs them) and <em>useful</em> (reward predictor
            needs them). This compression makes the dynamics model more
            accurate (easier to learn), faster (smaller state space),
            and generalizable (abstracts away irrelevant details).
            Dreamer can then ‚Äúimagine‚Äù thousands of trajectories in this
            compact latent space cheaply, training the policy without
            any real environment interaction ‚Äî essentially learning in a
            dream.</p>
            <h3 id="sample-efficiency-comparison">Sample Efficiency
            Comparison</h3>
            <pre><code>Sample Efficiency:

Model-Free (PPO):     [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 10M steps
Model-Based (Dreamer): [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100K steps

Same performance, 100√ó fewer samples!</code></pre>
            <h3 id="when-to-use-which-4">When to Use Which?</h3>
            <table>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Recommendation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Simple dynamics, cheap simulation</td>
            <td>Model-Free</td>
            </tr>
            <tr>
            <td>Complex dynamics, expensive real experience</td>
            <td>Model-Based</td>
            </tr>
            <tr>
            <td>Safety-critical (can‚Äôt explore freely)</td>
            <td>Model-Based</td>
            </tr>
            <tr>
            <td>High-dimensional observations</td>
            <td>Model-Based with latent space</td>
            </tr>
            <tr>
            <td>Real robotics</td>
            <td>Model-Based (sample efficiency crucial)</td>
            </tr>
            <tr>
            <td>Games with fast simulation</td>
            <td>Model-Free often sufficient</td>
            </tr>
            </tbody>
            </table>
            <h3 id="challenges-with-model-based-rl">Challenges with
            Model-Based RL</h3>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 36%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>Challenge</th>
            <th>Description</th>
            <th>Mitigation</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Model error</strong></td>
            <td>Learned model imperfect</td>
            <td>Ensemble models, uncertainty</td>
            </tr>
            <tr>
            <td><strong>Compounding errors</strong></td>
            <td>Errors accumulate over long rollouts</td>
            <td>Short planning horizons</td>
            </tr>
            <tr>
            <td><strong>Exploitation of model</strong></td>
            <td>Policy exploits model mistakes</td>
            <td>Model uncertainty penalties</td>
            </tr>
            <tr>
            <td><strong>Computational cost</strong></td>
            <td>Planning is expensive</td>
            <td>Amortized planning (policy)</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-the-difference-between-model-based-and-model-free-rl">Interview
            Q: ‚ÄúWhat‚Äôs the difference between model-based and model-free
            RL?‚Äù</h3>
            <p><strong>A</strong>: Model-free RL (Q-learning, PPO)
            learns a policy or value function directly from experience
            without modeling environment dynamics. Model-based RL first
            learns a world model <span
            class="math inline">\(\hat{P}(s&#39;|s,a)\)</span>, then
            uses it to plan or generate synthetic experience.</p>
            <p><strong>Model-based pros</strong>: Much more sample
            efficient (10-100√ó), can plan ahead, enables transfer.
            <strong>Cons</strong>: Model errors compound, can exploit
            model mistakes, computationally expensive.</p>
            <p>Modern approaches like Dreamer learn latent world models
            and ‚Äúimagine‚Äù trajectories to train policies. Dyna-Q
            combines both: direct RL + planning with learned model. Use
            model-based when real experience is expensive (robotics),
            model-free when simulation is cheap (games).</p>
            <hr />
            <h2 id="offline-rl-batch-rl">9.12 Offline RL / Batch RL</h2>
            <h3 id="the-problem-learning-without-interaction">The
            Problem: Learning Without Interaction</h3>
            <p><strong>Standard RL</strong>: Agent interacts with
            environment, learns from experience.</p>
            <p><strong>Offline RL</strong>: Agent learns from a
            <strong>fixed dataset</strong> ‚Äî no environment
            interaction!</p>
            <pre><code>Online RL:                          Offline RL:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Agent ‚Üê‚îÄ‚îÄ‚Üí Environment              Fixed Dataset D
  ‚îÇ          ‚îÇ                      (collected by other policies)
  ‚îî‚îÄ Interact ‚îÄ‚îò                           ‚îÇ
        ‚Üì                                  ‚Üì
    Learn œÄ                           Learn œÄ from D only
        ‚Üì                                  ‚îÇ
    More interaction                  No new data allowed!</code></pre>
            <h3 id="why-offline-rl-matters">Why Offline RL Matters</h3>
            <table>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Why Offline RL?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Healthcare</strong></td>
            <td>Can‚Äôt experiment on patients</td>
            </tr>
            <tr>
            <td><strong>Autonomous driving</strong></td>
            <td>Historical driving logs exist</td>
            </tr>
            <tr>
            <td><strong>Robotics</strong></td>
            <td>Real robot interaction expensive</td>
            </tr>
            <tr>
            <td><strong>Recommendation</strong></td>
            <td>Have user click logs</td>
            </tr>
            <tr>
            <td><strong>LLM alignment</strong></td>
            <td>SFT is essentially offline RL!</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-distribution-shift-problem">The Distribution
            Shift Problem</h3>
            <p><strong>The fundamental challenge</strong>: Policy visits
            states not in the dataset!</p>
            <pre><code>Dataset D was collected by behavior policy Œ≤:
States in D: ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
                     ‚Üë
        New policy œÄ goes here ‚Üí ‚óã (out of distribution!)
        
Q(s, a) is wrong for states not in D!</code></pre>
            <p><strong>What goes wrong</strong>:</p>
            <ol type="1">
            <li><strong>Overestimation</strong>: Q-learning uses <span
            class="math inline">\(\max_a Q(s&#39;, a)\)</span></li>
            <li>For unseen <span class="math inline">\((s, a)\)</span>:
            Q might be arbitrarily wrong</li>
            <li>Policy exploits these errors ‚Üí selects bad actions</li>
            <li>Leads to catastrophic failures</li>
            </ol>
            <h3 id="naive-approaches-fail">Naive Approaches Fail</h3>
            <div class="sourceCode" id="cb331"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb331-1"><a href="#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This DOESN&#39;T work for offline RL!</span></span>
<span id="cb331-2"><a href="#cb331-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> naive_offline_q_learning(dataset):</span>
<span id="cb331-3"><a href="#cb331-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> initialize_q_network()</span>
<span id="cb331-4"><a href="#cb331-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb331-5"><a href="#cb331-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s, a, r, s_next <span class="kw">in</span> dataset:</span>
<span id="cb331-6"><a href="#cb331-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard Q-learning</span></span>
<span id="cb331-7"><a href="#cb331-7" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> <span class="bu">max</span>(Q(s_next, a<span class="st">&#39;) for a&#39;</span>)  <span class="co"># Problem here!</span></span>
<span id="cb331-8"><a href="#cb331-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> (Q(s, a) <span class="op">-</span> target)<span class="op">^</span><span class="dv">2</span></span>
<span id="cb331-9"><a href="#cb331-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb331-10"><a href="#cb331-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Q</span></code></pre></div>
            <p><strong>Problem</strong>: <span
            class="math inline">\(\max_a Q(s&#39;, a)\)</span> might
            select an action never seen in data!</p>
            <h3 id="solution-1-conservative-q-learning-cql">Solution 1:
            Conservative Q-Learning (CQL)</h3>
            <p><strong>Key idea</strong>: Penalize Q-values for actions
            not in the dataset.</p>
            <p><span class="math display">\[\mathcal{L}_{CQL} =
            \mathcal{L}_{Q} + \alpha \cdot \mathbb{E}_{s \sim
            D}\left[\log \sum_a \exp(Q(s, a)) - \mathbb{E}_{a \sim
            \hat{\pi}_\beta}[Q(s, a)]\right]\]</span></p>
            <p><strong>Intuition</strong>:</p>
            <ul>
            <li>First term: Push down Q for ALL actions (especially
            OOD)</li>
            <li>Second term: Push up Q for actions IN the dataset</li>
            <li><strong>Net effect</strong>: Q is conservative (lower
            bound) for unseen actions</li>
            </ul>
            <div class="sourceCode" id="cb332"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb332-1"><a href="#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cql_loss(Q, states, actions, rewards, next_states, alpha<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb332-2"><a href="#cb332-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Standard Bellman loss</span></span>
<span id="cb332-3"><a href="#cb332-3" aria-hidden="true" tabindex="-1"></a>    q_values <span class="op">=</span> Q(states, actions)</span>
<span id="cb332-4"><a href="#cb332-4" aria-hidden="true" tabindex="-1"></a>    next_q <span class="op">=</span> Q(next_states, actions_from_policy(next_states))</span>
<span id="cb332-5"><a href="#cb332-5" aria-hidden="true" tabindex="-1"></a>    bellman_loss <span class="op">=</span> mse(q_values, rewards <span class="op">+</span> gamma <span class="op">*</span> next_q)</span>
<span id="cb332-6"><a href="#cb332-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb332-7"><a href="#cb332-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># CQL regularizer</span></span>
<span id="cb332-8"><a href="#cb332-8" aria-hidden="true" tabindex="-1"></a>    logsumexp_q <span class="op">=</span> torch.logsumexp(Q(states, all_actions), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb332-9"><a href="#cb332-9" aria-hidden="true" tabindex="-1"></a>    data_q <span class="op">=</span> Q(states, actions)  <span class="co"># Actions actually in dataset</span></span>
<span id="cb332-10"><a href="#cb332-10" aria-hidden="true" tabindex="-1"></a>    cql_penalty <span class="op">=</span> (logsumexp_q <span class="op">-</span> data_q).mean()</span>
<span id="cb332-11"><a href="#cb332-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb332-12"><a href="#cb332-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bellman_loss <span class="op">+</span> alpha <span class="op">*</span> cql_penalty</span></code></pre></div>
            <h3 id="solution-2-behavior-cloning-constraints">Solution 2:
            Behavior Cloning + Constraints</h3>
            <p><strong>Approach</strong>: Constrain policy to stay close
            to behavior policy.</p>
            <p><span class="math display">\[\pi^* = \arg\max_\pi
            \mathbb{E}_{s,a \sim D}[\hat{Q}(s, a)] \quad \text{s.t.}
            \quad D_{KL}(\pi || \hat{\pi}_\beta) \leq
            \epsilon\]</span></p>
            <p><strong>Algorithms</strong>: BCQ, BEAR, AWR</p>
            <h3 id="solution-3-decision-transformer">Solution 3:
            Decision Transformer</h3>
            <p><strong>Key insight</strong>: Frame offline RL as
            <strong>sequence modeling</strong>!</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Decision Transformer                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Input sequence:                                                   ‚îÇ
‚îÇ  [RÃÇ‚ÇÅ, s‚ÇÅ, a‚ÇÅ, RÃÇ‚ÇÇ, s‚ÇÇ, a‚ÇÇ, RÃÇ‚ÇÉ, s‚ÇÉ, ?]                            ‚îÇ
‚îÇ   ‚Üë                        ‚Üë                                       ‚îÇ
‚îÇ   Returns-to-go            Predict next action                     ‚îÇ
‚îÇ   (desired future return)                                          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  At test time:                                                     ‚îÇ
‚îÇ  - Specify desired return RÃÇ (e.g., &quot;I want 1000 points&quot;)          ‚îÇ
‚îÇ  - Transformer predicts actions to achieve it                      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>No Q-learning, no policy gradient ‚Äî just sequence
            prediction!</strong></p>
            <div class="sourceCode" id="cb334"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb334-1"><a href="#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decision_transformer_forward(returns_to_go, states, actions):</span>
<span id="cb334-2"><a href="#cb334-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb334-3"><a href="#cb334-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Predicts action given returns-to-go, states, actions history.</span></span>
<span id="cb334-4"><a href="#cb334-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb334-5"><a href="#cb334-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embed each modality</span></span>
<span id="cb334-6"><a href="#cb334-6" aria-hidden="true" tabindex="-1"></a>    ret_emb <span class="op">=</span> embed_returns(returns_to_go)</span>
<span id="cb334-7"><a href="#cb334-7" aria-hidden="true" tabindex="-1"></a>    state_emb <span class="op">=</span> embed_states(states)</span>
<span id="cb334-8"><a href="#cb334-8" aria-hidden="true" tabindex="-1"></a>    action_emb <span class="op">=</span> embed_actions(actions)</span>
<span id="cb334-9"><a href="#cb334-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb334-10"><a href="#cb334-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Interleave: [RÃÇ‚ÇÅ, s‚ÇÅ, a‚ÇÅ, RÃÇ‚ÇÇ, s‚ÇÇ, a‚ÇÇ, ...]</span></span>
<span id="cb334-11"><a href="#cb334-11" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> interleave(ret_emb, state_emb, action_emb)</span>
<span id="cb334-12"><a href="#cb334-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb334-13"><a href="#cb334-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Standard transformer</span></span>
<span id="cb334-14"><a href="#cb334-14" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> transformer(tokens)</span>
<span id="cb334-15"><a href="#cb334-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb334-16"><a href="#cb334-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict next action (at position after last state)</span></span>
<span id="cb334-17"><a href="#cb334-17" aria-hidden="true" tabindex="-1"></a>    action_pred <span class="op">=</span> action_head(output[action_positions])</span>
<span id="cb334-18"><a href="#cb334-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb334-19"><a href="#cb334-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> action_pred</span></code></pre></div>
            <h3 id="connection-to-llms">Connection to LLMs</h3>
            <p><strong>SFT is offline RL!</strong></p>
            <table>
            <thead>
            <tr>
            <th>LLM Training</th>
            <th>Offline RL Analog</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Prompts</td>
            <td>States</td>
            </tr>
            <tr>
            <td>Responses</td>
            <td>Actions</td>
            </tr>
            <tr>
            <td>High-quality responses</td>
            <td>Expert demonstrations</td>
            </tr>
            <tr>
            <td>Cross-entropy loss</td>
            <td>Behavior cloning</td>
            </tr>
            </tbody>
            </table>
            <p><strong>The insight</strong>: When we train LLMs on
            curated data, we‚Äôre doing offline RL ‚Äî learning a policy
            from a fixed dataset without environment interaction!</p>
            <h3 id="comparison-of-offline-rl-methods">Comparison of
            Offline RL Methods</h3>
            <table>
            <colgroup>
            <col style="width: 26%" />
            <col style="width: 33%" />
            <col style="width: 20%" />
            <col style="width: 20%" />
            </colgroup>
            <thead>
            <tr>
            <th>Method</th>
            <th>Approach</th>
            <th>Pros</th>
            <th>Cons</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>CQL</strong></td>
            <td>Conservative Q-values</td>
            <td>Provable guarantees</td>
            <td>Overly conservative</td>
            </tr>
            <tr>
            <td><strong>BCQ/BEAR</strong></td>
            <td>Constrained policy</td>
            <td>Stable</td>
            <td>Needs explicit density estimation</td>
            </tr>
            <tr>
            <td><strong>IQL</strong></td>
            <td>Implicit constraints</td>
            <td>Simple, stable</td>
            <td>May be suboptimal</td>
            </tr>
            <tr>
            <td><strong>Decision Transformer</strong></td>
            <td>Sequence modeling</td>
            <td>Elegant, scalable</td>
            <td>Needs good data</td>
            </tr>
            <tr>
            <td><strong>Behavior Cloning</strong></td>
            <td>Imitation only</td>
            <td>Simple</td>
            <td>No improvement over data</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-the-main-challenge-in-offline-rl">Interview
            Q: ‚ÄúWhat‚Äôs the main challenge in offline RL?‚Äù</h3>
            <p><strong>A</strong>: The main challenge is
            <strong>distribution shift</strong>: the policy being
            learned might want to take actions or visit states that
            don‚Äôt exist in the dataset. When this happens, the
            Q-function‚Äôs estimates for these out-of-distribution (OOD)
            actions are unreliable, and the policy can exploit these
            errors.</p>
            <p>Standard Q-learning‚Äôs <span class="math inline">\(\max_a
            Q(s&#39;, a)\)</span> will select actions that might have
            erroneously high Q-values simply because they were never
            seen. Solutions include: (1) <strong>CQL</strong>: penalize
            Q-values for OOD actions to be conservative, (2)
            <strong>constrained policies</strong>: keep policy close to
            behavior policy, (3) <strong>Decision Transformer</strong>:
            reframe as sequence modeling, avoiding Q-learning
            entirely.</p>
            <h3
            id="interview-q-how-does-decision-transformer-relate-to-standard-rl">Interview
            Q: ‚ÄúHow does Decision Transformer relate to standard
            RL?‚Äù</h3>
            <p><strong>A</strong>: Decision Transformer reframes RL as
            <strong>sequence modeling</strong> rather than value
            estimation or policy optimization. Instead of learning
            Q(s,a) or <span class="math inline">\(\pi(a|s)\)</span>, it
            learns to predict actions conditioned on <strong>desired
            returns-to-go</strong>.</p>
            <p>At test time, you specify the return you want (e.g.,
            ‚Äúachieve 1000 points‚Äù), and the model predicts actions to
            achieve it. This avoids the deadly triad (function
            approximation + bootstrapping + off-policy) that makes
            offline RL hard. It‚Äôs essentially a return-conditioned
            behavior cloning, leveraging the power of transformer
            architectures for sequence modeling.</p>
            <hr />
            <h2 id="multi-agent-rl-marl">9.13 Multi-Agent RL (MARL)</h2>
            <h3 id="the-setting">The Setting</h3>
            <p>Multiple agents interact in a shared environment:</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Multi-Agent Environment                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ     Agent 1                   Environment                Agent 2    ‚îÇ
‚îÇ        ‚îÇ                          ‚îÇ                         ‚îÇ       ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ a‚ÇÅ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ a‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                                   ‚îÇ                                 ‚îÇ
‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ s‚ÇÅ, r‚ÇÅ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí s‚ÇÇ, r‚ÇÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ        ‚îÇ                          ‚îÇ                         ‚îÇ       ‚îÇ
‚îÇ     Agent 1                                               Agent 2   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ     Both agents&#39; actions affect the environment!                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="types-of-multi-agent-settings">Types of Multi-Agent
            Settings</h3>
            <table>
            <thead>
            <tr>
            <th>Type</th>
            <th>Description</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Cooperative</strong></td>
            <td>All agents share same reward</td>
            <td>Team robotics</td>
            </tr>
            <tr>
            <td><strong>Competitive</strong></td>
            <td>Zero-sum game</td>
            <td>Chess, Go</td>
            </tr>
            <tr>
            <td><strong>Mixed</strong></td>
            <td>Some cooperation, some competition</td>
            <td>Traffic, economics</td>
            </tr>
            </tbody>
            </table>
            <h3 id="the-non-stationarity-problem">The Non-Stationarity
            Problem</h3>
            <p><strong>Single-agent RL</strong>: Environment is
            stationary (Markov)</p>
            <p><strong>Multi-agent RL</strong>: Other agents are
            learning too!</p>
            <pre><code>From Agent 1&#39;s perspective:

Transition: P(s&#39;|s, a‚ÇÅ) 
          = ‚àë_{a‚ÇÇ} P(s&#39;|s, a‚ÇÅ, a‚ÇÇ) ¬∑ œÄ‚ÇÇ(a‚ÇÇ|s)
                                    ‚Üë
                            Agent 2&#39;s policy is CHANGING!

Environment appears non-stationary!</code></pre>
            <h3 id="solution-1-independent-learning">Solution 1:
            Independent Learning</h3>
            <p><strong>Idea</strong>: Ignore other agents, treat as part
            of environment.</p>
            <div class="sourceCode" id="cb337"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> independent_q_learning(agents, env):</span>
<span id="cb337-2"><a href="#cb337-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Each agent learns independently, ignoring others.&quot;&quot;&quot;</span></span>
<span id="cb337-3"><a href="#cb337-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> [defaultdict(<span class="bu">float</span>) <span class="cf">for</span> _ <span class="kw">in</span> agents]</span>
<span id="cb337-4"><a href="#cb337-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb337-5"><a href="#cb337-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb337-6"><a href="#cb337-6" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> env.reset()</span>
<span id="cb337-7"><a href="#cb337-7" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb337-8"><a href="#cb337-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb337-9"><a href="#cb337-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb337-10"><a href="#cb337-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Each agent selects action independently</span></span>
<span id="cb337-11"><a href="#cb337-11" aria-hidden="true" tabindex="-1"></a>            actions <span class="op">=</span> [epsilon_greedy(Q[i], state) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(agents))]</span>
<span id="cb337-12"><a href="#cb337-12" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb337-13"><a href="#cb337-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Joint action executed</span></span>
<span id="cb337-14"><a href="#cb337-14" aria-hidden="true" tabindex="-1"></a>            next_state, rewards, done <span class="op">=</span> env.step(actions)</span>
<span id="cb337-15"><a href="#cb337-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb337-16"><a href="#cb337-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Each agent updates its own Q</span></span>
<span id="cb337-17"><a href="#cb337-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(agents)):</span>
<span id="cb337-18"><a href="#cb337-18" aria-hidden="true" tabindex="-1"></a>                Q[i][state][actions[i]] <span class="op">+=</span> alpha <span class="op">*</span> (</span>
<span id="cb337-19"><a href="#cb337-19" aria-hidden="true" tabindex="-1"></a>                    rewards[i] <span class="op">+</span> gamma <span class="op">*</span> <span class="bu">max</span>(Q[i][next_state].values()) </span>
<span id="cb337-20"><a href="#cb337-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb337-21"><a href="#cb337-21" aria-hidden="true" tabindex="-1"></a>                    <span class="op">-</span> Q[i][state][actions[i]]</span>
<span id="cb337-22"><a href="#cb337-22" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb337-23"><a href="#cb337-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb337-24"><a href="#cb337-24" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> next_state</span></code></pre></div>
            <p><strong>Pros</strong>: Simple, scalable
            <strong>Cons</strong>: No convergence guarantees, can
            oscillate</p>
            <h3
            id="solution-2-centralized-training-decentralized-execution-ctde">Solution
            2: Centralized Training, Decentralized Execution (CTDE)</h3>
            <p><strong>Key paradigm</strong> for cooperative MARL:</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Centralized Training, Decentralized Execution          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  TRAINING (has access to everything):                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                              ‚îÇ
‚îÇ  Central critic sees: all states, all actions                      ‚îÇ
‚îÇ  Q(s, a‚ÇÅ, a‚ÇÇ, ..., a‚Çô) ‚Üê joint action-value                       ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  EXECUTION (decentralized):                                        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                        ‚îÇ
‚îÇ  Each agent only uses its own observation                          ‚îÇ
‚îÇ  œÄ_i(a_i | o_i) ‚Üê local policy                                     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="maddpg-multi-agent-ddpg">MADDPG: Multi-Agent
            DDPG</h3>
            <p><strong>Algorithm</strong>: Each agent has actor (local)
            and critic (centralized)</p>
            <div class="sourceCode" id="cb339"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> maddpg_update(agents, transitions):</span>
<span id="cb339-2"><a href="#cb339-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb339-3"><a href="#cb339-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Multi-Agent Deep Deterministic Policy Gradient.</span></span>
<span id="cb339-4"><a href="#cb339-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb339-5"><a href="#cb339-5" aria-hidden="true" tabindex="-1"></a>    states, actions, rewards, next_states <span class="op">=</span> transitions</span>
<span id="cb339-6"><a href="#cb339-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb339-7"><a href="#cb339-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents):</span>
<span id="cb339-8"><a href="#cb339-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Centralized critic update</span></span>
<span id="cb339-9"><a href="#cb339-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Critic sees ALL agents&#39; states and actions</span></span>
<span id="cb339-10"><a href="#cb339-10" aria-hidden="true" tabindex="-1"></a>        all_actions <span class="op">=</span> [agent.actor(states[j]) <span class="cf">for</span> j, agent <span class="kw">in</span> <span class="bu">enumerate</span>(agents)]</span>
<span id="cb339-11"><a href="#cb339-11" aria-hidden="true" tabindex="-1"></a>        Q_target <span class="op">=</span> rewards[i] <span class="op">+</span> gamma <span class="op">*</span> agent.critic(next_states, next_all_actions)</span>
<span id="cb339-12"><a href="#cb339-12" aria-hidden="true" tabindex="-1"></a>        critic_loss <span class="op">=</span> mse(agent.critic(states, all_actions), Q_target)</span>
<span id="cb339-13"><a href="#cb339-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb339-14"><a href="#cb339-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Decentralized actor update</span></span>
<span id="cb339-15"><a href="#cb339-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Actor only uses agent i&#39;s observation</span></span>
<span id="cb339-16"><a href="#cb339-16" aria-hidden="true" tabindex="-1"></a>        actor_loss <span class="op">=</span> <span class="op">-</span>agent.critic(states, all_actions).mean()</span>
<span id="cb339-17"><a href="#cb339-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb339-18"><a href="#cb339-18" aria-hidden="true" tabindex="-1"></a>        update(agent.critic, critic_loss)</span>
<span id="cb339-19"><a href="#cb339-19" aria-hidden="true" tabindex="-1"></a>        update(agent.actor, actor_loss)</span></code></pre></div>
            <h3 id="self-play-learning-by-playing-yourself">Self-Play:
            Learning by Playing Yourself</h3>
            <p><strong>Key technique</strong> for competitive games:</p>
            <pre><code>Self-Play Evolution:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Generation 0: Random policy œÄ‚ÇÄ

Generation 1: œÄ‚ÇÅ = train(œÄ‚ÇÄ vs œÄ‚ÇÄ) ‚Üí beats œÄ‚ÇÄ

Generation 2: œÄ‚ÇÇ = train(œÄ‚ÇÅ vs œÄ‚ÇÅ) ‚Üí beats œÄ‚ÇÅ

...

Generation N: œÄ‚Çô = superhuman level!</code></pre>
            <p><strong>Used in</strong>: AlphaGo, AlphaZero, OpenAI Five
            (Dota 2)</p>
            <h3 id="nash-equilibrium">Nash Equilibrium</h3>
            <p><strong>Definition</strong>: Strategy profile where no
            agent can improve by unilaterally changing strategy.</p>
            <p><span class="math display">\[\pi_i^* \in \arg\max_{\pi_i}
            J_i(\pi_i, \pi_{-i}^*)\]</span></p>
            <p>for all agents <span
            class="math inline">\(i\)</span>.</p>
            <p><strong>In competitive games</strong>: Minimax
            equilibrium</p>
            <p><span class="math display">\[V^* = \max_{\pi_1}
            \min_{\pi_2} V(\pi_1, \pi_2)\]</span></p>
            <h3 id="emergent-communication">Emergent Communication</h3>
            <p><strong>Fascinating phenomenon</strong>: Agents can
            develop their own communication!</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Emergent Communication                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Agent 1 ‚îÄ‚îÄ‚Üí [Message m‚Çú] ‚îÄ‚îÄ‚Üí Agent 2                              ‚îÇ
‚îÇ     ‚îÇ                            ‚îÇ                                  ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ Both see partial ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îÇ           information                                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Agents learn to communicate useful information!                   ‚îÇ
‚îÇ  - Learned &quot;language&quot; emerges from task pressure                   ‚îÇ
‚îÇ  - Not human-interpretable initially                               ‚îÇ
‚îÇ  - Can be encouraged to be discrete/compositional                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="challenges-in-marl">Challenges in MARL</h3>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 36%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>Challenge</th>
            <th>Description</th>
            <th>Approaches</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Non-stationarity</strong></td>
            <td>Other agents changing</td>
            <td>CTDE, opponent modeling</td>
            </tr>
            <tr>
            <td><strong>Credit assignment</strong></td>
            <td>Who caused the reward?</td>
            <td>Difference rewards, COMA</td>
            </tr>
            <tr>
            <td><strong>Scalability</strong></td>
            <td>Many agents</td>
            <td>Mean-field, graph networks</td>
            </tr>
            <tr>
            <td><strong>Exploration</strong></td>
            <td>Coordinated exploration</td>
            <td>Population-based training</td>
            </tr>
            <tr>
            <td><strong>Equilibrium selection</strong></td>
            <td>Multiple equilibria</td>
            <td>Focal points, communication</td>
            </tr>
            </tbody>
            </table>
            <h3 id="marl-in-practice">MARL in Practice</h3>
            <table>
            <thead>
            <tr>
            <th>Application</th>
            <th>Agents</th>
            <th>Type</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>AlphaGo/AlphaZero</strong></td>
            <td>2 (self-play)</td>
            <td>Competitive</td>
            </tr>
            <tr>
            <td><strong>OpenAI Five</strong></td>
            <td>5 vs 5</td>
            <td>Cooperative + Competitive</td>
            </tr>
            <tr>
            <td><strong>Traffic control</strong></td>
            <td>Many vehicles</td>
            <td>Mixed</td>
            </tr>
            <tr>
            <td><strong>Robotics swarms</strong></td>
            <td>Many robots</td>
            <td>Cooperative</td>
            </tr>
            <tr>
            <td><strong>Economic markets</strong></td>
            <td>Buyers/sellers</td>
            <td>Competitive</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-what-makes-multi-agent-rl-harder-than-single-agent">Interview
            Q: ‚ÄúWhat makes multi-agent RL harder than
            single-agent?‚Äù</h3>
            <p><strong>A</strong>: Three main challenges:</p>
            <ol type="1">
            <li><strong>Non-stationarity</strong>: From each agent‚Äôs
            view, the environment is non-stationary because other agents
            are learning simultaneously. The Markov assumption breaks
            down ‚Äî optimal behavior today isn‚Äôt optimal tomorrow as
            others adapt.</li>
            <li><strong>Credit assignment</strong>: With shared rewards,
            hard to determine which agent‚Äôs actions caused
            success/failure.</li>
            <li><strong>Scalability</strong>: Joint action space grows
            exponentially with agents. <span
            class="math inline">\(n\)</span> agents with <span
            class="math inline">\(k\)</span> actions each: <span
            class="math inline">\(k^n\)</span> joint actions.</li>
            </ol>
            <p>Solutions include <strong>CTDE</strong> (train with full
            info, execute with local info), <strong>self-play</strong>
            for competitive games, and <strong>communication
            protocols</strong> for coordination.</p>
            <hr />
            <h2 id="monte-carlo-tree-search-mcts">9.14 Monte Carlo Tree
            Search (MCTS)</h2>
            <h3 id="what-is-mcts">What is MCTS?</h3>
            <p><strong>MCTS</strong> is a search algorithm that builds a
            search tree incrementally using random simulations to
            evaluate positions.</p>
            <p><strong>Famous for</strong>: AlphaGo, AlphaZero ‚Äî beating
            humans at Go!</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Why MCTS for Games?                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Game tree is HUGE:                                                ‚îÇ
‚îÇ  - Chess: ~10^120 possible games                                   ‚îÇ
‚îÇ  - Go: ~10^360 possible games                                      ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Can&#39;t search exhaustively!                                        ‚îÇ
‚îÇ  MCTS: Smart sampling + statistics to focus search                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="the-four-steps-of-mcts">The Four Steps of MCTS</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        MCTS Algorithm                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  REPEAT for N iterations:                                          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  1. SELECTION         2. EXPANSION                                 ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ     ‚îÇ  ‚óè  ‚îÇ ‚Üê‚îÄ root      ‚îÇ  ‚óè  ‚îÇ                                   ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ        ‚îÇ UCB              ‚îÇ                                        ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ     ‚îÇ  ‚óè  ‚îÇ            ‚îÇ  ‚óè  ‚îÇ                                     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê                                  ‚îÇ
‚îÇ        ‚îÇ UCB              ‚îÇ  ‚îÇ  ‚îÇ ‚Üê NEW NODE                       ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ     ‚îÇ  ‚óã  ‚îÇ ‚Üê leaf     ‚îÇ     ‚îÇ  ‚óã  ‚îÇ                               ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  3. SIMULATION         4. BACKPROPAGATION                          ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ     ‚îÇ  ‚óã  ‚îÇ ‚Üê start      ‚îÇ  ‚óè  ‚îÇ ‚Üê update N, W                     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ        ‚îÇ random              ‚îÇ propagate                           ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ     ‚îÇ  ?  ‚îÇ            ‚îÇ  ‚óè  ‚îÇ ‚Üê update N, W                       ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ        ‚îÇ random              ‚îÇ  ‚îÇ                                  ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê                               ‚îÇ
‚îÇ     ‚îÇ WIN ‚îÇ            ‚îÇ     ‚îÇ  ‚óè  ‚îÇ ‚Üê update N, W                 ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="step-1-selection-ucb1">Step 1: Selection (UCB1)</h3>
            <p><strong>Navigate tree using Upper Confidence
            Bound</strong>:</p>
            <p><span class="math display">\[UCB1(s, a) = \frac{W(s,
            a)}{N(s, a)} + c \sqrt{\frac{\ln N(s)}{N(s,
            a)}}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(W(s, a)\)</span> = total
            wins after taking action <span
            class="math inline">\(a\)</span> from state <span
            class="math inline">\(s\)</span></li>
            <li><span class="math inline">\(N(s, a)\)</span> = number of
            times action <span class="math inline">\(a\)</span> taken
            from <span class="math inline">\(s\)</span></li>
            <li><span class="math inline">\(N(s)\)</span> = number of
            times state <span class="math inline">\(s\)</span>
            visited</li>
            <li><span class="math inline">\(c\)</span> = exploration
            constant (typically <span
            class="math inline">\(\sqrt{2}\)</span>)</li>
            </ul>
            <p><strong>First term</strong>: Exploitation (prefer high
            win rate) <strong>Second term</strong>: Exploration (prefer
            less-tried actions)</p>
            <h3 id="step-2-expansion">Step 2: Expansion</h3>
            <p>When reaching a leaf node, <strong>expand</strong> by
            adding one or more children.</p>
            <div class="sourceCode" id="cb344"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb344-1"><a href="#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expand(node):</span>
<span id="cb344-2"><a href="#cb344-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Add child nodes for untried actions.&quot;&quot;&quot;</span></span>
<span id="cb344-3"><a href="#cb344-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> action <span class="kw">in</span> get_legal_actions(node.state):</span>
<span id="cb344-4"><a href="#cb344-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action <span class="kw">not</span> <span class="kw">in</span> node.children:</span>
<span id="cb344-5"><a href="#cb344-5" aria-hidden="true" tabindex="-1"></a>            child_state <span class="op">=</span> apply_action(node.state, action)</span>
<span id="cb344-6"><a href="#cb344-6" aria-hidden="true" tabindex="-1"></a>            child_node <span class="op">=</span> Node(state<span class="op">=</span>child_state, parent<span class="op">=</span>node)</span>
<span id="cb344-7"><a href="#cb344-7" aria-hidden="true" tabindex="-1"></a>            node.children[action] <span class="op">=</span> child_node</span>
<span id="cb344-8"><a href="#cb344-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> child_node  <span class="co"># Return first new child</span></span></code></pre></div>
            <h3 id="step-3-simulation-rollout">Step 3: Simulation
            (Rollout)</h3>
            <p><strong>Play randomly</strong> until game ends to
            estimate value.</p>
            <div class="sourceCode" id="cb345"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb345-1"><a href="#cb345-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate(state):</span>
<span id="cb345-2"><a href="#cb345-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Random playout to terminal state.&quot;&quot;&quot;</span></span>
<span id="cb345-3"><a href="#cb345-3" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> state</span>
<span id="cb345-4"><a href="#cb345-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb345-5"><a href="#cb345-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="kw">not</span> is_terminal(current_state):</span>
<span id="cb345-6"><a href="#cb345-6" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> random.choice(get_legal_actions(current_state))</span>
<span id="cb345-7"><a href="#cb345-7" aria-hidden="true" tabindex="-1"></a>        current_state <span class="op">=</span> apply_action(current_state, action)</span>
<span id="cb345-8"><a href="#cb345-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb345-9"><a href="#cb345-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> get_outcome(current_state)  <span class="co"># +1 win, 0 draw, -1 loss</span></span></code></pre></div>
            <h3 id="step-4-backpropagation">Step 4: Backpropagation</h3>
            <p><strong>Update statistics</strong> for all nodes on the
            path:</p>
            <div class="sourceCode" id="cb346"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb346-1"><a href="#cb346-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backpropagate(node, result):</span>
<span id="cb346-2"><a href="#cb346-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Update visit counts and win counts up the tree.&quot;&quot;&quot;</span></span>
<span id="cb346-3"><a href="#cb346-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> node <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb346-4"><a href="#cb346-4" aria-hidden="true" tabindex="-1"></a>        node.visits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb346-5"><a href="#cb346-5" aria-hidden="true" tabindex="-1"></a>        node.wins <span class="op">+=</span> result</span>
<span id="cb346-6"><a href="#cb346-6" aria-hidden="true" tabindex="-1"></a>        node <span class="op">=</span> node.parent</span>
<span id="cb346-7"><a href="#cb346-7" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> <span class="op">-</span>result  <span class="co"># Flip for opponent&#39;s perspective</span></span></code></pre></div>
            <h3 id="full-mcts-algorithm">Full MCTS Algorithm</h3>
            <div class="sourceCode" id="cb347"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb347-1"><a href="#cb347-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mcts(root_state, num_iterations<span class="op">=</span><span class="dv">1000</span>, exploration_constant<span class="op">=</span><span class="fl">1.41</span>):</span>
<span id="cb347-2"><a href="#cb347-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb347-3"><a href="#cb347-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Monte Carlo Tree Search.</span></span>
<span id="cb347-4"><a href="#cb347-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb347-5"><a href="#cb347-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb347-6"><a href="#cb347-6" aria-hidden="true" tabindex="-1"></a><span class="co">        root_state: Current game state</span></span>
<span id="cb347-7"><a href="#cb347-7" aria-hidden="true" tabindex="-1"></a><span class="co">        num_iterations: Number of MCTS iterations</span></span>
<span id="cb347-8"><a href="#cb347-8" aria-hidden="true" tabindex="-1"></a><span class="co">        exploration_constant: UCB exploration parameter (sqrt(2) typical)</span></span>
<span id="cb347-9"><a href="#cb347-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb347-10"><a href="#cb347-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb347-11"><a href="#cb347-11" aria-hidden="true" tabindex="-1"></a><span class="co">        Best action from root</span></span>
<span id="cb347-12"><a href="#cb347-12" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb347-13"><a href="#cb347-13" aria-hidden="true" tabindex="-1"></a>    root <span class="op">=</span> Node(state<span class="op">=</span>root_state)</span>
<span id="cb347-14"><a href="#cb347-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb347-15"><a href="#cb347-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb347-16"><a href="#cb347-16" aria-hidden="true" tabindex="-1"></a>        node <span class="op">=</span> root</span>
<span id="cb347-17"><a href="#cb347-17" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> root_state.copy()</span>
<span id="cb347-18"><a href="#cb347-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb347-19"><a href="#cb347-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Selection: Walk down tree using UCB</span></span>
<span id="cb347-20"><a href="#cb347-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> node.is_fully_expanded() <span class="kw">and</span> node.children:</span>
<span id="cb347-21"><a href="#cb347-21" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> select_child_ucb(node, exploration_constant)</span>
<span id="cb347-22"><a href="#cb347-22" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> apply_action(state, node.action)</span>
<span id="cb347-23"><a href="#cb347-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb347-24"><a href="#cb347-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Expansion: Add a new child</span></span>
<span id="cb347-25"><a href="#cb347-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> is_terminal(state):</span>
<span id="cb347-26"><a href="#cb347-26" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> get_untried_action(node, state)</span>
<span id="cb347-27"><a href="#cb347-27" aria-hidden="true" tabindex="-1"></a>            state <span class="op">=</span> apply_action(state, action)</span>
<span id="cb347-28"><a href="#cb347-28" aria-hidden="true" tabindex="-1"></a>            node <span class="op">=</span> node.add_child(action, state)</span>
<span id="cb347-29"><a href="#cb347-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb347-30"><a href="#cb347-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Simulation: Random playout</span></span>
<span id="cb347-31"><a href="#cb347-31" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> simulate(state)</span>
<span id="cb347-32"><a href="#cb347-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb347-33"><a href="#cb347-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Backpropagation: Update statistics</span></span>
<span id="cb347-34"><a href="#cb347-34" aria-hidden="true" tabindex="-1"></a>        backpropagate(node, result)</span>
<span id="cb347-35"><a href="#cb347-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb347-36"><a href="#cb347-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Return most visited child (most robust choice)</span></span>
<span id="cb347-37"><a href="#cb347-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(root.children, key<span class="op">=</span><span class="kw">lambda</span> c: c.visits).action</span></code></pre></div>
            <h3 id="mcts-neural-networks-alphagoalphazero">MCTS + Neural
            Networks: AlphaGo/AlphaZero</h3>
            <p><strong>Key innovation</strong>: Replace random rollouts
            with neural network evaluation!</p>
            <pre><code>Traditional MCTS:              AlphaZero MCTS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Simulation: Random playout     Simulation: Neural network
            until terminal                  instant evaluation
            (slow, noisy)                   (fast, accurate)

Selection: UCB1                Selection: PUCT
           W/N + c‚àö(ln(N)/n)              Q + c¬∑P¬∑‚àöN/(1+n)
                                              ‚Üë
                                          Prior from
                                          policy network</code></pre>
            <p><strong>AlphaZero PUCT formula</strong>:</p>
            <p><span class="math display">\[UCB(s, a) = Q(s, a) + c
            \cdot P(s, a) \cdot \frac{\sqrt{N(s)}}{1 + N(s,
            a)}\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(Q(s, a)\)</span> = mean
            action value from NN</li>
            <li><span class="math inline">\(P(s, a)\)</span> = prior
            probability from policy network</li>
            <li><span class="math inline">\(c\)</span> = exploration
            constant</li>
            </ul>
            <h3 id="alphazero-training-loop">AlphaZero Training
            Loop</h3>
            <pre><code>
1. Self-play with MCTS
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Use current neural network + MCTS to play games
   Store (state, MCTS_policy, outcome) tuples

2. Train neural network
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Policy head: Match MCTS visit distribution
   Value head: Predict game outcome
   
   Loss = (z - v)¬≤ - œÄ¬∑log(p) + Œª||Œ∏||¬≤
          ‚Üë          ‚Üë
        value     policy
        loss       loss

3. Repeat
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   New network ‚Üí better MCTS ‚Üí better training data ‚Üí ...</code></pre>
            <h3 id="why-mcts-nn-is-so-powerful">Why MCTS + NN is So
            Powerful</h3>
            <table>
            <thead>
            <tr>
            <th>Component</th>
            <th>Contribution</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MCTS</strong></td>
            <td>Look-ahead search, explores variations</td>
            </tr>
            <tr>
            <td><strong>Policy network</strong></td>
            <td>Guides search to promising moves</td>
            </tr>
            <tr>
            <td><strong>Value network</strong></td>
            <td>Fast position evaluation without rollout</td>
            </tr>
            <tr>
            <td><strong>Self-play</strong></td>
            <td>Infinite training data, curriculum</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-how-does-mcts-work-and-why-was-it-crucial-for-alphago">Interview
            Q: ‚ÄúHow does MCTS work and why was it crucial for
            AlphaGo?‚Äù</h3>
            <p><strong>A</strong>: MCTS builds a search tree
            incrementally using four steps: (1)
            <strong>Selection</strong> ‚Äî use UCB to balance exploitation
            and exploration down the tree, (2)
            <strong>Expansion</strong> ‚Äî add new nodes for unexplored
            actions, (3) <strong>Simulation</strong> ‚Äî random playout to
            terminal state, (4) <strong>Backpropagation</strong> ‚Äî
            update win/visit statistics up the tree.</p>
            <p>For AlphaGo/AlphaZero, MCTS was combined with neural
            networks: the <strong>policy network</strong> provides
            priors to guide search (replacing uniform exploration), and
            the <strong>value network</strong> evaluates positions
            (replacing random rollouts). This combination achieves
            superhuman play: MCTS provides look-ahead reasoning while
            neural networks provide pattern recognition and fast
            evaluation. The system improves through self-play ‚Äî MCTS
            generates training data, networks improve, which improves
            MCTS, creating a virtuous cycle.</p>
            <hr />
            <h2 id="distributional-rl">9.15 Distributional RL</h2>
            <h3 id="the-idea-model-the-full-distribution">The Idea:
            Model the Full Distribution</h3>
            <p><strong>Standard RL</strong>: Learn expected value <span
            class="math inline">\(Q(s, a) = \mathbb{E}[G]\)</span></p>
            <p><strong>Distributional RL</strong>: Learn the full
            distribution <span class="math inline">\(Z(s, a)\)</span>
            where <span class="math inline">\(Q(s, a) = \mathbb{E}[Z(s,
            a)]\)</span></p>
            <pre><code>Standard Q-Learning:           Distributional RL:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Q(s, a) = 7.5                 Z(s, a) = distribution
    ‚Üë                               /\
    ‚îÇ                              /  \
    ‚îÇ                             /    \
    ‚óè                            /      \
    ‚îÇ                           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                                5  6  7  8  9  10
                                    
Single number (mean)            Full distribution of returns</code></pre>
            <h3 id="why-distribution-matters">Why Distribution
            Matters</h3>
            <p><strong>Two scenarios with same expected
            value</strong>:</p>
            <pre><code>Scenario A:                    Scenario B:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Return = 50 always            Return = 100 with P=0.5
                                      = 0 with P=0.5

E[G] = 50                     E[G] = 50

Same Q-value, but very different!
Distributional RL captures this difference.</code></pre>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li>Better representation ‚Üí better features</li>
            <li>Risk-sensitive decisions possible</li>
            <li>More stable learning</li>
            <li>Auxiliary signal (distributional loss)</li>
            </ul>
            <h3 id="c51-algorithm">C51 Algorithm</h3>
            <p><strong>Idea</strong>: Represent distribution with 51
            atoms (fixed support)</p>
            <p><strong>Why is projection needed?</strong> In standard
            Q-learning, the Bellman update <span
            class="math inline">\(Q(s,a) \leftarrow r + \gamma
            Q(s&#39;,a&#39;)\)</span> shifts and scales a single number.
            In distributional RL, we apply this to the entire
            distribution: <span class="math inline">\(Z(s,a) \leftarrow
            r + \gamma Z(s&#39;,a&#39;)\)</span>. The problem is that
            adding reward <span class="math inline">\(r\)</span> and
            scaling by <span class="math inline">\(\gamma\)</span>
            <em>shifts</em> the distribution ‚Äî atoms that were at
            positions <span class="math inline">\([z_1, z_2,
            \ldots]\)</span> are now at <span class="math inline">\([r +
            \gamma z_1, r + \gamma z_2, \ldots]\)</span>. These new
            positions don‚Äôt align with our fixed support grid! So we
            need to <strong>project</strong> back: distribute each
            shifted atom‚Äôs probability mass to its neighboring atoms on
            the original grid, proportionally to how close it is to
            each. This is analogous to how you‚Äôd interpolate between
            grid points in image processing ‚Äî probability mass gets
            split between the two nearest bins.</p>
            <div class="sourceCode" id="cb352"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> C51:</span>
<span id="cb352-2"><a href="#cb352-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_atoms<span class="op">=</span><span class="dv">51</span>, v_min<span class="op">=-</span><span class="dv">10</span>, v_max<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb352-3"><a href="#cb352-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_atoms <span class="op">=</span> num_atoms</span>
<span id="cb352-4"><a href="#cb352-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.support <span class="op">=</span> torch.linspace(v_min, v_max, num_atoms)</span>
<span id="cb352-5"><a href="#cb352-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta_z <span class="op">=</span> (v_max <span class="op">-</span> v_min) <span class="op">/</span> (num_atoms <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb352-6"><a href="#cb352-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb352-7"><a href="#cb352-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> project_distribution(<span class="va">self</span>, next_dist, rewards, dones, gamma):</span>
<span id="cb352-8"><a href="#cb352-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb352-9"><a href="#cb352-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Project Bellman update onto fixed support.</span></span>
<span id="cb352-10"><a href="#cb352-10" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb352-11"><a href="#cb352-11" aria-hidden="true" tabindex="-1"></a><span class="co">        T_z = r + Œ≥z  (shift and scale the distribution)</span></span>
<span id="cb352-12"><a href="#cb352-12" aria-hidden="true" tabindex="-1"></a><span class="co">        Then project back onto our atoms.</span></span>
<span id="cb352-13"><a href="#cb352-13" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb352-14"><a href="#cb352-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute projected support</span></span>
<span id="cb352-15"><a href="#cb352-15" aria-hidden="true" tabindex="-1"></a>        Tz <span class="op">=</span> rewards.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> gamma <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> dones.unsqueeze(<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> <span class="va">self</span>.support</span>
<span id="cb352-16"><a href="#cb352-16" aria-hidden="true" tabindex="-1"></a>        Tz <span class="op">=</span> Tz.clamp(<span class="va">self</span>.v_min, <span class="va">self</span>.v_max)</span>
<span id="cb352-17"><a href="#cb352-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb352-18"><a href="#cb352-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute projection indices</span></span>
<span id="cb352-19"><a href="#cb352-19" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> (Tz <span class="op">-</span> <span class="va">self</span>.v_min) <span class="op">/</span> <span class="va">self</span>.delta_z</span>
<span id="cb352-20"><a href="#cb352-20" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> b.floor().<span class="bu">long</span>()</span>
<span id="cb352-21"><a href="#cb352-21" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> b.ceil().<span class="bu">long</span>()</span>
<span id="cb352-22"><a href="#cb352-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb352-23"><a href="#cb352-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Distribute probability to neighboring atoms</span></span>
<span id="cb352-24"><a href="#cb352-24" aria-hidden="true" tabindex="-1"></a>        projected_dist <span class="op">=</span> torch.zeros_like(next_dist)</span>
<span id="cb352-25"><a href="#cb352-25" aria-hidden="true" tabindex="-1"></a>        projected_dist.scatter_add_(<span class="op">-</span><span class="dv">1</span>, l, next_dist <span class="op">*</span> (u.<span class="bu">float</span>() <span class="op">-</span> b))</span>
<span id="cb352-26"><a href="#cb352-26" aria-hidden="true" tabindex="-1"></a>        projected_dist.scatter_add_(<span class="op">-</span><span class="dv">1</span>, u, next_dist <span class="op">*</span> (b <span class="op">-</span> l.<span class="bu">float</span>()))</span>
<span id="cb352-27"><a href="#cb352-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb352-28"><a href="#cb352-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> projected_dist</span></code></pre></div>
            <h3 id="qr-dqn-quantile-regression">QR-DQN: Quantile
            Regression</h3>
            <p><strong>Idea</strong>: Learn quantiles instead of fixed
            atoms</p>
            <pre><code>C51: Fixed atoms, learn probabilities
     Atoms:  [z‚ÇÅ, z‚ÇÇ, z‚ÇÉ, ..., z‚ÇÖ‚ÇÅ]
     Learn:  [p‚ÇÅ, p‚ÇÇ, p‚ÇÉ, ..., p‚ÇÖ‚ÇÅ]

QR-DQN: Fixed probabilities (quantiles), learn values
        Quantiles: [0.01, 0.02, ..., 0.99]  (N quantiles)
        Learn:     [Œ∏‚ÇÅ,  Œ∏‚ÇÇ,  ...,  Œ∏‚Çô]     (quantile values)</code></pre>
            <p><strong>Quantile Huber loss</strong>:</p>
            <p><span class="math display">\[\rho_\tau(u) = |\tau -
            \mathbf{1}_{u &lt; 0}| \cdot L_\kappa(u)\]</span></p>
            <p>where <span class="math inline">\(L_\kappa\)</span> is
            Huber loss and <span class="math inline">\(\tau\)</span> is
            the quantile.</p>
            <h3 id="iqn-implicit-quantile-networks">IQN: Implicit
            Quantile Networks</h3>
            <p><strong>Idea</strong>: Sample quantiles continuously,
            learn to output any quantile</p>
            <div class="sourceCode" id="cb354"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb354-1"><a href="#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IQN(nn.Module):</span>
<span id="cb354-2"><a href="#cb354-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, state_dim, action_dim, embedding_dim<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb354-3"><a href="#cb354-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb354-4"><a href="#cb354-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.state_encoder <span class="op">=</span> nn.Linear(state_dim, embedding_dim)</span>
<span id="cb354-5"><a href="#cb354-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quantile_encoder <span class="op">=</span> nn.Linear(embedding_dim, embedding_dim)</span>
<span id="cb354-6"><a href="#cb354-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(embedding_dim, action_dim)</span>
<span id="cb354-7"><a href="#cb354-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb354-8"><a href="#cb354-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, state, num_quantiles<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb354-9"><a href="#cb354-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode state</span></span>
<span id="cb354-10"><a href="#cb354-10" aria-hidden="true" tabindex="-1"></a>        state_embed <span class="op">=</span> F.relu(<span class="va">self</span>.state_encoder(state))</span>
<span id="cb354-11"><a href="#cb354-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb354-12"><a href="#cb354-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample random quantiles œÑ ‚àà [0, 1]</span></span>
<span id="cb354-13"><a href="#cb354-13" aria-hidden="true" tabindex="-1"></a>        tau <span class="op">=</span> torch.rand(num_quantiles)</span>
<span id="cb354-14"><a href="#cb354-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb354-15"><a href="#cb354-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode quantiles (cosine embedding)</span></span>
<span id="cb354-16"><a href="#cb354-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># œÜ(œÑ) = ReLU(Œ£·µ¢ cos(œÄiœÑ) w·µ¢)</span></span>
<span id="cb354-17"><a href="#cb354-17" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> torch.arange(<span class="dv">1</span>, <span class="va">self</span>.embedding_dim <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb354-18"><a href="#cb354-18" aria-hidden="true" tabindex="-1"></a>        cos_embed <span class="op">=</span> torch.cos(tau.unsqueeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> i <span class="op">*</span> math.pi)</span>
<span id="cb354-19"><a href="#cb354-19" aria-hidden="true" tabindex="-1"></a>        quantile_embed <span class="op">=</span> F.relu(<span class="va">self</span>.quantile_encoder(cos_embed))</span>
<span id="cb354-20"><a href="#cb354-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb354-21"><a href="#cb354-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine: element-wise product</span></span>
<span id="cb354-22"><a href="#cb354-22" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> state_embed.unsqueeze(<span class="dv">1</span>) <span class="op">*</span> quantile_embed</span>
<span id="cb354-23"><a href="#cb354-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb354-24"><a href="#cb354-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output quantile values for each action</span></span>
<span id="cb354-25"><a href="#cb354-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output(combined)  <span class="co"># [batch, num_quantiles, actions]</span></span></code></pre></div>
            <h3 id="rainbow-dqn">Rainbow DQN</h3>
            <p><strong>Combines</strong> many improvements including
            distributional RL:</p>
            <table>
            <thead>
            <tr>
            <th>Component</th>
            <th>Contribution</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>DQN</strong></td>
            <td>Base algorithm</td>
            </tr>
            <tr>
            <td><strong>Double DQN</strong></td>
            <td>Reduce overestimation</td>
            </tr>
            <tr>
            <td><strong>Prioritized replay</strong></td>
            <td>Focus on important transitions</td>
            </tr>
            <tr>
            <td><strong>Dueling networks</strong></td>
            <td>Separate value and advantage</td>
            </tr>
            <tr>
            <td><strong>Multi-step returns</strong></td>
            <td>Better credit assignment</td>
            </tr>
            <tr>
            <td><strong>Distributional (C51)</strong></td>
            <td>Richer value representation</td>
            </tr>
            <tr>
            <td><strong>Noisy networks</strong></td>
            <td>Learned exploration</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-whats-distributional-rl-and-why-does-it-help">Interview
            Q: ‚ÄúWhat‚Äôs distributional RL and why does it help?‚Äù</h3>
            <p><strong>A</strong>: Standard RL learns expected returns
            <span class="math inline">\(Q(s,a) = \mathbb{E}[G]\)</span>.
            Distributional RL learns the <strong>full
            distribution</strong> of returns <span
            class="math inline">\(Z(s,a)\)</span>. Two situations with
            the same mean can have very different distributions (certain
            vs risky).</p>
            <p>Benefits: (1) <strong>Richer representation</strong> ‚Äî
            the distribution provides more learning signal than just the
            mean, leading to better feature learning, (2)
            <strong>Risk-sensitive decisions</strong> ‚Äî can choose
            actions based on variance or worst-case, not just mean, (3)
            <strong>More stable</strong> ‚Äî distributional losses (like
            cross-entropy for C51) are often more stable than squared
            error.</p>
            <p>Implementations include <strong>C51</strong> (fixed
            atoms, learned probabilities), <strong>QR-DQN</strong>
            (fixed quantiles, learned values), and <strong>IQN</strong>
            (implicit quantiles, can output any quantile). Rainbow DQN
            combines distributional RL with other improvements for
            state-of-the-art Atari performance.</p>
            <hr />
            <h1 id="part-10-ml-systems-high-performance-computing">Part
            10: ML Systems &amp; High-Performance Computing</h1>
            <p>Modern machine learning at scale is fundamentally a
            systems problem. Training large models requires
            understanding not just the algorithms, but how computation
            is distributed across hardware, how data moves between
            processors, and how numerical precision affects both speed
            and correctness. This section covers the essential systems
            concepts that every ML practitioner working with large-scale
            models needs to understand.</p>
            <h2 id="spmd-computing-single-program-multiple-data">10.1
            SPMD Computing (Single Program Multiple Data)</h2>
            <h3 id="what-is-spmd">What is SPMD?</h3>
            <p>When training neural networks across multiple GPUs, we
            need a programming model that allows us to express
            parallelism without writing completely different code for
            each processor. SPMD (Single Program Multiple Data) provides
            exactly this abstraction: every processor executes the same
            program, but operates on different portions of the data.
            This simple idea underpins virtually all modern distributed
            deep learning.</p>
            <p><strong>SPMD</strong> = Same program runs on all
            processors, operating on different data.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         SPMD Execution                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  GPU 0: same_function(data_chunk_0) ‚îÄ‚îÄ‚Üí result_0              ‚îÇ
‚îÇ  GPU 1: same_function(data_chunk_1) ‚îÄ‚îÄ‚Üí result_1              ‚îÇ
‚îÇ  GPU 2: same_function(data_chunk_2) ‚îÄ‚îÄ‚Üí result_2              ‚îÇ
‚îÇ  GPU 3: same_function(data_chunk_3) ‚îÄ‚îÄ‚Üí result_3              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Same code         Different data       Parallel results      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Key insight</strong>: Data parallelism IS SPMD ‚Äî
            each GPU runs the same forward/backward pass on different
            mini-batch slices.</p>
            <h3 id="spmd-vs-other-paradigms">SPMD vs Other
            Paradigms</h3>
            <p>Understanding where SPMD fits in the hierarchy of
            parallel computing paradigms helps clarify its role. SIMD
            (Single Instruction Multiple Data) operates at the hardware
            level‚Äîa single instruction like ‚Äúadd‚Äù is applied to multiple
            data elements simultaneously, as seen in vector instructions
            (AVX) or within GPU warps where 32 threads execute the same
            instruction in lockstep. SPMD operates at a higher
            abstraction level: entire programs (not just single
            instructions) run on multiple processors. MIMD (Multiple
            Instruction Multiple Data) is the most general model where
            different processors can run completely different
            programs‚Äîpipeline parallelism is an example where different
            stages run different computations.</p>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 40%" />
            <col style="width: 28%" />
            </colgroup>
            <thead>
            <tr>
            <th>Paradigm</th>
            <th>Description</th>
            <th>Example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>SIMD</strong></td>
            <td>Single instruction, multiple data (vector ops)</td>
            <td>AVX, GPU warps</td>
            </tr>
            <tr>
            <td><strong>SPMD</strong></td>
            <td>Single program, multiple data (higher level)</td>
            <td>Data parallel training</td>
            </tr>
            <tr>
            <td><strong>MIMD</strong></td>
            <td>Multiple programs, multiple data</td>
            <td>Pipeline parallelism</td>
            </tr>
            </tbody>
            </table>
            <h3 id="jaxxla-model-for-spmd">JAX/XLA Model for SPMD</h3>
            <p>JAX, developed by Google, provides an elegant functional
            approach to SPMD programming. Its core philosophy is
            composable transformations: you write code for a single
            example or single device, then apply transformations like
            <code>jit</code>, <code>vmap</code>, and <code>pmap</code>
            to compile, vectorize, and parallelize it. This stands in
            contrast to PyTorch‚Äôs more imperative distributed APIs where
            you explicitly manage processes and communication.</p>
            <p>JAX provides powerful SPMD primitives:</p>
            <h4 id="jit-just-in-time-compilation"><code>jit</code> ‚Äî
            Just-In-Time Compilation</h4>
            <div class="sourceCode" id="cb356"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb356-1"><a href="#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb356-2"><a href="#cb356-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb356-3"><a href="#cb356-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb356-4"><a href="#cb356-4" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb356-5"><a href="#cb356-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(params, x):</span>
<span id="cb356-6"><a href="#cb356-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Compiled once, executed many times efficiently&quot;&quot;&quot;</span></span>
<span id="cb356-7"><a href="#cb356-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.dot(x, params[<span class="st">&#39;W&#39;</span>]) <span class="op">+</span> params[<span class="st">&#39;b&#39;</span>]</span>
<span id="cb356-8"><a href="#cb356-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb356-9"><a href="#cb356-9" aria-hidden="true" tabindex="-1"></a><span class="co"># First call: compiles to XLA</span></span>
<span id="cb356-10"><a href="#cb356-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Subsequent calls: uses cached compiled code</span></span>
<span id="cb356-11"><a href="#cb356-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> forward(params, x)</span></code></pre></div>
            <h4 id="vmap-vectorized-map-auto-batching"><code>vmap</code>
            ‚Äî Vectorized Map (Auto-batching)</h4>
            <div class="sourceCode" id="cb357"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> single_example_loss(params, x, y):</span>
<span id="cb357-2"><a href="#cb357-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Loss for ONE example&quot;&quot;&quot;</span></span>
<span id="cb357-3"><a href="#cb357-3" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> forward(params, x)</span>
<span id="cb357-4"><a href="#cb357-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.mean((pred <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb357-5"><a href="#cb357-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb357-6"><a href="#cb357-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatically vectorize over batch dimension</span></span>
<span id="cb357-7"><a href="#cb357-7" aria-hidden="true" tabindex="-1"></a>batch_loss <span class="op">=</span> jax.vmap(single_example_loss, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>, <span class="dv">0</span>))</span>
<span id="cb357-8"><a href="#cb357-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb357-9"><a href="#cb357-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Now works on batches!</span></span>
<span id="cb357-10"><a href="#cb357-10" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> batch_loss(params, X_batch, y_batch).mean()</span></code></pre></div>
            <p><strong>Why <code>vmap</code> matters</strong>: Write
            code for single examples, automatically get batched version.
            No manual batch dimension handling!</p>
            <h4
            id="pmap-parallel-map-multi-device-spmd"><code>pmap</code> ‚Äî
            Parallel Map (Multi-device SPMD)</h4>
            <div class="sourceCode" id="cb358"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb358-1"><a href="#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.pmap</span></span>
<span id="cb358-2"><a href="#cb358-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parallel_forward(params, x):</span>
<span id="cb358-3"><a href="#cb358-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Runs on each device with its data shard&quot;&quot;&quot;</span></span>
<span id="cb358-4"><a href="#cb358-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.dot(x, params[<span class="st">&#39;W&#39;</span>]) <span class="op">+</span> params[<span class="st">&#39;b&#39;</span>]</span>
<span id="cb358-5"><a href="#cb358-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb358-6"><a href="#cb358-6" aria-hidden="true" tabindex="-1"></a><span class="co"># x has shape (num_devices, batch_per_device, features)</span></span>
<span id="cb358-7"><a href="#cb358-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Each device processes its slice automatically</span></span>
<span id="cb358-8"><a href="#cb358-8" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> parallel_forward(replicated_params, sharded_x)</span></code></pre></div>
            <h3 id="sharding-strategies">Sharding Strategies</h3>
            <p>Once we have the SPMD programming model, the next
            question is: how do we divide up the work? Sharding
            strategies determine how data and model parameters are
            distributed across devices. The choice of sharding strategy
            profoundly affects memory usage, communication patterns, and
            achievable parallelism. Different strategies are often
            combined‚Äîfor example, data parallelism across nodes with
            tensor parallelism within a node.</p>
            <h4 id="data-sharding-most-common">Data Sharding (Most
            Common)</h4>
            <p>The simplest and most widely used approach: each device
            gets a different slice of the batch. This works well because
            the forward and backward passes are independent across batch
            elements until gradient synchronization. The model is
            replicated on each device, which limits model size to what
            fits on a single GPU but scales linearly with batch
            size.</p>
            <pre><code>Global batch: [B, seq_len, hidden]
              ‚Üì shard along batch
Device 0: [B/N, seq_len, hidden]
Device 1: [B/N, seq_len, hidden]
...
Device N: [B/N, seq_len, hidden]</code></pre>
            <h4 id="model-sharding-tensor-parallelism">Model Sharding
            (Tensor Parallelism)</h4>
            <p>When models become too large to fit on a single GPU even
            without activation memory, we need to split the model
            weights themselves. Tensor parallelism shards individual
            weight matrices across devices. For example, a large matrix
            multiply Y = XW can be computed by splitting W column-wise:
            each device computes X √ó W_i for its shard W_i. The outputs
            are then concatenated or reduced depending on the layer
            type. This requires careful handling of
            communication‚Äîtypically an AllReduce or AllGather at layer
            boundaries‚Äîbut allows training models far larger than
            single-GPU memory.</p>
            <pre><code>Weight matrix: [hidden, 4*hidden]
               ‚Üì shard along columns
Device 0: [hidden, hidden]   (first quarter)
Device 1: [hidden, hidden]   (second quarter)
...</code></pre>
            <h4 id="fsdp-style-sharding-zero">FSDP-style Sharding
            (ZeRO)</h4>
            <p>Fully Sharded Data Parallel (FSDP), based on the ZeRO
            (Zero Redundancy Optimizer) technique from DeepSpeed, takes
            a different approach: instead of each GPU holding a full
            copy of the model, parameters are sharded across GPUs and
            gathered on-demand. During forward pass, each layer gathers
            its full parameters via AllGather, computes, then discards
            the gathered parameters to free memory. During backward, the
            process repeats, and gradients are scattered back via
            ReduceScatter so each GPU only stores gradients for its
            parameter shard. This trades communication for memory,
            allowing training of models that wouldn‚Äôt fit with standard
            data parallelism.</p>
            <pre><code>Full params: [total_params]
             ‚Üì shard evenly
Device 0: [total_params / N]  (owns this shard)
Device 1: [total_params / N]  (owns this shard)
...
# All-gather when needed, discard after use</code></pre>
            <h3
            id="interview-q-what-is-spmd-and-how-does-data-parallelism-relate-to-it">Interview
            Q: ‚ÄúWhat is SPMD and how does data parallelism relate to
            it?‚Äù</h3>
            <p><strong>A</strong>: SPMD (Single Program Multiple Data)
            means the same code runs on all processors, but each
            operates on different data. Data parallelism is a form of
            SPMD: each GPU runs the identical forward/backward pass on
            different mini-batch slices. JAX expresses this with
            <code>pmap</code> ‚Äî you write code for one device, and it
            automatically runs across all devices with data sharded
            appropriately. The key benefit is simple programming model:
            write sequential code, get parallel execution.</p>
            <hr />
            <h2 id="communication-collectives">10.2 Communication
            Collectives</h2>
            <p>Communication is often the bottleneck in distributed
            training. While computation has scaled dramatically with
            more powerful GPUs, the need to synchronize data between
            devices creates overhead that can dominate training time if
            not carefully managed. Understanding communication
            collectives‚Äîstandardized patterns for exchanging data
            between processes‚Äîis essential for reasoning about
            distributed training performance.</p>
            <h3 id="why-communication-matters">Why Communication
            Matters</h3>
            <p>In distributed training, GPUs must exchange data:</p>
            <ul>
            <li><strong>Gradients</strong>: Average across workers (data
            parallelism)</li>
            <li><strong>Activations</strong>: Exchange between pipeline
            stages</li>
            <li><strong>Parameters</strong>: Gather for forward, scatter
            gradients (FSDP)</li>
            </ul>
            <p>The choice of collective operation, its implementation
            algorithm, and when it‚Äôs invoked relative to computation all
            significantly impact training throughput. Modern frameworks
            like NCCL (NVIDIA Collective Communications Library) provide
            highly optimized implementations, but understanding the
            underlying patterns helps you make informed decisions about
            parallelization strategies.</p>
            <h3 id="the-core-collectives">The Core Collectives</h3>
            <p>These six operations form the vocabulary of distributed
            communication. Each has distinct semantics and use cases.
            Understanding them conceptually makes it much easier to
            reason about what happens when you use higher-level APIs
            like PyTorch DDP or DeepSpeed.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Communication Collectives                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  REDUCE (many ‚Üí one)          ALL-REDUCE (many ‚Üí all)               ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  [A] ‚îÄ‚îê                       [A] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ [A+B+C+D]             ‚îÇ
‚îÇ  [B] ‚îÄ‚îº‚îÄ‚îÄ‚Üí [A+B+C+D]          [B] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [A+B+C+D]             ‚îÇ
‚îÇ  [C] ‚îÄ‚î§     (root only)       [C] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [A+B+C+D]             ‚îÇ
‚îÇ  [D] ‚îÄ‚îò                       [D] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ [A+B+C+D]             ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  BROADCAST (one ‚Üí many)       ALL-GATHER (parts ‚Üí all have all)     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  [A] ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí [A]               [A‚ÇÄ] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ [A‚ÇÄ,A‚ÇÅ,A‚ÇÇ,A‚ÇÉ]         ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ‚Üí [A]               [A‚ÇÅ] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [A‚ÇÄ,A‚ÇÅ,A‚ÇÇ,A‚ÇÉ]         ‚îÇ
‚îÇ        ‚îú‚îÄ‚îÄ‚Üí [A]               [A‚ÇÇ] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [A‚ÇÄ,A‚ÇÅ,A‚ÇÇ,A‚ÇÉ]         ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚Üí [A]               [A‚ÇÉ] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ [A‚ÇÄ,A‚ÇÅ,A‚ÇÇ,A‚ÇÉ]         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  SCATTER (one ‚Üí parts)        REDUCE-SCATTER (reduce + scatter)     ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  [A‚ÇÄ,A‚ÇÅ,A‚ÇÇ,A‚ÇÉ] ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚Üí [A‚ÇÄ]    [A] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ [(A+B+C+D)‚ÇÄ]          ‚îÇ
‚îÇ                  ‚îú‚îÄ‚îÄ‚Üí [A‚ÇÅ]    [B] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [(A+B+C+D)‚ÇÅ]          ‚îÇ
‚îÇ                  ‚îú‚îÄ‚îÄ‚Üí [A‚ÇÇ]    [C] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ [(A+B+C+D)‚ÇÇ]          ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚Üí [A‚ÇÉ]    [D] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ [(A+B+C+D)‚ÇÉ]          ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3
            id="allreduce-the-workhorse-of-data-parallelism">AllReduce:
            The Workhorse of Data Parallelism</h3>
            <p>AllReduce is the most important collective for data
            parallelism. It takes a tensor from each process, applies a
            reduction operation (typically sum), and distributes the
            result back to all processes. In data parallel training,
            each GPU computes gradients on its local batch, and
            AllReduce sums these gradients so every GPU has the same
            averaged gradient‚Äîensuring all model replicas stay
            synchronized. Without AllReduce, the models would diverge
            after just one step.</p>
            <p><strong>Used in</strong>: DDP gradient
            synchronization</p>
            <div class="sourceCode" id="cb363"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb363-1"><a href="#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Each GPU has local gradients</span></span>
<span id="cb363-2"><a href="#cb363-2" aria-hidden="true" tabindex="-1"></a><span class="co"># After AllReduce, each GPU has the SAME averaged gradients</span></span>
<span id="cb363-3"><a href="#cb363-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb363-4"><a href="#cb363-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudo-code</span></span>
<span id="cb363-5"><a href="#cb363-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_parallel_step(model, batch, optimizer):</span>
<span id="cb363-6"><a href="#cb363-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch)</span>
<span id="cb363-7"><a href="#cb363-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()  <span class="co"># Local gradients</span></span>
<span id="cb363-8"><a href="#cb363-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb363-9"><a href="#cb363-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb363-10"><a href="#cb363-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># AllReduce: sum across GPUs, then divide by world_size</span></span>
<span id="cb363-11"><a href="#cb363-11" aria-hidden="true" tabindex="-1"></a>        dist.all_reduce(param.grad, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb363-12"><a href="#cb363-12" aria-hidden="true" tabindex="-1"></a>        param.grad <span class="op">/=</span> world_size</span>
<span id="cb363-13"><a href="#cb363-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb363-14"><a href="#cb363-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div>
            <h3 id="allgather-collecting-distributed-data">AllGather:
            Collecting Distributed Data</h3>
            <p>AllGather concatenates data from all processes so that
            every process ends up with the complete collection. If each
            process starts with a shard of size S, after AllGather each
            process has all N shards concatenated (total size N√óS). This
            is essential for ZeRO-3/FSDP where parameters are sharded:
            before computing a layer, each GPU must gather the full
            weights from all other GPUs. The memory cost is
            temporary‚Äîyou can discard the gathered weights after use‚Äîbut
            the communication cost is real.</p>
            <p><strong>Used in</strong>: ZeRO-3 (gather params before
            forward), tensor parallelism</p>
            <div class="sourceCode" id="cb364"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb364-1"><a href="#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Each GPU has a shard of the weight</span></span>
<span id="cb364-2"><a href="#cb364-2" aria-hidden="true" tabindex="-1"></a><span class="co"># AllGather to reconstruct full weight</span></span>
<span id="cb364-3"><a href="#cb364-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb364-4"><a href="#cb364-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_with_allgather(local_weight_shard, x):</span>
<span id="cb364-5"><a href="#cb364-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gather full weight from all GPUs</span></span>
<span id="cb364-6"><a href="#cb364-6" aria-hidden="true" tabindex="-1"></a>    full_weight <span class="op">=</span> torch.empty(total_size)</span>
<span id="cb364-7"><a href="#cb364-7" aria-hidden="true" tabindex="-1"></a>    dist.all_gather_into_tensor(full_weight, local_weight_shard)</span>
<span id="cb364-8"><a href="#cb364-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb364-9"><a href="#cb364-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use full weight for computation</span></span>
<span id="cb364-10"><a href="#cb364-10" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> x <span class="op">@</span> full_weight</span>
<span id="cb364-11"><a href="#cb364-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb364-12"><a href="#cb364-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optionally discard full_weight to save memory</span></span>
<span id="cb364-13"><a href="#cb364-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
            <h3
            id="reducescatter-efficient-gradient-handling">ReduceScatter:
            Efficient Gradient Handling</h3>
            <p>ReduceScatter combines a reduction (sum) with a scatter
            operation in a single collective. Each process starts with a
            full tensor, all tensors are summed element-wise, and the
            result is partitioned so each process receives a different
            shard of the reduced result. This is the inverse of
            AllGather and is particularly useful in FSDP: after backward
            pass, each GPU has gradients for all parameters, but only
            needs to store/update gradients for its parameter shard.
            ReduceScatter both sums the gradients and distributes them
            appropriately in one communication step‚Äîmore efficient than
            AllReduce followed by discarding.</p>
            <p><strong>Used in</strong>: ZeRO, FSDP gradient
            synchronization</p>
            <div class="sourceCode" id="cb365"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb365-1"><a href="#cb365-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Each GPU has full gradients</span></span>
<span id="cb365-2"><a href="#cb365-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ReduceScatter: sum and scatter so each GPU gets 1/N of summed grads</span></span>
<span id="cb365-3"><a href="#cb365-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb365-4"><a href="#cb365-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reduce_scatter_gradients(full_grads, world_size):</span>
<span id="cb365-5"><a href="#cb365-5" aria-hidden="true" tabindex="-1"></a>    shard_size <span class="op">=</span> <span class="bu">len</span>(full_grads) <span class="op">//</span> world_size</span>
<span id="cb365-6"><a href="#cb365-6" aria-hidden="true" tabindex="-1"></a>    my_grad_shard <span class="op">=</span> torch.empty(shard_size)</span>
<span id="cb365-7"><a href="#cb365-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb365-8"><a href="#cb365-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combines reduce + scatter in one operation</span></span>
<span id="cb365-9"><a href="#cb365-9" aria-hidden="true" tabindex="-1"></a>    dist.reduce_scatter_tensor(my_grad_shard, full_grads, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb365-10"><a href="#cb365-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb365-11"><a href="#cb365-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> my_grad_shard</span></code></pre></div>
            <h3 id="reduce-broadcast-and-scatter">Reduce, Broadcast, and
            Scatter</h3>
            <p>While AllReduce, AllGather, and ReduceScatter are the
            workhorses of distributed training, three simpler
            collectives complete the picture:</p>
            <p><strong>Reduce</strong> is like AllReduce but only the
            ‚Äúroot‚Äù process gets the result. All processes contribute
            tensors that are combined (e.g., summed), but only one
            designated process (typically rank 0) receives the final
            reduced value. This is useful for computing global metrics
            (like total loss across all GPUs) that only need to be
            logged once, not known by every worker.</p>
            <p><strong>Broadcast</strong> is the inverse of Reduce: one
            process sends data to all others. The root process has a
            tensor that gets copied to every other process. Common uses
            include distributing model weights at initialization
            (ensuring all replicas start identical), sharing
            hyperparameters, or distributing a random seed so all
            processes generate the same ‚Äúrandom‚Äù sequence.</p>
            <p><strong>Scatter</strong> distributes different pieces of
            data from one process to all others. The root process has a
            tensor that gets partitioned, with each process receiving
            one partition. This is less common in training loops but
            useful for distributing work‚Äîfor example, scattering
            different validation batches to different workers for
            parallel evaluation.</p>
            <div class="sourceCode" id="cb366"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb366-1"><a href="#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reduce: sum on all GPUs, result only on rank 0</span></span>
<span id="cb366-2"><a href="#cb366-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb366-3"><a href="#cb366-3" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> torch.zeros_like(local_tensor)</span>
<span id="cb366-4"><a href="#cb366-4" aria-hidden="true" tabindex="-1"></a>dist.<span class="bu">reduce</span>(local_tensor, dst<span class="op">=</span><span class="dv">0</span>, op<span class="op">=</span>dist.ReduceOp.SUM)</span>
<span id="cb366-5"><a href="#cb366-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Only rank 0 has the sum; other ranks have garbage</span></span>
<span id="cb366-6"><a href="#cb366-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-7"><a href="#cb366-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Broadcast: rank 0 sends to all</span></span>
<span id="cb366-8"><a href="#cb366-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb366-9"><a href="#cb366-9" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb366-10"><a href="#cb366-10" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb366-11"><a href="#cb366-11" aria-hidden="true" tabindex="-1"></a>    tensor <span class="op">=</span> torch.empty(<span class="dv">4</span>)</span>
<span id="cb366-12"><a href="#cb366-12" aria-hidden="true" tabindex="-1"></a>dist.broadcast(tensor, src<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb366-13"><a href="#cb366-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Now all ranks have [1, 2, 3, 4]</span></span>
<span id="cb366-14"><a href="#cb366-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-15"><a href="#cb366-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter: rank 0 distributes different pieces to each rank</span></span>
<span id="cb366-16"><a href="#cb366-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> rank <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb366-17"><a href="#cb366-17" aria-hidden="true" tabindex="-1"></a>    tensors <span class="op">=</span> [torch.tensor([i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb366-18"><a href="#cb366-18" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb366-19"><a href="#cb366-19" aria-hidden="true" tabindex="-1"></a>    tensors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb366-20"><a href="#cb366-20" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> torch.empty(<span class="dv">1</span>)</span>
<span id="cb366-21"><a href="#cb366-21" aria-hidden="true" tabindex="-1"></a>dist.scatter(output, scatter_list<span class="op">=</span>tensors, src<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb366-22"><a href="#cb366-22" aria-hidden="true" tabindex="-1"></a><span class="co"># rank 0 gets [0], rank 1 gets [1], rank 2 gets [2], ...</span></span></code></pre></div>
            <h3 id="ring-allreduce-algorithm">Ring AllReduce
            Algorithm</h3>
            <p>The Ring AllReduce algorithm achieves optimal bandwidth
            utilization by arranging GPUs in a logical ring. The key
            insight is decomposing AllReduce into two phases: first a
            Reduce-Scatter (each GPU ends up with 1/N of the final sum),
            then an AllGather (distribute those partial sums to
            everyone). By sending data around the ring in chunks, every
            link is utilized simultaneously, achieving theoretical peak
            bandwidth.</p>
            <p><strong>Most bandwidth-efficient AllReduce for large
            tensors</strong></p>
            <pre><code>Step 1: Reduce-Scatter phase (N-1 steps)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
GPU 0: [A‚ÇÄ|A‚ÇÅ|A‚ÇÇ|A‚ÇÉ] ‚îÄ‚îÄsend A‚ÇÉ‚îÄ‚îÄ‚Üí GPU 1 ‚îÄ‚îÄreceive‚îÄ‚îÄ‚Üí [B‚ÇÄ|B‚ÇÅ|B‚ÇÇ|A‚ÇÉ+B‚ÇÉ]
GPU 1: [B‚ÇÄ|B‚ÇÅ|B‚ÇÇ|B‚ÇÉ] ‚îÄ‚îÄsend B‚ÇÄ‚îÄ‚îÄ‚Üí GPU 2
GPU 2: [C‚ÇÄ|C‚ÇÅ|C‚ÇÇ|C‚ÇÉ] ‚îÄ‚îÄsend C‚ÇÅ‚îÄ‚îÄ‚Üí GPU 3
GPU 3: [D‚ÇÄ|D‚ÇÅ|D‚ÇÇ|D‚ÇÉ] ‚îÄ‚îÄsend D‚ÇÇ‚îÄ‚îÄ‚Üí GPU 0

After N-1 steps: Each GPU has one fully-reduced chunk

Step 2: AllGather phase (N-1 steps)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Each GPU sends its reduced chunk around the ring
After N-1 steps: All GPUs have all reduced chunks</code></pre>
            <p><strong>Complexity</strong>:</p>
            <ul>
            <li><strong>Time</strong>: <span
            class="math inline">\(2(N-1) \cdot \frac{M}{N \cdot
            B}\)</span> where <span class="math inline">\(M\)</span> =
            message size, <span class="math inline">\(B\)</span> =
            bandwidth</li>
            <li><strong>Bandwidth optimal</strong>: Uses all links
            simultaneously</li>
            </ul>
            <h3 id="tree-allreduce-algorithm">Tree AllReduce
            Algorithm</h3>
            <p><strong>Better for latency-sensitive small
            tensors</strong></p>
            <pre><code>Reduce phase (log N steps):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Level 0: GPU 0 ‚Üê GPU 1,  GPU 2 ‚Üê GPU 3
Level 1: GPU 0 ‚Üê GPU 2

Broadcast phase (log N steps):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Reverse the tree

Total: 2 log N steps</code></pre>
            <p><strong>Comparison</strong>:</p>
            <table>
            <thead>
            <tr>
            <th>Algorithm</th>
            <th>Latency</th>
            <th>Bandwidth Efficiency</th>
            <th>Best For</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Ring</strong></td>
            <td><span class="math inline">\(O(N)\)</span></td>
            <td>Optimal</td>
            <td>Large tensors</td>
            </tr>
            <tr>
            <td><strong>Tree</strong></td>
            <td><span class="math inline">\(O(\log N)\)</span></td>
            <td>Lower</td>
            <td>Small tensors, high N</td>
            </tr>
            </tbody>
            </table>
            <h3 id="communication-cost-analysis">Communication Cost
            Analysis</h3>
            <p><strong>Key metrics</strong>:</p>
            <ul>
            <li><strong>Œ± (alpha)</strong>: Latency per message (startup
            cost)</li>
            <li><strong>Œ≤ (beta)</strong>: Time per byte (inverse
            bandwidth)</li>
            </ul>
            <p><strong>Ring AllReduce</strong>: <span
            class="math display">\[T = 2(N-1) \cdot \alpha + 2 \cdot
            \frac{N-1}{N} \cdot M \cdot \beta \approx 2(N-1)\alpha +
            2M\beta\]</span></p>
            <p><strong>Tree AllReduce</strong>: <span
            class="math display">\[T = 2\log_2(N) \cdot \alpha +
            2\log_2(N) \cdot M \cdot \beta\]</span></p>
            <h3
            id="interview-q-explain-allreduce-and-when-youd-use-ring-vs-tree">Interview
            Q: ‚ÄúExplain AllReduce and when you‚Äôd use Ring vs Tree‚Äù</h3>
            <p><strong>A</strong>: AllReduce sums tensors across all
            GPUs so each ends up with the same result. It‚Äôs used in data
            parallelism to average gradients. Ring AllReduce sends
            chunks around a ring ‚Äî it‚Äôs bandwidth-optimal (uses all
            links fully) but has <span
            class="math inline">\(O(N)\)</span> latency. Tree AllReduce
            uses a binary tree pattern ‚Äî it has <span
            class="math inline">\(O(\log N)\)</span> latency but doesn‚Äôt
            saturate bandwidth. Use Ring for large tensors (gradients in
            LLMs) where bandwidth dominates. Use Tree for small tensors
            or very large GPU counts where latency matters more.</p>
            <hr />
            <h2 id="numerical-computing-essentials">10.3 Numerical
            Computing Essentials</h2>
            <p>Numerical precision is a critical but often overlooked
            aspect of ML systems. The choice of floating-point format
            affects memory usage, compute speed, and training stability.
            Modern deep learning has moved away from 32-bit precision
            toward mixed-precision training, but this requires
            understanding the tradeoffs between different number formats
            and the numerical pitfalls that can destabilize
            training.</p>
            <h3 id="floating-point-formats">Floating Point Formats</h3>
            <h4 id="what-this-means-for-beginners-9">What This Means
            (For Beginners)</h4>
            <p>Think of floating point numbers like scientific notation:
            <span class="math inline">\(6.02 \times 10^{23}\)</span></p>
            <ul>
            <li>The <strong>mantissa</strong> (6.02) determines
            <strong>precision</strong> ‚Äî how many significant
            digits</li>
            <li>The <strong>exponent</strong> (23) determines
            <strong>range</strong> ‚Äî how big or small the number can
            be</li>
            </ul>
            <pre><code>Same idea in binary:
   FP32:  [1 sign bit][8 exponent bits][23 mantissa bits]
          ‚Üì           ‚Üì                ‚Üì
        positive/    &quot;power of 2&quot;     &quot;significant digits&quot;
        negative     (range)          (precision)</code></pre>
            <p><strong>The key trade-off</strong>: With fewer total bits
            (16 vs 32), you must choose: more range (bigger exponent) or
            more precision (bigger mantissa)?</p>
            <ul>
            <li><strong>FP16</strong> chose precision: 10 mantissa bits,
            only 5 exponent bits ‚Üí limited range (max ¬±65504)</li>
            <li><strong>BF16</strong> chose range: 7 mantissa bits, 8
            exponent bits ‚Üí same range as FP32 (max ¬±3.4√ó10¬≥‚Å∏)</li>
            </ul>
            <p><strong>Why range matters more for training</strong>:
            Gradients can occasionally be very large. If a gradient
            exceeds 65504, FP16 overflows to infinity ‚Üí training
            crashes. BF16 can represent the same huge numbers as FP32,
            so this never happens. The precision loss (fewer mantissa
            bits) is acceptable because we keep master weights in
            FP32.</p>
            <p>Floating point numbers trade off between range (how
            large/small values can be) and precision (how many
            significant digits). The key insight is that different parts
            of training have different requirements: forward/backward
            passes benefit most from speed and memory savings, while
            weight updates and optimizer states need higher precision to
            accumulate small gradient changes accurately.</p>
            <table>
            <thead>
            <tr>
            <th>Format</th>
            <th>Bits</th>
            <th>Exponent</th>
            <th>Mantissa</th>
            <th>Range</th>
            <th>Precision</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>FP32</strong></td>
            <td>32</td>
            <td>8</td>
            <td>23</td>
            <td>¬±3.4e38</td>
            <td>~7 decimal</td>
            </tr>
            <tr>
            <td><strong>FP16</strong></td>
            <td>16</td>
            <td>5</td>
            <td>10</td>
            <td>¬±65504</td>
            <td>~3 decimal</td>
            </tr>
            <tr>
            <td><strong>BF16</strong></td>
            <td>16</td>
            <td>8</td>
            <td>7</td>
            <td>¬±3.4e38</td>
            <td>~2 decimal</td>
            </tr>
            <tr>
            <td><strong>TF32</strong></td>
            <td>19</td>
            <td>8</td>
            <td>10</td>
            <td>¬±3.4e38</td>
            <td>~3 decimal</td>
            </tr>
            </tbody>
            </table>
            <h3 id="bf16-vs-fp16-why-bf16-wins-for-training">BF16 vs
            FP16: Why BF16 Wins for Training</h3>
            <pre><code>FP16:  [1 sign][5 exponent][10 mantissa]
       Range: ¬±65504 ‚Äî can overflow during training!
       
BF16:  [1 sign][8 exponent][7 mantissa]
       Range: ¬±3.4e38 ‚Äî same as FP32, safe for training
       
Key insight: Training needs RANGE more than PRECISION
             Gradients can be large, overflow is catastrophic
             Slight precision loss is tolerable</code></pre>
            <h3 id="mixed-precision-training-2">Mixed Precision
            Training</h3>
            <p><strong>Keep master weights in FP32, compute in lower
            precision</strong>:</p>
            <div class="sourceCode" id="cb371"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb371-1"><a href="#cb371-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic Mixed Precision (AMP) in PyTorch</span></span>
<span id="cb371-2"><a href="#cb371-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb371-3"><a href="#cb371-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-4"><a href="#cb371-4" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb371-5"><a href="#cb371-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-6"><a href="#cb371-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb371-7"><a href="#cb371-7" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb371-8"><a href="#cb371-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb371-9"><a href="#cb371-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass in FP16/BF16</span></span>
<span id="cb371-10"><a href="#cb371-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> autocast(dtype<span class="op">=</span>torch.bfloat16):</span>
<span id="cb371-11"><a href="#cb371-11" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch)</span>
<span id="cb371-12"><a href="#cb371-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb371-13"><a href="#cb371-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb371-14"><a href="#cb371-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass: scaler handles gradient scaling</span></span>
<span id="cb371-15"><a href="#cb371-15" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb371-16"><a href="#cb371-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb371-17"><a href="#cb371-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Unscale gradients, check for inf/nan, update</span></span>
<span id="cb371-18"><a href="#cb371-18" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb371-19"><a href="#cb371-19" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span></code></pre></div>
            <p><strong>Why it works</strong>:</p>
            <ul>
            <li>Forward/backward in FP16/BF16: 2√ó memory, faster
            compute</li>
            <li>Gradients accumulated in FP32: numerical stability</li>
            <li>Loss scaling: prevents gradient underflow in FP16</li>
            </ul>
            <h3 id="loss-scaling-for-fp16">Loss Scaling (for FP16)</h3>
            <p><strong>Problem</strong>: Small gradients underflow to
            zero in FP16</p>
            <p><strong>Solution</strong>: Scale loss up, scale gradients
            down</p>
            <div class="sourceCode" id="cb372"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb372-1"><a href="#cb372-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual loss scaling</span></span>
<span id="cb372-2"><a href="#cb372-2" aria-hidden="true" tabindex="-1"></a>LOSS_SCALE <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb372-3"><a href="#cb372-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb372-4"><a href="#cb372-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward</span></span>
<span id="cb372-5"><a href="#cb372-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> model(x, y)</span>
<span id="cb372-6"><a href="#cb372-6" aria-hidden="true" tabindex="-1"></a>scaled_loss <span class="op">=</span> loss <span class="op">*</span> LOSS_SCALE</span>
<span id="cb372-7"><a href="#cb372-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb372-8"><a href="#cb372-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Backward</span></span>
<span id="cb372-9"><a href="#cb372-9" aria-hidden="true" tabindex="-1"></a>scaled_loss.backward()</span>
<span id="cb372-10"><a href="#cb372-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb372-11"><a href="#cb372-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Unscale before optimizer step</span></span>
<span id="cb372-12"><a href="#cb372-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb372-13"><a href="#cb372-13" aria-hidden="true" tabindex="-1"></a>    param.grad <span class="op">/=</span> LOSS_SCALE</span></code></pre></div>
            <h3 id="numerical-stability-softmax">Numerical Stability:
            Softmax</h3>
            <p><strong>Naive implementation</strong>:</p>
            <div class="sourceCode" id="cb373"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb373-1"><a href="#cb373-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax_naive(x):</span>
<span id="cb373-2"><a href="#cb373-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(x) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(x))</span></code></pre></div>
            <p><strong>Problem</strong>: <code>exp(1000)</code> =
            overflow!</p>
            <p><strong>Stable implementation</strong>:</p>
            <div class="sourceCode" id="cb374"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb374-1"><a href="#cb374-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax_stable(x):</span>
<span id="cb374-2"><a href="#cb374-2" aria-hidden="true" tabindex="-1"></a>    x_max <span class="op">=</span> np.<span class="bu">max</span>(x)</span>
<span id="cb374-3"><a href="#cb374-3" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> np.exp(x <span class="op">-</span> x_max)  <span class="co"># Subtract max for stability</span></span>
<span id="cb374-4"><a href="#cb374-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> np.<span class="bu">sum</span>(exp_x)</span></code></pre></div>
            <p><strong>Why it works</strong>: <span
            class="math inline">\(\text{softmax}(x) = \text{softmax}(x -
            c)\)</span> for any constant <span
            class="math inline">\(c\)</span>.</p>
            <h3 id="numerical-stability-log-sum-exp">Numerical
            Stability: Log-Sum-Exp</h3>
            <p><strong>Problem</strong>: Computing <span
            class="math inline">\(\log(\sum_i e^{x_i})\)</span></p>
            <p><strong>Naive</strong>:</p>
            <div class="sourceCode" id="cb375"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsumexp_naive(x):</span>
<span id="cb375-2"><a href="#cb375-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.log(np.<span class="bu">sum</span>(np.exp(x)))  <span class="co"># Overflow!</span></span></code></pre></div>
            <p><strong>Stable</strong>:</p>
            <div class="sourceCode" id="cb376"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb376-1"><a href="#cb376-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsumexp_stable(x):</span>
<span id="cb376-2"><a href="#cb376-2" aria-hidden="true" tabindex="-1"></a>    x_max <span class="op">=</span> np.<span class="bu">max</span>(x)</span>
<span id="cb376-3"><a href="#cb376-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_max <span class="op">+</span> np.log(np.<span class="bu">sum</span>(np.exp(x <span class="op">-</span> x_max)))</span></code></pre></div>
            <p><strong>Identity</strong>: <span
            class="math inline">\(\log\sum_i e^{x_i} = x_{max} +
            \log\sum_i e^{x_i - x_{max}}\)</span></p>
            <h3 id="numerical-stability-cross-entropy">Numerical
            Stability: Cross-Entropy</h3>
            <p><strong>Combined softmax + cross-entropy is more
            stable</strong>:</p>
            <div class="sourceCode" id="cb377"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb377-1"><a href="#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DON&#39;T DO THIS:</span></span>
<span id="cb377-2"><a href="#cb377-2" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> softmax(logits)</span>
<span id="cb377-3"><a href="#cb377-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>np.<span class="bu">sum</span>(labels <span class="op">*</span> np.log(probs))  <span class="co"># log(0) = -inf!</span></span>
<span id="cb377-4"><a href="#cb377-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb377-5"><a href="#cb377-5" aria-hidden="true" tabindex="-1"></a><span class="co"># DO THIS (PyTorch does internally):</span></span>
<span id="cb377-6"><a href="#cb377-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy_stable(logits, labels):</span>
<span id="cb377-7"><a href="#cb377-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels is class index</span></span>
<span id="cb377-8"><a href="#cb377-8" aria-hidden="true" tabindex="-1"></a>    log_sum_exp <span class="op">=</span> logsumexp_stable(logits)</span>
<span id="cb377-9"><a href="#cb377-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_sum_exp <span class="op">-</span> logits[labels]</span></code></pre></div>
            <h3 id="gradient-accumulation-2">Gradient Accumulation</h3>
            <p><strong>Problem</strong>: Want large effective batch but
            GPU memory limited</p>
            <p><strong>Solution</strong>: Accumulate gradients over
            multiple micro-batches</p>
            <div class="sourceCode" id="cb378"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb378-1"><a href="#cb378-1" aria-hidden="true" tabindex="-1"></a>accumulation_steps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb378-2"><a href="#cb378-2" aria-hidden="true" tabindex="-1"></a>effective_batch_size <span class="op">=</span> micro_batch_size <span class="op">*</span> accumulation_steps</span>
<span id="cb378-3"><a href="#cb378-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb378-4"><a href="#cb378-4" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb378-5"><a href="#cb378-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb378-6"><a href="#cb378-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(batch) <span class="op">/</span> accumulation_steps  <span class="co"># Scale loss</span></span>
<span id="cb378-7"><a href="#cb378-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()  <span class="co"># Accumulate gradients</span></span>
<span id="cb378-8"><a href="#cb378-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb378-9"><a href="#cb378-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb378-10"><a href="#cb378-10" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb378-11"><a href="#cb378-11" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div>
            <p><strong>Memory</strong>: Same as small batch
            <strong>Effective batch</strong>: Much larger</p>
            <h3
            id="interview-q-why-use-bf16-instead-of-fp16-for-training">Interview
            Q: ‚ÄúWhy use BF16 instead of FP16 for training?‚Äù</h3>
            <p><strong>A</strong>: BF16 has the same exponent range as
            FP32 (8 bits, ¬±3.4e38) but with less mantissa precision (7
            vs 23 bits). FP16 has only 5 exponent bits, limiting range
            to ¬±65504. During training, gradients and activations can
            have large magnitudes that overflow FP16 but not BF16. The
            precision loss in BF16 is acceptable because we keep master
            weights in FP32. FP16 requires loss scaling to prevent
            gradient underflow, adding complexity. BF16 ‚Äújust works‚Äù for
            training without special handling.</p>
            <hr />
            <h2 id="memory-and-compute-analysis">10.4 Memory and Compute
            Analysis</h2>
            <h3 id="what-this-means-for-beginners-10">What This Means
            (For Beginners)</h3>
            <p>You might think: ‚ÄúMy model has 7 billion parameters √ó 4
            bytes = 28 GB. I have an 80 GB GPU. Easy!‚Äù</p>
            <p><strong>Wrong.</strong> Here‚Äôs why:</p>
            <pre><code>What you think:                What actually happens:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Model weights: 28 GB ‚îÇ       ‚îÇ Model weights (BF16):     14 GB     ‚îÇ
‚îÇ                      ‚îÇ       ‚îÇ Gradients (BF16):         14 GB     ‚îÇ
‚îÇ &quot;I have 52 GB left!&quot; ‚îÇ       ‚îÇ Optimizer states (FP32):  56 GB     ‚îÇ  ‚Üê SURPRISE!
‚îÇ                      ‚îÇ       ‚îÇ Activations:              18+ GB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ
                               ‚îÇ Total:                  ~100+ GB    ‚îÇ
                               ‚îÇ                                      ‚îÇ
                               ‚îÇ &quot;Wait... that&#39;s more than my GPU!&quot;  ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>The three surprises</strong>:</p>
            <ol type="1">
            <li><p><strong>Optimizer states are HUGE</strong>: Adam
            stores momentum (<span class="math inline">\(m\)</span>) and
            variance (<span class="math inline">\(v\)</span>) for each
            parameter. That‚Äôs 2√ó the parameters, kept in FP32 for
            stability, so 4√ó memory compared to BF16 weights.</p></li>
            <li><p><strong>Gradients need storage too</strong>: During
            backprop, you need to store the gradient for every
            parameter.</p></li>
            <li><p><strong>Activations grow with batch size</strong>:
            Every layer‚Äôs output must be stored for the backward pass.
            Longer sequences √ó larger batches = more memory.</p></li>
            </ol>
            <p><strong>This is why</strong>: - ZeRO exists (shard
            optimizer states across GPUs) - Activation checkpointing
            exists (recompute instead of store) - Mixed precision exists
            (halve parameter/gradient memory)</p>
            <p>Understanding memory consumption is crucial for training
            large models. Memory runs out before you expect it to, and
            knowing where the bytes go helps you make informed
            tradeoffs. The main consumers are model parameters,
            gradients, optimizer states (which often dominate), and
            activations. Modern techniques like ZeRO, activation
            checkpointing, and mixed precision all attack different
            parts of this memory equation.</p>
            <h3 id="memory-breakdown-for-training-1">Memory Breakdown
            for Training</h3>
            <p>For a model with <span class="math inline">\(P\)</span>
            parameters trained with Adam, memory requirements quickly
            multiply beyond just the model weights. A common surprise
            for newcomers: the optimizer state for Adam (momentum and
            variance for each parameter) typically consumes 4√ó the
            memory of the parameters themselves when kept in FP32.</p>
            <table>
            <thead>
            <tr>
            <th>Component</th>
            <th>Size (bytes)</th>
            <th>Example (7B params)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Parameters</strong></td>
            <td><span class="math inline">\(4P\)</span> (FP32) or <span
            class="math inline">\(2P\)</span> (BF16)</td>
            <td>14 GB (BF16)</td>
            </tr>
            <tr>
            <td><strong>Gradients</strong></td>
            <td>Same as params</td>
            <td>14 GB</td>
            </tr>
            <tr>
            <td><strong>Optimizer (Adam)</strong></td>
            <td><span class="math inline">\(8P\)</span> (m and v in
            FP32)</td>
            <td>56 GB</td>
            </tr>
            <tr>
            <td><strong>Activations</strong></td>
            <td>Varies with batch/seq</td>
            <td>10-100+ GB</td>
            </tr>
            <tr>
            <td><strong>Total</strong></td>
            <td>~<span class="math inline">\(16P\)</span> +
            activations</td>
            <td>84 GB + activations</td>
            </tr>
            </tbody>
            </table>
            <h3 id="activation-memory">Activation Memory</h3>
            <p><strong>For Transformer layer</strong>: <span
            class="math display">\[\text{Activation mem} \approx 34
            \cdot b \cdot s \cdot h\]</span></p>
            <p>where <span class="math inline">\(b\)</span> = batch
            size, <span class="math inline">\(s\)</span> = sequence
            length, <span class="math inline">\(h\)</span> = hidden
            dimension</p>
            <p><strong>For full model with <span
            class="math inline">\(L\)</span> layers</strong>: <span
            class="math display">\[\text{Total activations} \approx 34
            \cdot L \cdot b \cdot s \cdot h\]</span></p>
            <h3
            id="activation-checkpointing-gradient-checkpointing">Activation
            Checkpointing (Gradient Checkpointing)</h3>
            <p>During backpropagation, we need the activations from the
            forward pass to compute gradients. Normally, we store all
            intermediate activations, which consumes memory proportional
            to the number of layers. Activation checkpointing offers a
            tradeoff: instead of storing everything, we only store
            activations at certain ‚Äúcheckpoint‚Äù boundaries, then
            recompute the intermediate activations during backward pass.
            This trades compute (roughly 33% more, since we do an extra
            forward pass through checkpointed segments) for memory (can
            reduce from O(L) to O(‚àöL) with optimal checkpoint
            placement). For large models where memory is the bottleneck,
            this tradeoff is almost always worthwhile.</p>
            <p><strong>Trade compute for memory</strong>: Don‚Äôt store
            all activations, recompute during backward.</p>
            <div class="sourceCode" id="cb380"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Without checkpointing: store all activations</span></span>
<span id="cb380-2"><a href="#cb380-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_no_checkpoint(x, layers):</span>
<span id="cb380-3"><a href="#cb380-3" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> [x]</span>
<span id="cb380-4"><a href="#cb380-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb380-5"><a href="#cb380-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb380-6"><a href="#cb380-6" aria-hidden="true" tabindex="-1"></a>        activations.append(x)  <span class="co"># Store for backward</span></span>
<span id="cb380-7"><a href="#cb380-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, activations</span>
<span id="cb380-8"><a href="#cb380-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb380-9"><a href="#cb380-9" aria-hidden="true" tabindex="-1"></a><span class="co"># With checkpointing: recompute activations during backward</span></span>
<span id="cb380-10"><a href="#cb380-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.checkpoint <span class="im">import</span> checkpoint</span>
<span id="cb380-11"><a href="#cb380-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb380-12"><a href="#cb380-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_with_checkpoint(x, layers):</span>
<span id="cb380-13"><a href="#cb380-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb380-14"><a href="#cb380-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> checkpoint(layer, x)  <span class="co"># Recompute in backward</span></span>
<span id="cb380-15"><a href="#cb380-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
            <p><strong>Tradeoff</strong>:</p>
            <ul>
            <li><strong>Memory</strong>: O(‚àöL) instead of O(L) with
            optimal checkpointing</li>
            <li><strong>Compute</strong>: ~33% more FLOPs (one extra
            forward pass)</li>
            </ul>
            <h3 id="compute-vs-memory-bound">Compute vs Memory
            Bound</h3>
            <p><strong>Memory-bound</strong>: Waiting for data
            transfer</p>
            <ul>
            <li>Matrix-vector products</li>
            <li>Element-wise operations</li>
            <li>Small batch sizes</li>
            </ul>
            <p><strong>Compute-bound</strong>: GPU cores fully
            utilized</p>
            <ul>
            <li>Matrix-matrix products (large GEMM)</li>
            <li>Large batch sizes</li>
            <li>Convolutions</li>
            </ul>
            <h3 id="roofline-model">Roofline Model</h3>
            <p>The roofline model is a visual framework for
            understanding whether a workload is limited by compute or
            memory bandwidth. The x-axis is ‚Äúarithmetic intensity‚Äù
            (FLOPs per byte of memory accessed), and the y-axis is
            achievable FLOPS. For low arithmetic intensity (few
            operations per byte moved), you‚Äôre memory-bound‚Äîperformance
            is limited by how fast you can feed data to the compute
            units. For high arithmetic intensity (many operations per
            byte), you‚Äôre compute-bound‚Äîperformance is limited by the
            processor‚Äôs peak FLOPS. The ‚Äúridge point‚Äù where these two
            constraints meet tells you the minimum arithmetic intensity
            needed to fully utilize the hardware. Understanding where
            your workload falls on this curve guides optimization: if
            memory-bound, fuse operations and increase batch sizes; if
            compute-bound, you‚Äôre already getting good utilization.</p>
            <pre><code>FLOPS/s
    ‚Üë
    ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Peak compute (e.g., 312 TFLOPS)
    ‚îÇ        ‚îÇ
    ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚îÇ   /    ‚îÇ
    ‚îÇ  /     ‚îÇ
    ‚îÇ /      ‚îÇ  Compute-bound region
    ‚îÇ/       ‚îÇ
    ‚îÇ Memory-bound region
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
                  Arithmetic Intensity (FLOPS / byte)</code></pre>
            <p><strong>Arithmetic Intensity</strong> = FLOPs / Bytes
            accessed</p>
            <p><strong>Ridge point</strong>: Where memory bandwidth
            meets compute ceiling</p>
            <h3 id="throughput-optimization">Throughput
            Optimization</h3>
            <p><strong>Maximize GPU utilization</strong>:</p>
            <ol type="1">
            <li><strong>Increase batch size</strong> (until memory
            limit)</li>
            <li><strong>Use longer sequences</strong> (better arithmetic
            intensity)</li>
            <li><strong>Fuse operations</strong> (reduce memory
            traffic)</li>
            <li><strong>Overlap compute and communication</strong></li>
            </ol>
            <div class="sourceCode" id="cb382"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb382-1"><a href="#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Overlap AllReduce with backward pass</span></span>
<span id="cb382-2"><a href="#cb382-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Instead of:</span></span>
<span id="cb382-3"><a href="#cb382-3" aria-hidden="true" tabindex="-1"></a><span class="co">#   backward() ‚Üí allreduce()</span></span>
<span id="cb382-4"><a href="#cb382-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Do:</span></span>
<span id="cb382-5"><a href="#cb382-5" aria-hidden="true" tabindex="-1"></a><span class="co">#   backward_layer_n() ‚Üí start_allreduce(grad_n) ‚Üí backward_layer_n-1() ‚Üí ...</span></span></code></pre></div>
            <h3 id="common-memory-optimization-techniques">Common Memory
            Optimization Techniques</h3>
            <table>
            <thead>
            <tr>
            <th>Technique</th>
            <th>Memory Savings</th>
            <th>Compute Cost</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Mixed precision (BF16)</strong></td>
            <td>2√ó</td>
            <td>None (often faster)</td>
            </tr>
            <tr>
            <td><strong>Gradient accumulation</strong></td>
            <td>1√ó (same)</td>
            <td>None</td>
            </tr>
            <tr>
            <td><strong>Activation checkpointing</strong></td>
            <td>~‚àöL reduction</td>
            <td>~33% more</td>
            </tr>
            <tr>
            <td><strong>ZeRO Stage 1</strong></td>
            <td>4√ó</td>
            <td>Communication</td>
            </tr>
            <tr>
            <td><strong>ZeRO Stage 2</strong></td>
            <td>8√ó</td>
            <td>More communication</td>
            </tr>
            <tr>
            <td><strong>ZeRO Stage 3</strong></td>
            <td>Linear in GPUs</td>
            <td>Most communication</td>
            </tr>
            <tr>
            <td><strong>CPU offloading</strong></td>
            <td>Large</td>
            <td>Slow</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-how-would-you-estimate-memory-requirements-for-training-a-7b-parameter-model">Interview
            Q: ‚ÄúHow would you estimate memory requirements for training
            a 7B parameter model?‚Äù</h3>
            <p><strong>A</strong>: For 7B params with Adam in BF16:</p>
            <ul>
            <li>Parameters: 7B √ó 2 bytes = 14 GB</li>
            <li>Gradients: 7B √ó 2 bytes = 14 GB<br />
            </li>
            <li>Optimizer states (m, v): 7B √ó 4 √ó 2 = 56 GB (FP32)</li>
            <li>Total model state: ~84 GB</li>
            </ul>
            <p>Plus activations: ~34 √ó batch_size √ó seq_len √ó hidden_dim
            √ó num_layers bytes. For batch=1, seq=2048, hidden=4096,
            layers=32: ~34 √ó 1 √ó 2048 √ó 4096 √ó 32 √ó 2 ‚âà 18 GB.</p>
            <p>With activation checkpointing, activation memory drops
            significantly. For a single 80GB A100, you‚Äôd need at least
            ZeRO Stage 2 or gradient checkpointing to fit training.
            Multiple GPUs with ZeRO-3 is typical.</p>
            <hr />
            <h2 id="practical-systems-problems-pseudo-code">10.5
            Practical Systems Problems (Pseudo-code)</h2>
            <p>This section presents common systems programming tasks
            you might encounter in interviews or real-world ML
            engineering. These problems test your understanding of
            distributed training concepts, not just your ability to use
            high-level APIs. Being able to write out the logic of
            gradient synchronization, memory management, or
            communication patterns demonstrates deep understanding of
            what happens ‚Äúunder the hood‚Äù when you call
            <code>model.fit()</code> on a multi-GPU cluster.</p>
            <h3 id="problem-1-distributed-training-loop">Problem 1:
            Distributed Training Loop</h3>
            <p><strong>Task</strong>: Write pseudo-code for a
            distributed training step with gradient synchronization.</p>
            <div class="sourceCode" id="cb383"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb383-1"><a href="#cb383-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distributed_training_step(model, batch, optimizer, world_size, rank):</span>
<span id="cb383-2"><a href="#cb383-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb383-3"><a href="#cb383-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Single training step in data-parallel setup.</span></span>
<span id="cb383-4"><a href="#cb383-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb383-5"><a href="#cb383-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb383-6"><a href="#cb383-6" aria-hidden="true" tabindex="-1"></a><span class="co">        model: Neural network (replicated on each GPU)</span></span>
<span id="cb383-7"><a href="#cb383-7" aria-hidden="true" tabindex="-1"></a><span class="co">        batch: Local mini-batch for this GPU</span></span>
<span id="cb383-8"><a href="#cb383-8" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: Optimizer instance</span></span>
<span id="cb383-9"><a href="#cb383-9" aria-hidden="true" tabindex="-1"></a><span class="co">        world_size: Total number of GPUs</span></span>
<span id="cb383-10"><a href="#cb383-10" aria-hidden="true" tabindex="-1"></a><span class="co">        rank: This GPU&#39;s rank (0 to world_size-1)</span></span>
<span id="cb383-11"><a href="#cb383-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb383-12"><a href="#cb383-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Forward pass (local computation)</span></span>
<span id="cb383-13"><a href="#cb383-13" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(batch.<span class="bu">input</span>)</span>
<span id="cb383-14"><a href="#cb383-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_loss(output, batch.target)</span>
<span id="cb383-15"><a href="#cb383-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb383-16"><a href="#cb383-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Backward pass (local gradients)</span></span>
<span id="cb383-17"><a href="#cb383-17" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb383-18"><a href="#cb383-18" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb383-19"><a href="#cb383-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb383-20"><a href="#cb383-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Synchronize gradients across all GPUs</span></span>
<span id="cb383-21"><a href="#cb383-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb383-22"><a href="#cb383-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb383-23"><a href="#cb383-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># AllReduce: sum gradients, then average</span></span>
<span id="cb383-24"><a href="#cb383-24" aria-hidden="true" tabindex="-1"></a>            all_reduce(param.grad, op<span class="op">=</span>SUM)</span>
<span id="cb383-25"><a href="#cb383-25" aria-hidden="true" tabindex="-1"></a>            param.grad <span class="op">/=</span> world_size</span>
<span id="cb383-26"><a href="#cb383-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb383-27"><a href="#cb383-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Update weights (now all GPUs have same gradients ‚Üí same weights)</span></span>
<span id="cb383-28"><a href="#cb383-28" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb383-29"><a href="#cb383-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb383-30"><a href="#cb383-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span></code></pre></div>
            <h3
            id="problem-2-implementing-allreduce-with-sendrecv">Problem
            2: Implementing AllReduce with Send/Recv</h3>
            <p><strong>Task</strong>: Implement Ring AllReduce using
            point-to-point communication.</p>
            <div class="sourceCode" id="cb384"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb384-1"><a href="#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ring_allreduce(tensor, world_size, rank):</span>
<span id="cb384-2"><a href="#cb384-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb384-3"><a href="#cb384-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Ring AllReduce implementation.</span></span>
<span id="cb384-4"><a href="#cb384-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb384-5"><a href="#cb384-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb384-6"><a href="#cb384-6" aria-hidden="true" tabindex="-1"></a><span class="co">        tensor: Local tensor to reduce (will be modified in-place)</span></span>
<span id="cb384-7"><a href="#cb384-7" aria-hidden="true" tabindex="-1"></a><span class="co">        world_size: Number of processes</span></span>
<span id="cb384-8"><a href="#cb384-8" aria-hidden="true" tabindex="-1"></a><span class="co">        rank: This process&#39;s rank</span></span>
<span id="cb384-9"><a href="#cb384-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb384-10"><a href="#cb384-10" aria-hidden="true" tabindex="-1"></a>    chunk_size <span class="op">=</span> <span class="bu">len</span>(tensor) <span class="op">//</span> world_size</span>
<span id="cb384-11"><a href="#cb384-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb384-12"><a href="#cb384-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split tensor into chunks</span></span>
<span id="cb384-13"><a href="#cb384-13" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> [tensor[i<span class="op">*</span>chunk_size : (i<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>chunk_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(world_size)]</span>
<span id="cb384-14"><a href="#cb384-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb384-15"><a href="#cb384-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Phase 1: Reduce-Scatter</span></span>
<span id="cb384-16"><a href="#cb384-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each process ends up with one fully-reduced chunk</span></span>
<span id="cb384-17"><a href="#cb384-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(world_size <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb384-18"><a href="#cb384-18" aria-hidden="true" tabindex="-1"></a>        send_idx <span class="op">=</span> (rank <span class="op">-</span> step) <span class="op">%</span> world_size</span>
<span id="cb384-19"><a href="#cb384-19" aria-hidden="true" tabindex="-1"></a>        recv_idx <span class="op">=</span> (rank <span class="op">-</span> step <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-20"><a href="#cb384-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb384-21"><a href="#cb384-21" aria-hidden="true" tabindex="-1"></a>        send_to <span class="op">=</span> (rank <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-22"><a href="#cb384-22" aria-hidden="true" tabindex="-1"></a>        recv_from <span class="op">=</span> (rank <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-23"><a href="#cb384-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb384-24"><a href="#cb384-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Send chunk[send_idx] to next, receive into temp</span></span>
<span id="cb384-25"><a href="#cb384-25" aria-hidden="true" tabindex="-1"></a>        send_async(chunks[send_idx], dest<span class="op">=</span>send_to)</span>
<span id="cb384-26"><a href="#cb384-26" aria-hidden="true" tabindex="-1"></a>        temp <span class="op">=</span> recv(source<span class="op">=</span>recv_from)</span>
<span id="cb384-27"><a href="#cb384-27" aria-hidden="true" tabindex="-1"></a>        wait_all()</span>
<span id="cb384-28"><a href="#cb384-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb384-29"><a href="#cb384-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accumulate received chunk</span></span>
<span id="cb384-30"><a href="#cb384-30" aria-hidden="true" tabindex="-1"></a>        chunks[recv_idx] <span class="op">+=</span> temp</span>
<span id="cb384-31"><a href="#cb384-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb384-32"><a href="#cb384-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Phase 2: AllGather</span></span>
<span id="cb384-33"><a href="#cb384-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Distribute reduced chunks to all processes</span></span>
<span id="cb384-34"><a href="#cb384-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(world_size <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb384-35"><a href="#cb384-35" aria-hidden="true" tabindex="-1"></a>        send_idx <span class="op">=</span> (rank <span class="op">-</span> step <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-36"><a href="#cb384-36" aria-hidden="true" tabindex="-1"></a>        recv_idx <span class="op">=</span> (rank <span class="op">-</span> step) <span class="op">%</span> world_size</span>
<span id="cb384-37"><a href="#cb384-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb384-38"><a href="#cb384-38" aria-hidden="true" tabindex="-1"></a>        send_to <span class="op">=</span> (rank <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-39"><a href="#cb384-39" aria-hidden="true" tabindex="-1"></a>        recv_from <span class="op">=</span> (rank <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> world_size</span>
<span id="cb384-40"><a href="#cb384-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb384-41"><a href="#cb384-41" aria-hidden="true" tabindex="-1"></a>        send_async(chunks[send_idx], dest<span class="op">=</span>send_to)</span>
<span id="cb384-42"><a href="#cb384-42" aria-hidden="true" tabindex="-1"></a>        chunks[recv_idx] <span class="op">=</span> recv(source<span class="op">=</span>recv_from)</span>
<span id="cb384-43"><a href="#cb384-43" aria-hidden="true" tabindex="-1"></a>        wait_all()</span>
<span id="cb384-44"><a href="#cb384-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb384-45"><a href="#cb384-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruct tensor from chunks</span></span>
<span id="cb384-46"><a href="#cb384-46" aria-hidden="true" tabindex="-1"></a>    tensor[:] <span class="op">=</span> concatenate(chunks)</span>
<span id="cb384-47"><a href="#cb384-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tensor</span></code></pre></div>
            <h3 id="problem-3-gradient-accumulation">Problem 3: Gradient
            Accumulation</h3>
            <p><strong>Task</strong>: Implement gradient accumulation
            for effective large batch training.</p>
            <div class="sourceCode" id="cb385"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_with_gradient_accumulation(</span>
<span id="cb385-2"><a href="#cb385-2" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb385-3"><a href="#cb385-3" aria-hidden="true" tabindex="-1"></a>    dataloader, </span>
<span id="cb385-4"><a href="#cb385-4" aria-hidden="true" tabindex="-1"></a>    optimizer, </span>
<span id="cb385-5"><a href="#cb385-5" aria-hidden="true" tabindex="-1"></a>    accumulation_steps<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb385-6"><a href="#cb385-6" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span><span class="fl">1.0</span></span>
<span id="cb385-7"><a href="#cb385-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb385-8"><a href="#cb385-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb385-9"><a href="#cb385-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Training with gradient accumulation.</span></span>
<span id="cb385-10"><a href="#cb385-10" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb385-11"><a href="#cb385-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Effective batch size = micro_batch_size * accumulation_steps</span></span>
<span id="cb385-12"><a href="#cb385-12" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb385-13"><a href="#cb385-13" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb385-14"><a href="#cb385-14" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb385-15"><a href="#cb385-15" aria-hidden="true" tabindex="-1"></a>    accumulated_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb385-16"><a href="#cb385-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb385-17"><a href="#cb385-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb385-18"><a href="#cb385-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb385-19"><a href="#cb385-19" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch.<span class="bu">input</span>)</span>
<span id="cb385-20"><a href="#cb385-20" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> compute_loss(output, batch.target)</span>
<span id="cb385-21"><a href="#cb385-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb385-22"><a href="#cb385-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale loss by accumulation steps</span></span>
<span id="cb385-23"><a href="#cb385-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (so mean over effective batch is correct)</span></span>
<span id="cb385-24"><a href="#cb385-24" aria-hidden="true" tabindex="-1"></a>        scaled_loss <span class="op">=</span> loss <span class="op">/</span> accumulation_steps</span>
<span id="cb385-25"><a href="#cb385-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb385-26"><a href="#cb385-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass (gradients accumulate)</span></span>
<span id="cb385-27"><a href="#cb385-27" aria-hidden="true" tabindex="-1"></a>        scaled_loss.backward()</span>
<span id="cb385-28"><a href="#cb385-28" aria-hidden="true" tabindex="-1"></a>        accumulated_loss <span class="op">+=</span> loss.item()</span>
<span id="cb385-29"><a href="#cb385-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb385-30"><a href="#cb385-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights every accumulation_steps</span></span>
<span id="cb385-31"><a href="#cb385-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb385-32"><a href="#cb385-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Gradient clipping</span></span>
<span id="cb385-33"><a href="#cb385-33" aria-hidden="true" tabindex="-1"></a>            grad_norm <span class="op">=</span> clip_grad_norm(model.parameters(), max_grad_norm)</span>
<span id="cb385-34"><a href="#cb385-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb385-35"><a href="#cb385-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Optimizer step</span></span>
<span id="cb385-36"><a href="#cb385-36" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb385-37"><a href="#cb385-37" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb385-38"><a href="#cb385-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb385-39"><a href="#cb385-39" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>step<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>accumulated_loss<span class="sc">:.4f}</span><span class="ss">, Grad norm: </span><span class="sc">{</span>grad_norm<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb385-40"><a href="#cb385-40" aria-hidden="true" tabindex="-1"></a>            accumulated_loss <span class="op">=</span> <span class="fl">0.0</span></span></code></pre></div>
            <h3 id="problem-4-computing-global-batch-statistics">Problem
            4: Computing Global Batch Statistics</h3>
            <p><strong>Task</strong>: Compute mean and variance across
            all workers for batch normalization.</p>
            <div class="sourceCode" id="cb386"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb386-1"><a href="#cb386-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> distributed_batch_norm_stats(x, world_size):</span>
<span id="cb386-2"><a href="#cb386-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb386-3"><a href="#cb386-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute global mean and variance across all GPUs.</span></span>
<span id="cb386-4"><a href="#cb386-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb386-5"><a href="#cb386-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb386-6"><a href="#cb386-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Local activations [batch, channels, height, width]</span></span>
<span id="cb386-7"><a href="#cb386-7" aria-hidden="true" tabindex="-1"></a><span class="co">        world_size: Number of GPUs</span></span>
<span id="cb386-8"><a href="#cb386-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb386-9"><a href="#cb386-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb386-10"><a href="#cb386-10" aria-hidden="true" tabindex="-1"></a><span class="co">        global_mean, global_var: Statistics across all GPUs</span></span>
<span id="cb386-11"><a href="#cb386-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb386-12"><a href="#cb386-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Local statistics</span></span>
<span id="cb386-13"><a href="#cb386-13" aria-hidden="true" tabindex="-1"></a>    local_sum <span class="op">=</span> x.<span class="bu">sum</span>(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))      <span class="co"># [channels]</span></span>
<span id="cb386-14"><a href="#cb386-14" aria-hidden="true" tabindex="-1"></a>    local_sq_sum <span class="op">=</span> (x <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(dim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb386-15"><a href="#cb386-15" aria-hidden="true" tabindex="-1"></a>    local_count <span class="op">=</span> x.shape[<span class="dv">0</span>] <span class="op">*</span> x.shape[<span class="dv">2</span>] <span class="op">*</span> x.shape[<span class="dv">3</span>]</span>
<span id="cb386-16"><a href="#cb386-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb386-17"><a href="#cb386-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pack for single AllReduce (more efficient)</span></span>
<span id="cb386-18"><a href="#cb386-18" aria-hidden="true" tabindex="-1"></a>    stats <span class="op">=</span> torch.stack([local_sum, local_sq_sum, torch.tensor([local_count])])</span>
<span id="cb386-19"><a href="#cb386-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb386-20"><a href="#cb386-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AllReduce to get global sums</span></span>
<span id="cb386-21"><a href="#cb386-21" aria-hidden="true" tabindex="-1"></a>    all_reduce(stats, op<span class="op">=</span>SUM)</span>
<span id="cb386-22"><a href="#cb386-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb386-23"><a href="#cb386-23" aria-hidden="true" tabindex="-1"></a>    global_sum <span class="op">=</span> stats[<span class="dv">0</span>]</span>
<span id="cb386-24"><a href="#cb386-24" aria-hidden="true" tabindex="-1"></a>    global_sq_sum <span class="op">=</span> stats[<span class="dv">1</span>]</span>
<span id="cb386-25"><a href="#cb386-25" aria-hidden="true" tabindex="-1"></a>    global_count <span class="op">=</span> stats[<span class="dv">2</span>].item() <span class="op">*</span> world_size</span>
<span id="cb386-26"><a href="#cb386-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb386-27"><a href="#cb386-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute global statistics</span></span>
<span id="cb386-28"><a href="#cb386-28" aria-hidden="true" tabindex="-1"></a>    global_mean <span class="op">=</span> global_sum <span class="op">/</span> global_count</span>
<span id="cb386-29"><a href="#cb386-29" aria-hidden="true" tabindex="-1"></a>    global_var <span class="op">=</span> (global_sq_sum <span class="op">/</span> global_count) <span class="op">-</span> global_mean <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb386-30"><a href="#cb386-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb386-31"><a href="#cb386-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> global_mean, global_var</span></code></pre></div>
            <h3 id="problem-5-sharding-a-weight-matrix">Problem 5:
            Sharding a Weight Matrix</h3>
            <p><strong>Task</strong>: Shard a weight matrix across GPUs
            for tensor parallelism.</p>
            <div class="sourceCode" id="cb387"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb387-1"><a href="#cb387-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> column_parallel_linear(x, weight, bias, world_size, rank):</span>
<span id="cb387-2"><a href="#cb387-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb387-3"><a href="#cb387-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Column-parallel linear layer.</span></span>
<span id="cb387-4"><a href="#cb387-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb387-5"><a href="#cb387-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Full computation: Y = X @ W + b</span></span>
<span id="cb387-6"><a href="#cb387-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Sharded: Each GPU computes Y_i = X @ W_i where W = [W_0 | W_1 | ... | W_n]</span></span>
<span id="cb387-7"><a href="#cb387-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb387-8"><a href="#cb387-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb387-9"><a href="#cb387-9" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input [batch, in_features] (replicated on all GPUs)</span></span>
<span id="cb387-10"><a href="#cb387-10" aria-hidden="true" tabindex="-1"></a><span class="co">        weight: Local weight shard [in_features, out_features // world_size]</span></span>
<span id="cb387-11"><a href="#cb387-11" aria-hidden="true" tabindex="-1"></a><span class="co">        bias: Local bias shard [out_features // world_size]</span></span>
<span id="cb387-12"><a href="#cb387-12" aria-hidden="true" tabindex="-1"></a><span class="co">        world_size: Number of GPUs</span></span>
<span id="cb387-13"><a href="#cb387-13" aria-hidden="true" tabindex="-1"></a><span class="co">        rank: This GPU&#39;s rank</span></span>
<span id="cb387-14"><a href="#cb387-14" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb387-15"><a href="#cb387-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each GPU computes its portion of the output</span></span>
<span id="cb387-16"><a href="#cb387-16" aria-hidden="true" tabindex="-1"></a>    local_output <span class="op">=</span> x <span class="op">@</span> weight <span class="op">+</span> bias</span>
<span id="cb387-17"><a href="#cb387-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> local_output  <span class="co"># Shape: [batch, out_features // world_size]</span></span>
<span id="cb387-18"><a href="#cb387-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb387-19"><a href="#cb387-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb387-20"><a href="#cb387-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> row_parallel_linear(x, weight, world_size, rank):</span>
<span id="cb387-21"><a href="#cb387-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb387-22"><a href="#cb387-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Row-parallel linear layer (pairs with column-parallel).</span></span>
<span id="cb387-23"><a href="#cb387-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Input is sharded, output is AllReduced to be replicated.</span></span>
<span id="cb387-24"><a href="#cb387-24" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb387-25"><a href="#cb387-25" aria-hidden="true" tabindex="-1"></a>    partial_output <span class="op">=</span> x <span class="op">@</span> weight</span>
<span id="cb387-26"><a href="#cb387-26" aria-hidden="true" tabindex="-1"></a>    all_reduce(partial_output, op<span class="op">=</span>SUM)</span>
<span id="cb387-27"><a href="#cb387-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> partial_output</span></code></pre></div>
            <h3 id="problem-6-mixed-precision-training-step">Problem 6:
            Mixed Precision Training Step</h3>
            <p><strong>Task</strong>: Implement a training step with
            automatic mixed precision.</p>
            <div class="sourceCode" id="cb388"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb388-1"><a href="#cb388-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mixed_precision_step(model, batch, optimizer, scaler, use_fp16<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb388-2"><a href="#cb388-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb388-3"><a href="#cb388-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Training step with automatic mixed precision (AMP).</span></span>
<span id="cb388-4"><a href="#cb388-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb388-5"><a href="#cb388-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb388-6"><a href="#cb388-6" aria-hidden="true" tabindex="-1"></a><span class="co">        model: Neural network</span></span>
<span id="cb388-7"><a href="#cb388-7" aria-hidden="true" tabindex="-1"></a><span class="co">        batch: Training batch (input, target)</span></span>
<span id="cb388-8"><a href="#cb388-8" aria-hidden="true" tabindex="-1"></a><span class="co">        optimizer: Optimizer instance</span></span>
<span id="cb388-9"><a href="#cb388-9" aria-hidden="true" tabindex="-1"></a><span class="co">        scaler: GradScaler for FP16 gradient scaling</span></span>
<span id="cb388-10"><a href="#cb388-10" aria-hidden="true" tabindex="-1"></a><span class="co">        use_fp16: Whether to use FP16 (vs BF16 which needs no scaling)</span></span>
<span id="cb388-11"><a href="#cb388-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb388-12"><a href="#cb388-12" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb388-13"><a href="#cb388-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb388-14"><a href="#cb388-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass in lower precision</span></span>
<span id="cb388-15"><a href="#cb388-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> autocast(enabled<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>torch.float16 <span class="cf">if</span> use_fp16 <span class="cf">else</span> torch.bfloat16):</span>
<span id="cb388-16"><a href="#cb388-16" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch.<span class="bu">input</span>)</span>
<span id="cb388-17"><a href="#cb388-17" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> compute_loss(output, batch.target)</span>
<span id="cb388-18"><a href="#cb388-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb388-19"><a href="#cb388-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> use_fp16:</span>
<span id="cb388-20"><a href="#cb388-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale loss to prevent gradient underflow</span></span>
<span id="cb388-21"><a href="#cb388-21" aria-hidden="true" tabindex="-1"></a>        scaler.scale(loss).backward()</span>
<span id="cb388-22"><a href="#cb388-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb388-23"><a href="#cb388-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Unscale gradients for clipping</span></span>
<span id="cb388-24"><a href="#cb388-24" aria-hidden="true" tabindex="-1"></a>        scaler.unscale_(optimizer)</span>
<span id="cb388-25"><a href="#cb388-25" aria-hidden="true" tabindex="-1"></a>        clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb388-26"><a href="#cb388-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb388-27"><a href="#cb388-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step with scaler (skips if inf/nan detected)</span></span>
<span id="cb388-28"><a href="#cb388-28" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizer)</span>
<span id="cb388-29"><a href="#cb388-29" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb388-30"><a href="#cb388-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb388-31"><a href="#cb388-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># BF16: no scaling needed</span></span>
<span id="cb388-32"><a href="#cb388-32" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb388-33"><a href="#cb388-33" aria-hidden="true" tabindex="-1"></a>        clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb388-34"><a href="#cb388-34" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb388-35"><a href="#cb388-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb388-36"><a href="#cb388-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item()</span></code></pre></div>
            <hr />
            <h2 id="summary-and-key-takeaways">10.6 Summary and Key
            Takeaways</h2>
            <h3 id="the-big-picture-1">The Big Picture</h3>
            <p>ML systems at scale is about managing <strong>three
            fundamental constraints</strong>:</p>
            <pre><code>                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    MEMORY       ‚îÇ
                    ‚îÇ  (GPU VRAM)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ              ‚îÇ              ‚îÇ
              ‚ñº              ‚ñº              ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Params  ‚îÇ    ‚îÇ Compute ‚îÇ    ‚îÇ Comms   ‚îÇ
        ‚îÇ &amp; Grads ‚îÇ    ‚îÇ  Time   ‚îÇ    ‚îÇBandwidth‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Everything is a tradeoff</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 44%" />
            <col style="width: 28%" />
            <col style="width: 28%" />
            </colgroup>
            <thead>
            <tr>
            <th>Technique</th>
            <th>Saves</th>
            <th>Costs</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Mixed Precision (BF16)</strong></td>
            <td>Memory (2√ó), Compute</td>
            <td>Slight precision loss</td>
            </tr>
            <tr>
            <td><strong>Activation Checkpointing</strong></td>
            <td>Memory (~‚àöL)</td>
            <td>Compute (~33% more)</td>
            </tr>
            <tr>
            <td><strong>Gradient Accumulation</strong></td>
            <td>Memory</td>
            <td>Nothing! (just slower)</td>
            </tr>
            <tr>
            <td><strong>ZeRO/FSDP</strong></td>
            <td>Memory (scales with GPUs)</td>
            <td>Communication overhead</td>
            </tr>
            <tr>
            <td><strong>Tensor Parallelism</strong></td>
            <td>Memory (per GPU)</td>
            <td>Communication + complexity</td>
            </tr>
            </tbody>
            </table>
            <h3 id="decision-tree-how-do-i-train-this-model">Decision
            Tree: ‚ÄúHow Do I Train This Model?‚Äù</h3>
            <pre><code>Start: I want to train a model with P parameters
        ‚îÇ
        ‚ñº
    Does P √ó 16 bytes fit on one GPU?
        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
    Yes     No
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚ñº
    ‚îÇ   Can I use multiple GPUs?
    ‚îÇ       ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Yes     No ‚Üí You need bigger GPU(s)
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚ñº
    ‚îÇ   Does P √ó 16 / num_GPUs fit? (ZeRO-3)
    ‚îÇ       ‚îÇ
    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Yes     No ‚Üí Add more GPUs or use CPU offload
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                      ‚îÇ
    ‚ñº                      ‚ñº
Use mixed precision    Use ZeRO-3/FSDP
+ activation ckpt      + activation ckpt
+ grad accumulation    + mixed precision</code></pre>
            <h3 id="cheat-sheet-memory-formula">Cheat Sheet: Memory
            Formula</h3>
            <p>For a model with <span class="math inline">\(P\)</span>
            parameters, training with Adam:</p>
            <p><span class="math display">\[\text{Memory} =
            \underbrace{2P}_{\text{params (BF16)}} +
            \underbrace{2P}_{\text{grads (BF16)}} +
            \underbrace{8P}_{\text{Adam states (FP32)}} +
            \underbrace{\text{Activations}}_{\propto \text{batch} \times
            \text{seq} \times \text{depth}}\]</span></p>
            <p><strong>Rule of thumb</strong>: Training requires
            <strong>~16√ó the parameter memory</strong> plus
            activations.</p>
            <hr />
            <h2
            id="quick-reference-ml-systems-interview-questions">Quick
            Reference: ML Systems Interview Questions</h2>
            <h3 id="spmd-distributed-computing">SPMD &amp; Distributed
            Computing</h3>
            <ul>
            <li>What is SPMD and how does it relate to data
            parallelism?</li>
            <li>Explain the difference between data, tensor, and
            pipeline parallelism</li>
            <li>When would you choose ZeRO/FSDP over standard DDP?</li>
            <li>What are the tradeoffs between different sharding
            strategies?</li>
            </ul>
            <h3 id="communication-collectives-1">Communication
            Collectives</h3>
            <ul>
            <li>Explain AllReduce and its role in data parallel
            training</li>
            <li>When would you use Ring vs Tree AllReduce?</li>
            <li>What is ReduceScatter and when is it used?</li>
            <li>How does overlapping communication with computation
            work?</li>
            </ul>
            <h3 id="numerical-computing">Numerical Computing</h3>
            <ul>
            <li>Why use BF16 instead of FP16 for training?</li>
            <li>Explain loss scaling and when it‚Äôs needed</li>
            <li>How do you implement numerically stable softmax?</li>
            <li>What is the log-sum-exp trick and why is it
            necessary?</li>
            </ul>
            <h3 id="memory-compute">Memory &amp; Compute</h3>
            <ul>
            <li>How would you estimate memory for training a 7B
            model?</li>
            <li>Explain activation checkpointing ‚Äî what‚Äôs the
            tradeoff?</li>
            <li>What‚Äôs the difference between memory-bound and
            compute-bound operations?</li>
            <li>How does gradient accumulation affect memory and
            compute?</li>
            </ul>
            <h3 id="practical-systems">Practical Systems</h3>
            <ul>
            <li>Walk through a distributed training step with gradient
            synchronization</li>
            <li>How would you implement Ring AllReduce from
            scratch?</li>
            <li>Explain how tensor parallelism shards a linear
            layer</li>
            <li>What happens when training encounters inf/nan
            gradients?</li>
            </ul>
            <hr />
            <h1 id="part-11-advanced-transformer-topics">Part 11:
            Advanced Transformer Topics</h1>
            <h2 id="mixture-of-experts-moe">11.1 Mixture of Experts
            (MoE)</h2>
            <h3 id="what-is-moe">What is MoE?</h3>
            <p><strong>Mixture of Experts</strong> is a sparse
            architecture where only a subset of parameters are activated
            for each input. This allows scaling model capacity without
            proportionally scaling compute.</p>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Mixture of Experts Layer                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Input x ‚îÄ‚îÄ‚Üí [Router/Gating Network] ‚îÄ‚îÄ‚Üí Expert weights [0.6, 0.3, 0.1, 0, 0, 0, 0, 0]
‚îÇ                      ‚îÇ                                             ‚îÇ
‚îÇ                      ‚Üì                                             ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ         ‚îÇ   Select Top-K Experts    ‚îÇ                              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îÇ                     ‚îÇ                                              ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚îÇ
‚îÇ         ‚Üì                       ‚Üì                                  ‚îÇ
‚îÇ    [Expert 1]              [Expert 2]    (other experts inactive)  ‚îÇ
‚îÇ         ‚îÇ                       ‚îÇ                                  ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ                     ‚Üì                                              ‚îÇ
‚îÇ              Weighted Sum                                          ‚îÇ
‚îÇ                     ‚Üì                                              ‚îÇ
‚îÇ                 Output                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="the-key-insight-conditional-computation">The Key
            Insight: Conditional Computation</h3>
            <p><strong>Dense model</strong>: Every parameter used for
            every input <strong>Sparse MoE</strong>: Only ~10-25% of
            parameters used per input</p>
            <table>
            <thead>
            <tr>
            <th>Model Type</th>
            <th>Total Params</th>
            <th>Active Params</th>
            <th>Compute</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Dense 7B</td>
            <td>7B</td>
            <td>7B</td>
            <td>7B FLOPs</td>
            </tr>
            <tr>
            <td>MoE 47B (8 experts, top-2)</td>
            <td>47B</td>
            <td>~12B</td>
            <td>~12B FLOPs</td>
            </tr>
            </tbody>
            </table>
            <p><strong>You get capacity of 47B with compute of
            ~12B!</strong></p>
            <h3 id="routergating-mechanism">Router/Gating Mechanism</h3>
            <p>The router decides which experts process each token:</p>
            <p><span class="math display">\[G(x) = \text{softmax}(W_g
            \cdot x)\]</span></p>
            <p><strong>Top-K routing</strong>: Select K experts with
            highest gate values:</p>
            <div class="sourceCode" id="cb392"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb392-1"><a href="#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> top_k_routing(x, router_weights, num_experts<span class="op">=</span><span class="dv">8</span>, top_k<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb392-2"><a href="#cb392-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb392-3"><a href="#cb392-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Route input to top-k experts.</span></span>
<span id="cb392-4"><a href="#cb392-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb392-5"><a href="#cb392-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb392-6"><a href="#cb392-6" aria-hidden="true" tabindex="-1"></a><span class="co">        x: Input tensor [batch, seq_len, hidden]</span></span>
<span id="cb392-7"><a href="#cb392-7" aria-hidden="true" tabindex="-1"></a><span class="co">        router_weights: [hidden, num_experts]</span></span>
<span id="cb392-8"><a href="#cb392-8" aria-hidden="true" tabindex="-1"></a><span class="co">        num_experts: Total number of experts</span></span>
<span id="cb392-9"><a href="#cb392-9" aria-hidden="true" tabindex="-1"></a><span class="co">        top_k: Number of experts to use per token</span></span>
<span id="cb392-10"><a href="#cb392-10" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb392-11"><a href="#cb392-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute router logits</span></span>
<span id="cb392-12"><a href="#cb392-12" aria-hidden="true" tabindex="-1"></a>    router_logits <span class="op">=</span> x <span class="op">@</span> router_weights  <span class="co"># [batch, seq_len, num_experts]</span></span>
<span id="cb392-13"><a href="#cb392-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb392-14"><a href="#cb392-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get top-k experts and their weights</span></span>
<span id="cb392-15"><a href="#cb392-15" aria-hidden="true" tabindex="-1"></a>    top_k_logits, top_k_indices <span class="op">=</span> torch.topk(router_logits, top_k, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb392-16"><a href="#cb392-16" aria-hidden="true" tabindex="-1"></a>    top_k_weights <span class="op">=</span> F.softmax(top_k_logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Renormalize</span></span>
<span id="cb392-17"><a href="#cb392-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb392-18"><a href="#cb392-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> top_k_weights, top_k_indices</span></code></pre></div>
            <h3 id="expert-architecture">Expert Architecture</h3>
            <p>Each expert is typically a standard FFN (same as in
            regular Transformer):</p>
            <div class="sourceCode" id="cb393"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb393-1"><a href="#cb393-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Expert(nn.Module):</span>
<span id="cb393-2"><a href="#cb393-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_dim, ffn_dim):</span>
<span id="cb393-3"><a href="#cb393-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb393-4"><a href="#cb393-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w1 <span class="op">=</span> nn.Linear(hidden_dim, ffn_dim)</span>
<span id="cb393-5"><a href="#cb393-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w2 <span class="op">=</span> nn.Linear(ffn_dim, hidden_dim)</span>
<span id="cb393-6"><a href="#cb393-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> nn.GELU()</span>
<span id="cb393-7"><a href="#cb393-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb393-8"><a href="#cb393-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb393-9"><a href="#cb393-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.w2(<span class="va">self</span>.activation(<span class="va">self</span>.w1(x)))</span></code></pre></div>
            <h3 id="the-load-balancing-problem">The Load Balancing
            Problem</h3>
            <p><strong>Problem</strong>: Router might learn to send all
            tokens to one expert, leaving others unused.</p>
            <p><strong>Solution</strong>: Auxiliary loss to encourage
            balanced expert utilization.</p>
            <p><span class="math display">\[\mathcal{L}_{aux} = \alpha
            \cdot N \sum_{i=1}^{N} f_i \cdot P_i\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(f_i\)</span> = fraction of
            tokens routed to expert <span
            class="math inline">\(i\)</span></li>
            <li><span class="math inline">\(P_i\)</span> = average
            router probability for expert <span
            class="math inline">\(i\)</span></li>
            <li><span class="math inline">\(\alpha\)</span> = auxiliary
            loss weight (typically 0.01)</li>
            </ul>
            <div class="sourceCode" id="cb394"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb394-1"><a href="#cb394-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_balancing_loss(router_probs, expert_indices, num_experts):</span>
<span id="cb394-2"><a href="#cb394-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb394-3"><a href="#cb394-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Auxiliary loss for load balancing.</span></span>
<span id="cb394-4"><a href="#cb394-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb394-5"><a href="#cb394-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb394-6"><a href="#cb394-6" aria-hidden="true" tabindex="-1"></a><span class="co">        router_probs: [batch, seq_len, num_experts] - router softmax outputs</span></span>
<span id="cb394-7"><a href="#cb394-7" aria-hidden="true" tabindex="-1"></a><span class="co">        expert_indices: [batch, seq_len, top_k] - selected expert indices</span></span>
<span id="cb394-8"><a href="#cb394-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb394-9"><a href="#cb394-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fraction of tokens routed to each expert</span></span>
<span id="cb394-10"><a href="#cb394-10" aria-hidden="true" tabindex="-1"></a>    expert_mask <span class="op">=</span> F.one_hot(expert_indices, num_experts).<span class="bu">float</span>()</span>
<span id="cb394-11"><a href="#cb394-11" aria-hidden="true" tabindex="-1"></a>    tokens_per_expert <span class="op">=</span> expert_mask.<span class="bu">sum</span>(dim<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>])  <span class="co"># [num_experts]</span></span>
<span id="cb394-12"><a href="#cb394-12" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> tokens_per_expert <span class="op">/</span> tokens_per_expert.<span class="bu">sum</span>()</span>
<span id="cb394-13"><a href="#cb394-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb394-14"><a href="#cb394-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average router probability for each expert</span></span>
<span id="cb394-15"><a href="#cb394-15" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> router_probs.mean(dim<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># [num_experts]</span></span>
<span id="cb394-16"><a href="#cb394-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb394-17"><a href="#cb394-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Auxiliary loss: penalize imbalance</span></span>
<span id="cb394-18"><a href="#cb394-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> num_experts <span class="op">*</span> (f <span class="op">*</span> P).<span class="bu">sum</span>()</span></code></pre></div>
            <h3 id="token-choice-vs-expert-choice">Token Choice vs
            Expert Choice</h3>
            <p><strong>Token Choice</strong> (Original, Switch
            Transformer):</p>
            <ul>
            <li>Each token picks its top-K experts</li>
            <li>Simple but can cause load imbalance</li>
            </ul>
            <p><strong>Expert Choice</strong> (more recent):</p>
            <ul>
            <li>Each expert picks its top-K tokens</li>
            <li>Guaranteed balanced load</li>
            <li>Tokens may be dropped or duplicated</li>
            </ul>
            <div class="sourceCode" id="cb395"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb395-1"><a href="#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expert_choice_routing(x, router_weights, expert_capacity):</span>
<span id="cb395-2"><a href="#cb395-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb395-3"><a href="#cb395-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Each expert selects its top tokens.</span></span>
<span id="cb395-4"><a href="#cb395-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb395-5"><a href="#cb395-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb395-6"><a href="#cb395-6" aria-hidden="true" tabindex="-1"></a><span class="co">        expert_capacity: Max tokens per expert</span></span>
<span id="cb395-7"><a href="#cb395-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb395-8"><a href="#cb395-8" aria-hidden="true" tabindex="-1"></a>    router_logits <span class="op">=</span> x <span class="op">@</span> router_weights  <span class="co"># [batch*seq, num_experts]</span></span>
<span id="cb395-9"><a href="#cb395-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb395-10"><a href="#cb395-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each expert picks its top tokens</span></span>
<span id="cb395-11"><a href="#cb395-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transpose so experts are rows</span></span>
<span id="cb395-12"><a href="#cb395-12" aria-hidden="true" tabindex="-1"></a>    expert_scores <span class="op">=</span> router_logits.T  <span class="co"># [num_experts, batch*seq]</span></span>
<span id="cb395-13"><a href="#cb395-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb395-14"><a href="#cb395-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each expert selects top-capacity tokens</span></span>
<span id="cb395-15"><a href="#cb395-15" aria-hidden="true" tabindex="-1"></a>    top_scores, top_indices <span class="op">=</span> torch.topk(expert_scores, expert_capacity, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb395-16"><a href="#cb395-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb395-17"><a href="#cb395-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> top_scores, top_indices</span></code></pre></div>
            <h3 id="moe-in-practice-mixtral-deepseek-moe">MoE in
            Practice: Mixtral, DeepSeek-MoE</h3>
            <p><strong>Mixtral 8x7B</strong>:</p>
            <ul>
            <li>8 experts, top-2 routing</li>
            <li>Total: 47B params, Active: ~13B</li>
            <li>Matches or beats LLaMA-2 70B at much lower compute</li>
            </ul>
            <p><strong>DeepSeek-MoE</strong>:</p>
            <ul>
            <li>Fine-grained experts (more, smaller experts)</li>
            <li>Shared experts (some experts always active)</li>
            <li>Better expert specialization</li>
            </ul>
            <h3 id="expert-parallelism">Expert Parallelism</h3>
            <p><strong>How to distribute experts across
            GPUs</strong>:</p>
            <pre><code>Expert Parallelism (EP=4):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
GPU 0: Experts 0, 1
GPU 1: Experts 2, 3
GPU 2: Experts 4, 5
GPU 3: Experts 6, 7

All-to-All communication:

1. Route: Each GPU sends tokens to GPU hosting target expert
2. Compute: Each GPU processes its experts
3. Combine: Each GPU receives results for its tokens</code></pre>
            <p><strong>Communication pattern</strong>: All-to-All
            (unlike AllReduce in data parallelism)</p>
            <h3 id="moe-challenges">MoE Challenges</h3>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 38%" />
            <col style="width: 29%" />
            </colgroup>
            <thead>
            <tr>
            <th>Challenge</th>
            <th>Description</th>
            <th>Solution</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Load imbalance</strong></td>
            <td>Some experts overloaded</td>
            <td>Auxiliary loss, expert choice</td>
            </tr>
            <tr>
            <td><strong>Training instability</strong></td>
            <td>Router can collapse</td>
            <td>Careful init, dropout</td>
            </tr>
            <tr>
            <td><strong>Communication overhead</strong></td>
            <td>All-to-All is expensive</td>
            <td>Expert parallelism, capacity limits</td>
            </tr>
            <tr>
            <td><strong>Memory</strong></td>
            <td>Store all experts</td>
            <td>Expert parallelism</td>
            </tr>
            <tr>
            <td><strong>Dropped tokens</strong></td>
            <td>Capacity overflow</td>
            <td>Expert choice, auxiliary loss</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-how-does-moe-achieve-better-scaling-than-dense-models">Interview
            Q: ‚ÄúHow does MoE achieve better scaling than dense
            models?‚Äù</h3>
            <p><strong>A</strong>: MoE decouples model capacity (total
            parameters) from compute (active parameters). Each input
            token is processed by only K out of N experts (typically 2
            out of 8), so a 47B parameter MoE model uses ~13B FLOPs per
            token ‚Äî similar to a 13B dense model. The key is the
            <strong>router network</strong> that learns to dispatch
            tokens to specialized experts. This enables scaling model
            capacity without proportional compute increase. The main
            challenge is load balancing ‚Äî preventing the router from
            sending all tokens to one expert. This is addressed with
            auxiliary losses that penalize uneven expert
            utilization.</p>
            <h3
            id="interview-q-whats-the-difference-between-token-choice-and-expert-choice-routing">Interview
            Q: ‚ÄúWhat‚Äôs the difference between token choice and expert
            choice routing?‚Äù</h3>
            <p><strong>A</strong>: In <strong>token choice</strong>,
            each token picks its top-K experts ‚Äî simple but can cause
            load imbalance if many tokens pick the same expert. In
            <strong>expert choice</strong>, each expert picks its top-K
            tokens ‚Äî this guarantees balanced load but means some tokens
            might be processed by zero experts (dropped) or multiple
            times. Expert choice is more compute-efficient but may hurt
            quality if important tokens are dropped. Modern
            architectures like DeepSeek-MoE use hybrid approaches with
            shared experts (always active) plus routed experts.</p>
            <hr />
            <h2 id="flash-attention">11.2 Flash Attention</h2>
            <h3 id="the-problem-attention-is-memory-bound">The Problem:
            Attention is Memory-Bound</h3>
            <p>Standard attention materializes the full <span
            class="math inline">\(N \times N\)</span> attention
            matrix:</p>
            <div class="sourceCode" id="cb397"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb397-1"><a href="#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standard_attention(Q, K, V):</span>
<span id="cb397-2"><a href="#cb397-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb397-3"><a href="#cb397-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Standard attention - O(N¬≤) memory!</span></span>
<span id="cb397-4"><a href="#cb397-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb397-5"><a href="#cb397-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Q, K, V: [batch, seq_len, head_dim]</span></span>
<span id="cb397-6"><a href="#cb397-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb397-7"><a href="#cb397-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Compute attention scores - creates N√óN matrix!</span></span>
<span id="cb397-8"><a href="#cb397-8" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> Q <span class="op">@</span> K.T <span class="op">/</span> sqrt(d_k)  <span class="co"># [batch, N, N] ‚Üê O(N¬≤) memory</span></span>
<span id="cb397-9"><a href="#cb397-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb397-10"><a href="#cb397-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Softmax</span></span>
<span id="cb397-11"><a href="#cb397-11" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># [batch, N, N]</span></span>
<span id="cb397-12"><a href="#cb397-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb397-13"><a href="#cb397-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Apply to values</span></span>
<span id="cb397-14"><a href="#cb397-14" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> attn_weights <span class="op">@</span> V  <span class="co"># [batch, N, d]</span></span>
<span id="cb397-15"><a href="#cb397-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb397-16"><a href="#cb397-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
            <p><strong>Memory</strong>: <span
            class="math inline">\(O(N^2)\)</span> for attention matrix
            <strong>Problem</strong>: For N=32K, that‚Äôs 32K √ó 32K √ó 2
            bytes = <strong>2GB per layer per head!</strong></p>
            <h3 id="the-memory-hierarchy">The Memory Hierarchy</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      GPU Memory Hierarchy                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  SRAM (On-chip)     ‚îÇ  ~20 MB      ‚îÇ  ~19 TB/s    ‚îÇ  Very fast     ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  HBM (GPU memory)   ‚îÇ  40-80 GB    ‚îÇ  ~2 TB/s     ‚îÇ  10x slower    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  System RAM         ‚îÇ  ~1 TB       ‚îÇ  ~100 GB/s   ‚îÇ  20x slower    ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  Flash Attention Insight:                                          ‚îÇ
‚îÇ  Recompute &gt; Load from HBM (if SRAM is fast enough)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <p><strong>Key insight</strong>: It‚Äôs faster to
            <strong>recompute</strong> values than to
            <strong>load</strong> them from HBM!</p>
            <h3 id="flash-attention-tiling-recomputation">Flash
            Attention: Tiling + Recomputation</h3>
            <p><strong>Core idea</strong>: Never materialize the full
            N√óN matrix. Process in <strong>tiles</strong> that fit in
            SRAM.</p>
            <pre><code>Standard Attention:                Flash Attention:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Q @ K^T ‚Üí Full N√óN matrix        Process in Br √ó Bc blocks
         ‚Üì                       Never store full matrix
    Softmax(¬∑)                   Online softmax (running max/sum)
         ‚Üì                       
    ¬∑ @ V                        Output computed incrementally</code></pre>
            <h3 id="the-algorithm-1">The Algorithm</h3>
            <div class="sourceCode" id="cb400"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb400-1"><a href="#cb400-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> flash_attention(Q, K, V, block_size_q<span class="op">=</span><span class="dv">64</span>, block_size_kv<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb400-2"><a href="#cb400-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb400-3"><a href="#cb400-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Flash Attention - O(N) memory!</span></span>
<span id="cb400-4"><a href="#cb400-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb400-5"><a href="#cb400-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Process attention in blocks, never materializing full N√óN matrix.</span></span>
<span id="cb400-6"><a href="#cb400-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb400-7"><a href="#cb400-7" aria-hidden="true" tabindex="-1"></a>    N, d <span class="op">=</span> Q.shape</span>
<span id="cb400-8"><a href="#cb400-8" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> torch.zeros_like(Q)</span>
<span id="cb400-9"><a href="#cb400-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb400-10"><a href="#cb400-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Running statistics for online softmax</span></span>
<span id="cb400-11"><a href="#cb400-11" aria-hidden="true" tabindex="-1"></a>    row_max <span class="op">=</span> torch.full((N,), <span class="op">-</span><span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>))  <span class="co"># Running max</span></span>
<span id="cb400-12"><a href="#cb400-12" aria-hidden="true" tabindex="-1"></a>    row_sum <span class="op">=</span> torch.zeros(N)                    <span class="co"># Running sum of exp</span></span>
<span id="cb400-13"><a href="#cb400-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb400-14"><a href="#cb400-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over K, V blocks</span></span>
<span id="cb400-15"><a href="#cb400-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, block_size_kv):</span>
<span id="cb400-16"><a href="#cb400-16" aria-hidden="true" tabindex="-1"></a>        Kj <span class="op">=</span> K[j:j<span class="op">+</span>block_size_kv]</span>
<span id="cb400-17"><a href="#cb400-17" aria-hidden="true" tabindex="-1"></a>        Vj <span class="op">=</span> V[j:j<span class="op">+</span>block_size_kv]</span>
<span id="cb400-18"><a href="#cb400-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb400-19"><a href="#cb400-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Iterate over Q blocks</span></span>
<span id="cb400-20"><a href="#cb400-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, block_size_q):</span>
<span id="cb400-21"><a href="#cb400-21" aria-hidden="true" tabindex="-1"></a>            Qi <span class="op">=</span> Q[i:i<span class="op">+</span>block_size_q]</span>
<span id="cb400-22"><a href="#cb400-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-23"><a href="#cb400-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute block of attention scores</span></span>
<span id="cb400-24"><a href="#cb400-24" aria-hidden="true" tabindex="-1"></a>            Sij <span class="op">=</span> Qi <span class="op">@</span> Kj.T <span class="op">/</span> sqrt(d)  <span class="co"># Small block, fits in SRAM!</span></span>
<span id="cb400-25"><a href="#cb400-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-26"><a href="#cb400-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Online softmax update</span></span>
<span id="cb400-27"><a href="#cb400-27" aria-hidden="true" tabindex="-1"></a>            block_max <span class="op">=</span> Sij.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).values</span>
<span id="cb400-28"><a href="#cb400-28" aria-hidden="true" tabindex="-1"></a>            new_max <span class="op">=</span> torch.maximum(row_max[i:i<span class="op">+</span>block_size_q], block_max)</span>
<span id="cb400-29"><a href="#cb400-29" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-30"><a href="#cb400-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Rescale previous sum</span></span>
<span id="cb400-31"><a href="#cb400-31" aria-hidden="true" tabindex="-1"></a>            exp_diff <span class="op">=</span> torch.exp(row_max[i:i<span class="op">+</span>block_size_q] <span class="op">-</span> new_max)</span>
<span id="cb400-32"><a href="#cb400-32" aria-hidden="true" tabindex="-1"></a>            row_sum[i:i<span class="op">+</span>block_size_q] <span class="op">*=</span> exp_diff</span>
<span id="cb400-33"><a href="#cb400-33" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-34"><a href="#cb400-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Add new block contribution</span></span>
<span id="cb400-35"><a href="#cb400-35" aria-hidden="true" tabindex="-1"></a>            Pij <span class="op">=</span> torch.exp(Sij <span class="op">-</span> new_max.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb400-36"><a href="#cb400-36" aria-hidden="true" tabindex="-1"></a>            row_sum[i:i<span class="op">+</span>block_size_q] <span class="op">+=</span> Pij.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb400-37"><a href="#cb400-37" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-38"><a href="#cb400-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update output with rescaling</span></span>
<span id="cb400-39"><a href="#cb400-39" aria-hidden="true" tabindex="-1"></a>            output[i:i<span class="op">+</span>block_size_q] <span class="op">*=</span> exp_diff.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb400-40"><a href="#cb400-40" aria-hidden="true" tabindex="-1"></a>            output[i:i<span class="op">+</span>block_size_q] <span class="op">+=</span> Pij <span class="op">@</span> Vj</span>
<span id="cb400-41"><a href="#cb400-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb400-42"><a href="#cb400-42" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update running max</span></span>
<span id="cb400-43"><a href="#cb400-43" aria-hidden="true" tabindex="-1"></a>            row_max[i:i<span class="op">+</span>block_size_q] <span class="op">=</span> new_max</span>
<span id="cb400-44"><a href="#cb400-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb400-45"><a href="#cb400-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Final normalization</span></span>
<span id="cb400-46"><a href="#cb400-46" aria-hidden="true" tabindex="-1"></a>    output <span class="op">/=</span> row_sum.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb400-47"><a href="#cb400-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
            <h3 id="online-softmax-the-key-trick">Online Softmax: The
            Key Trick</h3>
            <p><strong>Standard softmax</strong> requires two
            passes:</p>
            <ol type="1">
            <li>Find max (for numerical stability)</li>
            <li>Compute exp(x - max) / sum</li>
            </ol>
            <p><strong>Online softmax</strong> maintains running
            statistics:</p>
            <p><span class="math display">\[m_{new} = \max(m_{old},
            \max(x_{block}))\]</span></p>
            <p><span class="math display">\[l_{new} = e^{m_{old} -
            m_{new}} \cdot l_{old} + \sum e^{x_{block} -
            m_{new}}\]</span></p>
            <p>This allows processing blocks sequentially without
            storing all values!</p>
            <h3 id="io-complexity-analysis">IO Complexity Analysis</h3>
            <p><strong>Standard Attention</strong>:</p>
            <ul>
            <li>Read Q, K, V: <span class="math inline">\(O(Nd)\)</span>
            from HBM</li>
            <li>Write attention matrix: <span
            class="math inline">\(O(N^2)\)</span> to HBM</li>
            <li>Read attention matrix: <span
            class="math inline">\(O(N^2)\)</span> from HBM</li>
            <li><strong>Total HBM access</strong>: <span
            class="math inline">\(O(N^2)\)</span></li>
            </ul>
            <p><strong>Flash Attention</strong>:</p>
            <ul>
            <li>Read Q, K, V in blocks: <span
            class="math inline">\(O(Nd)\)</span></li>
            <li>Keep intermediate in SRAM</li>
            <li>Write output: <span
            class="math inline">\(O(Nd)\)</span></li>
            <li><strong>Total HBM access</strong>: <span
            class="math inline">\(O(Nd)\)</span> ‚Äî <strong>linear, not
            quadratic!</strong></li>
            </ul>
            <h3 id="flash-attention-2-improvements">Flash Attention 2
            Improvements</h3>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>Improvement</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Better parallelism</strong></td>
            <td>Parallelize over sequence length, not just
            batch/heads</td>
            </tr>
            <tr>
            <td><strong>Reduced non-matmul FLOPs</strong></td>
            <td>Fewer register shuffles</td>
            </tr>
            <tr>
            <td><strong>Better work partitioning</strong></td>
            <td>Balance work across warps</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Speedup</strong>: 2x faster than Flash Attention
            1</p>
            <h3 id="flash-attention-3-hopper-gpus">Flash Attention 3
            (Hopper GPUs)</h3>
            <p>Leverages H100 features:</p>
            <ul>
            <li><strong>TMA</strong> (Tensor Memory Accelerator): Async
            memory loads</li>
            <li><strong>WGMMA</strong>: Warp Group
            Matrix-Multiply-Accumulate</li>
            <li><strong>FP8 support</strong>: Even faster with lower
            precision</li>
            </ul>
            <h3 id="when-flash-attention-helps-most">When Flash
            Attention Helps Most</h3>
            <table>
            <thead>
            <tr>
            <th>Scenario</th>
            <th>Benefit</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Long sequences (&gt;2K)</td>
            <td>Massive memory savings</td>
            </tr>
            <tr>
            <td>Training</td>
            <td>Avoids OOM, enables larger batches</td>
            </tr>
            <tr>
            <td>Memory-bound workloads</td>
            <td>Better HBM utilization</td>
            </tr>
            <tr>
            <td>Multi-query attention</td>
            <td>Same algorithm applies</td>
            </tr>
            </tbody>
            </table>
            <h3 id="code-example-using-flash-attention">Code Example:
            Using Flash Attention</h3>
            <div class="sourceCode" id="cb401"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb401-1"><a href="#cb401-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch 2.0+ has built-in Flash Attention</span></span>
<span id="cb401-2"><a href="#cb401-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb401-3"><a href="#cb401-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-4"><a href="#cb401-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatically uses Flash Attention when possible</span></span>
<span id="cb401-5"><a href="#cb401-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> F.scaled_dot_product_attention(</span>
<span id="cb401-6"><a href="#cb401-6" aria-hidden="true" tabindex="-1"></a>    query, key, value,</span>
<span id="cb401-7"><a href="#cb401-7" aria-hidden="true" tabindex="-1"></a>    attn_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb401-8"><a href="#cb401-8" aria-hidden="true" tabindex="-1"></a>    dropout_p<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb401-9"><a href="#cb401-9" aria-hidden="true" tabindex="-1"></a>    is_causal<span class="op">=</span><span class="va">True</span>  <span class="co"># For decoder/autoregressive models</span></span>
<span id="cb401-10"><a href="#cb401-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb401-11"><a href="#cb401-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-12"><a href="#cb401-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Or use the flash-attn library directly</span></span>
<span id="cb401-13"><a href="#cb401-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flash_attn <span class="im">import</span> flash_attn_func</span>
<span id="cb401-14"><a href="#cb401-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb401-15"><a href="#cb401-15" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> flash_attn_func(q, k, v, causal<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
            <h3
            id="interview-q-why-is-flash-attention-faster-despite-doing-more-flops">Interview
            Q: ‚ÄúWhy is Flash Attention faster despite doing more
            FLOPs?‚Äù</h3>
            <p><strong>A</strong>: Flash Attention is
            <strong>memory-bound</strong>, not compute-bound. Standard
            attention reads/writes the full N√óN attention matrix to GPU
            HBM (2 TB/s bandwidth). Flash Attention tiles the
            computation to fit in SRAM (19 TB/s) and <strong>never
            materializes</strong> the full matrix. Though it does
            slightly more FLOPs (due to recomputation in backward pass),
            it does far fewer memory accesses. For N=4K with d=64, this
            is ~10x fewer HBM accesses, making it 2-4x faster despite
            ~25% more FLOPs. The key insight is that memory bandwidth,
            not compute, is the bottleneck for attention.</p>
            <h3
            id="interview-q-how-does-flash-attention-handle-the-backward-pass">Interview
            Q: ‚ÄúHow does Flash Attention handle the backward pass?‚Äù</h3>
            <p><strong>A</strong>: The backward pass requires the
            attention weights, which Flash Attention doesn‚Äôt store.
            Instead, it <strong>recomputes</strong> them during backward
            ‚Äî this is the ‚Äúrecomputation‚Äù part. Specifically: (1) reload
            Q, K, V blocks, (2) recompute attention scores in blocks,
            (3) compute gradients using the recomputed values. This
            trades compute for memory ‚Äî recomputation is fast because
            it‚Äôs done in SRAM, while storing the N√óN matrix would
            require expensive HBM access. The total memory is O(N)
            instead of O(N¬≤).</p>
            <hr />
            <h2 id="kv-cache-and-inference-optimization">11.3 KV-Cache
            and Inference Optimization</h2>
            <h3 id="the-problem-autoregressive-generation-is-slow">The
            Problem: Autoregressive Generation is Slow</h3>
            <pre><code>Generating &quot;The cat sat on the mat&quot;:

Step 1: Process &quot;The&quot;           ‚Üí Predict &quot;cat&quot;     [1 token]
Step 2: Process &quot;The cat&quot;       ‚Üí Predict &quot;sat&quot;     [2 tokens] 
Step 3: Process &quot;The cat sat&quot;   ‚Üí Predict &quot;on&quot;      [3 tokens]
...
Step 6: Process &quot;The cat sat on the&quot; ‚Üí Predict &quot;mat&quot; [6 tokens]

Without KV-cache: Recompute attention for ALL tokens at every step!</code></pre>
            <h3 id="kv-cache-cache-past-key-values">KV-Cache: Cache Past
            Key-Values</h3>
            <p><strong>Key insight</strong>: For autoregressive
            generation, past tokens‚Äô K and V don‚Äôt change. Cache
            them!</p>
            <pre><code>With KV-Cache:

Step 1: Compute K‚ÇÅ, V‚ÇÅ for &quot;The&quot;       ‚Üí Cache [K‚ÇÅ], [V‚ÇÅ]
Step 2: Compute K‚ÇÇ, V‚ÇÇ for &quot;cat&quot;       ‚Üí Cache [K‚ÇÅ, K‚ÇÇ], [V‚ÇÅ, V‚ÇÇ]
Step 3: Compute K‚ÇÉ, V‚ÇÉ for &quot;sat&quot;       ‚Üí Cache [K‚ÇÅ, K‚ÇÇ, K‚ÇÉ], [V‚ÇÅ, V‚ÇÇ, V‚ÇÉ]
...

At each step: Only compute NEW token&#39;s Q, K, V
              Load cached K, V for attention
              Append new K, V to cache</code></pre>
            <h3 id="implementation-1">Implementation</h3>
            <div class="sourceCode" id="cb404"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb404-1"><a href="#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CachedAttention(nn.Module):</span>
<span id="cb404-2"><a href="#cb404-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_dim, num_heads):</span>
<span id="cb404-3"><a href="#cb404-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb404-4"><a href="#cb404-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb404-5"><a href="#cb404-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> hidden_dim <span class="op">//</span> num_heads</span>
<span id="cb404-6"><a href="#cb404-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Wq <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb404-7"><a href="#cb404-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Wk <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb404-8"><a href="#cb404-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Wv <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb404-9"><a href="#cb404-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Wo <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb404-10"><a href="#cb404-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb404-11"><a href="#cb404-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, kv_cache<span class="op">=</span><span class="va">None</span>, use_cache<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb404-12"><a href="#cb404-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb404-13"><a href="#cb404-13" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb404-14"><a href="#cb404-14" aria-hidden="true" tabindex="-1"></a><span class="co">            x: [batch, seq_len, hidden] - for prefill, full sequence</span></span>
<span id="cb404-15"><a href="#cb404-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb404-16"><a href="#cb404-16" aria-hidden="true" tabindex="-1"></a><span class="co">                                         - for decode, just new token</span></span>
<span id="cb404-17"><a href="#cb404-17" aria-hidden="true" tabindex="-1"></a><span class="co">            kv_cache: (cached_k, cached_v) or None</span></span>
<span id="cb404-18"><a href="#cb404-18" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb404-19"><a href="#cb404-19" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb404-20"><a href="#cb404-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb404-21"><a href="#cb404-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Project to Q, K, V</span></span>
<span id="cb404-22"><a href="#cb404-22" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.Wq(x).view(batch_size, seq_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb404-23"><a href="#cb404-23" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.Wk(x).view(batch_size, seq_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb404-24"><a href="#cb404-24" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.Wv(x).view(batch_size, seq_len, <span class="va">self</span>.num_heads, <span class="va">self</span>.head_dim)</span>
<span id="cb404-25"><a href="#cb404-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb404-26"><a href="#cb404-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append to cache</span></span>
<span id="cb404-27"><a href="#cb404-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> kv_cache <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb404-28"><a href="#cb404-28" aria-hidden="true" tabindex="-1"></a>            cached_k, cached_v <span class="op">=</span> kv_cache</span>
<span id="cb404-29"><a href="#cb404-29" aria-hidden="true" tabindex="-1"></a>            k <span class="op">=</span> torch.cat([cached_k, k], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb404-30"><a href="#cb404-30" aria-hidden="true" tabindex="-1"></a>            v <span class="op">=</span> torch.cat([cached_v, v], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb404-31"><a href="#cb404-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb404-32"><a href="#cb404-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Standard attention</span></span>
<span id="cb404-33"><a href="#cb404-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q: [batch, new_seq, heads, head_dim]</span></span>
<span id="cb404-34"><a href="#cb404-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># K, V: [batch, total_seq, heads, head_dim]</span></span>
<span id="cb404-35"><a href="#cb404-35" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> scaled_dot_product_attention(q, k, v)</span>
<span id="cb404-36"><a href="#cb404-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb404-37"><a href="#cb404-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.Wo(attn_output.view(batch_size, seq_len, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb404-38"><a href="#cb404-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb404-39"><a href="#cb404-39" aria-hidden="true" tabindex="-1"></a>        new_cache <span class="op">=</span> (k, v) <span class="cf">if</span> use_cache <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb404-40"><a href="#cb404-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, new_cache</span></code></pre></div>
            <h3 id="kv-cache-memory-analysis">KV-Cache Memory
            Analysis</h3>
            <p><strong>Memory per token per layer</strong>: <span
            class="math display">\[\text{KV memory} = 2 \times
            \text{num\_heads} \times \text{head\_dim} \times
            \text{bytes}\]</span></p>
            <p><strong>For LLaMA-70B</strong> (80 layers, 64 heads,
            head_dim=128, BF16): <span class="math display">\[\text{Per
            token} = 2 \times 64 \times 128 \times 2 \times 80 = 2.6
            \text{ MB}\]</span></p>
            <p><strong>For 4K context</strong>: <span
            class="math display">\[\text{Total KV cache} = 4096 \times
            2.6 \text{ MB} \approx 10 \text{ GB}\]</span></p>
            <p><strong>For 128K context</strong> (Claude, GPT-4): <span
            class="math display">\[\text{Total KV cache} = 131072 \times
            2.6 \text{ MB} \approx 340 \text{ GB!}\]</span></p>
            <h3 id="the-memory-problem-at-scale">The Memory Problem at
            Scale</h3>
            <pre><code>KV-Cache Memory Scaling:

Context     7B Model    70B Model
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
2K          0.5 GB      5 GB
8K          2 GB        20 GB
32K         8 GB        80 GB
128K        32 GB       320 GB    ‚Üê Doesn&#39;t fit on single GPU!</code></pre>
            <h3 id="solution-1-multi-query-attention-mqa">Solution 1:
            Multi-Query Attention (MQA)</h3>
            <p><strong>Idea</strong>: Share K, V across all query
            heads.</p>
            <pre><code>Multi-Head Attention (MHA):          Multi-Query Attention (MQA):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Q: [batch, heads, seq, d]           Q: [batch, heads, seq, d]  
K: [batch, heads, seq, d]           K: [batch, 1, seq, d]      ‚Üê Shared!
V: [batch, heads, seq, d]           V: [batch, 1, seq, d]      ‚Üê Shared!

KV memory: 2 √ó heads √ó seq √ó d      KV memory: 2 √ó 1 √ó seq √ó d</code></pre>
            <p><strong>Memory savings</strong>: <code>num_heads</code> √ó
            smaller KV cache! <strong>Tradeoff</strong>: Slight quality
            degradation</p>
            <h3 id="solution-2-grouped-query-attention-gqa">Solution 2:
            Grouped-Query Attention (GQA)</h3>
            <p><strong>Compromise</strong>: Share K, V among groups of
            heads (not all).</p>
            <pre><code>GQA with 8 heads, 2 KV groups:

Q heads:  [Q0, Q1, Q2, Q3, Q4, Q5, Q6, Q7]
                   ‚Üì           ‚Üì
KV groups: [K0, V0]        [K1, V1]
           (shared by      (shared by
            Q0-Q3)          Q4-Q7)</code></pre>
            <table>
            <thead>
            <tr>
            <th>Method</th>
            <th>KV Heads</th>
            <th>Memory Reduction</th>
            <th>Quality</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>MHA</strong></td>
            <td>num_heads</td>
            <td>1√ó (baseline)</td>
            <td>Best</td>
            </tr>
            <tr>
            <td><strong>GQA</strong></td>
            <td>num_heads / groups</td>
            <td>groups√ó</td>
            <td>Good</td>
            </tr>
            <tr>
            <td><strong>MQA</strong></td>
            <td>1</td>
            <td>num_heads√ó</td>
            <td>Acceptable</td>
            </tr>
            </tbody>
            </table>
            <p><strong>LLaMA-2 70B uses GQA</strong> (8 KV heads for 64
            query heads = 8√ó reduction)</p>
            <h3 id="solution-3-paged-attention-vllm">Solution 3: Paged
            Attention (vLLM)</h3>
            <p><strong>Problem</strong>: KV cache is pre-allocated for
            max sequence length ‚Üí memory waste</p>
            <p><strong>Solution</strong>: Manage KV cache like virtual
            memory ‚Äî allocate pages on demand.</p>
            <pre><code>Traditional:                    Paged Attention:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Pre-allocate for max_seq        Allocate pages on demand
[‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ]          [Page 0][Page 1][Page 2]...
   ‚Üë                                   
   Wasted space if seq &lt; max    Pages can be non-contiguous
                                Share pages across requests</code></pre>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li>Near-zero memory waste</li>
            <li>Support for longer contexts</li>
            <li>Better batching (pack more requests)</li>
            </ul>
            <h3 id="continuous-batching">Continuous Batching</h3>
            <p><strong>Static batching</strong>: Wait for all requests
            in batch to finish</p>
            <pre><code>Static Batching:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Request A: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Request B: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚Üê Waiting
Request C: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Time: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
      All wait for longest (C) to finish</code></pre>
            <p><strong>Continuous batching</strong>: New requests enter
            as old ones finish</p>
            <pre><code>Continuous Batching:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Request A: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
Request B: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà][D starts][‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
Request C: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà][E starts]
Request D:           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]
Request E:                           [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]

Time: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí
      New requests start immediately</code></pre>
            <p><strong>Result</strong>: Much higher throughput</p>
            <h3 id="speculative-decoding-connection">Speculative
            Decoding Connection</h3>
            <p>KV-cache enables speculative decoding:</p>
            <ol type="1">
            <li>Draft model generates K tokens (cheap)</li>
            <li>Target model verifies all K in <strong>parallel</strong>
            (uses KV-cache)</li>
            <li>Accept verified tokens, reject wrong ones</li>
            </ol>
            <p>Without KV-cache, verification couldn‚Äôt be
            parallelized!</p>
            <h3
            id="interview-q-whats-the-memory-cost-of-kv-cache-for-a-70b-model-at-128k-context">Interview
            Q: ‚ÄúWhat‚Äôs the memory cost of KV-cache for a 70B model at
            128K context?‚Äù</h3>
            <p><strong>A</strong>: For a 70B model like LLaMA-2 70B with
            80 layers, 64 heads (8 KV heads with GQA), head_dim=128, in
            BF16:</p>
            <p>Per token per layer: 2 (K and V) √ó 8 (KV heads) √ó 128
            (head_dim) √ó 2 (BF16 bytes) = 4KB Per token total: 4KB √ó 80
            layers = 320KB For 128K context: 320KB √ó 131072 = <strong>42
            GB</strong></p>
            <p>This is just for KV-cache ‚Äî add model weights (~140 GB in
            BF16) and you need 180+ GB, requiring multiple GPUs.
            Solutions include: (1) GQA (LLaMA already uses it), (2)
            quantized KV cache (INT8 = 2√ó, INT4 = 4√ó), (3) sliding
            window attention (limit cached tokens).</p>
            <h3
            id="interview-q-explain-the-difference-between-mha-mqa-and-gqa">Interview
            Q: ‚ÄúExplain the difference between MHA, MQA, and GQA‚Äù</h3>
            <p><strong>A</strong>:</p>
            <ul>
            <li><strong>MHA (Multi-Head Attention)</strong>: Each head
            has its own K, V projections. Best quality but highest
            memory for KV-cache.</li>
            <li><strong>MQA (Multi-Query Attention)</strong>: All query
            heads share a single K, V. Dramatically reduces KV-cache (by
            num_heads√ó) but can hurt quality.</li>
            <li><strong>GQA (Grouped-Query Attention)</strong>:
            Compromise ‚Äî groups of query heads share K, V. LLaMA-2 70B
            uses 8 KV heads for 64 query heads (8√ó reduction with
            minimal quality loss).</li>
            </ul>
            <p>GQA is the practical sweet spot: significant memory
            savings while maintaining quality. The savings are critical
            for long-context inference where KV-cache dominates
            memory.</p>
            <hr />
            <h2 id="speculative-decoding">11.4 Speculative Decoding</h2>
            <h3 id="the-problem-autoregressive-decoding-is-slow">The
            Problem: Autoregressive Decoding is Slow</h3>
            <pre><code>Standard Autoregressive Decoding:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Token 1: [Full forward pass through 70B model] ‚Üí 50ms
Token 2: [Full forward pass through 70B model] ‚Üí 50ms
Token 3: [Full forward pass through 70B model] ‚Üí 50ms
...
Token 100: [Full forward pass through 70B model] ‚Üí 50ms

Total: 100 √ó 50ms = 5 seconds

Problem: GPU is underutilized!

- Loading model weights: slow (memory-bound)
- Actual computation: fast but waiting for memory</code></pre>
            <h3 id="the-key-insight-2">The Key Insight</h3>
            <p>During autoregressive generation:</p>
            <ul>
            <li><strong>Batch size = 1</strong> (one token at a
            time)</li>
            <li><strong>Memory bandwidth bound</strong> (loading weights
            &gt;&gt; computation)</li>
            <li><strong>GPU compute largely idle</strong></li>
            </ul>
            <p><strong>But</strong>: Verifying K tokens takes same time
            as generating 1 token (parallel)!</p>
            <h3 id="speculative-decoding-algorithm">Speculative Decoding
            Algorithm</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Speculative Decoding                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  1. DRAFT: Small model generates K tokens quickly                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ     &quot;The&quot; ‚Üí [Small 7B Model] ‚Üí &quot;cat sat on the&quot;                    ‚îÇ
‚îÇ              (K=4 tokens, ~5ms each = 20ms total)                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  2. VERIFY: Large model scores ALL K tokens in parallel            ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ     &quot;The cat sat on the&quot; ‚Üí [Large 70B Model] ‚Üí probabilities       ‚îÇ
‚îÇ              (One forward pass = 50ms for ALL 4 tokens)            ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  3. ACCEPT/REJECT: Compare draft vs target probabilities           ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ     Token &quot;cat&quot;: P_target &gt; P_draft? ‚Üí ACCEPT                      ‚îÇ
‚îÇ     Token &quot;sat&quot;: P_target &gt; P_draft? ‚Üí ACCEPT                      ‚îÇ
‚îÇ     Token &quot;on&quot;:  P_target &lt; P_draft? ‚Üí REJECT (&amp; resample)         ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  4. Result: Accepted 2 tokens + 1 resampled = 3 tokens             ‚îÇ
‚îÇ             Time: 20ms + 50ms = 70ms for 3 tokens                  ‚îÇ
‚îÇ             vs. 3 √ó 50ms = 150ms standard decoding                 ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="the-acceptance-criterion">The Acceptance
            Criterion</h3>
            <p>For speculative decoding to be <strong>exact</strong>
            (same distribution as target model):</p>
            <p><span class="math display">\[P(\text{accept } x) =
            \min\left(1, \frac{p(x)}{q(x)}\right)\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(p(x)\)</span> = target model
            probability</li>
            <li><span class="math inline">\(q(x)\)</span> = draft model
            probability</li>
            </ul>
            <p>If rejected, <strong>resample</strong> from adjusted
            distribution: <span class="math display">\[p&#39;(x) =
            \text{norm}(\max(0, p(x) - q(x)))\]</span></p>
            <p>This ensures the final output distribution exactly
            matches the target model!</p>
            <h3 id="implementation-2">Implementation</h3>
            <div class="sourceCode" id="cb413"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb413-1"><a href="#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> speculative_decode(target_model, draft_model, prompt, K<span class="op">=</span><span class="dv">4</span>, max_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb413-2"><a href="#cb413-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb413-3"><a href="#cb413-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Speculative decoding with K draft tokens.</span></span>
<span id="cb413-4"><a href="#cb413-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb413-5"><a href="#cb413-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb413-6"><a href="#cb413-6" aria-hidden="true" tabindex="-1"></a><span class="co">        target_model: Large, accurate model</span></span>
<span id="cb413-7"><a href="#cb413-7" aria-hidden="true" tabindex="-1"></a><span class="co">        draft_model: Small, fast model  </span></span>
<span id="cb413-8"><a href="#cb413-8" aria-hidden="true" tabindex="-1"></a><span class="co">        prompt: Input token IDs</span></span>
<span id="cb413-9"><a href="#cb413-9" aria-hidden="true" tabindex="-1"></a><span class="co">        K: Number of speculative tokens</span></span>
<span id="cb413-10"><a href="#cb413-10" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb413-11"><a href="#cb413-11" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> <span class="bu">list</span>(prompt)</span>
<span id="cb413-12"><a href="#cb413-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb413-13"><a href="#cb413-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(generated) <span class="op">-</span> <span class="bu">len</span>(prompt) <span class="op">&lt;</span> max_tokens:</span>
<span id="cb413-14"><a href="#cb413-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Draft: Generate K tokens with small model</span></span>
<span id="cb413-15"><a href="#cb413-15" aria-hidden="true" tabindex="-1"></a>        draft_tokens <span class="op">=</span> []</span>
<span id="cb413-16"><a href="#cb413-16" aria-hidden="true" tabindex="-1"></a>        draft_probs <span class="op">=</span> []</span>
<span id="cb413-17"><a href="#cb413-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb413-18"><a href="#cb413-18" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> generated.copy()</span>
<span id="cb413-19"><a href="#cb413-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb413-20"><a href="#cb413-20" aria-hidden="true" tabindex="-1"></a>            q <span class="op">=</span> draft_model.get_probs(context)</span>
<span id="cb413-21"><a href="#cb413-21" aria-hidden="true" tabindex="-1"></a>            token <span class="op">=</span> sample(q)</span>
<span id="cb413-22"><a href="#cb413-22" aria-hidden="true" tabindex="-1"></a>            draft_tokens.append(token)</span>
<span id="cb413-23"><a href="#cb413-23" aria-hidden="true" tabindex="-1"></a>            draft_probs.append(q[token])</span>
<span id="cb413-24"><a href="#cb413-24" aria-hidden="true" tabindex="-1"></a>            context.append(token)</span>
<span id="cb413-25"><a href="#cb413-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb413-26"><a href="#cb413-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Verify: Score all K tokens with large model in parallel</span></span>
<span id="cb413-27"><a href="#cb413-27" aria-hidden="true" tabindex="-1"></a>        all_tokens <span class="op">=</span> generated <span class="op">+</span> draft_tokens</span>
<span id="cb413-28"><a href="#cb413-28" aria-hidden="true" tabindex="-1"></a>        target_probs <span class="op">=</span> target_model.get_probs_batch(</span>
<span id="cb413-29"><a href="#cb413-29" aria-hidden="true" tabindex="-1"></a>            generated, </span>
<span id="cb413-30"><a href="#cb413-30" aria-hidden="true" tabindex="-1"></a>            positions<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(generated), <span class="bu">len</span>(all_tokens))</span>
<span id="cb413-31"><a href="#cb413-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb413-32"><a href="#cb413-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb413-33"><a href="#cb413-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Accept/Reject</span></span>
<span id="cb413-34"><a href="#cb413-34" aria-hidden="true" tabindex="-1"></a>        accepted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb413-35"><a href="#cb413-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb413-36"><a href="#cb413-36" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> target_probs[i][draft_tokens[i]]</span>
<span id="cb413-37"><a href="#cb413-37" aria-hidden="true" tabindex="-1"></a>            q <span class="op">=</span> draft_probs[i]</span>
<span id="cb413-38"><a href="#cb413-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb413-39"><a href="#cb413-39" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Acceptance probability</span></span>
<span id="cb413-40"><a href="#cb413-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> random.random() <span class="op">&lt;</span> <span class="bu">min</span>(<span class="dv">1</span>, p <span class="op">/</span> q):</span>
<span id="cb413-41"><a href="#cb413-41" aria-hidden="true" tabindex="-1"></a>                generated.append(draft_tokens[i])</span>
<span id="cb413-42"><a href="#cb413-42" aria-hidden="true" tabindex="-1"></a>                accepted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb413-43"><a href="#cb413-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb413-44"><a href="#cb413-44" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Reject: resample from adjusted distribution</span></span>
<span id="cb413-45"><a href="#cb413-45" aria-hidden="true" tabindex="-1"></a>                adjusted <span class="op">=</span> np.maximum(<span class="dv">0</span>, target_probs[i] <span class="op">-</span> draft_probs[i] <span class="op">*</span> np.ones_like(target_probs[i]))</span>
<span id="cb413-46"><a href="#cb413-46" aria-hidden="true" tabindex="-1"></a>                adjusted <span class="op">/=</span> adjusted.<span class="bu">sum</span>()</span>
<span id="cb413-47"><a href="#cb413-47" aria-hidden="true" tabindex="-1"></a>                new_token <span class="op">=</span> sample(adjusted)</span>
<span id="cb413-48"><a href="#cb413-48" aria-hidden="true" tabindex="-1"></a>                generated.append(new_token)</span>
<span id="cb413-49"><a href="#cb413-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span>  <span class="co"># Stop accepting after first rejection</span></span>
<span id="cb413-50"><a href="#cb413-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb413-51"><a href="#cb413-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Bonus: Sample one more token from target (always do this)</span></span>
<span id="cb413-52"><a href="#cb413-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accepted <span class="op">==</span> K:</span>
<span id="cb413-53"><a href="#cb413-53" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> target_model.get_probs(generated)</span>
<span id="cb413-54"><a href="#cb413-54" aria-hidden="true" tabindex="-1"></a>            generated.append(sample(p))</span>
<span id="cb413-55"><a href="#cb413-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb413-56"><a href="#cb413-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated</span></code></pre></div>
            <h3 id="speedup-analysis">Speedup Analysis</h3>
            <p><strong>Expected accepted tokens per step</strong>: <span
            class="math display">\[\mathbb{E}[\text{accepted}] =
            \sum_{i=1}^{K} \prod_{j=1}^{i} P(\text{accept}_j) +
            1\]</span></p>
            <p><strong>Speedup factor</strong> (approximately): <span
            class="math display">\[\text{Speedup} \approx
            \frac{\mathbb{E}[\text{accepted}] + 1}{1 + K \cdot
            \frac{T_{draft}}{T_{target}}}\]</span></p>
            <p><strong>When speculative decoding helps</strong>:</p>
            <ul>
            <li>Draft model is much faster than target (e.g., 10√ó)</li>
            <li>Draft distribution is close to target (high acceptance
            rate)</li>
            <li>Target model is memory-bound (common for large
            models)</li>
            </ul>
            <h3 id="practical-considerations-1">Practical
            Considerations</h3>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>Factor</th>
            <th>Impact</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Draft model quality</strong></td>
            <td>Better draft ‚Üí more accepts ‚Üí more speedup</td>
            </tr>
            <tr>
            <td><strong>K (speculation length)</strong></td>
            <td>Higher K ‚Üí more parallelism but more rejects</td>
            </tr>
            <tr>
            <td><strong>Model size ratio</strong></td>
            <td>Larger gap ‚Üí more potential speedup</td>
            </tr>
            <tr>
            <td><strong>Task difficulty</strong></td>
            <td>Easy tokens (common words) ‚Üí high acceptance</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Common draft models</strong>:</p>
            <ul>
            <li>Same architecture, fewer layers (e.g., 7B draft for 70B
            target)</li>
            <li>Same model, early exit</li>
            <li>Smaller fine-tuned version</li>
            </ul>
            <h3 id="self-speculative-decoding">Self-Speculative
            Decoding</h3>
            <p><strong>Idea</strong>: Use early layers of the same model
            as the ‚Äúdraft‚Äù:</p>
            <pre><code>Layer 1-10:  Quick draft (early exit)
Layer 1-80:  Full verification

No separate draft model needed!</code></pre>
            <h3
            id="interview-q-how-does-speculative-decoding-achieve-speedup-without-changing-output-distribution">Interview
            Q: ‚ÄúHow does speculative decoding achieve speedup without
            changing output distribution?‚Äù</h3>
            <p><strong>A</strong>: Speculative decoding uses a small
            draft model to propose K tokens, then the large target model
            verifies all K tokens <strong>in parallel</strong> (one
            forward pass). The key is the acceptance criterion: accept
            token <span class="math inline">\(x\)</span> with
            probability <span class="math inline">\(\min(1,
            p(x)/q(x))\)</span> where <span
            class="math inline">\(p\)</span> is target and <span
            class="math inline">\(q\)</span> is draft probability. If
            rejected, resample from <span class="math inline">\(\max(0,
            p-q)\)</span>. This <strong>mathematically
            guarantees</strong> the output distribution matches exactly
            what you‚Äôd get from standard autoregressive sampling with
            the target model.</p>
            <p>The speedup comes from parallelism: verifying K tokens
            costs the same as generating 1 (both memory-bound). If 3 of
            4 draft tokens are accepted, you‚Äôve generated 4 tokens in
            roughly the time of 1.5 standard steps.</p>
            <hr />
            <h2 id="state-space-models-mamba">11.5 State Space Models
            (Mamba)</h2>
            <h3 id="the-problem-attention-is-on¬≤">The Problem: Attention
            is O(N¬≤)</h3>
            <p>Even with Flash Attention, attention has fundamental
            limitations:</p>
            <ul>
            <li><strong>Training</strong>: O(N¬≤) compute (can‚Äôt
            avoid)</li>
            <li><strong>Inference</strong>: KV-cache grows linearly with
            context</li>
            <li><strong>Very long contexts</strong>: Still
            expensive</li>
            </ul>
            <h3 id="state-space-models-a-different-approach">State Space
            Models: A Different Approach</h3>
            <p><strong>SSMs</strong> model sequences through
            continuous-time dynamics:</p>
            <p><span class="math display">\[h&#39;(t) = Ah(t) +
            Bx(t)\]</span></p>
            <p><span class="math display">\[y(t) = Ch(t) +
            Dx(t)\]</span></p>
            <p>where:</p>
            <ul>
            <li><span class="math inline">\(h(t)\)</span> = hidden
            state</li>
            <li><span class="math inline">\(A, B, C, D\)</span> =
            learnable parameters</li>
            <li><span class="math inline">\(x(t)\)</span> = input, <span
            class="math inline">\(y(t)\)</span> = output</li>
            </ul>
            <p><strong>Discretized version</strong> (for digital
            processing):</p>
            <p><span class="math display">\[h_t = \bar{A}h_{t-1} +
            \bar{B}x_t\]</span></p>
            <p><span class="math display">\[y_t = Ch_t +
            Dx_t\]</span></p>
            <h3 id="why-ssms-are-efficient">Why SSMs are Efficient</h3>
            <pre><code>Attention:                      SSM (Mamba):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Each token attends to ALL       Fixed-size state captures
previous tokens                 relevant history

Training: O(N¬≤)                 Training: O(N) with convolution
Inference: KV-cache grows       Inference: Constant state size

Memory: O(N) KV-cache           Memory: O(1) state</code></pre>
            <h3
            id="the-key-innovation-selective-state-spaces-s6mamba">The
            Key Innovation: Selective State Spaces (S6/Mamba)</h3>
            <p><strong>Problem with classic SSMs</strong>: Fixed A, B, C
            can‚Äôt do content-based reasoning.</p>
            <p><strong>Mamba‚Äôs solution</strong>: Make A, B, C
            <strong>input-dependent</strong>:</p>
            <p><span class="math display">\[B_t =
            \text{Linear}_B(x_t)\]</span></p>
            <p><span class="math display">\[C_t =
            \text{Linear}_C(x_t)\]</span></p>
            <p><span class="math display">\[\Delta_t =
            \text{softplus}(\text{Linear}_\Delta(x_t))\]</span></p>
            <p>where <span class="math inline">\(\Delta_t\)</span>
            controls discretization step size.</p>
            <p><strong>Why this matters</strong>: The model can now
            <strong>selectively</strong> remember or forget based on
            content!</p>
            <h3 id="mamba-architecture">Mamba Architecture</h3>
            <pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Mamba Block                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  Input x ‚îÄ‚îÄ‚Üí [Linear] ‚îÄ‚îÄ‚Üí [Conv1D] ‚îÄ‚îÄ‚Üí [SiLU] ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ          ‚îÇ                                       ‚îÇ                  ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚Üí [Linear] ‚îÄ‚îÄ‚Üí [SiLU] ‚îÄ‚îÄ‚Üí [SSM] ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí [√ó] ‚îÄ‚îÄ‚Üí [Linear] ‚îÄ‚îÄ‚Üí Output
‚îÇ                                                  ‚îÇ                  ‚îÇ
‚îÇ                              (gating)            ‚îÇ                  ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îÇ  SSM block:                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  x_t ‚Üí [Project to B, C, Œî]                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ          ‚Üì                                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  h_t = exp(ŒîA)h_{t-1} + ŒîBx_t   (selective state update)   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  y_t = Ch_t                      (output projection)        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
            <h3 id="efficient-implementation-parallel-scan">Efficient
            Implementation: Parallel Scan</h3>
            <p><strong>Naive SSM</strong>: Sequential (can‚Äôt
            parallelize)</p>
            <div class="sourceCode" id="cb417"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb417-1"><a href="#cb417-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb417-2"><a href="#cb417-2" aria-hidden="true" tabindex="-1"></a>    h[t] <span class="op">=</span> A <span class="op">*</span> h[t<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> B <span class="op">*</span> x[t]  <span class="co"># Depends on previous step!</span></span></code></pre></div>
            <p><strong>Parallel scan</strong>: Associative operation
            enables parallelism</p>
            <p><span class="math display">\[\begin{pmatrix} h_t \\ 1
            \end{pmatrix} = \begin{pmatrix} A &amp; B \\ 0 &amp; 1
            \end{pmatrix} \begin{pmatrix} h_{t-1} \\ x_t
            \end{pmatrix}\]</span></p>
            <p>Matrix multiplication is associative, so we can use
            <strong>parallel prefix sum</strong>:</p>
            <div class="sourceCode" id="cb418"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb418-1"><a href="#cb418-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parallel_scan(A, B, x):</span>
<span id="cb418-2"><a href="#cb418-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb418-3"><a href="#cb418-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute h_t = A*h_{t-1} + B*x_t for all t in O(log T) parallel steps.</span></span>
<span id="cb418-4"><a href="#cb418-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb418-5"><a href="#cb418-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Combine into tuples (a_i, b_i) where h_i = a_i * h_0 + b_i</span></span>
<span id="cb418-6"><a href="#cb418-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use associative combine: (a1, b1) ‚äï (a2, b2) = (a1*a2, a2*b1 + b2)</span></span>
<span id="cb418-7"><a href="#cb418-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parallel prefix scan in O(log T) steps</span></span>
<span id="cb418-8"><a href="#cb418-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span></code></pre></div>
            <p><strong>Result</strong>: O(N log N) parallel time instead
            of O(N) sequential!</p>
            <h3 id="mamba-vs-transformers-comparison">Mamba vs
            Transformers: Comparison</h3>
            <table>
            <thead>
            <tr>
            <th>Aspect</th>
            <th>Transformer</th>
            <th>Mamba</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Training FLOPs</strong></td>
            <td>O(N¬≤d)</td>
            <td>O(Nd¬≤)</td>
            </tr>
            <tr>
            <td><strong>Inference memory</strong></td>
            <td>O(N) KV-cache</td>
            <td>O(d) state</td>
            </tr>
            <tr>
            <td><strong>Long context</strong></td>
            <td>Expensive</td>
            <td>Cheap</td>
            </tr>
            <tr>
            <td><strong>Parallel training</strong></td>
            <td>Excellent</td>
            <td>Good (scan)</td>
            </tr>
            <tr>
            <td><strong>In-context learning</strong></td>
            <td>Strong</td>
            <td>Emerging</td>
            </tr>
            <tr>
            <td><strong>Copying/retrieval</strong></td>
            <td>Easy (attention)</td>
            <td>Harder</td>
            </tr>
            </tbody>
            </table>
            <h3 id="when-to-use-mamba-vs-transformers">When to Use Mamba
            vs Transformers</h3>
            <p><strong>Mamba strengths</strong>:</p>
            <ul>
            <li>Very long sequences (&gt;100K)</li>
            <li>Memory-constrained inference</li>
            <li>Continuous streaming data</li>
            <li>Linear-time requirements</li>
            </ul>
            <p><strong>Transformer strengths</strong>:</p>
            <ul>
            <li>Complex reasoning</li>
            <li>In-context learning</li>
            <li>Retrieval-heavy tasks</li>
            <li>Established ecosystem</li>
            </ul>
            <h3 id="hybrid-architectures-jamba-etc.">Hybrid
            Architectures (Jamba, etc.)</h3>
            <p><strong>Best of both worlds</strong>: Combine attention
            and SSM layers</p>
            <pre><code>Jamba Architecture:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[SSM Layer]      ‚Üê Efficient long-range
[SSM Layer]
[Attention Layer] ‚Üê Precise retrieval  
[SSM Layer]
[SSM Layer]
[Attention Layer]
...

Ratio: 7:1 (SSM:Attention) typical</code></pre>
            <p><strong>Benefits</strong>:</p>
            <ul>
            <li>Long context from SSM</li>
            <li>Precise recall from attention</li>
            <li>Much smaller KV-cache (only for attention layers)</li>
            </ul>
            <h3 id="mamba-2-improvements">Mamba-2 Improvements</h3>
            <table>
            <thead>
            <tr>
            <th>Improvement</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>Structured state</strong></td>
            <td>Scalar ‚Üí diagonal ‚Üí full matrices</td>
            </tr>
            <tr>
            <td><strong>Tensor parallelism</strong></td>
            <td>Better distributed training</td>
            </tr>
            <tr>
            <td><strong>Faster kernels</strong></td>
            <td>Optimized CUDA implementations</td>
            </tr>
            <tr>
            <td><strong>SSD (State Space Duality)</strong></td>
            <td>Connection to linear attention</td>
            </tr>
            </tbody>
            </table>
            <h3
            id="interview-q-how-is-mamba-different-from-transformers">Interview
            Q: ‚ÄúHow is Mamba different from Transformers?‚Äù</h3>
            <p><strong>A</strong>: Mamba uses <strong>State Space
            Models</strong> instead of attention. Key differences:</p>
            <ol type="1">
            <li><strong>Compute</strong>: O(N) vs O(N¬≤) ‚Äî Mamba scales
            linearly with sequence length</li>
            <li><strong>Memory</strong>: Constant state vs growing
            KV-cache ‚Äî Mamba uses O(d) memory regardless of context
            length</li>
            <li><strong>Mechanism</strong>: Mamba maintains a
            <strong>fixed-size hidden state</strong> that‚Äôs updated
            recurrently; Transformers do <strong>full pairwise
            attention</strong></li>
            </ol>
            <p>Mamba‚Äôs innovation is <strong>selective state
            spaces</strong> ‚Äî making A, B, C input-dependent so the
            model can learn what to remember/forget based on content.
            Training is efficient via <strong>parallel scan</strong>
            (O(log N) parallel time).</p>
            <p>Tradeoffs: Mamba is more efficient but attention is
            better at precise retrieval and in-context learning. Hybrid
            architectures (Jamba) combine both.</p>
            <h3
            id="interview-q-whats-the-key-insight-behind-selective-state-spaces">Interview
            Q: ‚ÄúWhat‚Äôs the key insight behind selective state
            spaces?‚Äù</h3>
            <p><strong>A</strong>: Classic SSMs have
            <strong>fixed</strong> transition matrices A, B, C ‚Äî they
            can‚Äôt adapt their behavior based on input content. This
            limits expressivity: the model processes ‚Äúimportant
            information‚Äù and ‚Äúfiller‚Äù the same way.</p>
            <p>Selective SSMs (Mamba) make these matrices
            <strong>input-dependent</strong>: <span
            class="math inline">\(B_t = f(x_t)\)</span>, <span
            class="math inline">\(C_t = g(x_t)\)</span>. This lets the
            model learn to <strong>selectively</strong> update its state
            ‚Äî like a learned gating mechanism. When seeing important
            content, it can use large <span
            class="math inline">\(\Delta\)</span> (big state update);
            for filler, small <span
            class="math inline">\(\Delta\)</span> (ignore). This is
            analogous to how attention ‚Äúselects‚Äù relevant tokens, but
            with O(1) state instead of O(N) cache.</p>
            <hr />
            <h1 id="part-12-question-bank">Part 12: Question Bank</h1>
            <blockquote>
            <p><strong>This chapter provides detailed answers with
            verbal expressions</strong> for common ML/AI question
            categories. Each answer includes explanations you can give
            verbally, key points to hit, and common follow-ups.</p>
            </blockquote>
            <hr />
            <h2 id="theoretical-foundations---math-ml-theory">12.1
            Theoretical Foundations - Math &amp; ML Theory</h2>
            <h3 id="linear-algebra-1">12.1.1 Linear Algebra</h3>
            <hr />
            <h4
            id="q-what-happens-if-your-weight-matrix-is-low-rank"><strong>Q:
            ‚ÄúWhat happens if your weight matrix is low
            rank?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúA low-rank weight matrix creates an <strong>information
            bottleneck</strong>. If I have a weight matrix W that‚Äôs
            supposed to map from dimension d to dimension d, but it only
            has rank r where r &lt; d, then I‚Äôm effectively projecting
            my data through a lower-dimensional subspace.</p>
            <p>Geometrically, think of it this way: a rank-r matrix can
            only span an r-dimensional subspace of the output space. So
            even if my input has rich d-dimensional information, the
            output can only capture r independent directions. Any
            information in the null space of W is <strong>completely
            lost</strong>.</p>
            <p>This has practical implications: if my network‚Äôs weight
            matrices become low-rank during training, it means the model
            isn‚Äôt using its full capacity. This can happen with poor
            initialization or when gradients don‚Äôt flow properly to
            certain dimensions.</p>
            <p>But interestingly, this ‚Äòbug‚Äô has become a
            <strong>feature</strong> in modern ML. <strong>LoRA</strong>
            ‚Äî Low-Rank Adaptation ‚Äî deliberately constrains fine-tuning
            updates to be low-rank: instead of updating the full W, we
            learn two small matrices A and B where the update is AB,
            with A being d√ór and B being r√ód for small r. This
            drastically reduces trainable parameters while preserving
            most of the model‚Äôs capability, because the
            <strong>change</strong> in weights during fine-tuning is
            often naturally low-rank.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Low rank = information bottleneck</li>
            <li>Projects to lower-dimensional subspace</li>
            <li>Null space information is lost</li>
            <li>Connection to LoRA (critical for modern ML)</li>
            <li>Why fine-tuning updates are naturally low-rank</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhy are fine-tuning updates
            low-rank?‚Äù</strong></p>
            <p>‚ÄúEmpirically, researchers found that the weight changes
            during fine-tuning have very low intrinsic dimensionality.
            Intuitively, fine-tuning is making small, targeted
            adjustments to already-good representations ‚Äî you don‚Äôt need
            to change everything, just nudge the model in a specific
            direction. That direction can be captured by a low-rank
            update. The LoRA paper showed you can get 90%+ of full
            fine-tuning performance with rank 8-16, which is remarkable
            for models with hidden dims of 4096+.‚Äù</p>
            <hr />
            <h4
            id="q-whats-the-relationship-between-the-hessians-eigenvalues-and-critical-point-stability"><strong>Q:
            ‚ÄúWhat‚Äôs the relationship between the Hessian‚Äôs eigenvalues
            and critical point stability?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúThe Hessian matrix contains second-order derivative
            information ‚Äî it tells us about the
            <strong>curvature</strong> of the loss landscape. At a
            critical point where the gradient is zero, the eigenvalues
            of the Hessian determine what <em>type</em> of critical
            point we‚Äôre at:</p>
            <ul>
            <li><strong>All positive eigenvalues</strong>: Local
            minimum. The loss curves upward in all directions, like
            sitting at the bottom of a bowl.</li>
            <li><strong>All negative eigenvalues</strong>: Local
            maximum. The loss curves downward everywhere ‚Äî we‚Äôre at a
            peak.</li>
            <li><strong>Mixed signs</strong>: Saddle point. This is the
            interesting one ‚Äî the loss curves up in some directions and
            down in others. It‚Äôs like sitting on a horse saddle: stable
            if you move sideways, but unstable if you move forward or
            back.</li>
            </ul>
            <p>For deep learning, this matters enormously. In high
            dimensions, <strong>saddle points are much more common than
            local minima</strong>. Think about it: for a point to be a
            minimum, ALL eigenvalues must be positive. With millions of
            parameters, that‚Äôs statistically unlikely. Most critical
            points in deep learning are saddles.</p>
            <p>The good news is that gradient descent naturally escapes
            saddles ‚Äî it will find the directions with negative
            curvature and slide down. The challenge is when eigenvalues
            are <strong>close to zero</strong> ‚Äî flat regions where
            gradients are tiny and training stalls. This is partly why
            methods like Adam, which adapt learning rates, work better
            than vanilla SGD.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Hessian = curvature information</li>
            <li>All positive eigenvalues ‚Üí local min</li>
            <li>Mixed signs ‚Üí saddle point</li>
            <li>High dimensions ‚Üí mostly saddles, not local minima</li>
            <li>Near-zero eigenvalues ‚Üí flat regions, slow training</li>
            <li>Connection to optimizer choice</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúHow does this relate to the
            vanishing gradient problem?‚Äù</strong></p>
            <p>‚ÄúIf the Hessian has many near-zero eigenvalues in certain
            directions, gradient updates in those directions will be
            tiny ‚Äî the loss surface is nearly flat. This is related to
            but distinct from vanishing gradients. Traditional vanishing
            gradients happen when the <strong>gradient itself</strong>
            is small due to repeated multiplication through layers. The
            Hessian view is about the <strong>curvature</strong> ‚Äî even
            with a non-zero gradient, if the curvature is nearly flat,
            progress is slow. Second-order methods like Newton‚Äôs method
            try to account for this by using the Hessian to scale
            updates, but computing the full Hessian for large models is
            intractable.‚Äù</p>
            <hr />
            <h4 id="q-why-do-we-use-eigenvectors-in-pca"><strong>Q: ‚ÄúWhy
            do we use eigenvectors in PCA?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúPCA‚Äôs goal is to find directions of <strong>maximum
            variance</strong> in the data. The eigenvectors of the
            covariance matrix are exactly those directions.</p>
            <p>Here‚Äôs the intuition: the covariance matrix C captures
            how each dimension varies with every other dimension. When
            we compute the eigenvectors of C, we‚Äôre finding directions v
            such that Cv = Œªv ‚Äî directions where the covariance matrix
            just <strong>scales</strong> the vector rather than rotating
            it.</p>
            <p>The eigenvalue Œª tells us the <strong>variance</strong>
            along that eigenvector direction. The largest eigenvalue
            corresponds to the direction with the most variance ‚Äî that‚Äôs
            our first principal component. The second-largest eigenvalue
            gives the direction with most variance <strong>orthogonal to
            the first</strong>, and so on.</p>
            <p>Why orthogonal? Because the covariance matrix is
            symmetric positive semi-definite, its eigenvectors are
            guaranteed to be orthogonal. This is geometrically nice ‚Äî
            our principal components form an orthonormal basis.</p>
            <p>When we project data onto the top k eigenvectors, we‚Äôre
            keeping the k dimensions that capture the most variance ‚Äî
            that‚Äôs dimensionality reduction with minimal information
            loss, in the sense that we‚Äôre maximizing preserved
            variance.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Eigenvectors = directions of maximum variance</li>
            <li>Eigenvalues = amount of variance in that direction</li>
            <li>Covariance matrix is symmetric ‚Üí orthogonal
            eigenvectors</li>
            <li>PCA = project onto top-k eigenvectors</li>
            <li>Minimal information loss (maximum variance
            preserved)</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhat‚Äôs the connection between PCA
            and SVD?‚Äù</strong></p>
            <p>‚ÄúThey‚Äôre deeply related. If X is my centered data matrix
            (n samples √ó d features), then:</p>
            <ul>
            <li>PCA computes eigenvectors of X^T X (the covariance
            matrix up to a scalar)</li>
            <li>SVD decomposes X = UŒ£V^T directly</li>
            </ul>
            <p>The right singular vectors V from SVD are exactly the
            eigenvectors of X^T X ‚Äî they‚Äôre the principal components!
            SVD is often preferred numerically because it‚Äôs more stable
            than explicitly forming X^T X and computing its
            eigendecomposition. The singular values œÉ in Œ£ relate to
            eigenvalues by Œª = œÉ¬≤/n.‚Äù</p>
            <hr />
            <h3 id="calculus-and-optimization">12.1.2 Calculus and
            Optimization</h3>
            <hr />
            <h4 id="q-design-a-simple-autograd-engine."><strong>Q:
            ‚ÄúDesign a simple Autograd engine.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúI‚Äôd build a computational graph where each operation
            creates nodes that remember their inputs and know how to
            compute local gradients. Let me walk through the key
            components:</p>
            <p>First, I need a <strong>Value</strong> class that wraps
            numbers and tracks the computation graph. Each Value stores:
            its data, its gradient (initially zero), a backward
            function, and pointers to its children in the graph.</p>
            <p>For the forward pass, when I do operations like addition
            or multiplication, I create new Value nodes and store the
            operation‚Äôs local gradient rule.</p>
            <p>For backward, I do a <strong>reverse topological
            sort</strong> of the graph ‚Äî this ensures I compute
            gradients for nodes only after I‚Äôve computed gradients for
            all nodes that depend on them. This is the key insight: I
            need to process the graph in reverse order.</p>
            <p>The chain rule is applied at each node: the gradient
            flowing into a node is the sum of gradients from all nodes
            that use it, each multiplied by the local gradient.‚Äù</p>
            <p><strong>Code (what you‚Äôd write on a
            whiteboard):</strong></p>
            <div class="sourceCode" id="cb420"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb420-1"><a href="#cb420-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Value:</span>
<span id="cb420-2"><a href="#cb420-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data, children<span class="op">=</span>(), op<span class="op">=</span><span class="st">&#39;&#39;</span>):</span>
<span id="cb420-3"><a href="#cb420-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.data <span class="op">=</span> data</span>
<span id="cb420-4"><a href="#cb420-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb420-5"><a href="#cb420-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span>  <span class="co"># Default: do nothing</span></span>
<span id="cb420-6"><a href="#cb420-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._children <span class="op">=</span> <span class="bu">set</span>(children)</span>
<span id="cb420-7"><a href="#cb420-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb420-8"><a href="#cb420-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</span>
<span id="cb420-9"><a href="#cb420-9" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb420-10"><a href="#cb420-10" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">+</span> other.data, (<span class="va">self</span>, other), <span class="st">&#39;+&#39;</span>)</span>
<span id="cb420-11"><a href="#cb420-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb420-12"><a href="#cb420-12" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb420-13"><a href="#cb420-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># d(a+b)/da = 1, d(a+b)/db = 1</span></span>
<span id="cb420-14"><a href="#cb420-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">+=</span> out.grad <span class="op">*</span> <span class="fl">1.0</span></span>
<span id="cb420-15"><a href="#cb420-15" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">+=</span> out.grad <span class="op">*</span> <span class="fl">1.0</span></span>
<span id="cb420-16"><a href="#cb420-16" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb420-17"><a href="#cb420-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb420-18"><a href="#cb420-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb420-19"><a href="#cb420-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other):</span>
<span id="cb420-20"><a href="#cb420-20" aria-hidden="true" tabindex="-1"></a>        other <span class="op">=</span> other <span class="cf">if</span> <span class="bu">isinstance</span>(other, Value) <span class="cf">else</span> Value(other)</span>
<span id="cb420-21"><a href="#cb420-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Value(<span class="va">self</span>.data <span class="op">*</span> other.data, (<span class="va">self</span>, other), <span class="st">&#39;*&#39;</span>)</span>
<span id="cb420-22"><a href="#cb420-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb420-23"><a href="#cb420-23" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward():</span>
<span id="cb420-24"><a href="#cb420-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># d(a*b)/da = b, d(a*b)/db = a</span></span>
<span id="cb420-25"><a href="#cb420-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">+=</span> out.grad <span class="op">*</span> other.data</span>
<span id="cb420-26"><a href="#cb420-26" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">+=</span> out.grad <span class="op">*</span> <span class="va">self</span>.data</span>
<span id="cb420-27"><a href="#cb420-27" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb420-28"><a href="#cb420-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb420-29"><a href="#cb420-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb420-30"><a href="#cb420-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>):</span>
<span id="cb420-31"><a href="#cb420-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Topological sort</span></span>
<span id="cb420-32"><a href="#cb420-32" aria-hidden="true" tabindex="-1"></a>        topo <span class="op">=</span> []</span>
<span id="cb420-33"><a href="#cb420-33" aria-hidden="true" tabindex="-1"></a>        visited <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb420-34"><a href="#cb420-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb420-35"><a href="#cb420-35" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> build_topo(v):</span>
<span id="cb420-36"><a href="#cb420-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> v <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb420-37"><a href="#cb420-37" aria-hidden="true" tabindex="-1"></a>                visited.add(v)</span>
<span id="cb420-38"><a href="#cb420-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> child <span class="kw">in</span> v._children:</span>
<span id="cb420-39"><a href="#cb420-39" aria-hidden="true" tabindex="-1"></a>                    build_topo(child)</span>
<span id="cb420-40"><a href="#cb420-40" aria-hidden="true" tabindex="-1"></a>                topo.append(v)</span>
<span id="cb420-41"><a href="#cb420-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb420-42"><a href="#cb420-42" aria-hidden="true" tabindex="-1"></a>        build_topo(<span class="va">self</span>)</span>
<span id="cb420-43"><a href="#cb420-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb420-44"><a href="#cb420-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backprop</span></span>
<span id="cb420-45"><a href="#cb420-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># dL/dL = 1</span></span>
<span id="cb420-46"><a href="#cb420-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> <span class="bu">reversed</span>(topo):</span>
<span id="cb420-47"><a href="#cb420-47" aria-hidden="true" tabindex="-1"></a>            node._backward()</span></code></pre></div>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Computational graph with nodes storing operation and
            children</li>
            <li>Forward pass builds the graph</li>
            <li>Backward pass requires topological sort (reverse
            order)</li>
            <li>Chain rule: accumulate gradients with <code>+=</code>
            (not <code>=</code>)</li>
            <li>Local gradients multiplied by upstream gradient</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhat‚Äôs the difference between
            forward mode and reverse mode autodiff?‚Äù</strong></p>
            <p>‚ÄúForward mode propagates derivatives
            <strong>alongside</strong> the forward computation ‚Äî you
            compute df/dx as you compute f.¬†It‚Äôs efficient when you have
            few inputs and many outputs.</p>
            <p>Reverse mode (backprop) computes the full forward pass
            first, then propagates gradients backward. It‚Äôs efficient
            when you have many inputs and few outputs ‚Äî which is exactly
            neural networks! One loss scalar, millions of parameters.
            That‚Äôs why we use reverse mode: one backward pass gives
            gradients for ALL parameters.‚Äù</p>
            <hr />
            <h4
            id="q-why-do-vanishing-gradients-happen-explain-mathematically."><strong>Q:
            ‚ÄúWhy do vanishing gradients happen? Explain
            mathematically.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúVanishing gradients occur because backpropagation
            involves <strong>repeated multiplication</strong> of
            Jacobian matrices through layers. Let me trace through the
            math.</p>
            <p>For a network with layers h‚ÇÅ ‚Üí h‚ÇÇ ‚Üí ‚Ä¶ ‚Üí h‚Çô ‚Üí L, the
            gradient of loss L with respect to early layer parameters
            flows through all subsequent layers. Specifically:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            h_1} = \frac{\partial L}{\partial h_n} \cdot \frac{\partial
            h_n}{\partial h_{n-1}} \cdot \ldots \cdot \frac{\partial
            h_2}{\partial h_1}\]</span></p>
            <p>Each term ‚àÇh‚Çú/‚àÇh‚Çú‚Çã‚ÇÅ is a Jacobian matrix. For a simple
            RNN with tanh activation:</p>
            <p><span class="math display">\[h_t = \tanh(W_{hh} h_{t-1} +
            W_{xh} x_t)\]</span></p>
            <p>The Jacobian is:</p>
            <p><span class="math display">\[\frac{\partial h_t}{\partial
            h_{t-1}} = \text{diag}(\tanh&#39;(z_t)) \cdot
            W_{hh}\]</span></p>
            <p>Now here‚Äôs the problem: tanh‚Äô(z) = 1 - tanh¬≤(z), which is
            bounded between 0 and 1. For saturated activations, tanh‚Äô ‚âà
            0. And if the spectral norm of W_{hh} is less than 1,
            repeated multiplication makes gradients
            <strong>exponentially small</strong>:</p>
            <p><span class="math display">\[\left\| \frac{\partial
            h_T}{\partial h_1} \right\| \leq \prod_{t=2}^{T}
            \|\text{diag}(\tanh&#39;)\| \cdot \|W_{hh}\| ‚âà
            (0.5)^T\]</span></p>
            <p>For T=100 steps, that‚Äôs 10^(-30) ‚Äî effectively zero.</p>
            <p>Conversely, if spectral norm &gt; 1 and activations
            aren‚Äôt saturated, gradients <strong>explode</strong>.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Backprop = product of Jacobians through layers</li>
            <li>Each Jacobian involves activation derivative AND weight
            matrix</li>
            <li>Tanh derivative bounded [0, 1], often small when
            saturated</li>
            <li>Repeated multiplication ‚Üí exponential decay or
            explosion</li>
            <li>Depends on eigenvalues/spectral norm of weight
            matrices</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúHow do modern architectures
            mitigate this?‚Äù</strong></p>
            <p>‚ÄúSeveral mechanisms:</p>
            <ol type="1">
            <li><strong>Residual connections</strong>: Instead of h_t =
            f(h_{t-1}), use h_t = h_{t-1} + f(h_{t-1}). Now the Jacobian
            is I + ‚àÇf/‚àÇh, and the identity term gives gradients a direct
            path. Even if f‚Äôs gradients vanish, the identity preserves
            them.</li>
            <li><strong>LayerNorm/BatchNorm</strong>: Normalizing
            activations prevents them from saturating, keeping
            activation derivatives away from zero.</li>
            <li><strong>LSTM gates</strong>: The cell state has an
            <strong>additive</strong> update path, not purely
            multiplicative. Gradients can flow through the cell state
            without repeated multiplication.</li>
            <li><strong>Careful initialization</strong>: Xavier/He init
            keeps variance stable across layers, preventing early
            saturation.</li>
            <li><strong>Gradient clipping</strong>: For exploding
            gradients, cap the gradient norm. Doesn‚Äôt help vanishing but
            prevents explosions.‚Äù</li>
            </ol>
            <hr />
            <h4
            id="q-derive-the-gradients-for-this-custom-layer-y-softmaxxw-b"><strong>Q:
            ‚ÄúDerive the gradients for this custom layer: y = softmax(xW
            + b)‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúLet me work through this step by step. I‚Äôll use the
            chain rule carefully.</p>
            <p>Let z = xW + b (the logits), and y = softmax(z).</p>
            <p><strong>Step 1: Softmax Jacobian</strong></p>
            <p>For softmax, y_i = exp(z_i) / Œ£‚±º exp(z_j), the Jacobian
            ‚àÇy/‚àÇz has a well-known form:</p>
            <p><span class="math display">\[\frac{\partial y_i}{\partial
            z_j} = \begin{cases} y_i(1 - y_i) &amp; \text{if } i = j \\
            -y_i y_j &amp; \text{if } i \neq j \end{cases}\]</span></p>
            <p>Or in matrix form: diag(y) - yy^T</p>
            <p><strong>Step 2: Gradient w.r.t. z</strong></p>
            <p>If we have upstream gradient ‚àÇL/‚àÇy, then:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z} = \frac{\partial L}{\partial y} \cdot \frac{\partial
            y}{\partial z} = \frac{\partial L}{\partial y} \cdot
            (\text{diag}(y) - yy^T)\]</span></p>
            <p>For cross-entropy loss L = -Œ£·µ¢ t_i log(y_i) where t is
            one-hot, this simplifies beautifully to:</p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            z} = y - t\]</span></p>
            <p><strong>Step 3: Gradient w.r.t. W and b</strong></p>
            <p>Since z = xW + b:</p>
            <ul>
            <li>‚àÇz/‚àÇW = x^T (each row of W gets gradient from
            corresponding output)</li>
            <li>‚àÇz/‚àÇb = 1</li>
            </ul>
            <p>So: <span class="math display">\[\frac{\partial
            L}{\partial W} = x^T \cdot \frac{\partial L}{\partial z} =
            x^T (y - t)\]</span></p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            b} = \frac{\partial L}{\partial z} = y - t\]</span></p>
            <p><strong>Step 4: Gradient w.r.t. x (for backprop to
            previous layer)</strong></p>
            <p><span class="math display">\[\frac{\partial L}{\partial
            x} = \frac{\partial L}{\partial z} \cdot W^T = (y - t)
            W^T\]</span>‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Softmax Jacobian: diag(y) - yy^T</li>
            <li>For cross-entropy + softmax, gradient simplifies to (y -
            t)</li>
            <li>dL/dW = x^T ¬∑ (dL/dz)</li>
            <li>dL/dx = (dL/dz) ¬∑ W^T for backprop</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhy is combining softmax and
            cross-entropy nice numerically?‚Äù</strong></p>
            <p>‚ÄúTwo reasons. First, the gradient is simple: y - t, no
            divisions or logs. Second, we can compute log-softmax in a
            numerically stable way: log(softmax(z)) = z - log(Œ£exp(z)),
            and we compute log-sum-exp with the max subtraction trick:
            log(Œ£exp(z)) = max(z) + log(Œ£exp(z - max(z))). This avoids
            overflow/underflow. PyTorch‚Äôs CrossEntropyLoss does this
            internally, which is why you should pass logits, not softmax
            outputs.‚Äù</p>
            <hr />
            <h3 id="probability-and-statistics-1">12.1.3 Probability and
            Statistics</h3>
            <hr />
            <h4
            id="q-derive-the-loss-function-for-logistic-regression-from-first-principles."><strong>Q:
            ‚ÄúDerive the loss function for logistic regression from first
            principles.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúI‚Äôll start from maximum likelihood estimation and arrive
            at binary cross-entropy.</p>
            <p><strong>Step 1: Model</strong></p>
            <p>In logistic regression, we model P(y=1|x) = œÉ(w¬∑x + b),
            where œÉ is the sigmoid function. This is a Bernoulli
            distribution:</p>
            <p><span class="math display">\[P(y|x) = \hat{y}^y
            (1-\hat{y})^{1-y}\]</span></p>
            <p>where <span class="math inline">\(\hat{y} = \sigma(w
            \cdot x + b)\)</span>.</p>
            <p><strong>Step 2: Likelihood</strong></p>
            <p>For N independent samples, the likelihood of observing
            our dataset is:</p>
            <p><span class="math display">\[L(w, b) = \prod_{i=1}^{N}
            \hat{y}_i^{y_i} (1-\hat{y}_i)^{1-y_i}\]</span></p>
            <p><strong>Step 3: Log-Likelihood</strong></p>
            <p>Taking the log (which is monotonic, so same optimum):</p>
            <p><span class="math display">\[\log L = \sum_{i=1}^{N}
            \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)
            \right]\]</span></p>
            <p><strong>Step 4: Negative Log-Likelihood =
            Loss</strong></p>
            <p>We want to maximize likelihood, which is equivalent to
            minimizing negative log-likelihood:</p>
            <p><span class="math display">\[\mathcal{L} = -\frac{1}{N}
            \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1-y_i)
            \log(1-\hat{y}_i) \right]\]</span></p>
            <p>This is exactly <strong>binary
            cross-entropy</strong>!</p>
            <p>The key insight: our loss function isn‚Äôt arbitrary ‚Äî it
            comes directly from probabilistic principles. We‚Äôre finding
            parameters that make our observed data most probable under
            the model.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Start with Bernoulli distribution for binary
            outcomes</li>
            <li>Write likelihood as product over samples</li>
            <li>Take log ‚Üí sum of log probabilities</li>
            <li>Flip sign ‚Üí loss function</li>
            <li>Connection: cross-entropy = negative log-likelihood</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúHow does this extend to
            multi-class?‚Äù</strong></p>
            <p>‚ÄúFor K classes, we use the categorical distribution
            (generalized Bernoulli). The model becomes P(y=k|x) =
            softmax(Wx + b)<em>k. The log-likelihood term for sample i
            with true class c_i is log(≈∑</em>{i,c_i}). Summing over
            samples and flipping sign:</p>
            <p><span class="math display">\[\mathcal{L} = -\frac{1}{N}
            \sum_{i=1}^{N} \log(\hat{y}_{i,c_i}) = -\frac{1}{N}
            \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k}
            \log(\hat{y}_{i,k})\]</span></p>
            <p>where y is one-hot. This is categorical cross-entropy,
            and it‚Äôs still just negative log-likelihood of a categorical
            distribution.‚Äù</p>
            <hr />
            <h4
            id="q-whats-the-difference-between-mle-and-map-estimation"><strong>Q:
            ‚ÄúWhat‚Äôs the difference between MLE and MAP
            estimation?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúMLE and MAP answer slightly different questions:</p>
            <p><strong>MLE asks</strong>: What parameters make my
            observed data most probable? <span
            class="math display">\[\hat{\theta}_{MLE} = \arg\max_\theta
            P(D|\theta)\]</span></p>
            <p><strong>MAP asks</strong>: Given my data AND prior
            beliefs about parameters, what are the most probable
            parameters? <span class="math display">\[\hat{\theta}_{MAP}
            = \arg\max_\theta P(\theta|D) = \arg\max_\theta P(D|\theta)
            P(\theta)\]</span></p>
            <p>The key difference is the prior P(Œ∏). In log form:</p>
            <ul>
            <li>MLE: maximize log P(D|Œ∏)</li>
            <li>MAP: maximize log P(D|Œ∏) + log P(Œ∏)</li>
            </ul>
            <p>That extra log P(Œ∏) term is
            <strong>regularization</strong>!</p>
            <p>If I choose a Gaussian prior P(Œ∏) ‚àù exp(-Œª||Œ∏||¬≤/2), then
            log P(Œ∏) = -Œª||Œ∏||¬≤/2, which is <strong>L2
            regularization</strong>.</p>
            <p>If I choose a Laplace prior P(Œ∏) ‚àù exp(-Œª||Œ∏||‚ÇÅ), then
            log P(Œ∏) = -Œª||Œ∏||‚ÇÅ, which is <strong>L1
            regularization</strong>.</p>
            <p>So MAP with a Gaussian prior is mathematically equivalent
            to L2-regularized MLE. Regularization has a Bayesian
            interpretation as encoding prior beliefs about parameter
            values.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>MLE maximizes likelihood</li>
            <li>MAP maximizes posterior (likelihood √ó prior)</li>
            <li>Prior becomes regularization in log space</li>
            <li>Gaussian prior ‚Üí L2 regularization</li>
            <li>Laplace prior ‚Üí L1 regularization</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhen would you prefer MAP over
            MLE?‚Äù</strong></p>
            <p>‚ÄúWhen you have <strong>limited data</strong> or
            <strong>want to prevent overfitting</strong>. MLE with
            limited data can give extreme parameter values ‚Äî it only
            cares about fitting the data, not about plausible parameter
            ranges. MAP‚Äôs prior encodes ‚Äòmost weights should be near
            zero,‚Äô which shrinks extreme values. With infinite data, MLE
            and MAP converge ‚Äî the likelihood dominates the prior. With
            finite data, the prior acts as a stabilizer.‚Äù</p>
            <hr />
            <h2 id="ml-coding-implementation-from-scratch">12.2 ML
            Coding &amp; Implementation from Scratch</h2>
            <h3 id="the-transformer-implementation">12.2.1 The
            Transformer Implementation</h3>
            <hr />
            <h4
            id="q-implement-multi-head-attention.-walk-me-through-the-tensor-shapes."><strong>Q:
            ‚ÄúImplement Multi-Head Attention. Walk me through the tensor
            shapes.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúLet me implement this step by step, tracking shapes
            carefully ‚Äî this is where most bugs happen.</p>
            <p>The input is typically (batch, seq_len, d_model). I‚Äôll
            call these B, S, D for brevity.‚Äù</p>
            <p><strong>Code (interview whiteboard style):</strong></p>
            <div class="sourceCode" id="cb421"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb421-1"><a href="#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb421-2"><a href="#cb421-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb421-3"><a href="#cb421-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb421-4"><a href="#cb421-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb421-5"><a href="#cb421-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb421-6"><a href="#cb421-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb421-7"><a href="#cb421-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, num_heads):</span>
<span id="cb421-8"><a href="#cb421-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb421-9"><a href="#cb421-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb421-10"><a href="#cb421-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb421-11"><a href="#cb421-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> d_model <span class="op">//</span> num_heads</span>
<span id="cb421-12"><a href="#cb421-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-13"><a href="#cb421-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Projection matrices</span></span>
<span id="cb421-14"><a href="#cb421-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb421-15"><a href="#cb421-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb421-16"><a href="#cb421-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb421-17"><a href="#cb421-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_o <span class="op">=</span> nn.Linear(d_model, d_model)</span>
<span id="cb421-18"><a href="#cb421-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb421-19"><a href="#cb421-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb421-20"><a href="#cb421-20" aria-hidden="true" tabindex="-1"></a>        B, S, D <span class="op">=</span> x.shape  <span class="co"># batch, seq_len, d_model</span></span>
<span id="cb421-21"><a href="#cb421-21" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> <span class="va">self</span>.num_heads</span>
<span id="cb421-22"><a href="#cb421-22" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> <span class="va">self</span>.head_dim</span>
<span id="cb421-23"><a href="#cb421-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-24"><a href="#cb421-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Project to Q, K, V</span></span>
<span id="cb421-25"><a href="#cb421-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (B, S, D) -&gt; (B, S, D)</span></span>
<span id="cb421-26"><a href="#cb421-26" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> <span class="va">self</span>.W_q(x)</span>
<span id="cb421-27"><a href="#cb421-27" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> <span class="va">self</span>.W_k(x)</span>
<span id="cb421-28"><a href="#cb421-28" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.W_v(x)</span>
<span id="cb421-29"><a href="#cb421-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-30"><a href="#cb421-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Reshape for multi-head: (B, S, D) -&gt; (B, S, H, d_k)</span></span>
<span id="cb421-31"><a href="#cb421-31" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> Q.view(B, S, H, d_k)</span>
<span id="cb421-32"><a href="#cb421-32" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> K.view(B, S, H, d_k)</span>
<span id="cb421-33"><a href="#cb421-33" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> V.view(B, S, H, d_k)</span>
<span id="cb421-34"><a href="#cb421-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-35"><a href="#cb421-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Transpose for attention: (B, S, H, d_k) -&gt; (B, H, S, d_k)</span></span>
<span id="cb421-36"><a href="#cb421-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We want batch and heads as the first dimensions for batched matmul</span></span>
<span id="cb421-37"><a href="#cb421-37" aria-hidden="true" tabindex="-1"></a>        Q <span class="op">=</span> Q.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, H, S, d_k)</span></span>
<span id="cb421-38"><a href="#cb421-38" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> K.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, H, S, d_k)</span></span>
<span id="cb421-39"><a href="#cb421-39" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> V.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># (B, H, S, d_k)</span></span>
<span id="cb421-40"><a href="#cb421-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-41"><a href="#cb421-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Attention scores</span></span>
<span id="cb421-42"><a href="#cb421-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q @ K^T: (B, H, S, d_k) @ (B, H, d_k, S) -&gt; (B, H, S, S)</span></span>
<span id="cb421-43"><a href="#cb421-43" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> Q <span class="op">@</span> K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> math.sqrt(d_k)</span>
<span id="cb421-44"><a href="#cb421-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-45"><a href="#cb421-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 5: Apply mask (for causal attention)</span></span>
<span id="cb421-46"><a href="#cb421-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb421-47"><a href="#cb421-47" aria-hidden="true" tabindex="-1"></a>            scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">&#39;-inf&#39;</span>))</span>
<span id="cb421-48"><a href="#cb421-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-49"><a href="#cb421-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 6: Softmax over last dimension (the keys)</span></span>
<span id="cb421-50"><a href="#cb421-50" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, H, S, S)</span></span>
<span id="cb421-51"><a href="#cb421-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-52"><a href="#cb421-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 7: Apply attention to values</span></span>
<span id="cb421-53"><a href="#cb421-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (B, H, S, S) @ (B, H, S, d_k) -&gt; (B, H, S, d_k)</span></span>
<span id="cb421-54"><a href="#cb421-54" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> attn_weights <span class="op">@</span> V</span>
<span id="cb421-55"><a href="#cb421-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-56"><a href="#cb421-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 8: Transpose back and reshape</span></span>
<span id="cb421-57"><a href="#cb421-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (B, H, S, d_k) -&gt; (B, S, H, d_k) -&gt; (B, S, D)</span></span>
<span id="cb421-58"><a href="#cb421-58" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, S, D)</span>
<span id="cb421-59"><a href="#cb421-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-60"><a href="#cb421-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 9: Final projection</span></span>
<span id="cb421-61"><a href="#cb421-61" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.W_o(out)</span>
<span id="cb421-62"><a href="#cb421-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb421-63"><a href="#cb421-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
            <p><strong>The Shape Journey:</strong></p>
            <pre><code>Input:           (B, S, D)        e.g., (32, 128, 512)
After W_q/k/v:   (B, S, D)        (32, 128, 512)
After view:      (B, S, H, d_k)   (32, 128, 8, 64)
After transpose: (B, H, S, d_k)   (32, 8, 128, 64)
Q @ K^T:         (B, H, S, S)     (32, 8, 128, 128)  &lt;- attention matrix!
After softmax:   (B, H, S, S)     (32, 8, 128, 128)
√ó V:             (B, H, S, d_k)   (32, 8, 128, 64)
After transpose: (B, S, H, d_k)   (32, 128, 8, 64)
After view:      (B, S, D)        (32, 128, 512)
Output:          (B, S, D)        (32, 128, 512)</code></pre>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Input shape: (batch, seq_len, d_model)</li>
            <li>Split into heads: view into (B, S, H, d_k) then
            transpose to (B, H, S, d_k)</li>
            <li>Attention matrix is (B, H, S, S) ‚Äî each head, each query
            attends to all keys</li>
            <li>Scale by ‚àöd_k before softmax</li>
            <li>Softmax on dim=-1 (the key dimension)</li>
            <li>After attention, transpose and reshape back</li>
            </ul>
            <hr />
            <h4
            id="q-whats-the-difference-between-view-and-reshape-in-pytorch"><strong>Q:
            ‚ÄúWhat‚Äôs the difference between view() and reshape() in
            PyTorch?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúThe key difference is about <strong>memory
            contiguity</strong>.</p>
            <p><code>view()</code> requires the tensor to be
            <strong>contiguous</strong> in memory ‚Äî meaning elements are
            stored sequentially without gaps. It creates a view of the
            same underlying data without copying.</p>
            <p><code>reshape()</code> will <strong>also</strong> avoid
            copying if possible, but if the tensor isn‚Äôt contiguous, it
            will copy the data to make it contiguous first.</p>
            <p>Here‚Äôs where this bites you in Transformers: after
            <code>transpose()</code>, the tensor is <strong>no longer
            contiguous</strong> because we‚Äôve reordered dimensions
            without moving data in memory. If you then call
            <code>view()</code>, PyTorch throws an error:</p>
            <div class="sourceCode" id="cb423"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb423-1"><a href="#cb423-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb423-2"><a href="#cb423-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.transpose(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># Now shape (2, 4, 3), but NOT contiguous!</span></span>
<span id="cb423-3"><a href="#cb423-3" aria-hidden="true" tabindex="-1"></a>x.view(<span class="dv">2</span>, <span class="dv">12</span>)  <span class="co"># ERROR: RuntimeError!</span></span></code></pre></div>
            <p>The fixes:</p>
            <ol type="1">
            <li><code>x.contiguous().view(2, 12)</code> ‚Äî explicitly
            copy to contiguous memory, then view</li>
            <li><code>x.reshape(2, 12)</code> ‚Äî let PyTorch handle it
            (will copy if needed)</li>
            </ol>
            <p>In the attention implementation, after transposing back
            from (B, H, S, d_k) to (B, S, H, d_k), I must call
            <code>.contiguous()</code> before <code>.view()</code>:</p>
            <div class="sourceCode" id="cb424"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb424-1"><a href="#cb424-1" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> out.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, S, D)</span></code></pre></div>
            <p><strong>Rule of thumb</strong>: Use
            <code>reshape()</code> when you don‚Äôt care about copies. Use
            <code>view()</code> when you want to guarantee no copy (and
            handle contiguity yourself).‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>view() requires contiguous memory</li>
            <li>transpose() breaks contiguity</li>
            <li>reshape() copies if needed, view() errors</li>
            <li>.contiguous() before .view() after transpose</li>
            <li>Performance: view() is zero-copy if contiguous</li>
            </ul>
            <hr />
            <h4
            id="q-why-do-we-mask-with--infinity-in-causal-attention-not-zero"><strong>Q:
            ‚ÄúWhy do we mask with -infinity in causal attention, not
            zero?‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúBecause of how softmax works. Softmax converts logits z
            to probabilities via:</p>
            <p><span class="math display">\[\text{softmax}(z_i) =
            \frac{e^{z_i}}{\sum_j e^{z_j}}\]</span></p>
            <p>The key insight: <span class="math inline">\(e^0 =
            1\)</span>, not 0!</p>
            <p>If I set masked positions to 0, then <span
            class="math inline">\(e^0 = 1\)</span>, so those positions
            still get <strong>non-zero probability mass</strong>. The
            model can still ‚Äòattend‚Äô to future tokens ‚Äî the mask doesn‚Äôt
            work.</p>
            <p>If I set masked positions to <span
            class="math inline">\(-\infty\)</span>, then <span
            class="math inline">\(e^{-\infty} = 0\)</span>, so those
            positions get <strong>exactly zero probability</strong>
            after softmax. That‚Äôs what we want for causal masking ‚Äî zero
            attention to future tokens.</p>
            <p>In practice, we use a large negative number like -1e9
            instead of actual infinity to avoid numerical issues:</p>
            <div class="sourceCode" id="cb425"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb425-1"><a href="#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb425-2"><a href="#cb425-2" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores.masked_fill(mask <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">&#39;-inf&#39;</span>))</span>
<span id="cb425-3"><a href="#cb425-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># or: scores = scores.masked_fill(mask == 0, -1e9)</span></span></code></pre></div>
            <p>The mask is typically a lower-triangular matrix of
            ones:</p>
            <pre><code>[[1, 0, 0, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 1]]</code></pre>
            <p>Positions with 0 get -inf, positions with 1 keep their
            original scores.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Softmax: e^0 = 1, not 0</li>
            <li>Zero mask ‚Üí non-zero attention (mask fails!)</li>
            <li>-inf mask ‚Üí zero attention after softmax (mask
            works)</li>
            <li>Use -1e9 or float(‚Äò-inf‚Äô) in practice</li>
            <li>Lower triangular matrix for causal mask</li>
            </ul>
            <hr />
            <h3 id="implementing-gradient-descent-from-scratch">12.2.2
            Implementing Gradient Descent from Scratch</h3>
            <hr />
            <h4
            id="q-implement-gradient-descent-from-scratch-with-numpy."><strong>Q:
            ‚ÄúImplement gradient descent from scratch with
            numpy.‚Äù</strong></h4>
            <p><strong>Code:</strong></p>
            <div class="sourceCode" id="cb427"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb427-1"><a href="#cb427-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb427-2"><a href="#cb427-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb427-3"><a href="#cb427-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(X, y, lr<span class="op">=</span><span class="fl">0.01</span>, epochs<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb427-4"><a href="#cb427-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb427-5"><a href="#cb427-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Linear regression with gradient descent from scratch.</span></span>
<span id="cb427-6"><a href="#cb427-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb427-7"><a href="#cb427-7" aria-hidden="true" tabindex="-1"></a><span class="co">    X: (N, D) features</span></span>
<span id="cb427-8"><a href="#cb427-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y: (N,) targets</span></span>
<span id="cb427-9"><a href="#cb427-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb427-10"><a href="#cb427-10" aria-hidden="true" tabindex="-1"></a>    N, D <span class="op">=</span> X.shape</span>
<span id="cb427-11"><a href="#cb427-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb427-12"><a href="#cb427-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize weights</span></span>
<span id="cb427-13"><a href="#cb427-13" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(D)</span>
<span id="cb427-14"><a href="#cb427-14" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb427-15"><a href="#cb427-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb427-16"><a href="#cb427-16" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb427-17"><a href="#cb427-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb427-18"><a href="#cb427-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb427-19"><a href="#cb427-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass: predictions</span></span>
<span id="cb427-20"><a href="#cb427-20" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> X <span class="op">@</span> w <span class="op">+</span> b</span>
<span id="cb427-21"><a href="#cb427-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb427-22"><a href="#cb427-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss (MSE)</span></span>
<span id="cb427-23"><a href="#cb427-23" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> np.mean((y_pred <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb427-24"><a href="#cb427-24" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb427-25"><a href="#cb427-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb427-26"><a href="#cb427-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gradients</span></span>
<span id="cb427-27"><a href="#cb427-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/dw = (2/N) * X^T @ (y_pred - y)</span></span>
<span id="cb427-28"><a href="#cb427-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dL/db = (2/N) * sum(y_pred - y)</span></span>
<span id="cb427-29"><a href="#cb427-29" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> y_pred <span class="op">-</span> y</span>
<span id="cb427-30"><a href="#cb427-30" aria-hidden="true" tabindex="-1"></a>        dw <span class="op">=</span> (<span class="dv">2</span> <span class="op">/</span> N) <span class="op">*</span> X.T <span class="op">@</span> error</span>
<span id="cb427-31"><a href="#cb427-31" aria-hidden="true" tabindex="-1"></a>        db <span class="op">=</span> (<span class="dv">2</span> <span class="op">/</span> N) <span class="op">*</span> np.<span class="bu">sum</span>(error)</span>
<span id="cb427-32"><a href="#cb427-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb427-33"><a href="#cb427-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb427-34"><a href="#cb427-34" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> w <span class="op">-</span> lr <span class="op">*</span> dw</span>
<span id="cb427-35"><a href="#cb427-35" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> b <span class="op">-</span> lr <span class="op">*</span> db</span>
<span id="cb427-36"><a href="#cb427-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb427-37"><a href="#cb427-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, b, losses</span></code></pre></div>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Forward pass: compute predictions</li>
            <li>Loss computation (MSE here)</li>
            <li>Gradient computation via calculus</li>
            <li>Parameter update: w = w - lr * grad</li>
            <li>That‚Äôs the entire algorithm!</li>
            </ul>
            <hr />
            <h3 id="k-means-clustering-from-scratch">12.2.3 K-Means
            Clustering from Scratch</h3>
            <hr />
            <h4
            id="q-implement-k-means-clustering-without-sklearn."><strong>Q:
            ‚ÄúImplement K-means clustering without
            sklearn.‚Äù</strong></h4>
            <p><strong>Code:</strong></p>
            <div class="sourceCode" id="cb428"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb428-1"><a href="#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb428-2"><a href="#cb428-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb428-3"><a href="#cb428-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kmeans(X, k, max_iters<span class="op">=</span><span class="dv">100</span>, tol<span class="op">=</span><span class="fl">1e-4</span>):</span>
<span id="cb428-4"><a href="#cb428-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb428-5"><a href="#cb428-5" aria-hidden="true" tabindex="-1"></a><span class="co">    K-means clustering from scratch.</span></span>
<span id="cb428-6"><a href="#cb428-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb428-7"><a href="#cb428-7" aria-hidden="true" tabindex="-1"></a><span class="co">    X: (N, D) data points</span></span>
<span id="cb428-8"><a href="#cb428-8" aria-hidden="true" tabindex="-1"></a><span class="co">    k: number of clusters</span></span>
<span id="cb428-9"><a href="#cb428-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb428-10"><a href="#cb428-10" aria-hidden="true" tabindex="-1"></a>    N, D <span class="op">=</span> X.shape</span>
<span id="cb428-11"><a href="#cb428-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb428-12"><a href="#cb428-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Initialize centroids randomly from data points</span></span>
<span id="cb428-13"><a href="#cb428-13" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(N, k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb428-14"><a href="#cb428-14" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> X[indices].copy()</span>
<span id="cb428-15"><a href="#cb428-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb428-16"><a href="#cb428-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb428-17"><a href="#cb428-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Assign each point to nearest centroid</span></span>
<span id="cb428-18"><a href="#cb428-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute distances: (N, k) matrix</span></span>
<span id="cb428-19"><a href="#cb428-19" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> np.zeros((N, k))</span>
<span id="cb428-20"><a href="#cb428-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb428-21"><a href="#cb428-21" aria-hidden="true" tabindex="-1"></a>            distances[:, j] <span class="op">=</span> np.linalg.norm(X <span class="op">-</span> centroids[j], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb428-22"><a href="#cb428-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb428-23"><a href="#cb428-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assign to nearest centroid</span></span>
<span id="cb428-24"><a href="#cb428-24" aria-hidden="true" tabindex="-1"></a>        assignments <span class="op">=</span> np.argmin(distances, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb428-25"><a href="#cb428-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb428-26"><a href="#cb428-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Update centroids to cluster means</span></span>
<span id="cb428-27"><a href="#cb428-27" aria-hidden="true" tabindex="-1"></a>        new_centroids <span class="op">=</span> np.zeros_like(centroids)</span>
<span id="cb428-28"><a href="#cb428-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb428-29"><a href="#cb428-29" aria-hidden="true" tabindex="-1"></a>            cluster_points <span class="op">=</span> X[assignments <span class="op">==</span> j]</span>
<span id="cb428-30"><a href="#cb428-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(cluster_points) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb428-31"><a href="#cb428-31" aria-hidden="true" tabindex="-1"></a>                new_centroids[j] <span class="op">=</span> cluster_points.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb428-32"><a href="#cb428-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb428-33"><a href="#cb428-33" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Empty cluster: reinitialize randomly</span></span>
<span id="cb428-34"><a href="#cb428-34" aria-hidden="true" tabindex="-1"></a>                new_centroids[j] <span class="op">=</span> X[np.random.randint(N)]</span>
<span id="cb428-35"><a href="#cb428-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb428-36"><a href="#cb428-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check convergence</span></span>
<span id="cb428-37"><a href="#cb428-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.linalg.norm(new_centroids <span class="op">-</span> centroids) <span class="op">&lt;</span> tol:</span>
<span id="cb428-38"><a href="#cb428-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb428-39"><a href="#cb428-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb428-40"><a href="#cb428-40" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> new_centroids</span>
<span id="cb428-41"><a href="#cb428-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb428-42"><a href="#cb428-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> centroids, assignments</span></code></pre></div>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Initialize centroids (random from data or random in
            space)</li>
            <li>Repeat: (1) assign points to nearest centroid, (2)
            update centroids to cluster means</li>
            <li>Convergence when centroids stop moving</li>
            <li>Handle empty clusters (reinitialize)</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhat are the limitations of
            K-means?‚Äù</strong></p>
            <p>‚ÄúSeveral limitations:</p>
            <ol type="1">
            <li><strong>Must specify k</strong> in advance</li>
            <li><strong>Sensitive to initialization</strong> ‚Äî different
            starts give different results (use k-means++)</li>
            <li><strong>Assumes spherical clusters</strong> ‚Äî doesn‚Äôt
            work well for elongated or irregular shapes</li>
            <li><strong>Sensitive to outliers</strong> ‚Äî means are
            pulled by outliers (k-medoids is more robust)</li>
            <li><strong>Only finds local optima</strong> ‚Äî run multiple
            times and take best‚Äù</li>
            </ol>
            <hr />
            <h3 id="auc-from-scratch">12.2.4 AUC from Scratch</h3>
            <hr />
            <h4
            id="q-implement-auc-area-under-roc-curve-without-sklearn."><strong>Q:
            ‚ÄúImplement AUC (Area Under ROC Curve) without
            sklearn.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúAUC measures how well a classifier ranks positive
            examples above negative ones. The ROC curve plots True
            Positive Rate vs False Positive Rate at different
            thresholds. AUC is the area under this curve.</p>
            <p>There‚Äôs a beautiful interpretation: AUC equals the
            probability that a randomly chosen positive example is
            ranked higher than a randomly chosen negative example. This
            gives us a simple O(N¬≤) algorithm, or we can sort and
            compute in O(N log N).‚Äù</p>
            <p><strong>Code:</strong></p>
            <div class="sourceCode" id="cb429"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb429-1"><a href="#cb429-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb429-2"><a href="#cb429-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb429-3"><a href="#cb429-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> auc_from_scratch(y_true, y_scores):</span>
<span id="cb429-4"><a href="#cb429-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb429-5"><a href="#cb429-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute AUC using the Mann-Whitney U statistic interpretation.</span></span>
<span id="cb429-6"><a href="#cb429-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb429-7"><a href="#cb429-7" aria-hidden="true" tabindex="-1"></a><span class="co">    y_true: binary labels (0 or 1)</span></span>
<span id="cb429-8"><a href="#cb429-8" aria-hidden="true" tabindex="-1"></a><span class="co">    y_scores: predicted probabilities or scores</span></span>
<span id="cb429-9"><a href="#cb429-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb429-10"><a href="#cb429-10" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> np.array(y_true)</span>
<span id="cb429-11"><a href="#cb429-11" aria-hidden="true" tabindex="-1"></a>    y_scores <span class="op">=</span> np.array(y_scores)</span>
<span id="cb429-12"><a href="#cb429-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-13"><a href="#cb429-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get positive and negative examples</span></span>
<span id="cb429-14"><a href="#cb429-14" aria-hidden="true" tabindex="-1"></a>    pos_scores <span class="op">=</span> y_scores[y_true <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb429-15"><a href="#cb429-15" aria-hidden="true" tabindex="-1"></a>    neg_scores <span class="op">=</span> y_scores[y_true <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb429-16"><a href="#cb429-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-17"><a href="#cb429-17" aria-hidden="true" tabindex="-1"></a>    n_pos <span class="op">=</span> <span class="bu">len</span>(pos_scores)</span>
<span id="cb429-18"><a href="#cb429-18" aria-hidden="true" tabindex="-1"></a>    n_neg <span class="op">=</span> <span class="bu">len</span>(neg_scores)</span>
<span id="cb429-19"><a href="#cb429-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-20"><a href="#cb429-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Count pairs where positive &gt; negative</span></span>
<span id="cb429-21"><a href="#cb429-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AUC = P(score_pos &gt; score_neg)</span></span>
<span id="cb429-22"><a href="#cb429-22" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb429-23"><a href="#cb429-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> pos <span class="kw">in</span> pos_scores:</span>
<span id="cb429-24"><a href="#cb429-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> neg <span class="kw">in</span> neg_scores:</span>
<span id="cb429-25"><a href="#cb429-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pos <span class="op">&gt;</span> neg:</span>
<span id="cb429-26"><a href="#cb429-26" aria-hidden="true" tabindex="-1"></a>                count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb429-27"><a href="#cb429-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> pos <span class="op">==</span> neg:</span>
<span id="cb429-28"><a href="#cb429-28" aria-hidden="true" tabindex="-1"></a>                count <span class="op">+=</span> <span class="fl">0.5</span>  <span class="co"># Tie: count as 0.5</span></span>
<span id="cb429-29"><a href="#cb429-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-30"><a href="#cb429-30" aria-hidden="true" tabindex="-1"></a>    auc <span class="op">=</span> count <span class="op">/</span> (n_pos <span class="op">*</span> n_neg)</span>
<span id="cb429-31"><a href="#cb429-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> auc</span>
<span id="cb429-32"><a href="#cb429-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb429-33"><a href="#cb429-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Faster O(N log N) version using sorting</span></span>
<span id="cb429-34"><a href="#cb429-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> auc_fast(y_true, y_scores):</span>
<span id="cb429-35"><a href="#cb429-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb429-36"><a href="#cb429-36" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute AUC using sorting (O(N log N)).</span></span>
<span id="cb429-37"><a href="#cb429-37" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb429-38"><a href="#cb429-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by scores descending</span></span>
<span id="cb429-39"><a href="#cb429-39" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> np.argsort(y_scores)[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb429-40"><a href="#cb429-40" aria-hidden="true" tabindex="-1"></a>    y_true_sorted <span class="op">=</span> y_true[order]</span>
<span id="cb429-41"><a href="#cb429-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-42"><a href="#cb429-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute TPR and FPR at each threshold</span></span>
<span id="cb429-43"><a href="#cb429-43" aria-hidden="true" tabindex="-1"></a>    tps <span class="op">=</span> np.cumsum(y_true_sorted)  <span class="co"># Cumulative true positives</span></span>
<span id="cb429-44"><a href="#cb429-44" aria-hidden="true" tabindex="-1"></a>    fps <span class="op">=</span> np.cumsum(<span class="dv">1</span> <span class="op">-</span> y_true_sorted)  <span class="co"># Cumulative false positives</span></span>
<span id="cb429-45"><a href="#cb429-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-46"><a href="#cb429-46" aria-hidden="true" tabindex="-1"></a>    n_pos <span class="op">=</span> y_true.<span class="bu">sum</span>()</span>
<span id="cb429-47"><a href="#cb429-47" aria-hidden="true" tabindex="-1"></a>    n_neg <span class="op">=</span> <span class="bu">len</span>(y_true) <span class="op">-</span> n_pos</span>
<span id="cb429-48"><a href="#cb429-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-49"><a href="#cb429-49" aria-hidden="true" tabindex="-1"></a>    tpr <span class="op">=</span> tps <span class="op">/</span> n_pos</span>
<span id="cb429-50"><a href="#cb429-50" aria-hidden="true" tabindex="-1"></a>    fpr <span class="op">=</span> fps <span class="op">/</span> n_neg</span>
<span id="cb429-51"><a href="#cb429-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb429-52"><a href="#cb429-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># AUC via trapezoidal rule</span></span>
<span id="cb429-53"><a href="#cb429-53" aria-hidden="true" tabindex="-1"></a>    auc <span class="op">=</span> np.trapz(tpr, fpr)</span>
<span id="cb429-54"><a href="#cb429-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> auc</span></code></pre></div>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>AUC = probability that random positive ranks above
            random negative</li>
            <li>O(N¬≤) straightforward, O(N log N) with sorting</li>
            <li>Handle ties by counting as 0.5</li>
            <li>Trapezoidal rule for integration</li>
            </ul>
            <hr />
            <h2 id="ml-debugging">12.3 ML Debugging</h2>
            <blockquote>
            <p><strong>A common practical exercise</strong>: You‚Äôre
            given a Jupyter notebook with code that compiles but doesn‚Äôt
            learn. The model trains but loss is flat or diverging. Your
            job is to find and fix the bugs.</p>
            </blockquote>
            <h3 id="approach-to-ml-debugging">Approach to ML
            Debugging</h3>
            <p><strong>Step 1: Check the Basics First</strong></p>
            <ul>
            <li>Is data loading correctly? Print shapes and sample
            values</li>
            <li>Is the model architecture correct? Print model
            summary</li>
            <li>Are loss values reasonable? (Not NaN, not exactly
            0)</li>
            </ul>
            <p><strong>Step 2: Check the Training Loop</strong></p>
            <ul>
            <li>Is optimizer.zero_grad() called?</li>
            <li>Is loss.backward() called on the right loss?</li>
            <li>Is optimizer.step() called?</li>
            </ul>
            <p><strong>Step 3: Check Data Flow</strong></p>
            <ul>
            <li>Are dimensions correct throughout?</li>
            <li>Is data normalized?</li>
            <li>Are labels in the right format?</li>
            </ul>
            <p><strong>Step 4: Check for Silent Failures</strong></p>
            <ul>
            <li>Broadcasting errors that don‚Äôt crash</li>
            <li>Wrong dimension in softmax/loss</li>
            <li>Data not shuffled</li>
            </ul>
            <hr />
            <hr />
            <h3 id="bug-1-broadcasting-silently-gone-wrong">Bug #1:
            Broadcasting Silently Gone Wrong</h3>
            <h4
            id="q-this-model-trains-but-doesnt-learn.-find-the-bug."><strong>Q:
            ‚ÄúThis model trains but doesn‚Äôt learn. Find the
            bug.‚Äù</strong></h4>
            <div class="sourceCode" id="cb430"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb430-1"><a href="#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb430-2"><a href="#cb430-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, hidden_dim, output_dim):</span>
<span id="cb430-3"><a href="#cb430-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb430-4"><a href="#cb430-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_dim, hidden_dim)</span>
<span id="cb430-5"><a href="#cb430-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_dim, output_dim)</span>
<span id="cb430-6"><a href="#cb430-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(output_dim))  <span class="co"># </span><span class="al">BUG</span><span class="co"> SETUP</span></span>
<span id="cb430-7"><a href="#cb430-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb430-8"><a href="#cb430-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb430-9"><a href="#cb430-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb430-10"><a href="#cb430-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb430-11"><a href="#cb430-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x shape: (batch_size, output_dim)</span></span>
<span id="cb430-12"><a href="#cb430-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.bias.unsqueeze(<span class="dv">0</span>)  <span class="co"># Looks fine...</span></span>
<span id="cb430-13"><a href="#cb430-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb430-14"><a href="#cb430-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb430-15"><a href="#cb430-15" aria-hidden="true" tabindex="-1"></a><span class="co"># But later, someone changed bias initialization:</span></span>
<span id="cb430-16"><a href="#cb430-16" aria-hidden="true" tabindex="-1"></a>model.bias <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, output_dim))  <span class="co"># Now (1, output_dim)</span></span>
<span id="cb430-17"><a href="#cb430-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb430-18"><a href="#cb430-18" aria-hidden="true" tabindex="-1"></a><span class="co"># And the data comes in as:</span></span>
<span id="cb430-19"><a href="#cb430-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(output_dim, batch_size)  <span class="co"># WRONG! Should be (batch_size, output_dim)</span></span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúThe bug is a <strong>shape mismatch</strong> that
            broadcasts silently. If my data accidentally comes in as
            (output_dim, batch_size) instead of (batch_size,
            output_dim), and my bias is (1, output_dim), PyTorch will
            broadcast:</p>
            <ul>
            <li>x: (output_dim, batch_size)</li>
            <li>bias: (1, output_dim)</li>
            </ul>
            <p>This broadcasts to (output_dim, output_dim, batch_size)!
            The code runs without error because PyTorch happily
            broadcasts, but the computation is nonsensical.‚Äù</p>
            <p><strong>How to Catch It:</strong></p>
            <p>‚ÄúAlways print shapes at key points during debugging:</p>
            <div class="sourceCode" id="cb431"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb431-1"><a href="#cb431-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;x shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)  <span class="co"># Should be (B, D), not (D, B)</span></span>
<span id="cb431-2"><a href="#cb431-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;bias shape: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>bias<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
            <p>Better yet, add explicit shape assertions:</p>
            <div class="sourceCode" id="cb432"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb432-1"><a href="#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> x.shape <span class="op">==</span> (batch_size, output_dim), <span class="ss">f&#39;Expected </span><span class="sc">{</span>(batch_size, output_dim)<span class="sc">}</span><span class="ss">, got </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">&#39;</span></span>
<span id="cb432-2"><a href="#cb432-2" aria-hidden="true" tabindex="-1"></a>```<span class="st">&quot;</span></span>
<span id="cb432-3"><a href="#cb432-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-4"><a href="#cb432-4" aria-hidden="true" tabindex="-1"></a><span class="er">---</span></span>
<span id="cb432-5"><a href="#cb432-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-6"><a href="#cb432-6" aria-hidden="true" tabindex="-1"></a><span class="er">---</span></span>
<span id="cb432-7"><a href="#cb432-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-8"><a href="#cb432-8" aria-hidden="true" tabindex="-1"></a><span class="er">### Bug #2: Softmax on Wrong Dimension</span></span>
<span id="cb432-9"><a href="#cb432-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-10"><a href="#cb432-10" aria-hidden="true" tabindex="-1"></a><span class="co">#### **Q: &quot;Loss is barely decreasing. What&#39;s wrong?&quot;**</span></span>
<span id="cb432-11"><a href="#cb432-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-12"><a href="#cb432-12" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb432-13"><a href="#cb432-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb432-14"><a href="#cb432-14" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.classifier(x)  <span class="co"># (batch_size, num_classes)</span></span>
<span id="cb432-15"><a href="#cb432-15" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># </span><span class="al">BUG</span><span class="co">!</span></span>
<span id="cb432-16"><a href="#cb432-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> probs</span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúThe softmax is applied along <code>dim=0</code>, which
            is the <strong>batch dimension</strong>. This means:</p>
            <ul>
            <li>Probabilities sum to 1 <strong>across different
            samples</strong> in the batch</li>
            <li>NOT across classes for each sample!</li>
            </ul>
            <p>Each sample gets a probability that depends on OTHER
            samples in the batch. This is completely wrong ‚Äî changing
            other samples in the batch changes this sample‚Äôs
            prediction.</p>
            <p>The fix: <code>dim=1</code> (or <code>dim=-1</code>) to
            sum over classes:</p>
            <div class="sourceCode" id="cb433"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb433-1"><a href="#cb433-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Correct: sum to 1 over classes</span></span>
<span id="cb433-2"><a href="#cb433-2" aria-hidden="true" tabindex="-1"></a>```<span class="st">&quot;</span></span>
<span id="cb433-3"><a href="#cb433-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-4"><a href="#cb433-4" aria-hidden="true" tabindex="-1"></a><span class="er">**How to Catch It:**</span></span>
<span id="cb433-5"><a href="#cb433-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-6"><a href="#cb433-6" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;Check that probabilities sum to 1 in the right way:</span></span>
<span id="cb433-7"><a href="#cb433-7" aria-hidden="true" tabindex="-1"></a><span class="er">```python</span></span>
<span id="cb433-8"><a href="#cb433-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(probs.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>))  <span class="co"># If using dim=0, this should be 1s</span></span>
<span id="cb433-9"><a href="#cb433-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(probs.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>))  <span class="co"># If using dim=1, this should be 1s</span></span>
<span id="cb433-10"><a href="#cb433-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-11"><a href="#cb433-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Correct behavior: each sample&#39;s probs sum to 1</span></span>
<span id="cb433-12"><a href="#cb433-12" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(probs.<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">1</span>), torch.ones(batch_size))</span>
<span id="cb433-13"><a href="#cb433-13" aria-hidden="true" tabindex="-1"></a>```<span class="st">&quot;</span></span>
<span id="cb433-14"><a href="#cb433-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-15"><a href="#cb433-15" aria-hidden="true" tabindex="-1"></a><span class="er">---</span></span>
<span id="cb433-16"><a href="#cb433-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-17"><a href="#cb433-17" aria-hidden="true" tabindex="-1"></a><span class="er">---</span></span>
<span id="cb433-18"><a href="#cb433-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-19"><a href="#cb433-19" aria-hidden="true" tabindex="-1"></a><span class="er">### Bug #3: Double Softmax with CrossEntropyLoss</span></span>
<span id="cb433-20"><a href="#cb433-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-21"><a href="#cb433-21" aria-hidden="true" tabindex="-1"></a><span class="co">#### **Q: &quot;My classifier&#39;s accuracy is stuck at random chance. Code looks fine.&quot;**</span></span>
<span id="cb433-22"><a href="#cb433-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-23"><a href="#cb433-23" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb433-24"><a href="#cb433-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb433-25"><a href="#cb433-25" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb433-26"><a href="#cb433-26" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb433-27"><a href="#cb433-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> probs</span>
<span id="cb433-28"><a href="#cb433-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb433-29"><a href="#cb433-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb433-30"><a href="#cb433-30" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb433-31"><a href="#cb433-31" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(x)</span>
<span id="cb433-32"><a href="#cb433-32" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(output, target)  <span class="co"># </span><span class="al">BUG</span><span class="co">!</span></span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúThis is a classic: <strong>double softmax</strong>.</p>
            <p>PyTorch‚Äôs <code>CrossEntropyLoss</code> internally
            applies <code>LogSoftmax</code> and then
            <code>NLLLoss</code>. It expects <strong>raw
            logits</strong>, not probabilities.</p>
            <p>When I pass probabilities (already softmaxed) to
            CrossEntropyLoss, it applies log-softmax again:</p>
            <pre><code>log(softmax(softmax(logits)))</code></pre>
            <p>This squashes the gradients and makes learning nearly
            impossible. The model sees almost no signal because:</p>
            <ul>
            <li>softmax(probs) where probs are already in [0,1] produces
            values very close to each other</li>
            <li>log of those gives similar values ‚Üí flat loss
            landscape‚Äù</li>
            </ul>
            <p><strong>The Fix:</strong></p>
            <p>‚ÄúEither:</p>
            <ol type="1">
            <li>Return logits and use
            <code>CrossEntropyLoss</code>:</li>
            </ol>
            <div class="sourceCode" id="cb435"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb435-1"><a href="#cb435-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb435-2"><a href="#cb435-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.fc(x)  <span class="co"># Return logits</span></span>
<span id="cb435-3"><a href="#cb435-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span></code></pre></div>
            <ol start="2" type="1">
            <li>Return log-probs and use <code>NLLLoss</code>:</li>
            </ol>
            <div class="sourceCode" id="cb436"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb436-1"><a href="#cb436-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb436-2"><a href="#cb436-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F.log_softmax(<span class="va">self</span>.fc(x), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb436-3"><a href="#cb436-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.NLLLoss()</span></code></pre></div>
            <p>Never: softmax output ‚Üí CrossEntropyLoss‚Äù</p>
            <hr />
            <hr />
            <h3 id="bug-4-missing-optimizer.zero_grad">Bug #4: Missing
            optimizer.zero_grad()</h3>
            <h4 id="q-loss-explodes-after-a-few-iterations."><strong>Q:
            ‚ÄúLoss explodes after a few iterations.‚Äù</strong></h4>
            <div class="sourceCode" id="cb437"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb437-1"><a href="#cb437-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb437-2"><a href="#cb437-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb437-3"><a href="#cb437-3" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch)</span>
<span id="cb437-4"><a href="#cb437-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb437-5"><a href="#cb437-5" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb437-6"><a href="#cb437-6" aria-hidden="true" tabindex="-1"></a>        optimizer.step()  <span class="co"># </span><span class="al">BUG</span><span class="co">: Where&#39;s zero_grad?</span></span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúGradients <strong>accumulate</strong> by default in
            PyTorch. Each <code>loss.backward()</code>
            <strong>adds</strong> to the existing <code>.grad</code>
            tensors. Without <code>optimizer.zero_grad()</code>,
            gradients grow every iteration:</p>
            <ul>
            <li>Iteration 1: grad = g‚ÇÅ</li>
            <li>Iteration 2: grad = g‚ÇÅ + g‚ÇÇ</li>
            <li>Iteration 3: grad = g‚ÇÅ + g‚ÇÇ + g‚ÇÉ</li>
            <li>‚Ä¶</li>
            </ul>
            <p>The effective learning rate grows larger and larger,
            causing the model to take increasingly wild steps and
            eventually diverge.‚Äù</p>
            <p><strong>The Fix:</strong></p>
            <div class="sourceCode" id="cb438"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb438-1"><a href="#cb438-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb438-2"><a href="#cb438-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb438-3"><a href="#cb438-3" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()  <span class="co"># Reset gradients!</span></span>
<span id="cb438-4"><a href="#cb438-4" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(batch)</span>
<span id="cb438-5"><a href="#cb438-5" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb438-6"><a href="#cb438-6" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb438-7"><a href="#cb438-7" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code></pre></div>
            <p><strong>Note</strong>: Sometimes gradient accumulation is
            <strong>intentional</strong> (to simulate larger batches).
            In that case, you zero_grad every N steps and divide loss by
            N:</p>
            <div class="sourceCode" id="cb439"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb439-1"><a href="#cb439-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb439-2"><a href="#cb439-2" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> criterion(model(batch), target) <span class="op">/</span> accumulation_steps</span>
<span id="cb439-3"><a href="#cb439-3" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb439-4"><a href="#cb439-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> accumulation_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb439-5"><a href="#cb439-5" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb439-6"><a href="#cb439-6" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span></code></pre></div>
            <hr />
            <hr />
            <h3 id="bug-5-dataloader-shufflefalse-for-training">Bug #5:
            DataLoader shuffle=False for Training</h3>
            <h4
            id="q-model-converges-but-generalizes-poorly-or-training-is-unstable."><strong>Q:
            ‚ÄúModel converges but generalizes poorly, or training is
            unstable.‚Äù</strong></h4>
            <div class="sourceCode" id="cb440"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb440-1"><a href="#cb440-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">False</span>)  <span class="co"># </span><span class="al">BUG</span><span class="co">!</span></span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúIf the training data is ordered (e.g., all class-0
            samples first, then class-1, etc.), the model sees highly
            correlated batches. This causes:</p>
            <ol type="1">
            <li><strong>Unstable training</strong>: Gradients are biased
            toward whatever class is in the current batch</li>
            <li><strong>Poor generalization</strong>: Model may learn to
            predict based on position in dataset rather than
            features</li>
            <li><strong>Mode collapse</strong>: For generative models,
            can collapse to generating one type of output</li>
            </ol>
            <p>Even if data isn‚Äôt ordered by class, lack of shuffling
            means the same sequence every epoch, which can lead to
            overfitting to that specific order.‚Äù</p>
            <p><strong>The Fix:</strong></p>
            <div class="sourceCode" id="cb441"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb441-1"><a href="#cb441-1" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)  <span class="co"># Always shuffle training!</span></span></code></pre></div>
            <p><strong>Additional tip</strong>: For validation/test,
            <code>shuffle=False</code> is correct ‚Äî we want reproducible
            evaluation.</p>
            <hr />
            <hr />
            <h3 id="bug-6-learning-rate-issues">Bug #6: Learning Rate
            Issues</h3>
            <h4
            id="q-loss-decreases-very-slowly-or-oscillates-wildly."><strong>Q:
            ‚ÄúLoss decreases very slowly or oscillates
            wildly.‚Äù</strong></h4>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúTwo common issues:</p>
            <p><strong>Too high LR</strong>: Loss oscillates or
            diverges. The steps are so large we jump over minima.</p>
            <pre><code>Loss: 2.3 ‚Üí 2.1 ‚Üí 2.5 ‚Üí 3.1 ‚Üí 8.7 ‚Üí NaN</code></pre>
            <p>Fix: Reduce LR by factor of 10.</p>
            <p><strong>Too low LR</strong>: Loss decreases painfully
            slowly. Might not converge in reasonable time.</p>
            <pre><code>Loss: 2.3 ‚Üí 2.29 ‚Üí 2.28 ‚Üí 2.27 ‚Üí ... (thousands of epochs later) ‚Üí 2.0</code></pre>
            <p>Fix: Increase LR or use learning rate finder.</p>
            <p><strong>Debugging approach</strong>:</p>
            <ol type="1">
            <li>Start with LR = 1e-3 (good default for Adam)</li>
            <li>If loss explodes ‚Üí decrease by 10x</li>
            <li>If loss barely moves ‚Üí increase by 10x</li>
            <li>Use LR warmup for Transformers</li>
            <li>Consider LR scheduler for long training‚Äù</li>
            </ol>
            <hr />
            <hr />
            <h3 id="bug-7-wrong-loss-function-for-task">Bug #7: Wrong
            Loss Function for Task</h3>
            <h4
            id="q-classification-accuracy-is-terrible-despite-loss-decreasing."><strong>Q:
            ‚ÄúClassification accuracy is terrible despite loss
            decreasing.‚Äù</strong></h4>
            <p><strong>Example:</strong></p>
            <div class="sourceCode" id="cb444"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb444-1"><a href="#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Binary classification</span></span>
<span id="cb444-2"><a href="#cb444-2" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()  <span class="co"># </span><span class="al">BUG</span><span class="co">!</span></span>
<span id="cb444-3"><a href="#cb444-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(x)  <span class="co"># Logits</span></span>
<span id="cb444-4"><a href="#cb444-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(output, target.<span class="bu">float</span>())</span></code></pre></div>
            <p><strong>The Bug:</strong></p>
            <p>‚ÄúUsing MSE loss for classification is problematic:</p>
            <ol type="1">
            <li>MSE doesn‚Äôt care about decision boundaries ‚Äî it
            penalizes 0.4 vs 0.6 equally whether the target is 0 or
            1</li>
            <li>Gradients are wrong for learning to classify</li>
            <li>Loss can decrease (predictions get closer to targets)
            without improving accuracy</li>
            </ol>
            <p>The right loss:</p>
            <div class="sourceCode" id="cb445"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb445-1"><a href="#cb445-1" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCEWithLogitsLoss()  <span class="co"># Binary classification</span></span>
<span id="cb445-2"><a href="#cb445-2" aria-hidden="true" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb445-3"><a href="#cb445-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># Multi-class</span></span></code></pre></div>
            <p>These losses are derived from maximum likelihood for the
            classification task and have gradients that actually push
            the decision boundary in the right direction.‚Äù</p>
            <hr />
            <h3 id="ml-debugging-checklist">ML Debugging Checklist</h3>
            <pre><code>‚ñ° Shapes: Print tensor shapes at each step
‚ñ° NaN/Inf: Check for numerical issues (torch.isnan, torch.isinf)
‚ñ° Gradients: Are they flowing? (param.grad is not None and not zero)
‚ñ° zero_grad(): Called before backward?
‚ñ° Loss function: Correct for task? Expecting logits or probs?
‚ñ° Softmax dimension: dim=-1 or dim=1 for class dimension?
‚ñ° Data shuffle: True for training?
‚ñ° Data normalization: Appropriate for model?
‚ñ° Learning rate: Try 10x higher and lower
‚ñ° Batch size: Not too small (unstable) or too large (memory)?
‚ñ° Labels: Correct format? (indices for CrossEntropyLoss, one-hot for BCELoss)
‚ñ° Model mode: model.train() for training, model.eval() for eval?
‚ñ° Dropout/BatchNorm: Behaving correctly in train vs eval mode?</code></pre>
            <hr />
            <h2 id="ml-system-design---distributed-training">12.4 ML
            System Design - Distributed Training</h2>
            <hr />
            <h3
            id="q-how-would-you-train-a-100b-parameter-model"><strong>Q:
            ‚ÄúHow would you train a 100B+ parameter model?‚Äù</strong></h3>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúA 100B parameter model requires roughly 400GB just for
            parameters and optimizer states in mixed precision ‚Äî way
            more than any single GPU‚Äôs 80GB. I‚Äôd use <strong>3D
            parallelism</strong>: combining data, tensor, and pipeline
            parallelism.</p>
            <p><strong>Step 1: Tensor Parallelism within
            nodes</strong></p>
            <p>First, I‚Äôd split the model <strong>horizontally</strong>
            using tensor parallelism. Each transformer layer‚Äôs weight
            matrices get split across GPUs on the same node. For an
            8-GPU node with NVLink, I‚Äôd use TP=8. This splits each
            matrix multiply:</p>
            <ul>
            <li>FFN: Column-parallel for first linear, row-parallel for
            second</li>
            <li>Attention: Split heads across GPUs</li>
            </ul>
            <p>The key constraint: TP requires high-bandwidth
            communication because every single layer needs
            synchronization. NVLink gives 600+ GB/s within a node, but
            cross-node is only ~50 GB/s with InfiniBand. So TP stays
            within a node.</p>
            <p><strong>Step 2: Pipeline Parallelism across
            nodes</strong></p>
            <p>For a 96-layer model, I‚Äôd partition layers across
            nodes:</p>
            <ul>
            <li>Node 0: Layers 0-23</li>
            <li>Node 1: Layers 24-47</li>
            <li>Node 2: Layers 48-71</li>
            <li>Node 3: Layers 72-95</li>
            </ul>
            <p>This is pipeline parallelism with PP=4. Communication is
            just activations between stages ‚Äî much lower bandwidth than
            TP.</p>
            <p>The <strong>bubble problem</strong>: GPUs sit idle
            waiting for activations. I‚Äôd use micro-batching with 32
            micro-batches to fill the pipeline. With GPipe or 1F1B
            scheduling, bubble overhead drops to ~10%.</p>
            <p><strong>Step 3: Data Parallelism for
            throughput</strong></p>
            <p>Finally, I‚Äôd replicate this entire TP√óPP setup across
            multiple node groups. Each replica processes different data.
            Gradients sync via AllReduce after each step.</p>
            <p>With 8 such replicas, total config is:</p>
            <ul>
            <li>TP=8 (within each node)</li>
            <li>PP=4 (across 4 nodes per replica)<br />
            </li>
            <li>DP=8 (8 replicas)</li>
            <li>Total: 8 √ó 4 √ó 8 = 256 GPUs</li>
            </ul>
            <p><strong>Additional techniques</strong>:</p>
            <ul>
            <li>Mixed precision (BF16) to halve memory and double
            throughput</li>
            <li>Gradient checkpointing to reduce activation memory</li>
            <li>ZeRO Stage 1 to shard optimizer states across DP
            ranks</li>
            <li>Gradient accumulation if effective batch size needs to
            be larger‚Äù</li>
            </ul>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Calculate memory requirements</li>
            <li>TP within node (NVLink bandwidth)</li>
            <li>PP across nodes (pipeline with micro-batching)</li>
            <li>DP for scaling replicas</li>
            <li>Mention bubble problem and solution</li>
            <li>Mixed precision, gradient checkpointing</li>
            </ul>
            <p><strong>Follow-up Q: ‚ÄúWhat‚Äôs the communication
            bottleneck?‚Äù</strong></p>
            <p>‚ÄúFor this setup:</p>
            <ul>
            <li><strong>TP</strong>: AllReduce of activations, every
            layer ‚Äî but on fast NVLink, not the bottleneck</li>
            <li><strong>PP</strong>: Point-to-point activation transfers
            between stages ‚Äî small and pipelined</li>
            <li><strong>DP</strong>: AllReduce of gradients at step end
            ‚Äî this is the bottleneck!</li>
            </ul>
            <p>The gradient AllReduce must sync 100B parameters √ó 2
            bytes (BF16) = 200GB across all DP replicas. Even with Ring
            AllReduce, that‚Äôs significant. Solutions:</p>
            <ol type="1">
            <li>Overlap AllReduce with backward pass (send gradients as
            they‚Äôre computed)</li>
            <li>Gradient compression (lower precision during
            communication)</li>
            <li>Larger micro-batch to amortize communication‚Äù</li>
            </ol>
            <hr />
            <h3
            id="q-whats-the-straggler-problem-how-do-you-handle-it"><strong>Q:
            ‚ÄúWhat‚Äôs the straggler problem? How do you handle
            it?‚Äù</strong></h3>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúThe straggler problem occurs in synchronous distributed
            training when one GPU is consistently slower than others. In
            data parallelism, all GPUs must sync gradients via AllReduce
            before any can proceed. If one GPU is 10% slower, the entire
            cluster waits for it every step.</p>
            <p>With 4000 GPUs, one straggler means 3999 GPUs are idle
            10% of the time. That‚Äôs effectively wasting 400 GPUs worth
            of compute!</p>
            <p><strong>Causes</strong>:</p>
            <ul>
            <li>Hardware variation (manufacturing differences)</li>
            <li>Thermal throttling (GPU too hot)</li>
            <li>Network congestion (slower communication)</li>
            <li>Uneven data batches (variable sequence lengths)</li>
            </ul>
            <p><strong>Solutions</strong>:</p>
            <ol type="1">
            <li><strong>Backup workers</strong>: Train with slight
            redundancy. If a worker is slow, use gradient from a backup.
            Google‚Äôs DistBelief used this.</li>
            <li><strong>Bounded staleness</strong>: Don‚Äôt wait
            indefinitely. If a worker is too slow, proceed without its
            gradient. Accept slightly stale gradients. Works okay in
            practice because SGD is already noisy.</li>
            <li><strong>Asynchronous SGD</strong>: Don‚Äôt synchronize at
            all ‚Äî each GPU updates parameters independently. Risk:
            gradient staleness can hurt convergence. Mitigation:
            learning rate scaling, momentum correction.</li>
            <li><strong>Proactive load balancing</strong>:
            <ul>
            <li>Monitor GPU utilization and communication times</li>
            <li>Redistribute data to balance computation</li>
            <li>For variable-length sequences: bucket by length so
            batches have similar total tokens</li>
            </ul></li>
            <li><strong>Hardware monitoring</strong>: Identify and
            replace faulty GPUs early. Modern clusters have automated
            health checks.</li>
            </ol>
            <p><strong>In practice</strong>: Most large-scale training
            uses synchronous with careful cluster management.
            Asynchronous is rarely used for LLMs because the quality
            impact is too high. Better to fix the stragglers than work
            around them.‚Äù</p>
            <hr />
            <h2 id="inference-optimization">12.5 Inference
            Optimization</h2>
            <hr />
            <h3
            id="q-explain-kv-caching-and-why-it-matters-for-inference."><strong>Q:
            ‚ÄúExplain KV caching and why it matters for
            inference.‚Äù</strong></h3>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúIn autoregressive generation, each new token depends on
            all previous tokens via self-attention. Naively, generating
            token N requires computing attention over all N tokens, and
            we‚Äôve already computed attention for tokens 1 to N-1 in
            previous steps ‚Äî that‚Äôs redundant work.</p>
            <p><strong>The insight</strong>: For past tokens, the Keys
            and Values in attention don‚Äôt change. Only the new token‚Äôs
            Query, Key, and Value are new. So we cache the K and V
            matrices from previous tokens.</p>
            <p><strong>Without KV cache</strong> (generating N
            tokens):</p>
            <ul>
            <li>Step 1: Compute K‚ÇÅ, V‚ÇÅ for token 1</li>
            <li>Step 2: Compute K‚ÇÅ, K‚ÇÇ, V‚ÇÅ, V‚ÇÇ for tokens 1-2</li>
            <li>Step N: Compute K‚ÇÅ‚Ä¶K‚Çô, V‚ÇÅ‚Ä¶V‚Çô</li>
            <li>Total: O(N¬≤) computation</li>
            </ul>
            <p><strong>With KV cache</strong>:</p>
            <ul>
            <li>Step 1: Compute K‚ÇÅ, V‚ÇÅ, cache them</li>
            <li>Step 2: Compute K‚ÇÇ, V‚ÇÇ, append to cache, attention uses
            cached K‚ÇÅV‚ÇÅ</li>
            <li>Step N: Compute K‚Çô, V‚Çô, append, attention uses full
            cache</li>
            <li>Total: O(N) computation per step, O(N¬≤) total but with
            smaller constant</li>
            </ul>
            <p><strong>Memory cost</strong>: For LLaMA 70B with 80
            layers, BF16:</p>
            <ul>
            <li>Per token: 2 (K and V) √ó num_heads √ó head_dim √ó 2 bytes
            √ó 80 layers</li>
            <li>For 4K context: ~10GB per batch</li>
            <li>For 128K context: ~300GB ‚Äî doesn‚Äôt fit on one GPU!</li>
            </ul>
            <p>This is why <strong>GQA</strong> (grouped-query
            attention) matters: sharing K,V across query heads reduces
            cache by 8x in LLaMA 2 70B.‚Äù</p>
            <p><strong>Key Points to Hit:</strong></p>
            <ul>
            <li>Past tokens‚Äô K,V don‚Äôt change ‚Äî cache them</li>
            <li>Reduces per-step compute from O(N) to O(1) attention
            re-computation</li>
            <li>Memory grows linearly with context</li>
            <li>GQA/MQA reduce cache size</li>
            <li>Critical for long-context models</li>
            </ul>
            <hr />
            <h3
            id="q-whats-speculative-decoding-why-is-it-exciting-for-2025"><strong>Q:
            ‚ÄúWhat‚Äôs speculative decoding? Why is it exciting for
            2025?‚Äù</strong></h3>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúSpeculative decoding breaks the sequential bottleneck of
            autoregressive generation. The key insight is that
            <strong>verifying is faster than generating</strong> for
            large models.</p>
            <p><strong>The algorithm</strong>:</p>
            <ol type="1">
            <li>Use a small ‚Äòdraft‚Äô model to generate K tokens quickly
            (say, K=4)</li>
            <li>Feed all K draft tokens to the large model in
            <strong>parallel</strong></li>
            <li>Large model outputs probabilities for all K positions in
            one forward pass</li>
            <li>Accept draft tokens where large model agrees, reject
            where it disagrees</li>
            <li>If rejected at position i, resample from large model and
            stop</li>
            </ol>
            <p><strong>Why it‚Äôs faster</strong>: The large model is
            memory-bound during generation ‚Äî loading 70B parameters
            takes the same time whether processing 1 token or 4. By
            verifying 4 tokens in one pass, we potentially generate 4
            tokens in the time of ~1.5 (draft time + verify time).</p>
            <p><strong>The math guarantee</strong>: If acceptance
            probability follows p(accept) = min(1, p_target/p_draft),
            the output distribution <strong>exactly matches</strong>
            target model. No quality loss!</p>
            <p><strong>Why it‚Äôs exciting for 2025</strong>:</p>
            <ul>
            <li>LLMs are getting larger but inference latency matters
            more</li>
            <li>Works especially well when draft model is high-quality
            (shared architecture)</li>
            <li>Can achieve 2-4x speedup on common text</li>
            <li>Combining with other techniques: continuous batching, KV
            cache quantization</li>
            <li>New variants: self-speculative (use early exit as
            draft), tree-based speculation</li>
            </ul>
            <p><strong>Limitations</strong>:</p>
            <ul>
            <li>Speedup depends on draft quality (high rejection = no
            benefit)</li>
            <li>Doesn‚Äôt help throughput, only latency</li>
            <li>Draft model adds memory overhead‚Äù</li>
            </ul>
            <hr />
            <h3
            id="q-explain-the-trade-offs-of-quantization-for-llm-inference."><strong>Q:
            ‚ÄúExplain the trade-offs of quantization for LLM
            inference.‚Äù</strong></h3>
            <p><strong>Verbal Answer:</strong></p>
            <p>‚ÄúQuantization reduces model weights from 16-bit to 8-bit
            or 4-bit, trading precision for memory and speed.</p>
            <p><strong>Memory savings</strong>:</p>
            <ul>
            <li>FP16 ‚Üí INT8: 2x smaller (70B model: 140GB ‚Üí 70GB)</li>
            <li>FP16 ‚Üí INT4: 4x smaller (70B model: 140GB ‚Üí 35GB)</li>
            </ul>
            <p><strong>Speed benefits</strong>:</p>
            <ol type="1">
            <li>Less memory bandwidth needed (memory-bound workloads
            speed up)</li>
            <li>INT8 tensor cores are faster than FP16 on modern
            GPUs</li>
            <li>Smaller model = fits in GPU memory without CPU
            offload</li>
            </ol>
            <p><strong>Quality impact</strong>:</p>
            <ul>
            <li>INT8 (W8A8): Usually &lt;1% accuracy loss, very
            safe</li>
            <li>INT8 weights, FP16 activations (W8A16): Even safer</li>
            <li>INT4 (W4A16): 1-3% loss, requires careful
            calibration</li>
            <li>INT4 (W4A4): Significant quality degradation</li>
            </ul>
            <p><strong>Key techniques</strong>:</p>
            <ol type="1">
            <li><strong>GPTQ</strong>: Optimal Brain Quantization
            adapted for LLMs. Quantizes layer-by-layer using calibration
            data to minimize error.</li>
            <li><strong>AWQ</strong>: Activation-aware Weight
            Quantization. Protects important weights (those with large
            activation magnitudes) from aggressive quantization.</li>
            <li><strong>QLoRA</strong>: Combines 4-bit quantization with
            LoRA fine-tuning. Keep base model in INT4, train LoRA
            adapters in FP16. Enables fine-tuning 65B models on single
            GPU!</li>
            </ol>
            <p><strong>When to use what</strong>:</p>
            <ul>
            <li>Production inference, quality matters: INT8 (W8A16)</li>
            <li>Memory-constrained deployment: INT4 with AWQ</li>
            <li>Fine-tuning large models: QLoRA</li>
            <li>Research/development: Keep FP16 for reproducibility</li>
            </ul>
            <p><strong>Trade-off summary</strong>:</p>
            <ul>
            <li>More aggressive quantization ‚Üí more memory savings,
            faster inference</li>
            <li>But: potential quality loss, quantization overhead, less
            flexibility‚Äù</li>
            </ul>
            <hr />
            <h2 id="quick-reference-common-interview-questions">12.6
            Quick Reference: Common Interview Questions</h2>
            <h3 id="linear-algebra-2">Linear Algebra</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />What happens if weight
            matrix is low rank? (‚Üí LoRA)</label></li>
            <li><label><input type="checkbox" />Hessian eigenvalues and
            critical points</label></li>
            <li><label><input type="checkbox" />Why eigenvectors in
            PCA?</label></li>
            <li><label><input type="checkbox" />SVD relationship to
            PCA</label></li>
            </ul>
            <h3 id="calculus-optimization">Calculus &amp;
            Optimization</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />Design a simple Autograd
            engine</label></li>
            <li><label><input type="checkbox" />Why do vanishing
            gradients happen? (Jacobian multiplication)</label></li>
            <li><label><input type="checkbox" />Derive gradients for
            softmax + cross-entropy</label></li>
            <li><label><input type="checkbox" />Forward vs reverse mode
            autodiff</label></li>
            </ul>
            <h3 id="probability-statistics">Probability &amp;
            Statistics</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />Derive logistic
            regression loss from MLE</label></li>
            <li><label><input type="checkbox" />MLE vs MAP (‚Üí
            regularization)</label></li>
            <li><label><input type="checkbox" />Properties of Gaussian
            distributions</label></li>
            </ul>
            <h3 id="transformer-implementation">Transformer
            Implementation</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />Implement multi-head
            attention with shape tracking</label></li>
            <li><label><input type="checkbox" />view() vs reshape() vs
            contiguous()</label></li>
            <li><label><input type="checkbox" />Why mask with -inf not
            0?</label></li>
            <li><label><input type="checkbox" />Why scale by
            ‚àöd_k?</label></li>
            </ul>
            <h3 id="ml-debugging-deepmind-style">ML Debugging (DeepMind
            Style)</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />Broadcasting shape
            errors</label></li>
            <li><label><input type="checkbox" />Softmax on wrong
            dimension</label></li>
            <li><label><input type="checkbox" />Double softmax with
            CrossEntropyLoss</label></li>
            <li><label><input type="checkbox" />Missing
            zero_grad()</label></li>
            <li><label><input type="checkbox" />DataLoader
            shuffle=False</label></li>
            </ul>
            <h3 id="distributed-training-1">Distributed Training</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />How to train 100B+
            model? (3D parallelism)</label></li>
            <li><label><input type="checkbox" />The straggler
            problem</label></li>
            <li><label><input type="checkbox" />ZeRO vs Tensor
            Parallelism trade-offs</label></li>
            </ul>
            <h3 id="inference-optimization-1">Inference
            Optimization</h3>
            <ul class="task-list">
            <li><label><input type="checkbox" />KV cache
            explanation</label></li>
            <li><label><input type="checkbox" />Speculative
            decoding</label></li>
            <li><label><input type="checkbox" />Quantization trade-offs
            (INT8 vs INT4)</label></li>
            </ul>
            <hr />
            <h2 id="appendix-verbal-expression-templates">Appendix:
            Verbal Expression Templates</h2>
            <h3 id="starting-an-answer">Starting an Answer</h3>
            <ul>
            <li>‚ÄúThe key insight here is‚Ä¶‚Äù</li>
            <li>‚ÄúLet me walk through this step by step‚Ä¶‚Äù</li>
            <li>‚ÄúThere are two main aspects to consider‚Ä¶‚Äù</li>
            <li>‚ÄúThis is a classic problem that comes down to‚Ä¶‚Äù</li>
            </ul>
            <h3 id="explaining-trade-offs">Explaining Trade-offs</h3>
            <ul>
            <li>‚ÄúThe trade-off is between X and Y‚Ä¶‚Äù</li>
            <li>‚ÄúIf we go with approach A, we gain X but lose Y‚Ä¶‚Äù</li>
            <li>‚ÄúThis works well when [condition], but struggles when
            [other condition]‚Ä¶‚Äù</li>
            </ul>
            <h3 id="connecting-to-practical-implications">Connecting to
            Practical Implications</h3>
            <ul>
            <li>‚ÄúThis matters in practice because‚Ä¶‚Äù</li>
            <li>‚ÄúWhere you see this in production is‚Ä¶‚Äù</li>
            <li>‚ÄúThe real-world implication is‚Ä¶‚Äù</li>
            </ul>
            <h3 id="admitting-uncertainty">Admitting Uncertainty</h3>
            <ul>
            <li>‚ÄúI‚Äôm not 100% certain, but my understanding is‚Ä¶‚Äù</li>
            <li>‚ÄúI‚Äôd need to verify this, but I believe‚Ä¶‚Äù</li>
            <li>‚ÄúThe exact numbers depend on [factors], but
            roughly‚Ä¶‚Äù</li>
            </ul>
            <h3 id="asking-clarifying-questions">Asking Clarifying
            Questions</h3>
            <ul>
            <li>‚ÄúWhen you say X, do you mean Y or Z?‚Äù</li>
            <li>‚ÄúIs this for training or inference?‚Äù</li>
            <li>‚ÄúWhat scale are we talking about ‚Äî millions or billions
            of parameters?‚Äù</li>
            </ul>
            <hr />
            <p><em>End of Part 12: Question Bank</em></p>
        </div>
    </div>
</body>
</html>
